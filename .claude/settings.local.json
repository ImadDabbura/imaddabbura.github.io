{
  "permissions": {
    "allow": [
      "Bash(xargs:*)",
      "WebSearch",
      "WebFetch(domain:pakstech.com)",
      "WebFetch(domain:silviacanelon.com)",
      "WebFetch(domain:github.com)",
      "WebFetch(domain:raw.githubusercontent.com)",
      "Bash(quarto render:*)",
      "Bash(python3:*)",
      "Bash(git log:*)",
      "Bash(ls:*)",
      "Bash(git checkout:*)",
      "Bash(/tmp/apply_notebook_changes.py:*)",
      "Bash(/tmp/rewrite_lstm.py:*)",
      "Bash(/tmp/update_lstm_cells.py:*)",
      "Bash(/tmp/update_gate_intuition.py << 'PYEOF'\nimport json\n\nwith open\\('posts/nlp/LSTM-Annotated-Implementation.ipynb'\\) as f:\n    nb = json.load\\(f\\)\n\ndef set_source\\(cell_idx, new_text\\):\n    lines = new_text.split\\('\\\\n'\\)\n    nb['cells'][cell_idx]['source'] = [line + '\\\\n' for line in lines[:-1]]\n    if lines[-1]:\n        nb['cells'][cell_idx]['source'].append\\(lines[-1]\\)\n\nset_source\\(6, r'''An `LSTMCell` computes four gates, then uses them to update the cell and hidden states. Each gate has the same dimension as the hidden state:\n\n\\\\begin{array}{ll} \\\\\\\\\ni_t = \\\\sigma\\(W_{ii} x_t + b_{ii} + W_{ih} h_{t-1} + b_{hi}\\) \\\\\\\\\nf_t = \\\\sigma\\(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}\\) \\\\\\\\\ng_t = \\\\tanh\\(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}\\) \\\\\\\\\no_t = \\\\sigma\\(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}\\) \\\\\\\\\nc_t = f_t \\\\odot c_{t-1} + i_t \\\\odot g_t \\\\\\\\\nh_t = o_t \\\\odot \\\\tanh\\(c_t\\) \\\\\\\\\n\\\\end{array}\n\nLet's build intuition for each gate:\n\n| Gate | Name | Activation | What It Does |\n|---|---|---|---|\n| $i_t$ | **Input gate** | Sigmoid \\(0–1\\) | How much of the *new* candidate values to write into the cell |\n| $f_t$ | **Forget gate** | Sigmoid \\(0–1\\) | How much of the *old* cell state to keep \\(1 = remember everything, 0 = forget everything\\) |\n| $g_t$ | **Cell gate** | Tanh \\(-1 to 1\\) | The candidate new values to potentially add to the cell state |\n| $o_t$ | **Output gate** | Sigmoid \\(0–1\\) | How much of the cell state to expose as the hidden state output |\n\n### Gates as Learned Pattern Detectors\n\nIt's tempting to think of gates as simple switches, but they're more than that. Each gate is a **learned pattern detector** — analogous to how a CNN filter learns to activate on specific visual patterns \\(edges, textures, shapes\\), a gate's weight matrix learns to activate on specific *contextual patterns* in the input and hidden state.\n\nConsider the forget gate: $f_t = \\\\sigma\\(W_{if} \\\\cdot x_t + W_{hf} \\\\cdot h_{t-1} + b_f\\)$. The weight matrix $W_{if}$ learns which patterns in the *current input* should trigger forgetting, while $W_{hf}$ learns which patterns in the *accumulated context* \\(hidden state\\) should trigger forgetting. After training, specific rows of these weight matrices become specialized detectors:\n\n- Some rows in the forget gate might learn to detect **\"end of clause\"** patterns \\(a period, a conjunction like \"but\"\\) — signaling that the topic is changing and old context should be flushed\n- Other rows might learn to detect **\"continuation\"** patterns \\(a comma, a relative pronoun like \"which\"\\) — signaling that existing context is still relevant and should be preserved\n- Rows in the input gate might learn to detect **\"salient new information\"** patterns \\(a named entity, a number, a negation word\\) — signaling that this input is important enough to write into memory\n\nThe key insight is that this happens **per dimension** of the hidden state. The gate output is a vector, not a scalar. Dimension 42 of the forget gate might be close to 0 \\(forget\\) while dimension 73 is close to 1 \\(keep\\) — because each dimension of the cell state stores different information, and each dimension of the gate learns to detect different patterns.\n\n::: {.callout-note}\n## The CNN Analogy\nIn a CNN, a filter is a small weight matrix that produces a high activation when the input patch matches its learned pattern \\(e.g., a horizontal edge\\). A gate weight matrix works the same way: it produces a high activation \\(close to 1 after sigmoid\\) when the combination of current input $x_t$ and previous context $h_{t-1}$ matches the pattern it has learned to detect. The difference is that CNN filters detect *spatial* patterns in pixel neighborhoods, while gate weights detect *contextual* patterns across the current token and the sequence history.\n:::\n\n::: {.callout-tip}\n## The Single-Matrix Trick\nEven though we describe four separate gates, in practice we compute them all in **one matrix multiplication** by concatenating the four weight matrices into a single `4 * hidden_size` matrix. We then split the result into four chunks. This is much faster because it replaces four small matrix multiplications with one large one — better utilizing GPU parallelism and memory bandwidth.\n:::\n\n::: {.callout-note}\n## Intuition: The LSTM as a Gated Memory Controller\nThink of the cell state as a **register** in a CPU. At each time step, the LSTM decides: \\(1\\) what to **erase** from the register \\(forget gate\\), \\(2\\) what to **write** into it \\(input gate × cell gate\\), and \\(3\\) what to **read out** as output \\(output gate applied to the register contents\\). This is why the cell state is sometimes compared to RAM — it supports read, write, and erase operations. The gate weights are the *learned logic* that determines which operation to perform based on the current context.\n:::\n\nLet's implement `LSTMCell` and verify it matches PyTorch.'''\\)\n\nwith open\\('posts/nlp/LSTM-Annotated-Implementation.ipynb', 'w'\\) as f:\n    json.dump\\(nb, f, indent=1, ensure_ascii=False\\)\n\nprint\\(\"Done.\"\\)\nPYEOF)",
      "Bash(/tmp/reduce_callouts.py:*)",
      "Bash(/tmp/update_two_states.py << 'PYEOF'\nimport json\n\nwith open\\('posts/nlp/LSTM-Annotated-Implementation.ipynb'\\) as f:\n    nb = json.load\\(f\\)\n\ndef set_source\\(cell_idx, new_text\\):\n    lines = new_text.split\\('\\\\n'\\)\n    nb['cells'][cell_idx]['source'] = [line + '\\\\n' for line in lines[:-1]]\n    if lines[-1]:\n        nb['cells'][cell_idx]['source'].append\\(lines[-1]\\)\n\nset_source\\(4, r'''At a high level, here's how an LSTM works at each time step $t$:\n\n- Two states flow through time: a **hidden state** $h_t$ and a **cell state** $c_t$, both vectors of length $n_h$\n- The cell state acts like a conveyor belt — information flows along it, and gates decide what to add or remove\n- Three gates control information flow: **forget** \\(what to erase from cell\\), **input** \\(what to write to cell\\), and **output** \\(what to expose as hidden state\\)\n\n### Why Two States?\n\nA vanilla RNN has a single hidden state that must do *everything*: store long-term memory, carry short-term context, and produce the output that downstream layers \\(softmax, attention, classifier\\) consume. That's too many jobs for one vector, and it's why RNNs fail — optimizing the hidden state for the current prediction destroys the long-term information stored in it.\n\nLSTMs split this into two specialized roles:\n\n**Cell state \\($c_t$\\): the long-term internal memory.** The cell state is the LSTM's private memory — it's never directly exposed to the rest of the network. Its job is to *retain information across long distances* without interference. Because it's updated additively \\(not passed through a nonlinearity at each step\\), gradients can flow through it across hundreds of time steps. Think of it as a notebook that the LSTM writes to and reads from, but never shows to anyone directly.\n\n**Hidden state \\($h_t$\\): the short-term working output.** The hidden state is what the LSTM *exposes* to the outside world — it's the input to the next layer, the softmax, or whatever comes next. It's computed by selectively reading from the cell state via the output gate: $h_t = o_t \\\\odot \\\\tanh\\(c_t\\)$. The output gate decides: *\"Given everything I know \\(cell state\\) and the current context, what's relevant right now?\"*\n\nThis separation is crucial. The cell state can hold information like \"the subject is plural\" or \"we're inside a quotation\" for as long as needed, without that information being distorted by the demands of predicting intermediate tokens. When it *is* needed — say, for subject-verb agreement or closing a quotation mark — the output gate reads it out into the hidden state at exactly the right moment.\n\n| | Cell State \\($c_t$\\) | Hidden State \\($h_t$\\) |\n|---|---|---|\n| **Role** | Long-term memory | Short-term working output |\n| **Visible to** | Only the LSTM itself \\(internal\\) | Next layer, softmax, classifier \\(external\\) |\n| **Updated by** | Forget gate \\(erase\\) + input gate \\(write\\) | Output gate reading from cell state |\n| **Gradient flow** | Additive — gradients pass through cleanly | Through tanh and output gate — more lossy |\n| **Analogy** | A notebook you write in privately | The answer you speak aloud when asked |\n\n::: {.callout-important}\n## Why Additive Updates Matter\nIn a vanilla RNN: $h_t = \\\\tanh\\(W_{hh} h_{t-1} + W_{xh} x_t\\)$ — the hidden state is *replaced* entirely at each step through a nonlinear transform. Gradients must pass through this transform at every time step, causing exponential decay.\n\nIn an LSTM: $c_t = f_t \\\\odot c_{t-1} + i_t \\\\odot g_t$ — the cell state is *updated additively*. The forget gate $f_t$ can be close to 1, letting gradients flow through almost unattenuated. This is the core mechanism that solves vanishing gradients.\n:::\n\nWe'll first implement `LSTMCell` \\(one time step\\), then wrap it in an `LSTM` module that handles sequences and multiple layers.'''\\)\n\nwith open\\('posts/nlp/LSTM-Annotated-Implementation.ipynb', 'w'\\) as f:\n    json.dump\\(nb, f, indent=1, ensure_ascii=False\\)\n\nprint\\(\"Done.\"\\)\nPYEOF)"
    ]
  }
}
