{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293884c7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Airflow Part 1 - What is Airflow?\"\n",
    "date: \"2022-01-10\"\n",
    "date-modified: \"2022-01-10\"\n",
    "image: feature.png\n",
    "title-block-banner: feature.png\n",
    "categories: [\"Data Engineering\", \"MLOps\", \"Airflow\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682e01d",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Other than my experience and the documentation, the main resource behind this post and figures is the fantastic book: [Data Pipelines with Apache. Airflow](https://www.manning.com/books/data-pipelines-with-apache-airflow).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc25065",
   "metadata": {},
   "source": [
    "**Airflow** is workflow orchestration tool that is written in Python at Airbnb. The workflow is also written in Python. It defines the workflow as a **DAG** so it is easy to determine the dependencies between tasks. If any task failed, we don't need to rerun the workflow again, we can just run the failed task and all the tasks that depend on it. We can also do *backfilling* by running the pipeline/tasks for time intervals in the past.\n",
    "\n",
    "Airflow consists of mainly three components:\n",
    "\n",
    "- **The Airflow scheduler**: Parses DAGs, checks their schedule interval, and (if the DAGs\u2019 schedule has passed) starts scheduling the DAGs\u2019 tasks for execution by passing them to the Airflow workers.\n",
    "- **The Airflow workers**: Pick up tasks that are scheduled for execution and execute them. As such, the workers are responsible for actually \u201cdoing the work.\u201d\n",
    "- **The Airflow webserver**: Visualizes the DAGs parsed by the scheduler and provides the main interface for users to monitor DAG runs and their results. It uses the metadata database which has all the logs and other metadata about tasks and workflows.\n",
    "\n",
    "Conceptually, the scheduling algorithm follows the following steps:\n",
    "\n",
    "1. For each open (= uncompleted) task in the graph, do the following:\n",
    "    \u2013 For each edge pointing toward the task, check if the \u201cupstream\u201d task on the other end of the edge has been completed.\n",
    "    \u2013 If all upstream tasks have been completed, add the task under consideration to a queue of tasks to be executed.\n",
    "2. Execute the tasks in the execution queue, marking them completed once they finish performing their work.\n",
    "3. Jump back to step 1 and repeat until all tasks in the graph have been completed.\n",
    "\n",
    "The scheduler in Airflow runs roughly through the following steps:\n",
    "\n",
    "1. Once users have written their workflows as DAGs, the files containing these DAGs are read by the scheduler to extract the corresponding tasks, dependen- cies, and schedule interval of each DAG.\n",
    "2. For each DAG, the scheduler then checks whether the schedule interval for the DAG has passed since the last time it was read. If so, the tasks in the DAG are scheduled for execution.\n",
    "3. For each scheduled task, the scheduler then checks whether the dependencies (= upstream tasks) of the task have been completed. If so, the task is added to the execution queue.\n",
    "4. The scheduler waits for several moments before starting a new loop by jumping back to step 1.\n",
    "\n",
    "<img src=\"images/scheduler-overview.png\">\n",
    "\n",
    "Airflow can be run:\n",
    "\n",
    "1. In python virtual environment\n",
    "2. Inside Docker containers. In this case, Airflow scheduler, webserver, and metastore would run each in separate containers\n",
    "\n",
    "The main disadvantages of Airflow are:\n",
    "\n",
    "1. It can get very messy and hard to understand for complex workflows\n",
    "2. It is best used for batch/recurring jobs NOT streaming jobs\n",
    "3. Mainly support static DAGs and hard to implement dynamic DAGs. Imagine you\u2019re reading from a database and you want to create a step to process each record in the database (e.g. to make a prediction), but you don\u2019t know in advance how many records there are in the database, Airflow won\u2019t be able to handle that.\n",
    "4. It is monolithic, which means it packages the entire workflow into one container. If two different steps in your workflow have different requirements, you can, in theory, create different containers for them using Airflow\u2019s DockerOperator, but it\u2019s not that easy to do so.\n",
    "5. Airflow\u2019s DAGs are not parameterized, which means you can\u2019t pass parameters into your workflows. So if you want to run the same model with different learning rates, you\u2019ll have to create different workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d3714",
   "metadata": {},
   "source": [
    "To setup Airflow locally inside Python virtual env:\n",
    "\n",
    "1. pip install apache-airflow\n",
    "2. airflow init db # Initialize metastore locally using SQLite; not recommended for production\n",
    "3. airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org # Create user\n",
    "4. airflow webserver # Start web server to use web UI\n",
    "5. airflow scheduler # Start scheduler, don't use sequential in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
