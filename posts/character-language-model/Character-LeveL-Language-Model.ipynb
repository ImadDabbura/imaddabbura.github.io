{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Character-Level Language Model\"\n",
    "subtitle: \"How an RNN learns to generate names one character at a time — and what it teaches us about language models\"\n",
    "image: feature.png\n",
    "title-block-banner: feature.png\n",
    "date: \"2018-02-22\"\n",
    "date-modified: \"2024-12-10\"\n",
    "categories: [\"NLP\",  \"Deep Learning\"]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Simplest Language Model You Can Actually Build\n",
    "\n",
    "Every time Gmail suggests a reply or a speech recognition system transcribes your words, a language model is predicting the next token. State-of-the-art systems like Google's Neural Machine Translation do this with millions of parameters and subword tokens — but the *core idea* is identical to what we'll build here: **predict the next character given everything that came before it.**\n",
    "\n",
    "In this post, we'll build a character-level language model from scratch using a Recurrent Neural Network (RNN). We'll train it on a dataset of human names, and by the end, it will generate plausible-sounding new names character by character. Along the way, we'll cover the same fundamental concepts that power production NLP systems — from this toy RNN to large-scale neural machine translation: sequential prediction, backpropagation through time, gradient instabilities, and the creativity-coherence trade-off in sampling.\n",
    "\n",
    "### Roadmap\n",
    "\n",
    "| Section | What You'll Learn | Why It Matters |\n",
    "|---|---|---|\n",
    "| **The Core Idea** | How sequence prediction decomposes into conditional probabilities | The same factorization underlies every autoregressive neural language model |\n",
    "| **Forward Pass** | One-hot encoding → hidden state → softmax → loss | The mechanics of a single training step |\n",
    "| **Backpropagation Through Time** | How gradients flow backward across shared weights | Why RNNs struggle with long sequences (and why we clip gradients) |\n",
    "| **Sampling** | Controlling randomness in generation | The entropy knob that controls text generation quality |\n",
    "| **Training & Results** | Putting it all together and generating names | Seeing the model go from gibberish to \"Yasira\" |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Idea: One Character at a Time\n",
    "\n",
    "A **statistical language model** learns the joint probability distribution over sequences of tokens. For a sequence of $T$ characters, we want to maximize:\n",
    "\n",
    "$$P(c_1, c_2, \\ldots, c_T) = \\prod_{t=1}^{T} P(c_t \\mid c_1, \\ldots, c_{t-1})$$\n",
    "\n",
    "In plain English: the probability of the full sequence equals the product of each character's probability *given everything before it*. This is the chain rule of probability — nothing more — and it's the same factorization used by large-scale neural language models for machine translation and speech recognition. The only difference is scale: we work with 27 characters, while production systems work with vocabularies of tens of thousands of subword tokens.\n",
    "\n",
    "::: {.callout-note}\n",
    "## The Same Core Loop\n",
    "Every autoregressive neural language model — from this character RNN to large LSTM language models used in machine translation — runs the same loop: (1) encode the input, (2) update an internal state, (3) predict the next token, (4) consume the prediction as the next input. The architecture and scale differ enormously, but the loop is identical.\n",
    ":::\n",
    "\n",
    "### Why RNNs?\n",
    "\n",
    "We need a model that can process sequences of *variable length* while maintaining memory of past inputs. An RNN does this by carrying a **hidden state** $h^t$ that gets updated at each time step as a function of the current input and the previous hidden state. In theory, the hidden state at the last time step captures the entire input history — it's a compressed summary of everything the model has seen so far.\n",
    "\n",
    "### Worked Example: Generating \"imad\"\n",
    "\n",
    "Let's trace through how the model processes a single name to make this concrete.\n",
    "\n",
    "**Step 1: Build a vocabulary.** Collect all unique characters and assign each an integer index:\n",
    "`{\"a\": 0, \"d\": 1, \"i\": 2, \"m\": 3}`. So \"imad\" becomes `[2, 3, 0, 1]`.\n",
    "\n",
    "**Step 2: Align inputs and targets.** The input at each time step is the *previous* character, and the target is the *current* character. We initialize $x^1 = \\vec{0}$ (a zero vector — \"no previous character\") and shift:\n",
    "\n",
    "| Time step | Input ($x^t$) | Target ($y^t$) |\n",
    "|---|---|---|\n",
    "| 1 | $\\vec{0}$ | \"i\" (2) |\n",
    "| 2 | \"i\" (2) | \"m\" (3) |\n",
    "| 3 | \"m\" (3) | \"a\" (0) |\n",
    "| 4 | \"a\" (0) | \"d\" (1) |\n",
    "\n",
    "**Step 3: At each time step**, the model: converts the input to a one-hot vector → computes the hidden state → produces a probability distribution over the vocabulary via softmax → measures the loss against the true target.\n",
    "\n",
    "The goal: make the probability assigned to the correct next character as high as possible. We measure this with **cross-entropy loss** and update parameters via gradient descent.\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src=\"images/char_level_example.png\" style=\"width: 800px; height: 600px\"><br>\n",
    "<caption><center><u><b><font color=\"00b7e4\">Figure 1:</font></b></u> Illustrative example of character-level language model using RNN. Green values are the target probabilities we want to maximize; red values should be minimized. Notice that $h^4$ carries information about all previous characters.</center></caption>\n",
    "</p>\n",
    "\n",
    "::: {.callout-tip}\n",
    "## Teacher Forcing\n",
    "During training, we feed the *true* target character as the next input — not the model's own prediction. This is called **teacher forcing** and it stabilizes training by preventing error accumulation across time steps. At generation time, we switch to feeding the model's own predictions back in (autoregressive decoding).\n",
    ":::\n",
    "\n",
    "**Key takeaway:** A character-level language model factorizes the probability of a name into a product of conditional probabilities, one per character. An RNN processes these sequentially, maintaining a hidden state that (in theory) summarizes all past context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Now that we understand the core idea, let's set up the training pipeline. The decisions made here — dataset, architecture variant, and optimization strategy — directly affect what the model can learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Architecture\n",
    "\n",
    "The [dataset](http://deron.meranda.us/data/census-derived-all-first.txt) contains **5,163 names** from US census data: 4,275 male names, 1,219 female names, and 331 names that can be either.\n",
    "\n",
    "We'll use a **many-to-many RNN** architecture where the number of input time steps equals the number of output time steps ($T_x = T_y$). At each step, the model reads one character and predicts the next — input and output are perfectly synced.\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src=\"images/rnn_architecture.png\" style=\"width: 600px; height: 600px\"><br>\n",
    "<caption><center><u><b><font color=\"00b7e4\">Figure 2:</font></b></u> RNN architecture: many to many — each time step produces a prediction</center></caption>\n",
    "</p>\n",
    "\n",
    "The character-level language model will be trained on names; which means after we're done with training the model, we'll be able to generate interesting names :).\n",
    "\n",
    "In this section, we'll go over four main parts:\n",
    "\n",
    "1. Forward propagation.\n",
    "2. Backpropagation\n",
    "3. Sampling\n",
    "4. Fitting the model\n",
    "\n",
    "::: {.callout-important}\n",
    "## SGD with Batch Size 1\n",
    "We train with **stochastic gradient descent** where each \"batch\" is a single name. The model runs forward and backward on one name, updates parameters, then moves to the next. This makes the loss noisy (high variance) but allows the model to learn from the idiosyncrasies of each name individually. We smooth the loss with an exponential moving average to track the trend.\n",
    ":::\n",
    "\n",
    "::: {.callout-note}\n",
    "## Practical Consideration: Batch Size & Throughput\n",
    "In production NLP systems (e.g., neural machine translation), batch size 1 would be very slow — the hardware can't parallelize across a single short sequence. Real systems use mini-batches of hundreds of sequences, padded to uniform length, to better utilize GPU resources. But the *math* is identical: gradient descent on cross-entropy loss over next-token predictions. The only difference is how many examples contribute to each gradient update.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation: From Characters to Probabilities\n",
    "\n",
    "The forward pass transforms raw characters into a probability distribution over the next character. Let's trace through each stage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Walkthrough\n",
    "\n",
    "**1. Vocabulary Construction**\n",
    "\n",
    "We build two dictionaries from the unique lowercase characters in the dataset:\n",
    "\n",
    "- `chars_to_idx`: maps each character to an integer (e.g., `\"a\" → 1`, `\"z\" → 26`). Index `0` is reserved for the newline character `\"\\n\"`, which serves as our **end-of-sequence (EOS)** token — the model learns to \"stop\" by predicting `\"\\n\"`.\n",
    "- `idx_to_chars`: the reverse mapping, used to decode model output back into characters.\n",
    "\n",
    "**2. Parameter Initialization**\n",
    "\n",
    "Weights are initialized from a small random normal distribution (to break symmetry so different hidden units learn different features). Biases are initialized to zeros.\n",
    "\n",
    "| Parameter | Connects | Shape |\n",
    "|---|---|---|\n",
    "| $W_{xh}$ | Input $x^t$ → Hidden $h^t$ | `(n_h, vocab_size)` |\n",
    "| $W_{hh}$ | Previous hidden $h^{t-1}$ → Current hidden $h^t$ | `(n_h, n_h)` |\n",
    "| $b$ | Hidden state bias | `(n_h, 1)` |\n",
    "| $W_{hy}$ | Hidden $h^t$ → Output $o^t$ | `(vocab_size, n_h)` |\n",
    "| $c$ | Output bias | `(vocab_size, 1)` |\n",
    "\n",
    "**3. One-Hot Encoding**\n",
    "\n",
    "Each character is converted to a one-hot vector of dimension `vocab_size × 1`. The first input $x^1 = \\vec{0}$ (all zeros — \"no previous character\"). From $t = 2$ onward, $x^{t} = y^{t-1}$ (the previous target character).\n",
    "\n",
    "The last target for every name is `\"\\n\"`, so the model learns *when to stop generating*.\n",
    "\n",
    "**4. Hidden State Computation**\n",
    "\n",
    "$$h^t = \\tanh(W_{hh} \\cdot h^{t-1} + W_{xh} \\cdot x^t + b) \\tag{1}$$\n",
    "\n",
    "The **tanh** activation squashes values to $[-1, 1]$. One practical advantage: near the origin, tanh resembles the identity function, which helps gradients flow during early training when weights are small.\n",
    "\n",
    "**5. Output & Softmax**\n",
    "\n",
    "$$o^t = W_{hy} \\cdot h^t + c \\tag{2}$$\n",
    "$$\\hat{y}^t = \\text{softmax}(o^t) = \\frac{e^{o^t_i}}{\\sum_j e^{o^t_j}} \\tag{3}$$\n",
    "\n",
    "The softmax converts raw logits into a valid probability distribution — all values between 0 and 1, summing to 1. Each entry $\\hat{y}^t[i]$ is the predicted probability that character $i$ comes next.\n",
    "\n",
    "**6. Cross-Entropy Loss**\n",
    "\n",
    "$$\\mathcal{L}^t = -\\log \\hat{y}^t[y^t] \\tag{4}$$\n",
    "$$\\mathcal{L} = \\sum_{t=1}^{T} \\mathcal{L}^t \\tag{5}$$\n",
    "\n",
    "We only care about the probability the model assigned to the *correct* next character. The negative log makes this a loss: high probability → low loss, low probability → high loss.\n",
    "\n",
    "Since we'll be using SGD, the loss will be noisy and have many oscillations, so it's a good practice to smooth out the loss using exponential weighted average.\n",
    "\n",
    "::: {.callout-tip}\n",
    "## Why Cross-Entropy?\n",
    "Cross-entropy loss is equivalent to minimizing the KL divergence between the model's predicted distribution and the true distribution (which is a one-hot vector). It's also the negative log-likelihood of the data under the model — so minimizing cross-entropy is the same as maximum likelihood estimation.\n",
    ":::\n",
    "\n",
    "::: {.callout-note}\n",
    "## Perplexity: The Standard Language Model Metric\n",
    "In language modeling, we typically report **perplexity** = $e^{\\mathcal{L}/T}$, which is the exponential of the average per-token cross-entropy loss. A perplexity of 27 means the model is as uncertain as if it were choosing uniformly among 27 characters. Lower is better. State-of-the-art word-level LSTM language models on the Penn Treebank benchmark achieve perplexity around 58 ([Merity et al., 2018](https://arxiv.org/abs/1708.02182)), down from over 80 just a couple of years ago.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "%load_ext lab_black\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "# | warning: false\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "os.chdir(\"../scripts/\")\n",
    "from character_level_language_model import (\n",
    "    initialize_parameters,\n",
    "    initialize_rmsprop,\n",
    "    softmax,\n",
    "    smooth_loss,\n",
    "    update_parameters_with_rmsprop,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, y, h_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement one Forward pass on one name.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : list\n",
    "        list of integers for the index of the characters in the example\n",
    "        shifted one character to the right.\n",
    "    y : list\n",
    "        list of integers for the index of the characters in the example.\n",
    "    h_prev : array\n",
    "        last hidden state from the previous example.\n",
    "    parameters : python dict\n",
    "        dictionary containing the parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        cross-entropy loss.\n",
    "    cache : tuple\n",
    "        contains three python dictionaries:\n",
    "            xs -- input of all time steps.\n",
    "            hs -- hidden state of all time steps.\n",
    "            probs -- probability distribution of each character at each time\n",
    "                step.\n",
    "    \"\"\"\n",
    "    # Retrieve parameters\n",
    "    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n",
    "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
    "\n",
    "    # Initialize inputs, hidden state, output, and probabilities dictionaries\n",
    "    xs, hs, os, probs = {}, {}, {}, {}\n",
    "\n",
    "    # Initialize x0 to zero vector\n",
    "    xs[0] = np.zeros((vocab_size, 1))\n",
    "\n",
    "    # Initialize loss and assigns h_prev to last hidden state in hs\n",
    "    loss = 0\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "\n",
    "    # Forward pass: loop over all characters of the name\n",
    "    for t in range(len(x)):\n",
    "        # Convert to one-hot vector\n",
    "        if t > 0:\n",
    "            xs[t] = np.zeros((vocab_size, 1))\n",
    "            xs[t][x[t]] = 1\n",
    "        # Hidden state\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n",
    "        # Logits\n",
    "        os[t] = np.dot(Why, hs[t]) + c\n",
    "        # Probs\n",
    "        probs[t] = softmax(os[t])\n",
    "        # Loss\n",
    "        loss -= np.log(probs[t][y[t], 0])\n",
    "\n",
    "    cache = (xs, hs, probs)\n",
    "\n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "Computing gradients for an RNN isn't quite like standard backpropagation — and the difference has profound consequences for what these models can and cannot learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why BPTT Is Different\n",
    "\n",
    "In a standard feedforward network, each layer has its *own* weights. In an RNN, **the same weights are shared across all time steps**. When we unroll the RNN across time, it looks like a very deep feedforward network — but with tied weights. This means the gradient of the loss with respect to any shared parameter must be **summed across all time steps**.\n",
    "\n",
    "We start at the last time step $T$ and propagate the loss backward through the entire sequence, accumulating gradients at each step.\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src=\"images/backprop.png\" style=\"width: 800px; height: 400px\"><br>\n",
    "<caption><center><u><b><font color=\"00b7e4\">Figure 3:</font></b></u> Backpropagation Through Time — gradients flow backward from the loss at each time step, accumulating across the shared weights</center></caption>\n",
    "</p>\n",
    "\n",
    "### The Gradient Clipping Problem\n",
    "\n",
    "RNN loss landscapes are known for having **steep cliffs** — regions where the loss changes dramatically over a tiny change in parameters. When the gradient hits one of these cliffs, it can become enormous, causing a single update to overshoot the minimum and undo many iterations of progress.\n",
    "\n",
    "Why does this happen? The gradient is a *linear* approximation of the loss surface. It captures the local slope but knows nothing about curvature. A steep cliff means the local slope is huge, but the optimal step size is actually tiny.\n",
    "\n",
    "The fix is simple and effective: **gradient clipping**. Before updating, we clip every gradient element to the interval $[-5, 5]$. If any gradient value exceeds these bounds, it's capped. This prevents catastrophic updates while preserving the gradient direction.\n",
    "\n",
    "::: {.callout-important}\n",
    "## Vanishing & Exploding Gradients\n",
    "Because the same weight matrix $W_{hh}$ is multiplied at each time step, the gradient either grows or shrinks *exponentially* with sequence length:\n",
    "\n",
    "- If the dominant eigenvalue of $W_{hh}$ is **< 1** → gradients **vanish** (the model can't learn long-range dependencies)\n",
    "- If it's **> 1** → gradients **explode** (training becomes unstable)\n",
    "\n",
    "Gradient clipping addresses exploding gradients. Vanishing gradients require architectural changes — which is exactly what LSTMs ([Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf)) and the recently proposed Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) were designed to solve.\n",
    ":::\n",
    "\n",
    "### The Gradient Equations\n",
    "\n",
    "For completeness, here are the BPTT gradient equations. The key insight is that each gradient sums contributions across all time steps, and the hidden state gradient at time $t$ receives contributions from *both* the output at time $t$ and the hidden state at time $t+1$ (the future).\n",
    "\n",
    "$$\\nabla_{o^t}\\mathcal{L} = \\widehat{y^t} - y^t\\tag{6}$$\n",
    "$$\\nabla_{W_{hy}}\\mathcal{L} = \\sum_t \\nabla_{o^t}\\mathcal{L}\\cdot{h^t}^T\\tag{7}$$\n",
    "$$\\nabla_{c}\\mathcal{L} = \\sum_t \\nabla_{o^t}\\mathcal{L} \\tag{8}$$\n",
    "$$\\nabla_{h^t}\\mathcal{L} = W_{hy}^T\\cdot\\nabla_{o^t}\\mathcal{L} + \\underbrace { W_{hh}^T\\cdot\\nabla_{h^{t + 1}}\\mathcal{L} * (1 - tanh(W_{hh}h^{t} + W_{xh}x^{t + 1} + b) ^ 2)}_{dh_{next}} \\tag{9}$$\n",
    "$$\\nabla_{h^{t - 1}}\\mathcal{L} = W_{hh}^T\\cdot\\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2)\\tag{10}$$\n",
    "$$\\nabla_{x^t}\\mathcal{L} = W_{xh}^T\\cdot\\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2)\\tag{11}$$\n",
    "$$\\nabla_{W_{hh}}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2)\\cdot{h^{t - 1}}^T\\tag{12}$$\n",
    "$$\\nabla_{W_{xh}}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2) . {x^t}^T\\tag{13}$$\n",
    "$$\\nabla_{b}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2) \\tag{14}$$\n",
    "\n",
    "At the last time step $T$, we initialize $dh_{next}$ to zeros since there is no future to backpropagate from.\n",
    "\n",
    "::: {.callout-tip}\n",
    "## Reading the Equations\n",
    "The $(1 - tanh^2)$ terms are the derivative of $\\tanh$. If you squint, the structure is always: **upstream gradient × local Jacobian × input to this operation**. That's the chain rule applied at each node — the same pattern used by automatic differentiation frameworks like TensorFlow and PyTorch.\n",
    ":::\n",
    "\n",
    "### Optimizer: RMSProp\n",
    "\n",
    "Since SGD with batch size 1 produces very noisy gradients, we use **[Root Mean Squared Propagation (RMSProp)](https://nbviewer.jupyter.org/github/ImadDabbura/Deep-Learning/blob/master/posts/Optimization-Algorithms.ipynb)** — an adaptive learning rate method that divides each gradient by a running average of its recent magnitude. This dampens updates for parameters with consistently large gradients and amplifies updates for parameters with consistently small gradients, leading to more stable convergence.\n",
    "\n",
    "::: {.callout-note}\n",
    "## Practical Consideration: Adaptive Optimizers\n",
    "RMSProp belongs to the family of adaptive learning rate methods, alongside Adagrad and Adam ([Kingma & Ba, 2015](https://arxiv.org/abs/1412.6980)). Adam combines RMSProp's adaptive second moment with a momentum term (first moment) and is currently the most popular optimizer for training deep networks, especially for NLP tasks like machine translation and language modeling.\n",
    ":::\n",
    "\n",
    "**Key takeaway:** BPTT computes gradients by unrolling the RNN through time and summing gradient contributions across all time steps. Gradient clipping prevents explosive updates, but vanishing gradients require architectural solutions (LSTM, GRU, or attention mechanisms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def clip_gradients(gradients, max_value):\n",
    "    \"\"\"\n",
    "    Implements gradient clipping element-wise on gradients to be between the\n",
    "    interval [-max_value, max_value].\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    gradients : python dict\n",
    "        dictionary that stores all the gradients.\n",
    "    max_value : scalar\n",
    "        edge of the interval [-max_value, max_value].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients : python dict\n",
    "        dictionary where all gradients were clipped.\n",
    "    \"\"\"\n",
    "    for grad in gradients.keys():\n",
    "        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def rnn_backward(y, parameters, cache):\n",
    "    \"\"\"\n",
    "    Implements Backpropagation on one name.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    y : list\n",
    "        list of integers for the index of the characters in the example.\n",
    "    parameters : python dict\n",
    "        dictionary containing the parameters.\n",
    "    cache : tuple\n",
    "            contains three python dictionaries:\n",
    "                xs -- input of all time steps.\n",
    "                hs -- hidden state of all time steps.\n",
    "                probs -- probability distribution of each character at each time\n",
    "                    step.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grads : python dict\n",
    "        dictionary containing all the gradients.\n",
    "    h_prev : array\n",
    "        last hidden state from the current example.\n",
    "    \"\"\"\n",
    "    # Retrieve xs, hs, and probs\n",
    "    xs, hs, probs = cache\n",
    "\n",
    "    # Initialize all gradients to zero\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n",
    "    grads = {}\n",
    "    for param_name in parameters_names:\n",
    "        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n",
    "\n",
    "    # Iterate over all time steps in reverse order starting from Tx\n",
    "    for t in reversed(range(len(xs))):\n",
    "        dy = np.copy(probs[t])\n",
    "        dy[y[t]] -= 1\n",
    "        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n",
    "        grads[\"dc\"] += dy\n",
    "        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n",
    "        dhraw = (1 - hs[t] ** 2) * dh\n",
    "        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n",
    "        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n",
    "        grads[\"db\"] += dhraw\n",
    "        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n",
    "\n",
    "    # Clip gradients after accumulating across all time steps\n",
    "    grads = clip_gradients(grads, 5)\n",
    "\n",
    "    # Get the last hidden state\n",
    "    h_prev = hs[len(xs) - 1]\n",
    "\n",
    "    return grads, h_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling: The Creativity-Coherence Trade-off\n",
    "\n",
    "Training teaches the model *what* to predict. Sampling determines *how* we use those predictions to generate text — and it's where the magic (and the control) lives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Entropy Spectrum\n",
    "\n",
    "At each time step, the model outputs a conditional probability distribution over the next character: $P(c_t \\mid c_1, \\ldots, c_{t-1})$. Suppose at time $t = 3$, the distribution is $(0.2, 0.3, 0.4, 0.1)$. How do we pick the next character?\n",
    "\n",
    "There are two extremes — and a useful middle ground:\n",
    "\n",
    "| Strategy | Entropy | Behavior | Result |\n",
    "|---|---|---|---|\n",
    "| **Uniform random** | Maximum | Ignore the model entirely; pick any character with equal probability | Gibberish — no structure |\n",
    "| **Argmax (greedy)** | Minimum | Always pick the highest-probability character | Coherent but repetitive and boring |\n",
    "| **Sample from distribution** | Medium | Pick characters proportionally to their predicted probability | Creative *and* structured |\n",
    "\n",
    "We use the middle option: **sample from the model's own distribution**. Character with probability 0.4 gets picked 40% of the time, character with probability 0.1 gets picked 10% of the time. This preserves the model's learned structure while allowing for variety. Using this sampling strategy on the above distribution, the index 0 has $20$% probability of being picked, while index 2 has $40$% probability to be picked.\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src=\"images/sampling.png\" style=\"width: 800px; height: 400px\"><br>\n",
    "<caption><center><u><b><font color=\"00b7e4\">Figure 4:</font></b></u> Sampling from the model's predicted distribution — a balance between randomness and coherence</center></caption>\n",
    "</p>\n",
    "\n",
    "As we increase randomness, text will loose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure.\n",
    "\n",
    "::: {.callout-note}\n",
    "## Temperature Scaling\n",
    "A common extension is to introduce a **temperature** parameter $\\tau$. Before applying softmax, the logits are divided by $\\tau$:\n",
    "\n",
    "$$P(c_i) = \\frac{e^{o_i / \\tau}}{\\sum_j e^{o_j / \\tau}}$$\n",
    "\n",
    "- $\\tau = 1.0$: standard sampling (what we do here)\n",
    "- $\\tau \\to 0$: approaches argmax (greedy, deterministic)\n",
    "- $\\tau > 1$: flattens the distribution (more random, more \"creative\")\n",
    "\n",
    "Temperature gives fine-grained control over the creativity-coherence trade-off without retraining the model. It's widely used in neural text generation systems.\n",
    ":::\n",
    "\n",
    "::: {.callout-tip}\n",
    "## Why Not Always Use Argmax?\n",
    "Greedy decoding (always picking the most likely character) produces the single most probable *next* character at each step, but this doesn't necessarily produce the most probable *sequence*. It can get stuck in repetitive loops and miss globally better paths. Sampling introduces the randomness needed to explore the distribution and generate diverse, interesting outputs. This is why beam search — which tracks multiple hypotheses simultaneously — is preferred over greedy decoding in tasks like machine translation.\n",
    ":::\n",
    "\n",
    "Therefore, sampling will be used at test time to generate names character by character.\n",
    "\n",
    "**Key takeaway:** Sampling strategy controls the trade-off between creativity and coherence. Sampling from the model's distribution is a sweet spot — it respects the learned probabilities while producing diverse outputs. Temperature scaling provides an additional dial to tune this balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sample(parameters, idx_to_chars, chars_to_idx, n, seed=None):\n",
    "    \"\"\"\n",
    "    Implements sampling of a squence of n characters characters length. The\n",
    "    sampling will be based on the probability distribution output of RNN.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    parameters : python dict\n",
    "        dictionary storing all the parameters of the model.\n",
    "    idx_to_chars : python dict\n",
    "        dictionary mapping indices to characters.\n",
    "    chars_to_idx : python dict\n",
    "        dictionary mapping characters to indices.\n",
    "    n : scalar\n",
    "        number of characters to output.\n",
    "    seed : int, optional\n",
    "        random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sequence : str\n",
    "        sequence of characters sampled.\n",
    "    \"\"\"\n",
    "    # Retrieve parameters, shapes, and vocab size\n",
    "    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n",
    "    Why, c = parameters[\"Why\"], parameters[\"c\"]\n",
    "    n_h, n_x = Wxh.shape\n",
    "    vocab_size = c.shape[0]\n",
    "\n",
    "    # Use new-style random generator for reproducibility\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Initialize a0 and x1 to zero vectors\n",
    "    h_prev = np.zeros((n_h, 1))\n",
    "    x = np.zeros((n_x, 1))\n",
    "\n",
    "    # Initialize empty sequence\n",
    "    indices = []\n",
    "    idx = -1\n",
    "    counter = 0\n",
    "    while counter <= n and idx != chars_to_idx[\"\\n\"]:\n",
    "        # Fwd propagation\n",
    "        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n",
    "        o = np.dot(Why, h) + c\n",
    "        probs = softmax(o)\n",
    "\n",
    "        # Sample the index of the character using generated probs distribution\n",
    "        idx = rng.choice(vocab_size, p=probs.ravel())\n",
    "\n",
    "        # Get the character of the sampled index\n",
    "        char = idx_to_chars[idx]\n",
    "\n",
    "        # Add the char to the sequence\n",
    "        indices.append(idx)\n",
    "\n",
    "        # Update a_prev and x\n",
    "        h_prev = np.copy(h)\n",
    "        x = np.zeros((n_x, 1))\n",
    "        x[idx] = 1\n",
    "\n",
    "        counter += 1\n",
    "    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together: Training and Results\n",
    "\n",
    "With forward propagation, BPTT, and sampling in place, we can now train the full model. Let's see how the generated names evolve as the model learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Overview\n",
    "\n",
    "After covering all the concepts/intuitions behind character-level language model, now we're ready to fit the model. The training loop is straightforward:\n",
    "\n",
    "1. **Shuffle** the names at the start of each epoch (reduces ordering bias)\n",
    "2. For each name: convert characters to indices, run **forward pass**, compute **smoothed loss**, run **backward pass** (BPTT with gradient clipping), **update parameters** with RMSProp\n",
    "3. Every 10 epochs: sample a name and print the smoothed loss\n",
    "\n",
    "We'll use the default settings for RMSProp's hyperparameters and run the model for 100 iterations. On each iteration, we'll print out one sampled name and smoothed loss to see how the names generated start to get more interesting with more iterations as well as the loss will start decreasing. When done with fitting the model, we'll plot the loss function and generate some names.\n",
    "\n",
    "::: {.callout-tip}\n",
    "## What to Watch For\n",
    "As training progresses, watch for two signals: (1) the smoothed loss should decrease steadily, and (2) the sampled names should transition from random character sequences to plausible-sounding names. If the loss plateaus early, the model may need more hidden units or a lower learning rate.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def model(\n",
    "    file_path,\n",
    "    chars_to_idx,\n",
    "    idx_to_chars,\n",
    "    hidden_layer_size,\n",
    "    vocab_size,\n",
    "    num_epochs=10,\n",
    "    learning_rate=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements RNN to generate characters.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    file_path : str\n",
    "        path to the file of the raw data.\n",
    "    num_epochs : int\n",
    "        number of passes the optimization algorithm to go over the training\n",
    "        data.\n",
    "    learning_rate : float\n",
    "        step size of learning.\n",
    "    chars_to_idx : python dict\n",
    "        dictionary mapping characters to indices.\n",
    "    idx_to_chars : python dict\n",
    "        dictionary mapping indices to characters.\n",
    "    hidden_layer_size : int\n",
    "        number of hidden units in the hidden layer.\n",
    "    vocab_size : int\n",
    "        size of vocabulary dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters : python dict\n",
    "        dictionary storing all the parameters of the model.\n",
    "    overall_loss : list\n",
    "        list stores smoothed loss per epoch.\n",
    "    \"\"\"\n",
    "    # Get the data\n",
    "    with open(file_path) as f:\n",
    "        data = f.readlines()\n",
    "    examples = [x.lower().strip() for x in data]\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n",
    "\n",
    "    # Initialize Adam parameters\n",
    "    s = initialize_rmsprop(parameters)\n",
    "\n",
    "    # Initialize loss\n",
    "    smoothed_loss = -np.log(1 / vocab_size) * 7\n",
    "\n",
    "    # Initialize hidden state h0 and overall loss\n",
    "    h_prev = np.zeros((hidden_layer_size, 1))\n",
    "    overall_loss = []\n",
    "\n",
    "    # Iterate over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle examples\n",
    "        np.random.shuffle(examples)\n",
    "\n",
    "        # Iterate over all examples (SGD)\n",
    "        for example in examples:\n",
    "            x = [None] + [chars_to_idx[char] for char in example]\n",
    "            y = x[1:] + [chars_to_idx[\"\\n\"]]\n",
    "            # Fwd pass\n",
    "            loss, cache = rnn_forward(x, y, h_prev, parameters)\n",
    "            # Compute smooth loss\n",
    "            smoothed_loss = smooth_loss(smoothed_loss, loss)\n",
    "            # Bwd pass\n",
    "            grads, h_prev = rnn_backward(y, parameters, cache)\n",
    "            # Update parameters\n",
    "            parameters, s = update_parameters_with_rmsprop(parameters, grads, s)\n",
    "\n",
    "        overall_loss.append(smoothed_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n",
    "            print(f\"\\033[1m\\033[92m=======\")\n",
    "            # Sample one name\n",
    "            print(\n",
    "                f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n",
    "                10).capitalize()}\"\"\"\n",
    "            )\n",
    "            print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n",
    "\n",
    "    return parameters, overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36121 characters and 27 unique characters.\n",
      "\u001b[1m\u001b[94mEpoch 0\n",
      "\u001b[1m\u001b[92m=======\n",
      "Sampled name: Ia\n",
      "Smoothed loss: 17.8206\n",
      "\n",
      "\u001b[1m\u001b[94mEpoch 10\n",
      "\u001b[1m\u001b[92m=======\n",
      "Sampled name: Rioee\n",
      "Smoothed loss: 15.8061\n",
      "\n",
      "\u001b[1m\u001b[94mEpoch 20\n",
      "\u001b[1m\u001b[92m=======\n",
      "Sampled name: Allise\n",
      "Smoothed loss: 15.8609\n",
      "\n",
      "\u001b[1m\u001b[94mEpoch 30\n",
      "\u001b[1m\u001b[92m=======\n",
      "Sampled name: Ininyo\n",
      "Smoothed loss: 15.7734\n",
      "\n",
      "\u001b[1m\u001b[94mEpoch 40\n",
      "\u001b[1m\u001b[92m=======\n",
      "Sampled name: Miadoe\n",
      "Smoothed loss: 15.7312\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Smoothed loss')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load names\n",
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Convert characters to lower case\n",
    "data = data.lower()\n",
    "\n",
    "# Construct vocabulary using unique characters, sort it in ascending order,\n",
    "# then construct two dictionaries that maps character to index and index to\n",
    "# characters.\n",
    "chars = list(sorted(set(data)))\n",
    "chars_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_chars = {i: ch for ch, i in chars_to_idx.items()}\n",
    "\n",
    "# Get the size of the data and vocab size\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars_to_idx)\n",
    "print(f\"There are {data_size} characters and {vocab_size} unique characters.\")\n",
    "\n",
    "# Fitting the model\n",
    "parameters, loss = model(\n",
    "    \"../data/names.txt\", chars_to_idx, idx_to_chars, 10, vocab_size, 50, 0.01\n",
    ")\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(range(len(loss)), loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Smoothed loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "As training progresses, the generated names evolve from random character soup to increasingly plausible names. By around epoch 15, the model has learned basic phonotactic patterns — which character combinations sound like real names. One of the interesting generated names is \"Yasira,\" which is an actual Arabic name — the model has learned cross-cultural naming patterns purely from statistical regularities!\n",
    "\n",
    "The loss curve shows a typical pattern for character-level models: rapid initial decrease (learning basic character frequencies), followed by slower improvement (learning positional and contextual patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Learned — and What Comes Next"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a complete character-level language model from scratch: vocabulary construction, forward propagation through an RNN, backpropagation through time, gradient clipping, and probabilistic sampling. The model learned to generate plausible names from 5,163 training examples using just 10 hidden units.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **The core autoregressive loop** — predict next token, feed prediction back as input — is identical across all neural language models, from this character RNN to large-scale LSTM systems used in production.\n",
    "- **Sampling strategy matters.** The same model produces gibberish (uniform sampling), boring repetition (greedy), or creative-yet-plausible names (distribution sampling). Temperature scaling gives fine-grained control over this spectrum.\n",
    "- **Gradient instabilities are fundamental to RNNs.** Shared weights across time steps cause gradients to explode or vanish exponentially with sequence length. Clipping handles explosions; architectural changes (LSTM, GRU, attention) handle vanishing.\n",
    "- **SGD with batch size 1 works but is slow.** Increasing batch size (e.g., packing multiple names into sequences of 50 characters) would speed up learning and improve gradient estimates.\n",
    "- With the sampling technique we're using, don't expect the RNN to generate meaningful sequence of characters (names).\n",
    "- We can control the level of randomness using the sampling strategy. Here, we balanced between what the model thinks its the right character and the level of randomness.\n",
    "\n",
    "### Scaling Up: What Would Change?\n",
    "\n",
    "This model is a starting point. If we have more data, bigger model, and train longer we may get more interesting results. To generate more interesting and realistic text, the natural next steps are:\n",
    "\n",
    "| This Model | Production Systems |\n",
    "|---|---|\n",
    "| Characters (27 tokens) | Word-level or BPE subword vocabularies (10K–50K tokens) |\n",
    "| Single-layer RNN (10 hidden units) | Multi-layer LSTM (1,000+ hidden units per layer) |\n",
    "| SGD + RMSProp, batch size 1 | Adam optimizer, mini-batches of 64–256 sequences |\n",
    "| 5K names | Millions of sentences (Wikipedia, books, web text) |\n",
    "| CPU, seconds to train | GPUs, hours to days of training |\n",
    "\n",
    "People have used 3-layer deep LSTM models with dropout and achieved impressive results on tasks like generating Shakespeare poems and cooking recipes. LSTM models outperform simple RNNs due to their ability to capture longer-range temporal dependencies. The recently proposed Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which replaces recurrence entirely with self-attention, is showing very promising results for sequence modeling and may reshape how we build language models going forward.\n",
    "\n",
    "::: {.callout-note}\n",
    "## What Stays the Same at Scale\n",
    "Regardless of scale, the *concepts* remain the same: conditional probability factorization, teacher forcing, cross-entropy loss, gradient-based optimization, and the sampling trade-off. Understanding them at this small scale makes the jump to larger systems far more intuitive.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
