{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf3687f-8696-44d1-acab-0ac5af311162",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Tokenization Uncovered: How BPE Shapes the Mind of a Language Model\"\n",
    "subtitle: A deep dive into the algorithm that compresses text, balances trade-offs, and defines how AI perceives meaning\n",
    "date: 2024-04-10\n",
    "date-modified: 2025-01-13\n",
    "categories: [NLP, Deep Learning]\n",
    "image: bpe-tokenizer.jpg\n",
    "title-block-banner: bpe-tokenizer.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8c6cc-8438-4963-8fee-4a206d1a95d8",
   "metadata": {},
   "source": [
    "<img src=\"bpe-tokenizer.jpg\" height=\"500px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4d799-bbd1-499d-9c13-ebc82a8864d3",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be7d8f-b75f-49ef-bd3b-5547f58483b4",
   "metadata": {},
   "source": [
    "What if I told you that the way a language model \"sees\" the world is entirely shaped by one invisible yet critical process? A process so fundamental that it can make or break the model\u2019s ability to understand context, predict meaning, and even handle different languages. That process is tokenization\u2014the art of breaking down text into smaller, digestible pieces called tokens. And it\u2019s not just a technical detail; it\u2019s the very lens through which language models perceive the world.\n",
    "\n",
    "But here\u2019s the twist: tokenization isn\u2019t perfect. It\u2019s a balancing act, full of trade-offs and challenges. For instance, languages like English, which dominate training datasets, are tokenized into fewer, denser tokens, allowing models to process more context efficiently. Meanwhile, less-represented languages like Korean are fragmented into far more tokens, forcing the model to work harder to understand the same amount of text. This imbalance can lead to inefficiencies, biases, and even a loss of meaning.\n",
    "\n",
    "So, how do we strike the right balance? Enter Byte Pair Encoding (BPE)\u2014a clever algorithm that merges frequent character pairs into subwords, creating a compact yet expressive vocabulary. Starting from raw bytes, BPE builds tokens step by step, like assembling a puzzle, ensuring flexibility while maintaining efficiency. It\u2019s a method that not only compresses text but also shapes how much context a model can attend to, directly influencing its performance.\n",
    "\n",
    "In this post, we\u2019ll unravel the mysteries of Byte Pair Encoding. We\u2019ll explore how it works, why it\u2019s so effective, and the surprising ways it impacts everything from multilingual understanding to computational efficiency. By the end, you\u2019ll see how this seemingly simple process holds the key to unlocking the true potential of language models. Ready to dive in? Let\u2019s decode the magic of tokenization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a4c37-17e2-456e-9ccf-9ad8ea2ea242",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4d86e-4998-44bd-b78b-c20978bc102e",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE) is a subword tokenization algorithm originally developed for data compression, now widely used in NLP to handle unknown or rare words by breaking text into frequent subword units. BPE strikes a balance between character-level and word-level representations, enabling open-vocabulary modeling.\n",
    "\n",
    "Algorithm Steps:\n",
    "- Initialize Vocabulary: Begin with a base vocabulary containing all 256 possible byte values (0\u2013255), ensuring full coverage of any input text\n",
    "    - Treat each word in the training corpus as a sequence of characters\n",
    "- Count Symbol Pairs: Scan the corpus to count all adjacent symbol (character or subword) pairs\n",
    "- Merge Most Frequent Pair: Identify the most frequent pair of symbols\n",
    "    - Merge them into a new symbol (e.g., merging \"l\" and \"o\" -> \"lo\")\n",
    "- Update Vocabulary: Replace all occurrences of the merged pair in the corpus with the new symbol\n",
    "- Repeat steps 2\u20134 for a predefined number of merge operations (vocabulary size) or until no pairs remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660e14ce-2270-4cf4-9078-36759f24db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc957dc1-22af-4978-835a-21881cea06cb",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1517b8b6-f809-44a3-a368-2ecf7073c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "from typing import Iterable\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a672ce35-a11c-4452-92fa-09b54198aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"Byte-pair encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_sz: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_sz (int): Vocabulary size.\n",
    "        \"\"\"\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "\n",
    "    def train(self, text: Iterable[str]):\n",
    "        \"\"\"Train Byte-pair encoder.\"\"\"\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        for idx in range(256, self.vocab_sz):\n",
    "            stats = self._get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            self.merges[pair] = idx\n",
    "            ids = self._merge(ids, pair, idx)\n",
    "        self.vocab = self._build_vocab(ids)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode string to bytes using vocabulary built during training.\"\"\"\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "\n",
    "        # If text is empty or has one character -> it is already encoded from previous step\n",
    "        while len(ids) >= 2:\n",
    "            # stats is used only for getting pairs next to each other\n",
    "            stats = self._get_stats(ids)\n",
    "            # Because we built vocab (and merges) bottom-up, we need to encode\n",
    "            # idx from smallest index because some later pairs depend on pairs\n",
    "            # occured before\n",
    "            # If a pair doesn't exist, it wouldn't participate in the list\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break  # No more pairs to merge\n",
    "            idx = self.merges[pair]\n",
    "            ids = self._merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, tokens: Iterable[int]):\n",
    "        \"\"\"Decode tokens into string using the vocabulary built during training.\"\"\"\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in tokens)\n",
    "        # It is important to replace tokens that were not seen during training\n",
    "        # with `?`; otherwise, it would fail\n",
    "        return tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def _get_stats(self, ids: Iterable[int]):\n",
    "        \"\"\"Get pair counts.\"\"\"\n",
    "        counts = {}\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def _merge(self, ids: Iterable[int], pair: Iterable[int], idx: int):\n",
    "        \"\"\"Merge pairs that match `pair` with new index `idx`.\"\"\"\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and pair[0] == ids[i] and pair[1] == ids[i + 1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def _build_vocab(self, ids: Iterable[int]):\n",
    "        \"\"\"Build vocabulary from 0-255 bytes and merges.\"\"\"\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        # Here we assume the items returned would be in the same order they were inserted.\n",
    "        # This is Okay Python 3.7+\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            # This would be a concatenation of the bytes\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26b9c11-cf65-45ca-a03a-7f2acef56f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(\"https://docs.python.org/3/library/stdtypes.html#bytes.decode\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4368c372-793e-4ca7-be14-c8e62b9c9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a4e7562-ca4c-4e81-907c-70479b2448ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ba82c99-8c22-4c9e-be22-717b6be33ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text)) == text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf45227-5d5c-47ac-ad38-7b161b658b7a",
   "metadata": {},
   "source": [
    "# GPT2 BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696bba5-7b57-4a93-bb24-d36e0b24bf3a",
   "metadata": {},
   "source": [
    "Vanilla byte-level BPE applies merges directly on raw byte sequences using a greedy frequency-based heuristic. While this reduces the initial vocabulary size to 256, it leads to inefficient vocabulary usage. For example, common words like \"play\" might appear with different punctuation (e.g., \"play:\", \"play?\", \"play!\") and the BPE algorithm tends to learn separate tokens for each variant. This redundancy consumes vocabulary slots that could be better utilized.\n",
    "\n",
    "Additionally, standard BPE implementations operating on Unicode code points would require a base vocabulary of over 130,000 symbols to handle all Unicode strings. This is impractically large for most models, which typically use vocabularies in the 32k\u201364k range.\n",
    "\n",
    "To address these issues, GPT-2 uses a modified byte-level BPE approach:\n",
    "\n",
    "It starts with a 256-symbol byte vocabulary.\n",
    "\n",
    "It prevents merges across character categories (e.g., letters and punctuation), reducing redundancy.\n",
    "\n",
    "It allows merging with spaces to improve compression without overly fragmenting words.\n",
    "\n",
    "This approach balances the open-vocabulary benefits of byte-level encoding with the compression and efficiency gains of BPE, while preserving the ability to assign probabilities to any Unicode string without requiring preprocessing or token normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba2a5f-af1c-4407-a144-b8e0a3801d5a",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b12d6e-c7b2-41bb-adfa-6d3fe086fe65",
   "metadata": {},
   "source": [
    "- [UTF-8 Everywhere](https://utf8everywhere.org/)\n",
    "- [A Programmer\u2019s Introduction to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)\n",
    "- [Karpathy's video on BPE](https://www.youtube.com/watch?v=zduSFxRajkE)\n",
    "- [Good tokenization web app](https://tiktokenizer.vercel.app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}