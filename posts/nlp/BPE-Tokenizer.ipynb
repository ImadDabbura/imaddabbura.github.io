{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf3687f-8696-44d1-acab-0ac5af311162",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Byte Pair Encoding from Scratch\"\n",
    "subtitle: \"Building a BPE tokenizer step by step — the algorithm that decides how language models see text\"\n",
    "date: 2024-04-10\n",
    "date-modified: 2025-01-13\n",
    "categories: [NLP, Deep Learning]\n",
    "image: bpe-tokenizer.jpg\n",
    "title-block-banner: bpe-tokenizer.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4d799-bbd1-499d-9c13-ebc82a8864d3",
   "metadata": {},
   "source": [
    "## Why Tokenization Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be7d8f-b75f-49ef-bd3b-5547f58483b4",
   "metadata": {},
   "source": [
    "When you type \"unhappiness\" into ChatGPT, the model doesn't see the word \"unhappiness.\" It sees something like `[\"un\", \"happ\", \"iness\"]` — three **tokens** that were chosen by an algorithm months before the model was even trained. That algorithm decided, based on statistics from a massive training corpus, that these three pieces are the right granularity. Not individual characters (too many tokens, too little meaning per token). Not whole words (too many unique words, no way to handle words never seen in training). Subwords — the sweet spot.\n",
    "\n",
    "This isn't a minor preprocessing detail. Tokenization defines **what the model can see**. Consider three strategies on the same sentence:\n",
    "\n",
    "| Strategy | \"The cat sat unhappily\" becomes | Tokens | Vocab Size |\n",
    "|---|---|---|---|\n",
    "| **Character-level** | `[\"T\",\"h\",\"e\",\" \",\"c\",\"a\",\"t\",\" \",\"s\",\"a\",\"t\",\" \",\"u\",\"n\",\"h\",\"a\",\"p\",\"p\",\"i\",\"l\",\"y\"]` | 21 | ~256 |\n",
    "| **Word-level** | `[\"The\", \"cat\", \"sat\", \"unhappily\"]` | 4 | 100,000+ |\n",
    "| **Subword (BPE)** | `[\"The\", \" cat\", \" sat\", \" un\", \"happ\", \"ily\"]` | 6 | ~50,000 |\n",
    "\n",
    "With characters, a fixed context window of 2048 tokens covers ~400 words. With subwords, the same window covers ~1500 words — nearly 4× more context for the model to reason over. Word-level is compact but brittle: \"unhappily\" might never appear in training data, making it an `<UNK>` token the model is completely blind to. But \"un\", \"happ\", and \"ily\" almost certainly do appear — the model can compose meaning from pieces it knows.\n",
    "\n",
    "The algorithm that learns these splits is **Byte Pair Encoding (BPE)** — originally a data compression technique ([Gage, 1994](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)), adapted for NLP by [Sennrich et al. (2016)](https://arxiv.org/abs/1508.07909), and now used in GPT-2, GPT-3/4, LLaMA, and most modern language models. In this post, we'll understand how it works, implement it from scratch, and see how GPT-2 refined the basic algorithm for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a4c37-17e2-456e-9ccf-9ad8ea2ea242",
   "metadata": {},
   "source": [
    "## How BPE Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4d86e-4998-44bd-b78b-c20978bc102e",
   "metadata": {},
   "source": [
    "The core insight is simple: **if two symbols frequently appear next to each other, they probably belong together.** Merge them into a single token, then look for the next most frequent pair, and repeat. It's exactly how you'd compress a text file — find repeated patterns and replace them with shorter symbols. Frequent patterns get absorbed into single tokens; rare patterns stay as smaller pieces.\n",
    "\n",
    "Think of it like learning abbreviations. If you keep writing \"machine learning\" in your notes, you'd eventually start writing \"ML.\" Then if \"ML model\" keeps appearing, maybe you'd abbreviate that too. BPE does the same thing, but systematically and bottom-up — starting from the smallest units (bytes) and building up to subwords.\n",
    "\n",
    "### Seeing It in Action\n",
    "\n",
    "Before formalizing the algorithm, let's watch it work on a real example. Consider a tiny training corpus containing the words `\"low lower lowest\"`:\n",
    "\n",
    "| Step | Token Sequence | Most Frequent Pair | New Token |\n",
    "|---|---|---|---|\n",
    "| Start | `l o w _ l o w e r _ l o w e s t` | — | — |\n",
    "| Merge 1 | `lo w _ lo w e r _ lo w e s t` | `(l, o)` → `lo` | 3× |\n",
    "| Merge 2 | `low _ low e r _ low e s t` | `(lo, w)` → `low` | 3× |\n",
    "| Merge 3 | `low _ lowe r _ lowe s t` | `(low, e)` → `lowe` | 2× |\n",
    "\n",
    "BPE discovered that `l` and `o` always appear together, then that `lo` and `w` always appear together, building up `low` as a token — effectively learning the word stem. Then it found `lowe` as a shared prefix of \"lower\" and \"lowest.\" Without any linguistic rules, purely from frequency, BPE learned morphological structure.\n",
    "\n",
    "Notice what happened in merge 2: the algorithm merged `lo` with `w`, where `lo` itself was created in merge 1. BPE builds tokens **hierarchically** — later merges compose earlier ones, the same way that syllables compose into words.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "With the intuition in place, here's the formal procedure:\n",
    "\n",
    "1. **Initialize**: Start with a base vocabulary of all 256 byte values (0–255). Every string can be represented as bytes, so this guarantees full coverage — no `<UNK>` tokens, ever.\n",
    "\n",
    "2. **Count pairs**: Scan the corpus and count every adjacent pair of tokens.\n",
    "\n",
    "3. **Merge the most frequent pair**: Create a new token for it and replace all occurrences in the corpus.\n",
    "\n",
    "4. **Repeat** steps 2–3 until you've done `vocab_size - 256` merges.\n",
    "\n",
    "The output is a **merge table**: an ordered list of pair → token mappings. This table *is* the tokenizer.\n",
    "\n",
    "### Training vs. Encoding: A Subtle Difference\n",
    "\n",
    "There's an important asymmetry between how BPE *learns* merges (training) and how it *applies* them to new text (encoding).\n",
    "\n",
    "During **training**, we always merge the globally most *frequent* pair — that's how we decide which merges to learn. But during **encoding**, we apply merges in the *order they were learned*, not by their frequency in the new text. Why? Because later merges depend on earlier ones. The token `low` only exists after `lo` has been created. If we tried to merge `(lo, w)` before creating `lo`, we'd never find the pair.\n",
    "\n",
    "In the implementation, this shows up as a subtle line: `min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))` — instead of picking the most frequent pair, it picks the pair with the *lowest merge index* (i.e., learned earliest). Pairs not in the merge table get `inf`, so they're never merged.\n",
    "\n",
    "### Why Bytes, Not Characters?\n",
    "\n",
    "Starting from bytes (0–255) rather than Unicode code points is a practical decision. Unicode has over 150,000 code points — that's an impractically large base vocabulary. By working at the byte level, we start with just 256 symbols and can represent *any* string in *any* language or script. BPE merges then learn to compose bytes into characters, characters into subwords, and subwords into common words — all driven by frequency in the training data.\n",
    "\n",
    "The trade-off: languages underrepresented in training data get less efficient tokenization. English \"hello\" might be one token, but the same greeting in a low-resource language could take 3–4 tokens because the byte sequences were never frequent enough to merge. This means the model burns more of its context window on the same content — a real and well-documented source of multilingual inefficiency ([Petrov et al., 2023](https://arxiv.org/abs/2311.09071)).\n",
    "\n",
    "### Vocabulary Size: A Key Hyperparameter\n",
    "\n",
    "How many merges should we do? This is the vocabulary size, and it's a meaningful trade-off:\n",
    "\n",
    "| Vocab Size | Tokens per Text | Embedding Table | Character |\n",
    "|---|---|---|---|\n",
    "| **Small** (~1k) | Many — close to character-level | Tiny | Better generalization on rare words, but sequences are long and training is slow |\n",
    "| **Medium** (~32k–50k) | Moderate — good compression | Manageable | The sweet spot for most models (GPT-2: 50k, LLaMA: 32k) |\n",
    "| **Large** (~100k+) | Few — common phrases become single tokens | Very large | Risk of overfitting to training distribution; rare tokens get poorly trained embeddings |\n",
    "\n",
    "Larger vocabularies mean each token carries more information, so sequences are shorter and the model sees more context per forward pass. But each token also needs an embedding vector, so the embedding table grows linearly. And tokens that appear rarely in training will have poorly learned embeddings — they've simply not been seen enough times.\n",
    "\n",
    "Most modern models settle in the 32k–100k range. GPT-2 uses ~50k tokens. LLaMA uses 32k. GPT-4 reportedly uses ~100k. The right size depends on the training data, the target languages, and the compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660e14ce-2270-4cf4-9078-36759f24db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## | echo: false\n",
    "## %load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc957dc1-22af-4978-835a-21881cea06cb",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's turn the algorithm into code. The `BPETokenizer` class below has four core methods, each mapping directly to a step we've discussed:\n",
    "\n",
    "- **`train`**: The learning loop — encode the corpus to bytes, then greedily merge the most frequent pair `vocab_size - 256` times. Each merge is recorded in `self.merges` as a `(pair) → index` mapping. This ordered dictionary *is* the tokenizer.\n",
    "- **`encode`**: The encoding step — convert new text to bytes, then apply merges in *learned order* (earliest first, using the `min` trick we discussed). This is where training-order matters: we pick the pair with the smallest merge index, not the most frequent.\n",
    "- **`decode`**: The inverse — look up each token ID in the vocabulary to get its byte sequence, concatenate, and decode back to a string.\n",
    "- **`_get_stats` / `_merge`**: Helpers that count adjacent pairs and replace a specific pair with its merged token throughout a sequence.\n",
    "\n",
    "One implementation detail: `_build_vocab` relies on Python 3.7+ dictionary insertion order. Since merges are inserted chronologically, iterating `self.merges` replays them in order — each merged token is the byte-concatenation of its two parents, which must already exist in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1517b8b6-f809-44a3-a368-2ecf7073c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## | code-fold: true\n",
    "from typing import Iterable\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a672ce35-a11c-4452-92fa-09b54198aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"Byte-pair encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_sz: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_sz (int): Vocabulary size.\n",
    "        \"\"\"\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "\n",
    "    def train(self, text: Iterable[str]):\n",
    "        \"\"\"Train Byte-pair encoder.\"\"\"\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        for idx in range(256, self.vocab_sz):\n",
    "            stats = self._get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            self.merges[pair] = idx\n",
    "            ids = self._merge(ids, pair, idx)\n",
    "        self.vocab = self._build_vocab(ids)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode string to bytes using vocabulary built during training.\"\"\"\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "\n",
    "        ## If text is empty or has one character -> it is already encoded from previous step\n",
    "        while len(ids) >= 2:\n",
    "            ## stats is used only for getting pairs next to each other\n",
    "            stats = self._get_stats(ids)\n",
    "            ## Because we built vocab (and merges) bottom-up, we need to encode\n",
    "            ## idx from smallest index because some later pairs depend on pairs\n",
    "            ## occured before\n",
    "            ## If a pair doesn't exist, it wouldn't participate in the list\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break  ## No more pairs to merge\n",
    "            idx = self.merges[pair]\n",
    "            ids = self._merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, tokens: Iterable[int]):\n",
    "        \"\"\"Decode tokens into string using the vocabulary built during training.\"\"\"\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in tokens)\n",
    "        ## It is important to replace tokens that were not seen during training\n",
    "        ## with `?`; otherwise, it would fail\n",
    "        return tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def _get_stats(self, ids: Iterable[int]):\n",
    "        \"\"\"Get pair counts.\"\"\"\n",
    "        counts = {}\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def _merge(self, ids: Iterable[int], pair: Iterable[int], idx: int):\n",
    "        \"\"\"Merge pairs that match `pair` with new index `idx`.\"\"\"\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and pair[0] == ids[i] and pair[1] == ids[i + 1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def _build_vocab(self, ids: Iterable[int]):\n",
    "        \"\"\"Build vocabulary from 0-255 bytes and merges.\"\"\"\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        ## Here we assume the items returned would be in the same order they were inserted.\n",
    "        ## This is Okay Python 3.7+\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            ## This would be a concatenation of the bytes\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26b9c11-cf65-45ca-a03a-7f2acef56f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(\"https://docs.python.org/3/library/stdtypes.html#bytes.decode\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4368c372-793e-4ca7-be14-c8e62b9c9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a4e7562-ca4c-4e81-907c-70479b2448ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ba82c99-8c22-4c9e-be22-717b6be33ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text)) == text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf45227-5d5c-47ac-ad38-7b161b658b7a",
   "metadata": {},
   "source": [
    "## From Vanilla BPE to GPT-2's Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696bba5-7b57-4a93-bb24-d36e0b24bf3a",
   "metadata": {},
   "source": [
    "The implementation above is vanilla byte-level BPE — it works, but it has a practical problem. Because merges are purely frequency-driven, the algorithm doesn't respect word boundaries. The word \"play\" might appear in the corpus as \"play.\", \"play!\", \"play,\", and \"play \" — and BPE will learn separate tokens for each variant, wasting vocabulary slots on what is essentially the same word with different punctuation.\n",
    "\n",
    "GPT-2 ([Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) introduced a key refinement: **pre-tokenization with a regex pattern** that splits text into chunks *before* BPE runs. The regex prevents merges from crossing certain boundaries — letters can't merge with digits, punctuation stays separate from words, and spaces attach to the *beginning* of words rather than the end.\n",
    "\n",
    "The GPT-2 regex pattern:\n",
    "\n",
    "```\n",
    "'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\n",
    "```\n",
    "\n",
    "This ensures that:\n",
    "\n",
    "- **Contractions** are split cleanly: \"don't\" → `[\"don\", \"'t\"]`\n",
    "- **Spaces attach to the next word**: \" hello\" stays together, preserving word boundaries\n",
    "- **Punctuation stays isolated**: \"play!\" → `[\"play\", \"!\"]` instead of learning \"play!\" as one token\n",
    "- **Digits don't merge with letters**: \"h3llo\" → `[\"h\", \"3\", \"llo\"]`\n",
    "\n",
    "BPE then runs *within* each chunk independently. The result: a much cleaner vocabulary where tokens correspond to linguistically meaningful units rather than artifacts of adjacent punctuation.\n",
    "\n",
    "This pre-tokenization pattern has been refined in later models. GPT-4 uses a [more sophisticated pattern](https://github.com/openai/tiktoken) that handles apostrophes, numbers, and whitespace more carefully, and also limits the length of digit sequences to avoid learning overly specific number tokens. The core idea remains the same: constrain where merges can happen to produce a more useful vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba2a5f-af1c-4407-a144-b8e0a3801d5a",
   "metadata": {},
   "source": [
    "## References & Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b12d6e-c7b2-41bb-adfa-6d3fe086fe65",
   "metadata": {},
   "source": [
    "- **Gage, P.** (1994). [A New Algorithm for Data Compression](https://www.derczynski.com/papers/archive/BPE_Gage.pdf). *The C Users Journal*. The original BPE paper — a compression algorithm that found new life in NLP.\n",
    "- **Sennrich, R. et al.** (2016). [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909). *ACL 2016*. The paper that adapted BPE for NLP tokenization.\n",
    "- **Radford, A. et al.** (2019). [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). The GPT-2 paper that introduced byte-level BPE with regex pre-tokenization.\n",
    "- **Karpathy, A.** (2024). [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE). Excellent video walkthrough of building a BPE tokenizer from scratch.\n",
    "- [A Programmer's Introduction to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/) — why bytes vs. code points matters.\n",
    "- [UTF-8 Everywhere](https://utf8everywhere.org/) — the case for UTF-8 as the universal encoding.\n",
    "- [Tiktokenizer](https://tiktokenizer.vercel.app) — interactive web app to visualize how different tokenizers split text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}