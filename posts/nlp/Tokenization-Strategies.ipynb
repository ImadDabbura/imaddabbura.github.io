{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc5376c-9773-45a6-8ab6-9d5ba2d33c3d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tokenization Strategies\n",
    "date: 2023-02-14\n",
    "image: tokenization.jpg\n",
    "categories: [\"NLP\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3470d82-5a53-4fe7-82df-b8d5f1b7ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32b0c-410a-469a-8d3c-4cfd69e3a77f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- ![](tokenization.jpg) -->\n",
    "<img src=\"tokenization.jpg\" alt=\"Imad\" height=\"400px\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c572f-b5be-40c4-ac8a-c658e6511e65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d09c1-ddb7-4475-9e4a-0ccc546304d7",
   "metadata": {},
   "source": [
    "**Tokenization** is the process of breaking down a string into smaller units of information that will be used by the model. This process sometimes involves some preprocessing steps such as converting to lowercase, [stemming & lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), etc. There are many tokenizations strategies that each has its own advantages and drawbacks. We will first consider the two extreme tokenization strategies: *character tokenization* and *word tokenization*. Then we will discuss subword tokenizations where statistical methods and language heuristics are used to learn the optimal splitting of words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe7ad1-9ece-432d-8ee1-47fca2d8063b",
   "metadata": {},
   "source": [
    "## Character Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9e311-07cf-43be-a874-9a4ddd1230c7",
   "metadata": {},
   "source": [
    "This is the simplest tokenization strategy where we simply break down the text at the character level. Then the characters will be fed to the model. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ba1e07-4dff-4f3a-9b31-0daf88028379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', ' ', 'l', 'o', 'v', 'e', ' ', 'N', 'L', 'P', '!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love NLP!\"\n",
    "list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74335a31-7dd7-4db3-ad47-727a91e84cfa",
   "metadata": {},
   "source": [
    "From here, it is easy to convert each character into integers that would be fed to the model. This step is called *numericalization*. We can numericalize the above text by first building the vocabulary, and then convert each character to its corresponding index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88aa86ee-95da-4173-82e0-20dd5f5c3fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, 'I': 3, 'L': 4, 'N': 5, 'P': 6, 'e': 7, 'l': 8, 'o': 9, 'v': 10}\n"
     ]
    }
   ],
   "source": [
    "vocab = {char: idx for idx, char in enumerate(sorted(list(text)))}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfb348-1c04-4bcc-8ea7-fa0104dd830b",
   "metadata": {},
   "source": [
    "Now we can simply map each token (character in this case) to its own corresponding index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabb3953-a7e5-4bab-9ae6-5f3f3c319e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 8, 9, 10, 7, 1, 5, 4, 6, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[char] for char in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed05d79-7479-4201-8985-486b8d4aea37",
   "metadata": {},
   "source": [
    "- **Advantages**:\n",
    "    - Helps us avoid misspellings and rare words\n",
    "    - Very small vocabulary\n",
    "- **Drawbacks**:\n",
    "    - Sequences length will be very long\n",
    "    - Linguistic structures such as words now need to be learned from data. This requires much more data, memory, and computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e3384-f0b7-4349-b587-4c819d2891d1",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a6da8-1947-4391-920f-7a05fd30d671",
   "metadata": {},
   "source": [
    "The other extreme of word tokenization is to split text into words and then map each word to its corresponding index in the vocabulary. The simplest form would be to split on whitespaces (which work well for English but not other languages such as Japanes that don't have a well-defined idea of a word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6e7b4b-99d2-4ce4-a3d7-3176cdf65250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'NLP!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5daf75d-a025-42d4-a21a-35b3b96391b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'NLP!': 1, 'love': 2}\n"
     ]
    }
   ],
   "source": [
    "vocab = {char: idx for idx, char in enumerate(sorted(text.split()))}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af9c548-5537-4a74-aee4-904ddfd45184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[word] for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a52e4-f454-46db-af7f-aec66b7fb7e0",
   "metadata": {},
   "source": [
    "Most tokenizers would include rules and heauristics that try to separate parts of meaning even when there are no spaces such as \"doesn't\" into \"does n't\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cdb1d6-88b8-42e1-be36-2c71478ff486",
   "metadata": {},
   "source": [
    "- **Advantages**:\n",
    "    - Sequences length will be short\n",
    "- **Drawbacks**:\n",
    "    - Size of the vocabulary will explode for large corpus due to the fact that words can include declinations, misspellings, or punctuations. If the vocabulary size has 1m words and the embedding dimension is 512 -> the first embedding layer would be ~ 0.5 billion parameters!\n",
    "        - We can work around this issue by includtion top n most frequent words. For example, if we include top 100,000 words -> the first embedding layer would be ~ 0.5 million parameters. However, because all other words will be mapped to the `UNK` token, the model has no idea about the words associated with the `UNK` token and we may lose some important information\n",
    "    - Some languages don't have well-defined idea of what constitute a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407a4e2-3ad2-4910-be32-e361ae6bf049",
   "metadata": {},
   "source": [
    "## Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13daa0-f4ff-4fd3-9196-a72cc192c70c",
   "metadata": {},
   "source": [
    "Split words inro smaller parts based on the most frequent sub-strings. Therefore, we want to split keep the most frequent words as unique entities but split the rare words into smaller units to allow us to deal with misspellings and complex words. This will help us achieve the best of both wolds: 1) manageable vocabulary size, 2) keep frequent words as their own entities, and 3) deal with complex and misspelling words.\n",
    "\n",
    "The subword tokenizers are typically learned from pretraining corpus using statistical rules and algorithms. Let's illustrate an example from the tokenizer used by the [DistilBERT](https://arxiv.org/abs/1910.01108) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b30f6a-2857-4850-9b6d-7ccfbd2ec75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 17953, 2361, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "encoded_text = tokenizer(text)\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0dbfc78-fdec-485e-84ac-795cb9ea3be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'love', 'nl', '##p', '!', '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b7c2d-78f4-4e67-a6a7-cc4150027b3b",
   "metadata": {},
   "source": [
    "Let's explain the output of the DistilBERT tokenizer:\n",
    "\n",
    "- [CLS] is a special token that is used to indixate the start of a sequence\n",
    "- [SEP] is also a special token to separate multiple sequences\n",
    "- `##` prefix indicates that the previous string isn't white space\n",
    "    - This shows that nlp is not common token, so it was split into two tokens\n",
    "- We can also see that `!` has its own token\n",
    "\n",
    "We can reconstruct that encoded text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "211d4098-2821-4fd3-9a0c-c404f4ac3ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i love nlp ! [SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec08c2-b7d5-421c-a3a9-0ed7b6492632",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3c94d-bc0c-4c64-9a17-c651b8e1d41a",
   "metadata": {},
   "source": [
    "Through this post, we covered three tokenization strategies along with their advantages and challenges/limitations. We mostly use tokenizers from well-known libraries such as [spaCy](https://spacy.io/) because it is very hard to get it right ourselves.\n",
    "\n",
    "When using pretrained models such as DistilBERT, we must use the same tokenizer that the model used during training. Otherwise, what the model assumes token_id = 1 is will be completely different that what the new token_id = 1 represents. It has the same effect as shuffling the vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
