{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e56cb6-ab82-4e90-b43e-b61baa27d834",
   "metadata": {},
   "source": [
    "---\n",
    "title: Coding GPT2/3 (124M) From Scratch\n",
    "date: 2024-04-10\n",
    "image: gpt2.jpeg\n",
    "categories: [NLP, Deep Learning]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da25df6",
   "metadata": {},
   "source": [
    "<img src=\"gpt2.jpeg\" height=\"600px\" width=800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2505dd-227b-46b9-be34-d42c2712ecc2",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa2c7f-a2dc-4f0e-a5ed-b2f25b173537",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e069ba2-e1ee-4798-b2f2-035e5e63583b",
   "metadata": {},
   "source": [
    "- In the first iteration of the training, we want all tokens to have almost\n",
    "  same probability and thus the loss on each one would be the same. The\n",
    "  probability of each token would be $1/vocab\\_sz$ -> $loss \\approx -log(1/vocab\\_sz)$\n",
    "  because the probability distribution would be diffused.\n",
    "- 3e-4 learning rate is good for `AdamW` optimizer for debugging\n",
    "- We want the model to overfit 1 batch to make sure it is running correctly\n",
    "- Weight sharing between token embedding and the final linear layer (also\n",
    "  called classifier or LM head) because we want the tokens that are semantically\n",
    "  similar to have similar probability when predicting next token.\n",
    "  - This also has huge advantage on computational efficiency as those matrices\n",
    "    have a lot parameters. For GPT2, each one has $50257 * 768 \\approx 38.5M$ which\n",
    "    is $1/3$ of the GPT2 model.\n",
    "  - As a result of Weight sharing, gradient update will be addition from the\n",
    "    two branches: classifier and token embedding\n",
    "- For tokens that don't appear in the training data, we want their\n",
    "  probabilities to be very close to zero\n",
    "- CPU can continue running even if Cuda kernels are not done. This is because\n",
    "  CPU is kinda scheduling the kernels on the GPU and doesn't wait for them to\n",
    "  finish -> Use `torch.cuda.synchronize()` so CPU only presumes when scheduled\n",
    "  kernels finish execution to get better timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfaa3eb-dca9-467e-a480-b0f3553d43a8",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660e14ce-2270-4cf4-9078-36759f24db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial, wraps\n",
    "from typing import Callable\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from torch.distributed import destroy_process_group, init_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6e093-b414-4fb9-a165-04ae9e312c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "def annealer(func: Callable):\n",
    "    wraps(func)\n",
    "\n",
    "    def annealer_wrapper(*args, **kwargs):\n",
    "        return partial(func, *args, **kwargs)\n",
    "\n",
    "    return annealer_wrapper\n",
    "\n",
    "\n",
    "@annealer\n",
    "def lin_sched(start, end, pos):\n",
    "    \"\"\"Linear scheduler.\"\"\"\n",
    "    return start + (end - start) * pos\n",
    "\n",
    "\n",
    "@annealer\n",
    "def cos_sched(start, end, pos):\n",
    "    \"\"\"Cosine scheduler.\"\"\"\n",
    "    return start + (1 + math.cos(math.pi * (1 - pos))) * (end - start) / 2\n",
    "\n",
    "\n",
    "def combine_scheds(pcts, scheds):\n",
    "    \"\"\"\n",
    "    Combine multiple schedulers, each run for a given percentage of the\n",
    "    training process.\n",
    "    \"\"\"\n",
    "    assert len(pcts) == len(scheds), \"Each scheduler should have its `pct`.\"\n",
    "    assert sum(pcts) == 1.0, \"Sum of the `pcts` should be equal to 1.\"\n",
    "    pcts = torch.tensor([0] + listify(pcts))\n",
    "    assert (pcts >= 0).all(), \"All percentages should be non-negative.\"\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "\n",
    "    def _inner(pos):\n",
    "        idx = (pos >= pcts).nonzero().max()\n",
    "        actual_pos = (pos - pcts[idx]) / (pcts[idx + 1] - pcts[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f98324-58a9-49f1-9ad1-789c1a6b20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_sz: int = 1024\n",
    "    vocab_sz: int = (\n",
    "        50257  # 50000 BPE merges + 256 byte tokens + 1 for <|endoftext|> token\n",
    "        # which will delimits different documents. This token's index is 50256\n",
    "    )\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "    lr: int = 3e-4\n",
    "    batch_sz: int = 4\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        # Point-wise feed-forward network that applies non-linearity\n",
    "        # on every token sepearately. THERE IS NO INTERACTION BETWEEN TOKENS\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.c_proj(self.gelu(self.c_fc(x)))\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # NOTE: Bias is not needed when we use Pytorch's Flash attention\n",
    "        # self.register_buffer(\n",
    "        #     \"bias\",\n",
    "        #     torch.tril(torch.ones(config.block_sz, config.block_sz)).view(\n",
    "        #         1, 1, config.block_sz, config.block_sz\n",
    "        #     ),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        # q/k/v is B x T x n_embd each\n",
    "        q, k, v = torch.split(qkv, self.n_embd, dim=-1)\n",
    "        # Reshape q/k/v to B x n_head x T x (n_embd / n_head)\n",
    "        # So each head would be learning different kind of\n",
    "        # relationships\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        # attn is B x T x T\n",
    "        # attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.shape[-1]))\n",
    "        # # Mask out future tokens\n",
    "        # attn = attn.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        # attn = F.softmax(attn, dim=-1)\n",
    "        # # y is B x T x n_embd\n",
    "        # y = attn @ v\n",
    "        # Uses Flash attention that never materialize attention matrices for\n",
    "        # each head and is aware of the memory hierarchy and tries to reduce\n",
    "        # read/writes with more FLOPs -> Speed up since we're memory bound\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use Pre-layer normalization which deviates from the\n",
    "        # transformer original paper that uses post-layer normalization.\n",
    "        # This should help stabilize training\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_sz, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_sz, config.n_embd),\n",
    "                h=nn.ModuleList(\n",
    "                    [Block(config) for _ in range(config.n_layer)]\n",
    "                ),\n",
    "                # Final layer norm after all transformer layers\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_sz, bias=False)\n",
    "\n",
    "        # Weigth sharing between the token embedding layer and\n",
    "        # last linear layer (LM head classifier). The rationale is\n",
    "        # that tokens that are semantically similar to each other in\n",
    "        # the embedding space should have similar probabilities in the\n",
    "        # softmax of the LM head layer\n",
    "        # Also, these matrices are one of the biggest matrices in the the model\n",
    "        # This means, for model like GPT2, we save almost 30 % of the parameters\n",
    "        # by sharing the weight matrices (50257 * 768) / 124M = ~31%\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # The following initialization comes from gpt2 src code\n",
    "        # NOTE: Becuase token embedding and classifier weights are shared,\n",
    "        # out initialization logic will initialize the weight matrix twice\n",
    "        # but shouldn't be an issue since they're being initialized with the\n",
    "        # same std and mean\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            # We're changing std because residual path affect std\n",
    "            # by increasing it on every layer so we need to adjust\n",
    "            # it so we still have the same std = 0.02\n",
    "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "                # `2` here becauase every layer has two blocks:\n",
    "                # Attention block and MLP block\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            nn.init.normal_(module.weight, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # We're initializing the token and positional embeddings\n",
    "            # with the same std but the paper initialized the positional\n",
    "            # embedding with std = 0.01\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        T = x.shape[-1]\n",
    "        assert (\n",
    "            T <= self.config.block_sz\n",
    "        ), f\"Sequence length must be <= {self.config.block_sz}, got {T}\"\n",
    "        pos_emb = self.transformer.wpe(\n",
    "            torch.arange(0, T, dtype=torch.long, device=x.device)\n",
    "        )\n",
    "        tok_emb = self.transformer.wte(x)\n",
    "        x = pos_emb + tok_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        # logits is B x T x vocab_sz\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # F.cross_entropy expects the 2nd dimension to be probabilities\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config.vocab_sz), targets.view(-1)\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizer(self, weight_decay, lr, device):\n",
    "        params_dict = {\n",
    "            pn: p for pn, p in self.named_parameters() if p.requires_grad\n",
    "        }\n",
    "        decay_params = [p for p in params_dict.values() if p.ndim >= 2]\n",
    "        nondecay_params = [p for p in params_dict.values() if p.ndim < 2]\n",
    "        params_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nondecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        fused_available = \"fused\" in inspect.signature(opt.AdamW).parameters\n",
    "        use_fused = fused_available and \"cuda\" in device\n",
    "        return opt.AdamW(\n",
    "            params_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
    "        )\n",
    "\n",
    "    @torch.no_grad\n",
    "    def generate(self, idxs: torch.tensor, max_tokens: int = 5):\n",
    "        for i in range(max_tokens):\n",
    "            # x would be B x T x vocab_sz\n",
    "            idxs = idxs[:, -self.config.block_sz :]\n",
    "            logits, _ = self(idxs)\n",
    "            # Get probs for last token to predict next token\n",
    "            # This would be B x vocab_sz\n",
    "            logits = logits[:, -1, :]\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Each would be B x 50\n",
    "            topk_probs, topk_idxs = torch.topk(probs, 50, dim=-1)\n",
    "            # idx is B x 1\n",
    "            idx = torch.multinomial(topk_probs, 1)\n",
    "            idx = torch.gather(topk_idxs, -1, idx)\n",
    "            idxs = torch.cat([idxs, idx], dim=1)\n",
    "        return idxs\n",
    "\n",
    "\n",
    "class DataLoaderLight:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_sz: int,\n",
    "        block_sz: int,\n",
    "        process_rank: int = 0,\n",
    "        number_processes: int = 1,\n",
    "    ) -> None:\n",
    "        self.batch_sz = batch_sz\n",
    "        self.block_sz = block_sz\n",
    "        self.process_rank = process_rank\n",
    "        self.number_processes = number_processes\n",
    "        with open(\"input.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.tokens = torch.tensor(encoder.encode(text), dtype=torch.long)\n",
    "        self.current_pos = batch_sz * block_sz * process_rank\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // (self.batch_sz * self.block_sz)\n",
    "\n",
    "    def next_batch(self):\n",
    "        buf = self.tokens[\n",
    "            self.current_pos : self.current_pos\n",
    "            + self.batch_sz * self.block_sz\n",
    "            + 1\n",
    "        ]\n",
    "        x = buf[:-1].view(self.batch_sz, self.block_sz)\n",
    "        y = buf[1:].view(self.batch_sz, self.block_sz)\n",
    "        # Each process will process batch_sz x block_sz tokens in each\n",
    "        # iteration -> with number_processes processes, total tokens processed\n",
    "        # in each iteration is batch_sz x block_sz x number_processes. In the\n",
    "        # case of one process, total tokens would be batch_sz x block_sz\n",
    "        self.current_pos += (\n",
    "            self.batch_sz * self.block_sz * self.number_processes\n",
    "        )\n",
    "        if self.current_pos + (\n",
    "            self.batch_sz * self.block_sz * self.number_processes\n",
    "        ) + self.number_processes > len(self):\n",
    "            self.current_pos = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ###########\n",
    "    # Distributed Data Parallel\n",
    "    ###########\n",
    "    # torchrun command sets the following environment variables:\n",
    "    # RANK: Id of the process in the process group. It is an int 0-WORLD_SIZE\n",
    "    # LOCAL_RANK: In the case of multi-nodes, LOCAL_RANK is the id of\n",
    "    #             the process in the same node\n",
    "    # WORLD_SIZE: Total number of processes\n",
    "    ddp = int(os.getenv(\"RANK\", -1)) != -1  # Check if it is a ddp run\n",
    "    if ddp:\n",
    "        # DDP requires CUDA so we need to set the device for each process\n",
    "        # so only one process can run per device\n",
    "        assert torch.cuda.is_available(), \"DDP requires CUDA\"\n",
    "        init_process_group(backend=\"nccl\")\n",
    "        ddp_rank = int(os.getenv(\"RANK\"))\n",
    "        ddp_local_rank = int(os.getenv(\"LOCAL_RANK\"))\n",
    "        ddp_world_size = int(os.getenv(\"WORLD_SIZE\"))\n",
    "        device = f\"cuda:{ddp_local_rank}\"\n",
    "        torch.cuda.set_device(device)\n",
    "        # master process will do more things such as checkpointing and logging\n",
    "        # while other processes would assist in the computations\n",
    "        master_process = ddp_rank == 0\n",
    "    else:\n",
    "        ddp_rank = 0\n",
    "        ddp_local_rank = 0\n",
    "        ddp_world_size = 1\n",
    "        master_process = True\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_built():\n",
    "            device = \"mps\"\n",
    "            torch.mps.manual_seed(1337)\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "    print(device)\n",
    "    torch.manual_seed(1337)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "    ##########\n",
    "    # Initialize model and optimizer\n",
    "    ##########\n",
    "    # Everything in GPUs is a power of 2 such as tiling ops\n",
    "    # So try to always have matrices be power of 2. Here we\n",
    "    # change the vocab_sz by rounding it up to the closest\n",
    "    # number that is power of. This will increase space overhead\n",
    "    # but would speed up computations\n",
    "    model = GPT2(GPTConfig(vocab_sz=50304)).to(device)\n",
    "    # Speed up model by building statis graph that analyzes all ops\n",
    "    # and optimizes them such as fusing some of them to avoid unnecessary\n",
    "    # trips to memory\n",
    "    # model = torch.compile(model)\n",
    "    if ddp:\n",
    "        model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    raw_model = model.module if ddp else model\n",
    "    max_lr = 3e-4\n",
    "    min_lr = max_lr * 0.1\n",
    "    warmup_steps = 10\n",
    "    max_steps = 50\n",
    "    sched = combine_scheds(\n",
    "        [warmup_steps / max_steps, 1 - (warmup_steps / max_steps)],\n",
    "        [lin_sched(min_lr, max_lr), cos_sched(max_lr, min_lr)],\n",
    "    )\n",
    "    # TODO: Move building of optimizer inside GPT2\n",
    "    # Don't decay biases and 1D tensors such as layer norm tensors (scales and\n",
    "    # biases) linear layer's tensors\n",
    "    # optimizer = opt.AdamW(\n",
    "    #     model.parameters(), lr=GPTConfig.lr, betas=(0.9, 0.95), eps=1e-8\n",
    "    # )\n",
    "    optimizer = raw_model.configure_optimizer(\n",
    "        weight_decay=0.1, lr=max_lr, device=device\n",
    "    )\n",
    "\n",
    "    ##########\n",
    "    # Run training loop\n",
    "    #########\n",
    "    # NOTE: In order to run 0.5M tokens per fwd/bwd iteration, we need to\n",
    "    # use gradient accumulation because we can't fit it in almost any commodity\n",
    "    # GPY -> We only do backward after we loop through ~0.5M tokens.\n",
    "    total_batch_sz = 2**19  # closest number to 0.5M\n",
    "    assert (\n",
    "        total_batch_sz\n",
    "        % (GPTConfig.batch_sz * GPTConfig.block_sz * ddp_world_size)\n",
    "        == 0,\n",
    "        \"total batch size must be divisible by micro batch_sz x block_sz x ddp_world_size\",\n",
    "    )\n",
    "    grad_accum_steps = total_batch_sz // (\n",
    "        GPTConfig.batch_sz * GPTConfig.block_sz * ddp_world_size\n",
    "    )\n",
    "    if master_process:\n",
    "        print(f\"Total desired batch size: {total_batch_sz}\")\n",
    "        print(f\"Calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "    train_dl = DataLoaderLight(\n",
    "        batch_sz=GPTConfig.batch_sz, block_sz=GPTConfig.block_sz\n",
    "    )\n",
    "    # Pytorch will use TensorFloat32 if available, else use FP32\n",
    "    # But the weights will still be stored as FP32. It is just the\n",
    "    # operations would be executed as TF32 if available\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        start = time.time()\n",
    "        x, y = train_dl.next_batch()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # code.interact(local=locals())\n",
    "        optimizer.zero_grad()\n",
    "        loss_accum = 0.0\n",
    "        for macro_step in range(grad_accum_steps):\n",
    "            if device == \"cuda\":\n",
    "                # Tensors that will be greatly affected by less precission such\n",
    "                # loss, layernorm would still be in FP32\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "            else:\n",
    "                logits, loss = model(x, y)\n",
    "            # Just accumulation gradients yield to summation of objective but\n",
    "            # we want mean so we weight each loss by 1/grad_accum_steps\n",
    "            loss /= grad_accum_steps\n",
    "            loss_accum += loss.detach()\n",
    "            # To avoid syncing the gradients between the processes after every\n",
    "            # macro step, we disable it and only allows the sync up of\n",
    "            # gradients after we finish all gradient accumulation in each\n",
    "            # process\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (\n",
    "                    macro_step == grad_accum_steps - 1\n",
    "                )\n",
    "            loss.backward()\n",
    "        # Each process would have its own loss_accum tensor, so to get the\n",
    "        # average loss_accum across all processes, we to compute the average of\n",
    "        # all loss_accum in all processes\n",
    "        if ddp:\n",
    "            dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "        # Clips gradient to global norm. It is very useful to avoid having a\n",
    "        # very high loss for some batch(es) that would have very high loss\n",
    "        # which would learn to high gradients and huge updates\n",
    "        # In the beginning of training it is normal to have high norms as the\n",
    "        # model initialized randomly\n",
    "        norm = nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # TODO: Use ParamScheduler from `cmn_ai`\n",
    "        lr = sched(step / max_steps)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "        optimizer.step()\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        token_per_sec = (\n",
    "            GPTConfig.batch_sz\n",
    "            * GPTConfig.block_sz\n",
    "            * grad_accum_steps\n",
    "            * ddp_world_size\n",
    "        ) / (elapsed_time)\n",
    "        print(\n",
    "            f\"step {step}, loss: {loss.item()}, lr {lr:.4e}, norm: {norm:.2f}, time: {elapsed_time:.2f}s, tok/sec: {token_per_sec:.2f}\"\n",
    "        )\n",
    "\n",
    "    if ddp:\n",
    "        # Kills all processes\n",
    "        destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3c71c-38ff-4cb1-aef2-d66ab1311cea",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41228847-7fdc-400c-ad84-a4efa076c1b1",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86abf3e9-a468-495f-9d33-df2063bc340e",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bed6b-f58c-48aa-a1ad-93f12c9bf0fb",
   "metadata": {},
   "source": [
    "- [GPT2: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [GPT3: Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
