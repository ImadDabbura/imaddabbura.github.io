{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a7fbef-378e-45f4-a688-1fa6824eb5a3",
   "metadata": {},
   "source": [
    "- Check the dimension returned by each attention head and multihead attention\n",
    "    - It should take embed_dim not hidden_sz as second argument for Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f972acd-a9f7-4a1f-8dea-1dfda848d51d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformer Architecture Explained\n",
    "date: 2023-02-14\n",
    "image: transformer-arch.png\n",
    "categories: [NLP, Deep Learning]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba6fc1-638c-4aac-9e53-d5f93d33d78b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"full-transformer-architecture.png\" width=\"600px\"><br>\n",
    "<caption><center><u><b>Figure 1:</b></u> The architecture of the vanilla Transformer model ([source](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/))</center></caption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b26629-0fa2-476f-8f2d-ab5490b96270",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9f471-e14e-4fb0-ae58-347e9d30703e",
   "metadata": {},
   "source": [
    "Transformer architecture was first introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper in 2017. It outperformed RNN-based models on all NLP related tasks. It has an encoder-decoder architecture that is used in tasks such as *Neural Machine Translation*. The most common examples of models that use transformer architecture is **BERT**, which uses encoder-only architecture, and **GPT**, that uses decoder-only architecture.\n",
    "\n",
    "The main motivation behind creating the Transformer architecture is to overcome issues that RNN-based models have:\n",
    "\n",
    "1. Hard to learn long distance dependencies due to gradient problems (vanishing/exploding). For example, if the last word of the sequence depends on the early words in the sequence, the hidden state by the time it reaches the last word wouldn't have much of the information of the early words especially as the length of the sequence gets longer. Such models assume linear order of words, which is not the right way to think about it.\n",
    "2. These are sequential models, which means we can only start processing $w_t$ once we finish $w_{t - 1}$ because it is dependent on the previous hidden state that computed from $w_{t - 1}$ and $h_{t - 1}$. Therefore, it is not parallelizable.\n",
    "\n",
    "In this post, we will implement and explain the main building blocks of the transformer architecture (see figure 1). By the end of this post, we should be able to:\n",
    "\n",
    "1. Implement vanilla transformer from scratch, including full encoder-decoder architecture, encoder-only architecture, and decoder-only architecture.\n",
    "2. Understand the role of each block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779785fb-451a-4365-a9de-4bbf2cebfe77",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f69795-143f-4182-83a4-b50f4fdcc3c9",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44074864-bc19-41c7-a4f0-8db939d23900",
   "metadata": {},
   "source": [
    "After the input sequence is tokenized and numericalized, we need to project each token into lower dimension space. Such projection is called **embedding** and it captures the semantic representation of tokens based on the context the token mostly occurs in.\n",
    "\n",
    "Attention operation is a permutation equivariant, this means that if we permute the input then the corresponding output will be permuted in exactly the same way. In other words, attention mechanism is not aware of the relative ordering of the tokens. Therefore, we need some way to encode the positions of the tokens in each sequence. This is where **positional encoding** comes into play. There are two types of encodings:\n",
    "\n",
    "- *Absolute Positional Encoding*: Use token absolute position. Can use either static patterns such as **sign** function, or learned parameters\n",
    "- *Relative Positional Encoding*: Encode the relative position of tokens. We need to adjust the attention mechanism itself by adding new terms to be used when dot-products are used to encode the relative position between tokens up to maximum relative position.\n",
    "- *Rotary Encoding*: Combine both absolute and relative position of tokens to achieve great results. This can be done by encoding the absolute positions with a rotation matrix that will be multiplied with key and value matrices of each attenetion layer to add the relative position information at every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a092f68-2275-4456-acfa-39f02a2ffe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a6d2afd-e157-421f-bfe6-d4dc0f73af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "config = {\n",
    "    \"vocab_sz\": 1000,\n",
    "    \"block_sz\": 8,\n",
    "    \"intermediare_sz\": 4 * 64,  # 4x hidden_dim\n",
    "    \"hidden_dropout_prob\": 0.2,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"hidden_sz\": 64,           # embed_dim / num_attention_head = 768 / 12 = 64\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"embed_dim\": 768,\n",
    "    \"num_classes\": 2,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51562c60-9883-4e62-b7c0-6dd0091ef203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_sz, config.embed_dim)\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            config.block_sz, config.embed_dim\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            config.embed_dim, eps=config.layer_norm_eps\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X:                   B x T\n",
    "        # token_embeddings:    B x T x embed_dim\n",
    "        # position_embeddings: T x embed_dim\n",
    "        embeddings = self.token_embedding(x) + self.position_embedding(\n",
    "            torch.arange(x.shape[1])\n",
    "        )\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        return self.dropout(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce85768-15af-4cab-93c8-78f24490499c",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29291a1-9e3b-49fe-af3f-56c3e4b9f7c1",
   "metadata": {},
   "source": [
    "**Attention** is a communication mechanism that is used by NN model to learn to make predictions by attending to some tokens in the context window (only current and previous tokens for decoder-only architectute). The attention weights, which are learned, are used to construct the weighted average of all the tokens attended to by each token. This will help each token focus on what is important in the context. As a reminder, with attention, there is no notion of space. This means it operates on a set of vectors. This is why we need positional encoding for tokens.\n",
    "\n",
    "The results of the attention layer would be contextualized embeddings, since the output of the embedding layer is contextless embeddings. This is very useful because we know that the meaning of a word changes according to the context, and embeddings from the embedding layer for a token is the same regardless of its context. For example, the word \"bear\" has the same embedding vector whether it comes in \"teddy bear\" or \"to bear\".\n",
    "\n",
    "**Self-attention** is a type of attention mechanism where the keys and values come from the same source as the queries, which is the input $x$. Whereas in **cross-attention**, the queries still get produced from the input $x$, but the keys and values come from some other external source (encoder module in the case of encoder-decoder architecture).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"scaled-dot-product-attention.png\" height=\"400px\"><br>\n",
    "<caption><center><u><b>Figure 2:</b></u> Scaled Dot-Product Attention ([source](https://ar5iv.labs.arxiv.org/html/1706.03762))</center></caption>\n",
    "</p>\n",
    "\n",
    "\n",
    "For self-attention, we have:\n",
    "\n",
    "- Query matrix  $Q$ (hidden_sz x head_dim): what each token is looking for\n",
    "- Key matrix $K$ (hidden_sz x head_dim): what each token contains\n",
    "- Value matrix $V$ (hidden_sz x head_dim): what each token communicate with\n",
    "\n",
    "Then,\n",
    "\n",
    "- The dot-product of query with all the keys of the tokens give us the affinities. Dot-product is just used as a form of computing similarities. Other form of attention include additive attention.\n",
    "    - If query and key vectors are aligned -> very high value -> get to know more about that token as opposed to other tokens\n",
    "    - All the tokens in all positions in B x T matrix produce query/key/value vectors in parallel and independently from each other and no communication is happening\n",
    "    - Then all queries will be dot-product with all the keys\n",
    "    - We scale attention by dividing it with $sqrt(head\\_sz)$. This makes it so when input Q,K are unit variance, weights will be unit variance too and *softmax* will stay diffuse and not saturate too much\n",
    "- Finally, we multiply the attention weights with the value matrix $V$ to get the contextualized embeddings\n",
    "\n",
    "In equations:\n",
    "$$attn(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt d_k})V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0540e571-f6de-49d2-9535-e91755a7a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config, head_dim, is_decoder=False) -> None:\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(config.embed_dim, head_dim, bias=False)\n",
    "        self.q = nn.Linear(config.embed_dim, head_dim, bias=False)\n",
    "        self.v = nn.Linear(config.embed_dim, head_dim, bias=False)\n",
    "        self.is_decoder = is_decoder\n",
    "        if self.is_decoder:\n",
    "            self.register_buffer(\n",
    "                \"mask\", torch.tril(torch.ones(config.block_sz, config.block_sz))\n",
    "        )\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # query, key, value are each B x T x embed_dim\n",
    "        q = self.q(query)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "        # w is B x T x T\n",
    "        w = q @ k.transpose(2, 1) / (k.shape[-1] ** 0.5)\n",
    "        if self.is_decoder:\n",
    "            w = w.masked_fill(self.mask == 0, -float(\"inf\"))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        # output is B x T x head_dim\n",
    "        return w @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582aec62-5e06-4d46-a036-860ce73af009",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f9596-56e8-4710-bbf2-4b0f57763b0f",
   "metadata": {},
   "source": [
    "What we described in the previous section was self-attention mechanism with one-head. Since each attention head focuses on one specific characteristic of the data in terms of similarity such as subject-verb interaction, other heads are needed to focus on other aspects such as adjectives. We can also think of having multiple heads as if each head focuses on one or few other tokens. Remember that all of this is done in parallel and there is no communication between heads. This means that each head has no idea what other heads are doing.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"multi-head-attention.png\" height=\"400px\"><br>\n",
    "<caption><center><u><b>Figure 3:</b></u> Mutli-Head Attention with several attention layers running in parallel ([source](https://ar5iv.labs.arxiv.org/html/1706.03762))</center></caption>\n",
    "</p>\n",
    "\n",
    "In multi-head layer, we typically have the `head_sz` be the result of dividing the `hidden_sz` (or the `embeddind_dim` if it is the first layer) by the number of heads.\n",
    "\n",
    "Once we get all contextualized embeddings from all heads, we concatenate them. Then we pass the output through a projection layer with the same dimension as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf91afcc-af53-4bd9-84c6-bd3dd69bd49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config, is_decoder=False) -> None:\n",
    "        super().__init__()\n",
    "        head_dim = config.embed_dim // config.num_attention_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(head_dim, config, is_decoder)\n",
    "                for _ in range(config.num_attention_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.output = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fbacb-64e3-48dd-b65e-7de96b486d89",
   "metadata": {},
   "source": [
    "## Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516daf9-e662-4411-9ae5-b33aad731eee",
   "metadata": {},
   "source": [
    "Because there are no elementwise nonlinearities involved in the calculation of the attention, stacking multiple layers of attention wouldn't help much because the output would still be linear transformation of the input. As a result, feed-forward NN is added to add such nonlinearities to post-process each output vector from the attention layer. Therefore, each embedding vector is processed independently in the batched sequence, which leads to the position-wise feed-forward layer.\n",
    "\n",
    "We typically first project the output vector into new space 4x the hidden_sz. Therefore, most of the capacity and memorization is expected to happen in the first layer, which is what gets scaled when the model is scaled up. Then we project it back to the original dimension. We use [`GELU`](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html?highlight=gelu#torch.nn.GELU) as the activation function, which is a Gaussian Error Linear Units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7ceda3b-1880-495c-8a32-04005afe4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # intermediate_sz is typically 4 x embed_dim\n",
    "        self.l1 = nn.Linear(config.embed_dim, config.intermediate_sz)\n",
    "        self.l2 = nn.Linear(config.intermediate_sz, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.l2(F.gelu(self.l1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fa91b-efa0-4970-8d32-92688e60acce",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b73ef-5b17-4b92-bee3-26aac96bb393",
   "metadata": {},
   "source": [
    "**Layer normalization** was introduced in [this paper](https://arxiv.org/abs/1607.06450) to overcome the main challenges of **Batch normalization**, which are 1) how do we handles batches with 1 or few examples because we would have infinite variance or unstable training and 2) how do we handle RNNs. The main differences with batch normalization are 1) we don't have moving averages/standard deviations and 2) we average over the hidden dimesnion(s), so it is indepenedent of the batch size. It has two learnable parameters (scalars): $\\beta$ and $\\gamma$ (see the equation below):\n",
    "\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "It is used as a trick to train complex models, such as Transformer, faster. In our case, we would normalize the hidden vectors to zero mean and unit standard deviation. This trick helps maintain consistent distribution of signals by cutting down uninformative variations in hidden vector values.\n",
    "\n",
    "There are two arrangements for the layer normalization as illustrated in Figure-4:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"layer-norm-arrangement.png\" width=\"600px\"><br>\n",
    "<caption><center><u><b>Figure 4:</b></u> Different LayerNorm arrangement ([source](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/))</center></caption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85573a-4dfa-47e5-b93b-94801c027b5c",
   "metadata": {},
   "source": [
    "- *Prelayer normalization*: Places the layer normalization within the span of skip connections. This arrangement is easier to train.\n",
    "- *Postlayer normalization*: Places the layer normalization in between skip connections. This arrangement is used in the Transformer paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d24d9-b049-4107-9770-750250ad9c7b",
   "metadata": {},
   "source": [
    "## Skip Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07932db-6543-4d6f-bb79-890dee394756",
   "metadata": {},
   "source": [
    "**Skip connections** help train deeper and more complex models faster as well as avoid the issue of vanishing gradients that deeper networks face. It provides paths for the gradient to flow through back to the input. In our case, we are using skip connections with addition, which means we take a copy of the inputs and added it to the output of a block (involves some computations). If we assume $y = x + F(x)$, then it is as if we are asking the block to predict $y - x$. In other words, it means to backpropagate through the identity function, which leads to multiply the gradient of $y$ by one and retain its value in the earlier layers.\n",
    "\n",
    "Skip connections help also smooth out the loss landscape (see Figure-5), and make it easier for the gradients to flow back as *addition* operator split the gradients equally. This means that small changes in the input can still find their way to the output. Additionally, it preserves the original input sequence, which means there is no way for the current word to forget to attend to its position because we always add it back.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"loss-landscape-with-skip-connections.png\" width=\"600px\"><br>\n",
    "<caption><center><u><b>Figure 5:</b></u> The loss surfaces of ResNet-56 with/without skip connections ([source](https://arxiv.labs.arxiv.org/html/1712.09913))</center></caption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1146541-296a-4cbb-9770-4a297ca91ebd",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b85205-ee73-4c42-bb1c-74ca9ce4b10e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"dropout.png\" width=\"600px\"><br>\n",
    "<caption><center><u><b>Figure 6:</b></u> \n",
    "Dropout Neural Net Model. **Left**: A standard neural net with 2 hidden layers. **Right**: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped.\n",
    " ([source](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf))</center></caption>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3fe2c-9c4d-4b15-a9ff-9bdd48cfeb59",
   "metadata": {},
   "source": [
    "**Dropout** is a regularization technique that was introduced by Geoffrey Hinton et al. in this [paper](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf). On each iteration, we randomly shut down some outputs from the previous layer and don't use those outputs in both forward propagation and back-propagation. Since the outputs that will be dropped out on each iteration will be random, the learning algorithm will have no idea which neurons will be shut down on every iteration; therefore, force the learning algorithm to spread out the weights and not focus on some specific feattures. \n",
    "Moreover, dropout help improving generalization error by:\n",
    "\n",
    "- Since we drop some units on each iteration, this will lead to smaller network which in turns means simpler network (regularization).\n",
    "- Can be seen as an approximation to bagging techniques. Each iteration can be viewed as different model since we're dropping randomly different units on each layer. This means that the error would be the average of errors from all different models (iterations). Therefore, averaging errors from different models especially if those errors are uncorrelated would reduce the overall errors. In the worst case where errors are perfectly correlated, averaging among all models won't help at all; however, we know that in practice errors have some degree of uncorrelation. As result, it will always improve generalization error.\n",
    "\n",
    "Dropout is used in the Transformer in embeddings layer after adding the token and positional embeddings as well as after each multi-head/feed-forward layers in both the encoder and decoder layers.\n",
    "\n",
    "For more information on dropout, check out my [previous post](https://imaddabbura.github.io/posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a72f3-bfcb-4568-8a82-354c80bdfff5",
   "metadata": {},
   "source": [
    "# Transformer Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027159a1-bdfe-434a-b210-9c3bd46a65e2",
   "metadata": {},
   "source": [
    "## Encoder-only Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec7ce1-4871-4ecb-ade7-fcc99f5b6b48",
   "metadata": {},
   "source": [
    "Encoder-only architecture are well suited for classification tasks. The most common model that uses encoder-only branch of the Transformer architecture is [BERT](https://arxiv.org/abs/1810.04805) and all its variants such as [RoBERTa](https://arxiv.org/abs/1907.11692). In this architecture, we would have:\n",
    "\n",
    "- body: Stacked encoder layers. The output would be `B x T x hidden_sz`.\n",
    "- head: A classification head which consists of linear layer that project the hidden_sz into num_classes. We take the hidden vector of the first token, which is the special token `[CLS]` in the case of *BERT* (indicates the beginning of sequence), and pass it through the linear layer to get the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b25d08fb-7099-40a1-9175-f53442e43b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ff = FeedForwardNN(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # There are two arrangements for layer_norm:\n",
    "        # Prelayer normalization & Postlayer normalization\n",
    "        # we are using postlayer normalization arrangement\n",
    "        x = self.layer_norm_1(x + self.attn(x))\n",
    "        x = self.layer_norm_2(x + self.ff(x))\n",
    "        # Prelayer normalization\n",
    "        # x = self.layer_norm_1(x)\n",
    "        # x = x + self.attn(x)\n",
    "        # x = x + self.ff(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af565905-5b13-4773-b44f-d0a2e78a3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder_blocks = nn.Sequential(\n",
    "            *[EncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        return self.encoder_blocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df92c42f-8650-4ef6-9865-686d183d61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.embed_dim, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We take the hidden state of the [CLS] token as\n",
    "        # input to the classifier\n",
    "        x = self.encoder(x)[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424974d-fa5e-4cd7-91ed-9dbef979ef41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decoder-only Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6753a-00fb-43d3-8f7b-4f311fc56272",
   "metadata": {},
   "source": [
    "These models are typically used as language models such as [GPT](https://openai.com/blog/language-unsupervised/) and all its variants. In this architecture, as opposed to the encoder-only architecture, the token can only see past tokens but **not future** tokens because this would be a kind of cheating since we are trying to predict the next token. Therefore, we need to mask all future tokens in the attention layer. In this architecture, we would have:\n",
    "\n",
    "- body: Stacked decoder layers. The output would be `B x T x hidden_sz`.\n",
    "- head: A classification head which consists of linear layer that project the hidden_sz into vocab_sz. The output would then be passed through `softmax` to get the probability distribution over all tokens in the vocabulary. The token with the highest probability would be chosen during training.\n",
    "\n",
    "At inference, we can use many sampling algorithms such as the greedy algorithm or top-k algorithm using the probability distribution obtained from the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d684880a-3132-487a-ab51-fc3b20978e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config, is_decoder=True)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.head_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.head_dim)\n",
    "        self.ff = FeedForwardNN(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm_1(x + self.attn(x))\n",
    "        x = self.layer_norm_2(x + self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89f920e8-82b9-4975-841d-94613c5bfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.decoder_blocks = nn.Sequential(\n",
    "            *[DecoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        return self.decoder_blocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d2737a7-c025-4578-882e-0352341d7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.lm_head = nn.Linear(config.head_dim, config.vacab_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.lm_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8990ad-ce4a-469e-9906-cba71097db80",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e47849-d3ce-4924-b268-73d4882f8174",
   "metadata": {},
   "source": [
    "The encoder-decoder architecture is the first Transformer architecture used in the [Attention Is All You Need]() paper for Neural Machine Translation Task. It is typically used for tasks that have both their input and output as text such as *summarization*. [T5](https://arxiv.org/abs/1910.10683) and [BART](https://arxiv.org/abs/1910.13461) are the most common models that use such architecture.\n",
    "\n",
    "For each decoder layer, we add masked multi-head attention layer in the middle that 1) takes the hidden state from the last encoder layer to compute the keys and values and 2) takes the hidden state from layer norm to compute the query. This means, this additional middle multi-head attention layer attends to the all tokens in the input sequence. This is a kind of cross-attention that we defined earlier where keys and values come from different source (input sequence) while the query comes from other source.\n",
    "\n",
    "It is very easy to extend or modify our implementation of DecoderLayer to use it in this architecture, so I will leave it for you as an exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41f034-47a7-44b8-a704-18281e843376",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5bddd-ab6e-491a-bf2b-b2ff93062ec9",
   "metadata": {},
   "source": [
    "In this post we started with a brief introduction of Transformer architecture and the motivation behind it such as overcoming RNN-based models. We then covered the main building blocks of the Transformer architecture including attention mechanism. We then briefly went over few tricks that are helpful to train complex models faster such as skip connections and layer normalization. Along the way, we implemented main sublayers used in the architecure. We concluded with different branches of the Transformer architecture that can be used separately such as encoder-only or decoder-only.\n",
    "\n",
    "I hope that you found this post helpful and provided and a good background about the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b830b-c37c-44d6-b2b0-affebb6638f7",
   "metadata": {},
   "source": [
    "# Credits/Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a426b9-40f4-4c92-a900-e93ef04b940c",
   "metadata": {},
   "source": [
    "- [Attention Is All You Need](https://arxiv.labs.arxiv.org/html/1706.03762)\n",
    "- [Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf (Oâ€™Reilly)](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "- [Lillian Weng's The Transformer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
