{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f972acd-a9f7-4a1f-8dea-1dfda848d51d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformer Architecture Explained\n",
    "date: 2023-02-01\n",
    "image: transformer-arch.png\n",
    "categories: [NLP, Deep Learning]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2a1c8-ad8c-465b-9ffe-8b0083dcdf01",
   "metadata": {},
   "source": [
    "![](transformer-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465ac08-7cb7-4bd6-b624-e5a6b197d628",
   "metadata": {},
   "source": [
    "Detailed post to come, stay tuned ðŸ˜Š! \n",
    "\n",
    "Most of the boilerplate code can be found below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d8113-cd99-48ee-8316-ce3cd7d9825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# config.head_dim is typically config.embed_dim / n_heads\n",
    "# config.embed_dim is sometimes also called hidden_size\n",
    "# Hidden size of the first layer in the FF NN is typically 4x the size of the embedding\n",
    "# Most of the capacity and memorization is expected to happen in the first layer of the\n",
    "# FF NN, which is what gets scaled when the model is scaled up\n",
    "# FF NN uses 2 linear layers -> since the input has shape B x T x D, the linear layer is\n",
    "# applied to each embedding vector independently in the sequence and batch, which leads to\n",
    "# position-wise feedforward layer.\n",
    "\n",
    "CONFIG = {\n",
    "    \"vocab_sz\": 1000,\n",
    "    \"block_sz\": 8,\n",
    "    \"intermediare_sz\": None,\n",
    "    \"hidden_dropout_prob\": \"0.2\",\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"hidden_sz\": 64,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"embed_dim\": 768,\n",
    "    \"num_classes\": 2,\n",
    "    \"layer_norm_rps\": 1e-12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51562c60-9883-4e62-b7c0-6dd0091ef203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_sz, config.embed_dim)\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            config.block_sz, config.embed_dim\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            config.embed_dim, eps=config.layer_norm_eps\n",
    "        )\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X is B x T\n",
    "        # token_embeddings are B x T x config.embed_dim\n",
    "        # position_embeddings are T x config.embed_dim\n",
    "        embeddings = self.token_embedding(x) + self.position_embedding(\n",
    "            torch.arange(x.shape[1])\n",
    "        )\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        return self.dropout(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0540e571-f6de-49d2-9535-e91755a7a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config, head_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(config.hidden_sz, head_dim, bias=False)\n",
    "        self.q = nn.Linear(config.hidden_sz, head_dim, bias=False)\n",
    "        self.v = nn.Linear(config.hidden_sz, head_dim, bias=False)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.tril(torch.ones(config.block_sz, config.block_sz))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # k,q,v are each B x T x config.hidden_sz\n",
    "        k, q, v = [func(x) for func in (self.k, self.q, self.v)]\n",
    "        # w is B x T x T\n",
    "        w = q @ k.transpose(2, 1) / (k.shape[-1] ** 0.5)\n",
    "        w = w.masked_fill(self.mask == 0, -float(\"inf\"))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        # output is B x T x config.hidden_sz\n",
    "        return w @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91afcc-af53-4bd9-84c6-bd3dd69bd49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        head_dim = config.hidden_sz // config.num_attention_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(head_dim, config)\n",
    "                for _ in range(config.num_attention_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.output = nn.Linear(config.hidden_sz, config.hidden_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ceda3b-1880-495c-8a32-04005afe4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # config.intermediate_sz is typically 4x hidden_sz\n",
    "        self.l1 = nn.Linear(config.hidden_sz, config.intermediate_sz)\n",
    "        self.l2 = nn.Linear(config.intermediate_sz, config.hidden_sz)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.l2(F.gelu(self.l1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d08fb-7099-40a1-9175-f53442e43b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.head_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.head_dim)\n",
    "        self.ff = FeedForwardNN(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # There are two arrangements for layer_norm:\n",
    "        # Prelayer normalization & Postlayer normalization\n",
    "        # we are using postlayer normalization arrangement\n",
    "        x = self.layer_norm_1(x + self.attn(x))\n",
    "        x = self.layer_norm_2(x + self.ff(x))\n",
    "        # Prelayer normalization\n",
    "        # x = self.layer_norm_1(x)\n",
    "        # x = x + self.attn(x)\n",
    "        # x = x + self.ff(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af565905-5b13-4773-b44f-d0a2e78a3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            *[EncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2737a7-c025-4578-882e-0352341d7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.head_dim, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We take the hidden state of the [CLS] token as\n",
    "        # input to the classifier\n",
    "        x = self.encoder(x)[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
