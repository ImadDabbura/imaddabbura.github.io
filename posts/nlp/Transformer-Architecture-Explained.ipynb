{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f972acd-a9f7-4a1f-8dea-1dfda848d51d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformer Architecture Explained\n",
    "date: 2023-02-01\n",
    "image: transformer-arch.png\n",
    "categories: [NLP, Deep Learning]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba322bf-737b-442f-9a61-eea90414dbe8",
   "metadata": {},
   "source": [
    "WIP (initial draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2a1c8-ad8c-465b-9ffe-8b0083dcdf01",
   "metadata": {},
   "source": [
    "<!-- ![](transformer-logo.png) -->\n",
    "![](full-transformer-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b26629-0fa2-476f-8f2d-ab5490b96270",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9f471-e14e-4fb0-ae58-347e9d30703e",
   "metadata": {},
   "source": [
    "Transformer architecture was first introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper in 2017. It outperformed RNN-based models on all NLP related tasks. It has an encoder-decoder architecture that is used in tasks such as *Neural Machine Translation*. The most common examples of models that use transformer architecture is **BERT**, which uses encoder-only architecture, and **GPT**, that uses decoder-only architecture.\n",
    "\n",
    "RNN-based models suffer from two main issues:\n",
    "\n",
    "1. Hard to learn long distance dependencies due to gradient problems (vanishing/exploding). For example, if the last word of the sequence depends on on of the early words in the sequence, the hidden state by the time it reaches the last word wouldn't have much of the information of the early words especially as the length of the sequence gets longer. Such models assume linear order of words, which is not the right way to think about it.\n",
    "2. These are sequential models, which means we can only start $w_t$ once we finish $w_{t - 1}$ because it is dependent on the previous hidden state. Therefore, it is not parallelizable\n",
    "\n",
    "Transformer architecture with its attention mechanism came to solve the above issues as well as providing much better design and architecture choices.\n",
    "\n",
    "In this post, we will implement and explain the main building blocks of the transformer architecture (see figure 1). By the end of this post, we should be able to:\n",
    "1. Implement vanilla transformer from scratch, including full encoder-decoder architecture, encoder-only architecture, and decoder-only architecture.\n",
    "2. Understand the role of each block\n",
    "\n",
    "<!-- ![](transformer-arch.png) -->\n",
    "<!-- ![](full-transformer-architecture.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a69f5-14cf-4314-98fa-c7da85ccaeb2",
   "metadata": {},
   "source": [
    "# Code Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb1a3a-0630-4f2c-ade5-f1657d4f5cb6",
   "metadata": {},
   "source": [
    "To avoid creating functions/classes with large number of parameters, we will use `config` dictionary that hosts all the required configurations needed for the transformer architecture such as `vocab_sz` and `hidden_sz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909d8113-cd99-48ee-8316-ce3cd7d9825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# config.head_dim is typically config.embed_dim / n_heads\n",
    "# config.embed_dim is sometimes also called hidden_size\n",
    "# Hidden size of the first layer in the FF NN is typically 4x the size of the embedding\n",
    "# Most of the capacity and memorization is expected to happen in the first layer of the\n",
    "# FF NN, which is what gets scaled when the model is scaled up\n",
    "# FF NN uses 2 linear layers -> since the input has shape B x T x D, the linear layer is\n",
    "# applied to each embedding vector independently in the sequence and batch, which leads to\n",
    "# position-wise feedforward layer.\n",
    "\n",
    "config = {\n",
    "    \"vocab_sz\": 1000,\n",
    "    \"block_sz\": 8,\n",
    "    \"intermediare_sz\": None,\n",
    "    \"hidden_dropout_prob\": \"0.2\",\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"hidden_sz\": 64,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"embed_dim\": 768,\n",
    "    \"num_classes\": 2,\n",
    "    \"layer_norm_rps\": 1e-12,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f69795-143f-4182-83a4-b50f4fdcc3c9",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44074864-bc19-41c7-a4f0-8db939d23900",
   "metadata": {},
   "source": [
    "After the input sequence is tokenized and numericalized, we need to project each token into lower dimension space. Such projection is called **embedding** and it captures the semantic representation of tokens based on the context the token mostly occurs in.\n",
    "\n",
    "Attention operation is a permutation equivariant, this means that if we permute the input then the corresponding output will be permuted in exactly the same way. In other words, attention mechanism is not aware of the relative ordering of the tokens. Therefore, we need some way to encode the positions of the tokens in each sequence. This is where **positional encoding** comes into play. There are two types of encodings:\n",
    "\n",
    "- *Absolute Positional Encoding*: Use static patterns of the token absolute position.Can use static patters such as **sign** function, or learned parameters\n",
    "- *Relative Positional Encoding*: Encode the relative position of tokens. We need to adjust the attention mechanism itself by adding new terms to be used when dot-products are used to encode the relative position between tokens up to maximum relative position.\n",
    "- *Rotary Encoding*: Combine both absolute and relative position of tokens to achieve great results. This can be done by encoding the absolute positions with a rotation matrix that will be multiplied with key and value matrices of each attenetion layer to add the relative position information at every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51562c60-9883-4e62-b7c0-6dd0091ef203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_sz, config.embed_dim)\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            config.block_sz, config.embed_dim\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            config.embed_dim, eps=config.layer_norm_eps\n",
    "        )\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X:                   B x T\n",
    "        # token_embeddings:    B x T x embed_dim\n",
    "        # position_embeddings: T x embed_dim\n",
    "        embeddings = self.token_embedding(x) + self.position_embedding(\n",
    "            torch.arange(x.shape[1])\n",
    "        )\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        return self.dropout(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce85768-15af-4cab-93c8-78f24490499c",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29291a1-9e3b-49fe-af3f-56c3e4b9f7c1",
   "metadata": {},
   "source": [
    "**Attention** is a communication mechanism that is used by NN model to learn to make predictions by attending to some tokens in the context window (only current and previous tokens for decoder-only architectute). The attention weights, which are learned, are used to construct the weighted average of all the tokens attended to by each token. This will help each token focus on what is important in the context. As a reminder, with attention, there is no notion of space. This means it operates on a set of vectors. This is why we need positional encoding for tokens.\n",
    "\n",
    "The results of the attention layer would be contextualized embeddings, since the output of the embedding layer is contextless embeddings. This is very useful because we know that the meaning of a word changes according to the context, and embeddings from the embedding layer for a token is the same regardles of its context. For example, the word \"bear\" has the same embedding vector whether it comes in \"teddy bear\" or \"to bear\".\n",
    "\n",
    "**Self-attention** is a type of attention mechanism where the keys and values come from the same source as the queries, which is the input $x$. Whereas in **cross-attention**, the queries still get produced from the input $x$, but the keys and values come from some other, external source (encoder module in the case of encoder-decoder architecure).\n",
    "\n",
    "![](scaled-dot-product-attention.png)\n",
    "\n",
    "For self-attention, we have:\n",
    "\n",
    "- Query matrix  $Q$ (hidden_sz x head_dim): what each token is looking for\n",
    "- Key matrix $K$ (hidden_sz x head_dim): what each token contains\n",
    "- Value matrix $V$ (hidden_sz x head_dim): what each token communicate with\n",
    "\n",
    "Then,\n",
    "\n",
    "- The dot-product of query with all the keys of the tokens give us the affinities. Dot-product is just used as a form of computing similarities.\n",
    "    - If query and key vectors are aligned -> very high value -> get to know more about that token as opposed to other tokens\n",
    "    - All the tokens in all positions in B x T matrix produce key/query/value vectors in parallel and independently from each other and no communication is happening\n",
    "    - Then all queries will be dot-product with all the keys\n",
    "    - We scale attention by dividing it with $sqrt(head\\_sz)$. This makes it so when input Q,K are unit variance, weights will be unit variance too and *softmax* will stay diffuse and not saturate too much\n",
    "- Finally, we multiply the attention weights with the value matrix $V$ to get the contextualized embeddings\n",
    "\n",
    "In equations:\n",
    "$$attn(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt d_k})V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0540e571-f6de-49d2-9535-e91755a7a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config, head_dim, is_decoder=False) -> None:\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(config.hidden_sz, head_dim, bias=False)\n",
    "        self.q = nn.Linear(config.hidden_sz, head_dim, bias=False)\n",
    "        self.v = nn.Linear(config.hidden_sz, head_dim, bias=False)\n",
    "        self.is_decoder = is_decoder\n",
    "        if self.is_decoder:\n",
    "            self.register_buffer(\n",
    "                \"mask\", torch.tril(torch.ones(config.block_sz, config.block_sz))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # k,q,v are each B x T x config.hidden_sz\n",
    "        k, q, v = [func(x) for func in (self.k, self.q, self.v)]\n",
    "        # w is B x T x T\n",
    "        w = q @ k.transpose(2, 1) / (k.shape[-1] ** 0.5)\n",
    "        if self.is_decoder:\n",
    "            w = w.masked_fill(self.mask == 0, -float(\"inf\"))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        # output is B x T x config.hidden_sz\n",
    "        return w @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582aec62-5e06-4d46-a036-860ce73af009",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f9596-56e8-4710-bbf2-4b0f57763b0f",
   "metadata": {},
   "source": [
    "What we described in the previous section was the one selt-attention mechanism. Since each attention head focuses on one specific characteristic of the data in terms of similarity such as subject-verb interaction, other heads are needed to focus on other aspects such as adjectives. We can also think of having multiple heads as if head head focuses on one or few other tokens. Remember that all of this is done in parallel and there is no communication between heads. This means that each head has no idea what other heads are doing.\n",
    "\n",
    "![](multi-head-attention.png)\n",
    "\n",
    "In multi-head layer, we typically have the `head_sz` be the result of dividing the `hidden_sz` (or the `embeddind_dim` if it is the first layer) by the number of heads.\n",
    "\n",
    "Once we get all contextualized embeddings from all heads, we concatenate them. Then we pass the output through a projection layer with the same dimension as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf91afcc-af53-4bd9-84c6-bd3dd69bd49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config, is_decoder=False) -> None:\n",
    "        super().__init__()\n",
    "        head_dim = config.hidden_sz // config.num_attention_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(head_dim, config, is_decoder)\n",
    "                for _ in range(config.num_attention_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.output = nn.Linear(config.hidden_sz, config.hidden_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fbacb-64e3-48dd-b65e-7de96b486d89",
   "metadata": {},
   "source": [
    "# Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516daf9-e662-4411-9ae5-b33aad731eee",
   "metadata": {},
   "source": [
    "Because there are no elementwise nonlinearities involved in the calculation of the attention, stacking multiple layers of attention wouldn't help much because the output would still be linear transformation of the input. As a result, feed-forward NN is added to add such nonlinearities to post-process each output vector from the attention layer. Therefore, each embedding vector is processed independently in the batched sequence, which leads to the position-wise feed-forward layer.\n",
    "\n",
    "We typically first project the output vector into new space 4x the hidden_sz. Therefore, most of the capacity and memorization is expected to happen in the first layer, which is what gets scaled when the model is scaled up. Then we project it back to the original dimension. We use [`GELU`](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html?highlight=gelu#torch.nn.GELU) as the activation function, which is a Gaussian Error Linear Units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ceda3b-1880-495c-8a32-04005afe4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # intermediate_sz is typically 4x hidden_sz\n",
    "        self.l1 = nn.Linear(config.hidden_sz, config.intermediate_sz)\n",
    "        self.l2 = nn.Linear(config.intermediate_sz, config.hidden_sz)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.l2(F.gelu(self.l1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fa91b-efa0-4970-8d32-92688e60acce",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b73ef-5b17-4b92-bee3-26aac96bb393",
   "metadata": {},
   "source": [
    "**Layer normalization** was introduced in [this paper](https://arxiv.org/abs/1607.06450) to overcome the main challenges of **Batch normalization**, which are 1) how do we handles batches with 1 or few examples because we would have infinite variance or unstable training and 2) how do we handle RNNs. The main differences with batch normalization are 1) we don't have moving averages/standard deviations and 2) we average over the hidden dimesnion(s), so it is indepenedent of the batch size. It has two learnable parameters (scalars): $\\beta$ and $\\gamma$ (see the equation below):\n",
    "\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "It is used as a trick to train complex models such as Transformer faster. In our case, we would normalize the hidden vectors to zero mean and unit standard deviation. This trick helps maintain consistent distribution of signals by cutting down uninformative variations in hidden vector values.\n",
    "\n",
    "There are two arrangements for the layer normalization (see examples of the different arrangements in the annotated code below):\n",
    "\n",
    "![](layer-norm-arrangement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85573a-4dfa-47e5-b93b-94801c027b5c",
   "metadata": {},
   "source": [
    "- *Prelayer normalization*: Places the layer normalization within the span of skip connections. This arrangement is easier to train.\n",
    "- *Postlayer normalization*: Places the layer normalization in between skip connections. This arrangement is used in the Transformer paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d24d9-b049-4107-9770-750250ad9c7b",
   "metadata": {},
   "source": [
    "# Skip Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07932db-6543-4d6f-bb79-890dee394756",
   "metadata": {},
   "source": [
    "**Skip connections** were as a trick to train deeper and more complex models faster as well as avoid the issue of vanishing gradients that deeper networks face. It provides paths for the gradient to flow through back to the input. In our case, we are using skip connections with addition, which means we take a copy of the inputs and added to the output of a block (involves some computations). If we assume $y = x + F(x)$, then it is as if we asking to block to predict $y - x$. In other words, it means to backpropagate through the identity function, which leads to multiply the gradient of $y$ by one and retain its value in the earlier layers.\n",
    "\n",
    "In conclusion, skip connections smooth out the loss landscape , as shown in [this paper](https://arxiv.labs.arxiv.org/html/1712.09913)(see figure below), and make it easier for the gradients to flow back as *addition* operator split the gradients equally. This means that small changes in the input can still find their way to the output. Additionally, it preserves the original input sequence, which means there is no way for the current word to forget to attend to its position because we always add it back.\n",
    "\n",
    "\n",
    "![](loss-landscape-with-skip-connections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027159a1-bdfe-434a-b210-9c3bd46a65e2",
   "metadata": {},
   "source": [
    "# Encoder-only Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec7ce1-4871-4ecb-ade7-fcc99f5b6b48",
   "metadata": {},
   "source": [
    "Encoder-only architecture are well suited for classification tasks. The most common that uses encoder-only branch of the Transformer architecture is [BERT](https://arxiv.org/abs/1810.04805) and all its variants such as [RoBERTa](https://arxiv.org/abs/1907.11692). In this architecture, we would have:\n",
    "- body: Stacked encoder layers. The output would be `B x T x hidden_sz`.\n",
    "- head: A classification head which consists of linear layer that project the hidden_sz into num_classes. We take the hidden vector of the first token, which is the special token `[CLS]` in the case of *BERT* (indicates the beginning of sequence), and pass it through the linear layer to get the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d08fb-7099-40a1-9175-f53442e43b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.head_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.head_dim)\n",
    "        self.ff = FeedForwardNN(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # There are two arrangements for layer_norm:\n",
    "        # Prelayer normalization & Postlayer normalization\n",
    "        # we are using postlayer normalization arrangement\n",
    "        x = self.layer_norm_1(x + self.attn(x))\n",
    "        x = self.layer_norm_2(x + self.ff(x))\n",
    "        # Prelayer normalization\n",
    "        # x = self.layer_norm_1(x)\n",
    "        # x = x + self.attn(x)\n",
    "        # x = x + self.ff(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af565905-5b13-4773-b44f-d0a2e78a3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder_blocks = nn.Sequential(\n",
    "            *[EncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        return self.encoder_blocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df92c42f-8650-4ef6-9865-686d183d61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.head_dim, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We take the hidden state of the [CLS] token as\n",
    "        # input to the classifier\n",
    "        x = self.encoder(x)[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424974d-fa5e-4cd7-91ed-9dbef979ef41",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decoder-only Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6753a-00fb-43d3-8f7b-4f311fc56272",
   "metadata": {},
   "source": [
    "These models are typically used as language models such as [GPT](https://openai.com/blog/language-unsupervised/) and all its variants. In this architecture, as opposed to the encoder-only architecture, the token can only see past tokens but NOT THE FUTUTE tokens because this would be a kind of cheating since we are trying to predict the next token. Therefore, we need to mask all future tokens in the attention layer. In this architecture, we would have:\n",
    "- body: Stacked decoder layers. The output would be `B x T x hidden_sz`.\n",
    "- head: A classification head which consists of linear layer that project the hidden_sz into vocab_sz. The output would then be passed through `softmax` to get the probability distribution over all tokens in the vocabulary. The token with the highest probability would be chosen during training.\n",
    "\n",
    "At inference, we can use many sampling algorithms such as the greedy algorithm of top-k algorithm using the probability distribution obtained from the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d684880a-3132-487a-ab51-fc3b20978e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config, is_decoder=True)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.head_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.head_dim)\n",
    "        self.ff = FeedForwardNN(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm_1(x + self.attn(x))\n",
    "        x = self.layer_norm_2(x + self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89f920e8-82b9-4975-841d-94613c5bfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.decoder_blocks = nn.Sequential(\n",
    "            *[DecoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        return self.decoder_blocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d2737a7-c025-4578-882e-0352341d7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.lm_head = nn.Linear(config.head_dim, config.vacab_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        return self.lm_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41f034-47a7-44b8-a704-18281e843376",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5bddd-ab6e-491a-bf2b-b2ff93062ec9",
   "metadata": {},
   "source": [
    "Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b830b-c37c-44d6-b2b0-affebb6638f7",
   "metadata": {},
   "source": [
    "# Credits/Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a426b9-40f4-4c92-a900-e93ef04b940c",
   "metadata": {},
   "source": [
    "- [Attention Is All You Need](https://arxiv.labs.arxiv.org/html/1706.03762)\n",
    "- [Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf (O’Reilly)](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "- [Lillian Weng's The Transformer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
