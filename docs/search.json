[
  {
    "objectID": "projects-index.html",
    "href": "projects-index.html",
    "title": "Projects",
    "section": "",
    "text": "Machine Learning\n\n\nDeep Learning\n\n\n\nBuild a classifier to predict whether a given patient will have TIA and deploy the best model as a RESTful API.\n\n\n\nImad Dabbura\n\n\nApr 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Engineering\n\n\n\nBuild a Relational Database using Star Schema of song’s users activities data.\n\n\n\nImad Dabbura\n\n\nApr 8, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#introduction",
    "href": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#introduction",
    "title": "Predicting Loan Repayment",
    "section": "Introduction",
    "text": "Introduction\nThe two most critical questions in the lending industry are: 1) How risky is the borrower? 2) Given the borrower’s risk, should we lend him/her? The answer to the first question determines the interest rate the borrower would have. Interest rate measures among other things (such as time value of money) the riskness of the borrower, i.e. the riskier the borrower, the higher the interest rate. With interest rate in mind, we can then determine if the borrower is eligible for the loan.\nInvestors (lenders) provide loans to borrowers in exchange for the promise of repayment with interest. That means the lender only makes profit (interest) if the borrower pays off the loan. However, if he/she doesn’t repay the loan, then the lender loses money.\nWe’ll be using publicly available data from LendingClub.com. The data covers the 9,578 loans funded by the platform between May 2007 and February 2010. The interest rate is provided to us for each borrower. Therefore, so we’ll address the second question indirectly by trying to predict if the borrower will repay the loan by its mature date or not. Through this excerise we’ll illustrate three modeling concepts:\n\nWhat to do with missing values.\nTechniques used with imbalanced classification problems.\nIllustrate how to build an ensemble model using two methods: blending and stacking, which most likely gives us a boost in performance.\n\nBelow is a short description of each feature in the data set:\n\ncredit_policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\npurpose: The purpose of the loan such as: credit_card, debt_consolidation, etc.\nint_rate: The interest rate of the loan (proportion).\ninstallment: The monthly installments ($) owed by the borrower if the loan is funded.\nlog_annual_inc: The natural log of the annual income of the borrower.\ndti: The debt-to-income ratio of the borrower.\nfico: The FICO credit score of the borrower.\ndays_with_cr_line: The number of days the borrower has had a credit line.\nrevol_bal: The borrower’s revolving balance.\nrevol_util: The borrower’s revolving line utilization rate.\ninq_last_6mths: The borrower’s number of inquiries by creditors in the last 6 months.\ndelinq_2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\npub_rec: The borrower’s number of derogatory public records.\nnot_fully_paid: indicates whether the loan was not paid back in full (the borrower either defaulted or the borrower was deemed unlikely to pay it back).\n\nLet’s load the data and check:\n\nData types of each feature\nIf we have missing values\nIf we have imbalanced data\n\n\n\nCode\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport fancyimpute\nfrom imblearn.pipeline import make_pipeline as imb_make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.ensemble import BalancedBaggingClassifier, EasyEnsemble\nfrom mlens.visualization import corrmat\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import Imputer, RobustScaler, FunctionTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (roc_auc_score, confusion_matrix,\n                             accuracy_score, roc_curve,\n                             precision_recall_curve, f1_score)\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\nfrom keras import models, layers, optimizers\n\nos.chdir(\"../\")\nfrom scripts.plot_roc import plot_roc_and_pr_curves\nos.chdir(\"notebooks/\")\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\n\n\n\nCode\n# Load the data\ndf = pd.read_csv(\"../data/loans.csv\")\n\n# Check both the datatypes and if there is missing values\nprint(f\"\\033[1m\\033[94mData types:\\n{11 * '-'}\")\nprint(f\"\\033[30m{df.dtypes}\\n\")\nprint(f\"\\033[1m\\033[94mSum of null values in each feature:\\n{35 * '-'}\")\nprint(f\"\\033[30m{df.isnull().sum()}\")\ndf.head()\n\n\nData types:\n-----------\ncredit_policy          int64\npurpose               object\nint_rate             float64\ninstallment          float64\nlog_annual_inc       float64\ndti                  float64\nfico                   int64\ndays_with_cr_line    float64\nrevol_bal              int64\nrevol_util           float64\ninq_last_6mths       float64\ndelinq_2yrs          float64\npub_rec              float64\nnot_fully_paid         int64\ndtype: object\n\nSum of null values in each feature:\n-----------------------------------\ncredit_policy         0\npurpose               0\nint_rate              0\ninstallment           0\nlog_annual_inc        4\ndti                   0\nfico                  0\ndays_with_cr_line    29\nrevol_bal             0\nrevol_util           62\ninq_last_6mths       29\ndelinq_2yrs          29\npub_rec              29\nnot_fully_paid        0\ndtype: int64\n\n\n\n\n\n\n  \n    \n      \n      credit_policy\n      purpose\n      int_rate\n      installment\n      log_annual_inc\n      dti\n      fico\n      days_with_cr_line\n      revol_bal\n      revol_util\n      inq_last_6mths\n      delinq_2yrs\n      pub_rec\n      not_fully_paid\n    \n  \n  \n    \n      0\n      1\n      debt_consolidation\n      0.1189\n      829.10\n      11.350407\n      19.48\n      737\n      5639.958333\n      28854\n      52.1\n      0.0\n      0.0\n      0.0\n      0\n    \n    \n      1\n      1\n      credit_card\n      0.1071\n      228.22\n      11.082143\n      14.29\n      707\n      2760.000000\n      33623\n      76.7\n      0.0\n      0.0\n      0.0\n      0\n    \n    \n      2\n      1\n      debt_consolidation\n      0.1357\n      366.86\n      10.373491\n      11.63\n      682\n      4710.000000\n      3511\n      25.6\n      1.0\n      0.0\n      0.0\n      0\n    \n    \n      3\n      1\n      debt_consolidation\n      0.1008\n      162.34\n      11.350407\n      8.10\n      712\n      2699.958333\n      33667\n      73.2\n      1.0\n      0.0\n      0.0\n      0\n    \n    \n      4\n      1\n      credit_card\n      0.1426\n      102.92\n      11.299732\n      14.97\n      667\n      4066.000000\n      4740\n      39.5\n      0.0\n      1.0\n      0.0\n      0\n    \n  \n\n\n\n\n\n\nCode\n# Get number of positve and negative examples\npos = df[df[\"not_fully_paid\"] == 1].shape[0]\nneg = df[df[\"not_fully_paid\"] == 0].shape[0]\nprint(f\"Positive examples = {pos}\")\nprint(f\"Negative examples = {neg}\")\nprint(f\"Proportion of positive to negative examples = {(pos / neg) * 100:.2f}%\")\nplt.figure(figsize=(8, 6))\nsns.countplot(df[\"not_fully_paid\"])\nplt.xticks((0, 1), [\"Paid fully\", \"Not paid fully\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Count\")\nplt.title(\"Class counts\", y=1, fontdict={\"fontsize\": 20});\n\n\nPositive examples = 1533\nNegative examples = 8045\nProportion of positive to negative examples = 19.06%\n\n\n\n\n\nIt looks like we have only one categorical feature (“purpose”). Also, six features have missing values (no missing values in labels). Moreover, the data set is pretty imbalanced as expected where positive examples (“not paid fully”) are only 19%. We’ll explain in the next section how to handle all of them after giving an overview of ensemble methods."
  },
  {
    "objectID": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#modeling",
    "href": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#modeling",
    "title": "Predicting Loan Repayment",
    "section": "Modeling",
    "text": "Modeling\nEnsemble methods can be defined as combining several different models (base learners) into final model (meta learner) to reduce the generalization error. It relies on the assumption that each model would look at a different aspect of the data which yield to capturing part of the truth. Combining good performing models the were trained independently will capture more of the truth than a single model. Therefore, this would result in more accurate predictions and lower generalization errors.\n\nAlmost always ensemble model performance gets improved as we add more models.\nTry to combine models that are as much different as possible. This will reduce the correlation between the models that will improve the performance of the ensemble model that will lead to significantly outperform the best model. In the worst case where all models are perfectly correlated, the ensemble would have the same performance as the best model and sometimes even lower if some models are very bad. As a result, pick models that are as good as possible.\n\nDiﬀerent ensemble methods construct the ensemble of models in diﬀerent ways. Below are the most common methods:\n\nBlending: Averaging the predictions of all models.\nBagging: Build different models on different datasets and then take the majority vote from all the models. Given the original dataset, we sample with replacement to get the same size of the original dataset. Therefore, each dataset will include, on average, 2/3 of the original data and the rest 1/3 will be duplicates. Since each model will be built on a different dataset, it can be seen as a different model. Random Forest improves on default bagging trees by reducing the likelihood of strong features to picked on every split. In other words, it reduces the number of features available at each split from \\(n\\) features to, for example, \\(n/2\\) or \\(log(n)\\) features. This will reduce correlation –> reduce variance.\nBoosting: Build models sequentially. That means each model learns from the residuals of the previous model. The output will be all output of each single model weighted by the learning rate (\\(\\lambda\\)). It reduces the bias resulted from bagging by learning sequentially from residuals of previous trees (models).\nStacking: Build k models called base learners. Then fit a model to the output of the base learners to predict the final output.\n\nSince we’ll be using Random Fores (bagging) and Gradient Boosting (boosting) classifiers as base learners in the ensemble model, we’ll illustrate only averaging and stacking ensemble methods. Therefore, modeling parts would be consisted of three parts:\n\nStrategies to deal with missing values.\nStrategies to deal with imbalanced datasets.\nBuild ensemble models.\n\nBefore going further, the following data preprocessing steps will be applicable to all models:\n\nCreate dummy variables from the feature “purpose” since its nominal (not ordinal) categorical variable. It’s also a good practice to drop the first one to avoid linear dependency between the resulted features since some algorithms may struggle with this issue.\nSplit the data into training set (70%), and test set (30%). Training set will be used to fit the model, and test set will be to evaluate the best model to get an estimation of generalization error. Instead of having validation set to tune hyperparameters and evaluate different models, we’ll use 10-folds cross validation because it’s more reliable estimate of generalization error.\nStandardize the data. We’ll be using RobustScaler so that the standarization will be less influenced by the outliers, i.e. more robust. It centers the data around the median and scale it using interquartile range (IQR). This step will be included in the pipelines for each model as a transformer so we will not do it separately.\n\n\n\nCode\n# Create dummy variables from the feature purpose\ndf = pd.get_dummies(df, columns=[\"purpose\"], drop_first=True)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      credit_policy\n      int_rate\n      installment\n      log_annual_inc\n      dti\n      fico\n      days_with_cr_line\n      revol_bal\n      revol_util\n      inq_last_6mths\n      delinq_2yrs\n      pub_rec\n      not_fully_paid\n      purpose_credit_card\n      purpose_debt_consolidation\n      purpose_educational\n      purpose_home_improvement\n      purpose_major_purchase\n      purpose_small_business\n    \n  \n  \n    \n      0\n      1\n      0.1189\n      829.10\n      11.350407\n      19.48\n      737\n      5639.958333\n      28854\n      52.1\n      0.0\n      0.0\n      0.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      0.1071\n      228.22\n      11.082143\n      14.29\n      707\n      2760.000000\n      33623\n      76.7\n      0.0\n      0.0\n      0.0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0.1357\n      366.86\n      10.373491\n      11.63\n      682\n      4710.000000\n      3511\n      25.6\n      1.0\n      0.0\n      0.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      1\n      0.1008\n      162.34\n      11.350407\n      8.10\n      712\n      2699.958333\n      33667\n      73.2\n      1.0\n      0.0\n      0.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1\n      0.1426\n      102.92\n      11.299732\n      14.97\n      667\n      4066.000000\n      4740\n      39.5\n      0.0\n      1.0\n      0.0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nStrategies to deal with missing value\nAlmost always real world data sets have missing values. This can be due, for example, users didn’t fill some part of the forms or some transformations happened while collecting and cleaning the data before they send it to you. Sometimes missing values are informative and weren’t generated randomly. Therefore, it’s a good practice to add binary features to check if there is missing values in each row for each feature that has missing values. In our case, six features have missing values so we would add six binary features one for each feature. For example, “log_annual_inc” feature has missing values, so we would add a feature “is_log_annual_inc_missing” that takes the values \\(\\in \\{0, 1\\}\\). Good thing is that the missing values are in the predictors only and not the labels. Below are some of the most common strategies for dealing with missing values:\n\nSimply delete all examples that have any missing values. This is usually done if the missing values are very small compared to the size of the data set and the missing values were random. In other words, the added binary features did not improve the model. One disadvantage for this strategy is that the model will throw an error when test data has missing values at prediction.\nImpute the missing values using the mean of each feature separately.\nImpute the missing values using the median of each feature separately.\nUse Multivariate Imputation by Chained Equations (MICE). The main disadvantage of MICE is that we can’t use it as a transformer in sklearn pipelines and it requires to use the full data set when imputing the missing values. This means that there will be a risk of data leakage since we’re using both training and test sets to impute the missing values. The following steps explain how MICE works:\n\nFirst step: Impute the missing values using the mean of each feature separately.\nSecond step: For each feature that has missing values, we take all other features as predictors (including the ones that had missing values) and try to predict the values for this feature using linear regression for example. The predicted values will replace the old values for that feature. We do this for all features that have missing values, i.e. each feature will be used once as a target variable to predict its values and the rest of the time as a predictor to predict other features’ values. Therefore, one complete cycle (iteration) will be done once we run the model \\(k\\) times to predict the \\(k\\) features that have missing values. For our data set, each iteration will run the linear regression 6 times to predict the 6 features.\nThird step: Repeat step 2 until there is not much of change between predictions.\n\nImpute the missing values using K-Nearest Neighbors. We compute distance between all examples (excluding missing values) in the data set and take the average of k-nearest neighbors of each missing value. There’s no implementation for it yet in sklearn and it’s pretty inefficient to compute it since we’ll have to go through all examples to calculate distances. Therefore, we’ll skip this strategy in this notebook.\n\nTo evaluate each strategy, we’ll use Random Forest classifier with hyperparameters’ values guided by Data-driven Advice for Applying Machine Learning to Bioinformatics Problems as a starting point.\nLet’s first create binary features for missing values and then prepare the data for each strategy discussed above. Next, we’ll compute the 10-folds cross validation AUC score for all the models using training data.\n\n\nCode\n# Create binary features to check if the example is has missing values for all features that have missing values\nfor feature in df.columns:\n    if np.any(np.isnan(df[feature])):\n        df[\"is_\" + feature + \"_missing\"] = np.isnan(df[feature]) * 1\n\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      credit_policy\n      int_rate\n      installment\n      log_annual_inc\n      dti\n      fico\n      days_with_cr_line\n      revol_bal\n      revol_util\n      inq_last_6mths\n      ...\n      purpose_educational\n      purpose_home_improvement\n      purpose_major_purchase\n      purpose_small_business\n      is_log_annual_inc_missing\n      is_days_with_cr_line_missing\n      is_revol_util_missing\n      is_inq_last_6mths_missing\n      is_delinq_2yrs_missing\n      is_pub_rec_missing\n    \n  \n  \n    \n      0\n      1\n      0.1189\n      829.10\n      11.350407\n      19.48\n      737\n      5639.958333\n      28854\n      52.1\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      0.1071\n      228.22\n      11.082143\n      14.29\n      707\n      2760.000000\n      33623\n      76.7\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0.1357\n      366.86\n      10.373491\n      11.63\n      682\n      4710.000000\n      3511\n      25.6\n      1.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      1\n      0.1008\n      162.34\n      11.350407\n      8.10\n      712\n      2699.958333\n      33667\n      73.2\n      1.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1\n      0.1426\n      102.92\n      11.299732\n      14.97\n      667\n      4066.000000\n      4740\n      39.5\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 25 columns\n\n\n\n\n\nCode\n# Original Data\nX = df.loc[:, df.columns != \"not_fully_paid\"].values\ny = df.loc[:, df.columns == \"not_fully_paid\"].values.flatten()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=True, random_state=123, stratify=y)\nprint(f\"Original data shapes: {X_train.shape, X_test.shape}\")\n\n# Drop NA and remove binary columns\ntrain_indices_na = np.max(np.isnan(X_train), axis=1)\ntest_indices_na = np.max(np.isnan(X_test), axis=1)\nX_train_dropna, y_train_dropna = X_train[~train_indices_na, :][:, :-6], y_train[~train_indices_na]\nX_test_dropna, y_test_dropna = X_test[~test_indices_na, :][:, :-6], y_test[~test_indices_na]\nprint(f\"After dropping NAs: {X_train_dropna.shape, X_test_dropna.shape}\")\n\n# MICE data\nmice = fancyimpute.MICE(verbose=0)\nX_mice = mice.complete(X)\nX_train_mice, X_test_mice, y_train_mice, y_test_mice = train_test_split(\n    X_mice, y, test_size=0.2, shuffle=True, random_state=123, stratify=y)\nprint(f\"MICE data shapes: {X_train_mice.shape, X_test_mice.shape}\")\n\n\nOriginal data shapes: ((7662, 24), (1916, 24))\nAfter dropping NAs: ((7611, 18), (1905, 18))\nMICE data shapes: ((7662, 24), (1916, 24))\n\n\n\n\nCode\n# Build random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=500,\n                                max_features=0.25,\n                                criterion=\"entropy\",\n                                class_weight=\"balanced\")\n# Build base line model -- Drop NA's\npip_baseline = make_pipeline(RobustScaler(), rf_clf)\nscores = cross_val_score(pip_baseline,\n                         X_train_dropna, y_train_dropna,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mBaseline model's average AUC: {scores.mean():.3f}\")\n\n# Build model with mean imputation\npip_impute_mean = make_pipeline(Imputer(strategy=\"mean\"),\n                                RobustScaler(), rf_clf)\nscores = cross_val_score(pip_impute_mean,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mMean imputation model's average AUC: {scores.mean():.3f}\")\n\n# Build model with median imputation\npip_impute_median = make_pipeline(Imputer(strategy=\"median\"),\n                                  RobustScaler(), rf_clf)\nscores = cross_val_score(pip_impute_median,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mMedian imputation model's average AUC: {scores.mean():.3f}\")\n\n# Build model using MICE imputation\npip_impute_mice = make_pipeline(RobustScaler(), rf_clf)\nscores = cross_val_score(pip_impute_mice,\n                         X_train_mice, y_train_mice,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mMICE imputation model's average AUC: {scores.mean():.3f}\")\n\n\nBaseline model's average AUC: 0.651\nMean imputation model's average AUC: 0.651\nMedian imputation model's average AUC: 0.651\nMICE imputation model's average AUC: 0.656\n\n\nLet’s plot the feature importances to check if the added binary features added anything to the model.\n\n\nCode\n# fit RF to plot feature importances\nrf_clf.fit(RobustScaler().fit_transform(Imputer(strategy=\"median\").fit_transform(X_train)), y_train)\n\n# Plot features importance\nimportances = rf_clf.feature_importances_\nindices = np.argsort(rf_clf.feature_importances_)[::-1]\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 25), importances[indices], align=\"center\")\nplt.xticks(range(1, 25), df.columns[df.columns != \"not_fully_paid\"][indices], rotation=90)\nplt.title(\"Feature Importance\", {\"fontsize\": 16});\n\n\n\n\n\nGuided by the 10-fold cross validation AUC scores, it looks like all strategies have comparable results and missing values were generated randomly. Also, the added six binary features showed no importance when plotting feature importances from Random Forest classifier. Therefore, it’s safe to drop those features and use Median Imputation method as a transformer later on in the pipeline.\n\n\nCode\n# Drop generated binary features\nX_train = X_train[:, :-6]\nX_test = X_test[:, :-6]\n\n\n\n\nStrategies to deal with imbalanced data\nClassification problems in most real world applications have imbalanced data sets. In other words, the positive examples (minority class) are a lot less than negative examples (majority class). We can see that in spam detection, ads click, loan approvals, etc. In our example, the positive examples (people who haven’t fully paid) were only 19% from the total examples. Therefore, accuracy is no longer a good measure of performance for different models because if we simply predict all examples to belong to the negative class, we achieve 81% accuracy. Better metrics for imbalanced data sets are AUC (area under the ROC curve) and f1-score. However, that’s not enough because class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. As a result, we’ll explore different methods to overcome class imbalance problem.\n\nUnder-Sample: Under-sample the majority class with or w/o replacement by making the number of positive and negative examples equal. One of the drawbacks of under-sampling is that it ignores a good portion of training data that has valuable information. In our example, it would loose around 6500 examples. However, it’s very fast to train.\nOver-Sample: Over-sample the minority class with or w/o replacement by making the number of positive and negative examples equal. We’ll add around 6500 samples from the training data set with this strategy. It’s a lot more computationally expensive than under-sampling. Also, it’s more prune to overfitting due to repeated examples.\nEasyEnsemble: Sample several subsets from the majority class, build a classifier on top of each sampled data, and combine the output of all classifiers. More details can be found here.\nSynthetic Minority Oversampling Technique (SMOTE): It over-samples the minority class but using synthesized examples. It operates on feature space not the data space. Here how it works:\n\nCompute the k-nearest neighbors for all minority samples.\nRandomly choose number between 1-k.\nFor each feature:\n\nCompute the difference between minority sample and its randomly chosen neighbor (from previous step).\nMultiply the difference by random number between 0 and 1.\nAdd the obtained feature to the synthesized sample attributes.\n\nRepeat the above until we get the number of synthesized samples needed. More information can be found here.\n\n\nThere are other methods such as EditedNearestNeighbors and CondensedNearestNeighbors that we will not cover in this notebook and are rarely used in practice.\nIn most applications, misclassifying the minority class (false negative) is a lot more expensive than misclassifying the majority class (false positive). In the context of lending, loosing money by lending to a risky borrower who is more likely to not fully pay the loan back is a lot more costly than missing the opportunity of lending to trust-worthy borrower (less risky). As a result, we can use class_weight that changes the weight of misclassifying positive example in the loss function. Also, we can use different cut-offs assign examples to classes. By default, 0.5 is the cut-off; however, we see more often in applications such as lending that the cut-off is less than 0.5. Note that changing the cut-off from the default 0.5 reduce the overall accuracy but may improve the accuracy of predicting positive/negative examples.\nWe’ll evaluate all the above methods plus the original model without resampling as a baseline model using the same Random Forest classifier we used in the missing values section.\n\n\nCode\n# Build random forest classifier (same config)\nrf_clf = RandomForestClassifier(n_estimators=500,\n                                max_features=0.25,\n                                criterion=\"entropy\",\n                                class_weight=\"balanced\")\n\n# Build model with no sampling\npip_orig = make_pipeline(Imputer(strategy=\"mean\"),\n                         RobustScaler(),\n                         rf_clf)\nscores = cross_val_score(pip_orig,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mOriginal model's average AUC: {scores.mean():.3f}\")\n\n# Build model with undersampling\npip_undersample = imb_make_pipeline(Imputer(strategy=\"mean\"),\n                                    RobustScaler(),\n                                    RandomUnderSampler(), rf_clf)\nscores = cross_val_score(pip_undersample,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mUnder-sampled model's average AUC: {scores.mean():.3f}\")\n\n# Build model with oversampling\npip_oversample = imb_make_pipeline(Imputer(strategy=\"mean\"),\n                                    RobustScaler(),\n                                    RandomOverSampler(), rf_clf)\nscores = cross_val_score(pip_oversample,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mOver-sampled model's average AUC: {scores.mean():.3f}\")\n\n# Build model with EasyEnsemble\nresampled_rf = BalancedBaggingClassifier(base_estimator=rf_clf,\n                                         n_estimators=10, random_state=123)\npip_resampled = make_pipeline(Imputer(strategy=\"mean\"),\n                              RobustScaler(), resampled_rf)\n                             \nscores = cross_val_score(pip_resampled,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mEasyEnsemble model's average AUC: {scores.mean():.3f}\")\n\n# Build model with SMOTE\npip_smote = imb_make_pipeline(Imputer(strategy=\"mean\"),\n                              RobustScaler(),\n                              SMOTE(), rf_clf)\nscores = cross_val_score(pip_smote,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mSMOTE model's average AUC: {scores.mean():.3f}\")\n\n\nOriginal model's average AUC: 0.652\nUnder-sampled model's average AUC: 0.656\nOver-sampled model's average AUC: 0.651\nEasyEnsemble model's average AUC: 0.665\nSMOTE model's average AUC: 0.641\n\n\nEasyEnsemble method has the highest 10-folds CV with average AUC = 0.665.\n\n\nBuild Ensemble methods\nWe’ll build ensemble models using three different models as base learners: - Extra Gradient Boosting - Support Vector Classifier - Random Forest\nThe ensemble models will be built using two different methods: - Blending (average) ensemble model. Fits the base learners to the training data and then, at test time, average the predictions generated by all the base learners. - Use VotingClassifier from sklearn that: - fit all the base learners on the training data - at test time, use all base learners to predict test data and then take the average of all predictions. - Stacked ensemble model: Fits the base learners to the training data. Next, use those trained base learners to generate predictions (meta-features) used by the meta-learner (assuming we have only one layer of base learners). There are few different ways of training stacked ensemble model: - Fitting the base learners to all training data and then generate predictions using the same training data it was used to fit those learners. This method is more prune to overfitting because the meta learner will give more weights to the base learner who memorized the training data better, i.e. meta-learner won’t generate well and would overfit. - Split the training data into 2 to 3 different parts that will be used for training, validation, and generate predictions. It’s a suboptimal method because held out sets usually have higher variance and different splits give different results as well as learning algorithms would have fewer data to train. - Use k-folds cross validation where we split the data into k-folds. We fit the base learners to the (k - 1) folds and use the fitted models to generate predictions of the held out fold. We repeat the process until we generate the predictions for all the k-folds. When done, refit the base learners to the full training data. This method is more reliable and will give models that memorize the data less weight. Therefore, it generalizes better on future data.\nWe’ll use logistic regression as the meta-learner for the stacked model. Note that we can use k-folds cross validation to validate and tune the hyperparameters of the meta learner. We will not tune the hyperparameters of any of the base learners or the meta-learner; however, we will use some of the values recommended by the Pennsylvania Benchmarking Paper. Additionally, we won’t use EasyEnsemble in training because, after some experimentation, it didn’t improve the AUC of the ensemble model more than 2% on average and it was computationally very expensive. In practice, we sometimes are willing to give up small improvements if the model would become a lot more complex computationally. Therefore, we will use RandomUnderSampler. Also, we’ll impute the missing values and standardize the data beforehand so that it would shorten the code of the ensemble models and allows use to avoid using Pipeline. Additionally, we will plot ROC and PR curves using test data and evaluate the performance of all models.\n\n\nCode\n# Impute the missing data using features means\nimp = Imputer()\nimp.fit(X_train)\nX_train = imp.transform(X_train)\nX_test = imp.transform(X_test)\n\n# Standardize the data\nstd = RobustScaler()\nstd.fit(X_train)\nX_train = std.transform(X_train)\nX_test = std.transform(X_test)\n\n# Implement RandomUnderSampler\nrandom_undersampler = RandomUnderSampler()\nX_res, y_res = random_undersampler.fit_sample(X_train, y_train)\n# Shuffle the data\nperms = np.random.permutation(X_res.shape[0])\nX_res = X_res[perms]\ny_res = y_res[perms]\nX_res.shape, y_res.shape\n\n\n((2452, 18), (2452,))\n\n\n\n\nCode\n# Define base learners\nxgb_clf = xgb.XGBClassifier(objective=\"binary:logistic\",\n                            learning_rate=0.03,\n                            n_estimators=500,\n                            max_depth=1,\n                            subsample=0.4,\n                            random_state=123)\n\nsvm_clf = SVC(gamma=0.1,\n                C=0.01,\n                kernel=\"poly\",\n                degree=3,\n                coef0=10.0,\n                probability=True)\n\nrf_clf = RandomForestClassifier(n_estimators=300,\n                                max_features=\"sqrt\",\n                                criterion=\"gini\",\n                                min_samples_leaf=5,\n                                class_weight=\"balanced\")\n\n# Define meta-learner\nlogreg_clf = LogisticRegression(penalty=\"l2\",\n                                C=100,\n                                fit_intercept=True)\n\n# Fitting voting clf --> average ensemble\nvoting_clf = VotingClassifier([(\"xgb\", xgb_clf),\n                               (\"svm\", svm_clf),\n                               (\"rf\", rf_clf)],\n                              voting=\"soft\",\n                              flatten_transform=True)\nvoting_clf.fit(X_res, y_res)\nxgb_model, svm_model, rf_model = voting_clf.estimators_\nmodels = {\"xgb\": xgb_model, \"svm\": svm_model,\n          \"rf\": rf_model, \"avg_ensemble\": voting_clf}\n\n# Build first stack of base learners\nfirst_stack = make_pipeline(voting_clf,\n                            FunctionTransformer(lambda X: X[:, 1::2]))\n# Use CV to generate meta-features\nmeta_features = cross_val_predict(first_stack,\n                                  X_res, y_res,\n                                  cv=10,\n                                  method=\"transform\")\n# Refit the first stack on the full training set\nfirst_stack.fit(X_res, y_res)\n# Fit the meta learner\nsecond_stack = logreg_clf.fit(meta_features, y_res)\n\n# Plot ROC and PR curves using all models and test data\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nfor name, model in models.items():\n            model_probs = model.predict_proba(X_test)[:, 1:]\n            model_auc_score = roc_auc_score(y_test, model_probs)\n            fpr, tpr, _ = roc_curve(y_test, model_probs)\n            precision, recall, _ = precision_recall_curve(y_test, model_probs)\n            axes[0].plot(fpr, tpr, label=f\"{name}, auc = {model_auc_score:.3f}\")\n            axes[1].plot(recall, precision, label=f\"{name}\")\nstacked_probs = second_stack.predict_proba(first_stack.transform(X_test))[:, 1:]\nstacked_auc_score = roc_auc_score(y_test, stacked_probs)\nfpr, tpr, _ = roc_curve(y_test, stacked_probs)\nprecision, recall, _ = precision_recall_curve(y_test, stacked_probs)\naxes[0].plot(fpr, tpr, label=f\"stacked_ensemble, auc = {stacked_auc_score:.3f}\")\naxes[1].plot(recall, precision, label=\"stacked_ensembe\")\naxes[0].legend(loc=\"lower right\")\naxes[0].set_xlabel(\"FPR\")\naxes[0].set_ylabel(\"TPR\")\naxes[0].set_title(\"ROC curve\")\naxes[1].legend()\naxes[1].set_xlabel(\"recall\")\naxes[1].set_ylabel(\"precision\")\naxes[1].set_title(\"PR curve\")\nplt.tight_layout()\n\n\n\n\n\nAs we can see from the chart above, stacked ensemble model didn’t improve the performance. One of the major reasons are that the base learners are considerably highly correlated especially Random Forest and Gradient Boosting (see the correlation matrix below).\n\n\nCode\n# Plot the correlation between base learners\nprobs_df = pd.DataFrame(meta_features, columns=[\"xgb\", \"svm\", \"rf\"])\ncorrmat(probs_df.corr(), inflate=True);\n\n\n\n\n\nIn addition, with classification problems where False Negatives are a lot more expensive than False Positives, we may want to have a model with a high precision rather than high recall, i.e. the probability of the model to identify positive examples from randomly selected examples. Below is the confusion matrix:\n\n\nCode\nsecond_stack_probs = second_stack.predict_proba(first_stack.transform(X_test))\nsecond_stack_preds = second_stack.predict(first_stack.transform(X_test))\nconf_mat = confusion_matrix(y_test, second_stack_preds)\n# Define figure size and figure ratios\nplt.figure(figsize=(16, 8))\nplt.matshow(conf_mat, cmap=plt.cm.Reds, alpha=0.2)\nfor i in range(2):\n    for j in range(2):\n        plt.text(x=j, y=i, s=conf_mat[i, j], ha=\"center\", va=\"center\")\nplt.title(\"Confusion matrix\", y=1.1, fontdict={\"fontsize\": 20})\nplt.xlabel(\"Predicted\", fontdict={\"fontsize\": 14})\nplt.ylabel(\"Actual\", fontdict={\"fontsize\": 14});\n\n\n<matplotlib.figure.Figure at 0x12eb8e320>\n\n\n\n\n\nLet’s finally check the partial dependence plots to see what are the most important features and their relationships with whether the borrower will most likely pay the loan in full before mature data. we will plot only the top 8 features to make it easier to read.\n\n\nCode\n# Plot partial dependence plots\ngbrt = GradientBoostingClassifier(loss=\"deviance\",\n                                  learning_rate=0.1,\n                                  n_estimators=100,\n                                  max_depth=3,\n                                  random_state=123)\ngbrt.fit(X_res, y_res)\nfig, axes = plot_partial_dependence(gbrt, X_res,\n                                    np.argsort(gbrt.feature_importances_)[::-1][:8],\n                                    n_cols=4,\n                                    feature_names=df.columns[:-6],\n                                    figsize=(14, 8))\nplt.subplots_adjust(top=0.9)\nplt.suptitle(\"Partial dependence plots of borrower not fully paid\\n\"\n             \"the loan based on top most influential features\")\nfor ax in axes: ax.set_xticks(())\nfor ax in [axes[0], axes[4]]: ax.set_ylabel(\"Partial dependence\")\n\n\n\n\n\nAs we might expected, borrowers with lower annual income and less FICO scores are less likely to pay the loan fully; however, borrowers with lower interest rates (riskier) and smaller installments are more likely to pay the loan fully."
  },
  {
    "objectID": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#conclusion",
    "href": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#conclusion",
    "title": "Predicting Loan Repayment",
    "section": "Conclusion",
    "text": "Conclusion\nMost classification problems in the real world are imbalanced. Also, almost always data sets have missing values. In this notebook, we covered strategies to deal with both missing values and imbalanced data sets. We also explored different ways of building ensembles in sklearn. Below are some takeaway points:\n\nThere is no definitive guide of which algorithms to use given any situation. What may work on some data sets may not necessarily work on others. Therefore, always evaluate methods using cross validation to get a reliable estimates.\nSometimes we may be willing to give up some improvement to the model if that would increase the complexity much more than the percentage change in the improvement to the evaluation metrics.\nIn some classification problems, False Negatives are a lot more expensive than False Positives. Therefore, we can reduce cut-off points to reduce the False Negatives.\nWhen building ensemble models, try to use good models that are as different as possible to reduce correlation between the base learners. We could’ve enhanced our stacked ensemble model by adding Dense Neural Network and some other kind of base learners as well as adding more layers to the stacked model.\nEasyEnsemble usually performs better than any other resampling methods.\nMissing values sometimes add more information to the model than we might expect. One way of capturing it is to add binary features for each feature that has missing values to check if each example is missing or not."
  },
  {
    "objectID": "posts/anomaly-detection/Anomaly-Detection.html#introduction",
    "href": "posts/anomaly-detection/Anomaly-Detection.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly Detection is the identification of examples or events that don’t confront to an expected pattern or the majority of examples. Roughly speaking, it’s the process of identifying an example that is not normal (outlier) given the distribution of the data. Outlier is an example that deviates so much from the other examples that arouse suspicions that it was generated by different data generating process. Mainly, such outliers would have a very low probability (on the very end of both left and right tails of the probability density function) that they belong to the same data generating process.\nThe algorithm works as follows: 1. Fit a Gaussian Probability Density Function (PDF) for each feature in the training dataset. 1. Calculate the mean and the variance of each feature: \\[\\mu_j = \\frac{1}{m}\\sum_{i = 1}^mx_j^i\\\\{}\\] \\[\\sigma^2_j = \\frac{1}{m}\\sum_{i = 1}^m(x_j^i - \\mu_j)^2\\\\{}\\] Where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance that controls the shape of the density function. 2. Compute the density function for each feature using the following formula:\n\\[p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\\\\{}\\] Since the mean and the variance are sensitive to outliers, we use training dataset that has only normal examples to fit the model and calculate both the mean vector and the covariance matrix. 2. Compute the gaussian density by taking the product of all features’ density functions. 3. If \\(p(x) < \\epsilon\\) then anomaly; otherwise, normal. Epsilon controls how sensitive the detection algorithm is. If \\(\\epsilon\\) is large \\(\\rightarrow\\) flag a lot of the examples as anomalous and that would increase the False Positives. However, If \\(\\epsilon\\) is small \\(\\rightarrow\\) very small portion of the examples will be flagged as anomalous and that would increase the False Negatives. 4. Use Cross Validation for tuning the hyper-parameter \\(\\epsilon\\) that yields the best performance metrics value. F1 score is commonly used: \\[F_1 = 2 \\frac{precision * recall}{precision + recall}\\\\{}\\] Where:\\[precision = \\frac{tp}{tp + fp}\\\\{}\\] \\[recall = \\frac{tp}{tp + fn}\\\\{}\\] tp: True Positive, fp: False Positive, fn: False Negative.\nWe have two kinds of anomaly detection algorithms: 1. Univariate Gaussian Density Function \\[p(x) = \\prod_{j = 1}^{n}p(x_j; \\mu_j, \\sigma_j^2)\\\\{}\\] \\[ = p(x_1; \\mu_1, \\sigma_1^2)*p(x_2; \\mu_2, \\sigma_2^2)* ... * p(x_n; \\mu_n, \\sigma_j^n)\\\\{}\\] * It assumes that all features are independent. Therefore, the covariance between all pairs of features is zero. * It’s computationally faster and more efficient. * Use it if we have very large number of features. * Make sure to add features manually that captures unusual values for combination of features; such as \\(x_3 = \\frac {x_2}{x_1}\\). Otherwise, the algorithm may fail to detect anomalies that takes values that are considered normal when looked at each feature separately but are unusual when looking at values of all features together such as having high value for feature 2 compared to low value for feature 1.\n\nMultivariate Gaussian Density Function \\[p(x) = \\prod_{j = 1}^{n}p(x_j; \\mu_j, \\sigma_j^2)\\\\{}\\] \\[p(x; \\mu, \\sigma^2) = \\frac{1}{(2\\pi)^{(n / 2)}(\\det\\sum)^{1 / 2}}e^{\\frac{-1}{2}(x - \\mu)^T\\sum^{-1}(x - \\mu)}\\\\{}\\] Where \\(\\sum\\) is n x n covariance matrix: \\[\\sum = \\begin{bmatrix}\n\\sigma_1^2&\\sigma_{12}&\\cdots&\\sigma_{1n}\\\\\n\\sigma_{21}&\\sigma_2^2&\\cdots&0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{n1} & 0 & 0 & \\sigma_n^2\n\\end{bmatrix}\\] Where \\(\\sigma_{12} = \\sigma_{21}\\) is the covariance between features 1&2. Therefore, the covariance matrix is symmetric positive (semi) definite.\n\nComputationally expensive\nUse it when number of examples \\(\\geq\\) 10 times number of features, i.e. \\(m \\geq 10n\\)\nIf some features are linearly dependent or number of examples is less than number of features \\(\\rightarrow\\) covariance matrix won’t be invertible\nNo need to add more features to capture unusual values of combination of features because it captures that through covariances of all pairs of features\nUnivariate density function can be derived from Multivariate density function where covariance matrix would be a diagonal matrix. Therefore, \\(\\sigma_{ij} = 0\\) for all \\(i \\neq j\\)\n\n\nThere are some assumptions made implicitly here: - For each feature, \\(X_i\\)’s are IID (independently and identically distributed). - Using Central Theorem (CLT): the distribution of sum of iid random variable are approximately normal. Therefore, this would allow us to fit normal distribution that’s parameterized by \\(\\mu\\) and \\(\\sigma^2\\). - \\(\\mu\\) and \\(\\sum\\) will be estimated using maximum-likelihood estimation method.\nWhen fitting multivariate probability distribution using the above assumptions, we’ll use that pdf to estimate the probability that each example from the validation/test set was generated by this pdf. If the probability is smaller that \\(\\epsilon\\), then we believe that such example was generated by different mutlivariate PDF and, therefor, classified as anomaly (outlier).\nIn this exercise, we’ll implement an anomaly detection algorithm to detect anomalous behavior in server computers. The features measure the throughput (mb/s) and latency (ms) of response of each server. While servers were operating, \\(m = 307\\) examples of how they were behaving were captured. We suspect that the vast majority of them are normal (non-anomalous) examples of the servers operating normally.\nLet’s first load and plot the data:\n\n\nCode\nimport numpy as np\nfrom numpy.linalg import pinv, det\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.io import loadmat, whosmat\nimport scipy.optimize as opt\nimport seaborn as sns\nfrom warnings import filterwarnings\n\n%matplotlib inline\nsns.set_context('notebook')\nplt.style.use('fivethirtyeight')\nfilterwarnings('ignore')"
  },
  {
    "objectID": "posts/anomaly-detection/Anomaly-Detection.html#functions",
    "href": "posts/anomaly-detection/Anomaly-Detection.html#functions",
    "title": "Anomaly Detection",
    "section": "Functions",
    "text": "Functions\n\n\nCode\n# Compute guassian distribution fn\ndef gaussian_estimate(X_train, X_val, gaussian_type='univariate'):\n    '''\n    parameters\n    ----------\n    X_train: array-like\n        training features matrix m x n that has only normal examples.\n    X_val: array-like\n        cross validation features matrix that has anomalous and normal\n        examples.\n    gussian_type: str\n        univariate or multivariate.\n    \n    Returns\n    -------\n    pdf: array-like\n        multivariate pdf vector of n x 1\n    '''\n    # number of training examples and features\n    m, n = X_train.shape\n    # number of cv examples\n    mval = X_val.shape[0]\n\n    # compute mean and covariance matrix\n    mu = X_train.mean(axis=0)\n    cov = (1 / (m)) * (X_train - mu).T.dot(X_train - mu)\n\n    # convert the covariance matrix to diagonal if it's a univariate\n    if gaussian_type == 'univariate':\n        z = np.zeros_like(cov)\n        np.fill_diagonal(z, np.diagonal(cov))\n        cov = z\n\n    # compute determinant and inverse of covariance matrix\n    cov_det = det(cov)\n    cov_inv = pinv(cov)\n\n    # compute pdf vector\n    pdf = ((2 * np.pi) ** (-n / 2)) * (cov_det ** (-0.5)) *\\\n        np.exp(-0.5 * np.sum(np.multiply((X_val - mu).dot(cov_inv),\n                                         (X_val - mu)), axis=1))\n\n    return pdf\n\n\n# Hyperparameter tuning of epsilon using cv dataset\ndef select_threshold(y_val, p_val):\n    '''\n    parameters\n    ----------\n    y_val: array-like\n        label whether a validation example is normal (0) or anomaly (1).\n    p_val: array-like\n        pdf for validated examples.\n    \n    Returns\n    -------\n    eplsion : float\n        best epsilon value tuned on validation data.\n    F1_score : float\n        F1 score using epsilon tuned on validation data.\n    '''\n    # initialize epsilon and F1 score values\n    best_epsilon = 0\n    best_F1 = 0\n\n    # compute stepsize for each iteration\n    epsilon_stepsize = (p_val.max() - p_val.min()) / 1000\n\n    for epsilon in np.arange(p_val.min(), p_val.max(), epsilon_stepsize):\n        # get predictions vector\n        pred = ((p_val < epsilon) * 1).reshape(-1, 1)\n\n        # compute true positives, false positives, false negatives\n        tp = np.sum((pred == 1) & (y_val == 1))\n        fp = np.sum((pred == 1) & (y_val == 0))\n        fn = np.sum((pred == 0) & (y_val == 1))\n\n        # compute precision and recall\n        precision_ = tp / (tp + fp)\n        recall_ = tp / (tp + fn)\n\n        # compute F1 score\n        F1 = 2 * ((precision_ * recall_) / (precision_ + recall_))\n        # if F1 score > best_F1, set best_F1 = F1\n        if F1 > best_F1:\n            best_F1 = F1\n            best_epsilon = epsilon\n\n    return best_epsilon, best_F1\n\n\n\n\nCode\n# Load data\ndata = loadmat('../data/servers_anomaly_detection.mat')\n\n# Training data\nX = data['X']\n\n# Cross validation data\nX_val = data['Xval']\ny_val = data['yval']\n\n# Plot data\nfig, ax = plt.subplots(figsize = (8, 8))\nplt.scatter(X[:, 0], X[:, 1], s = 50, c = 'blue')\nplt.axis([0, 30, 0, 30])\nplt.xlabel('Latency (ms)')\nplt.ylabel('Throughput (mb/s)')\nplt.gca().set_aspect('equal')\n# plt.title('Scatter plot of the first dataset');\n\n\n\n\n\n\n\nCode\n# plt.subplots(1, 2, 1)\nsns.kdeplot(X[:, 0])\nsns.kdeplot(X[:, 1])\n\n\n<AxesSubplot:ylabel='Density'>\n\n\n\n\n\nNow, we’ll first estimate the Gaussian distribution for both the training and cross validation sets. Note that we use training dataset that has ONLY normal examples when computing mean and covariance and then use cross validation that has both normal and anomalous examples to know the best epsilon.\n\n\nCode\n# Fit guassian distribution on both training and CV examples\nptrain = gaussian_estimate(X, X)\npval = gaussian_estimate(X, X_val, gaussian_type='multivariate')\n\n# Tune epsilon\nepsilon, F1 = select_threshold(y_val, pval)\nprint(f'The best epsilon tuned using CV that yielded the best' +\n      f'F1-score {F1:.3f} is: {epsilon}.')\n\n\nThe best epsilon tuned using CV that yielded the bestF1-score 0.875 is: 9.065769728392737e-05.\n\n\nWe’ll use the value of epsilon that we tuned using CV to see what examples were anomalous based on our algorithm. Below is the scatter plot of the training data where red points are anomalous examples.\n\n\nCode\n# Get the index of the outlier\noutliers = np.where(ptrain < epsilon)\n\n# Plot data\nfig, ax = plt.subplots(figsize=(10, 6))\nplt.scatter(X[:, 0], X[:, 1], s=50, c='blue', label='Normal Examples')\nplt.scatter(X[outliers[0], 0], X[outliers[0], 1], s=60, c='red', label='Anomalous Examples')\nplt.axis([0, 30, 0, 30])\nplt.xlabel('Latency (ms)')\nplt.ylabel('Throughput (mb/s)')\nplt.legend(loc='upper right')\nplt.title('Scatter plot of the training dataset');\n\n\n\n\n\nFinally, we’ll try to fit Gaussian distribution on training dataset that has 1000 examples and 11 features. Note that in both examples we used Multivariate not Univariate Gaussian distribution.\n\n\nCode\n# Load data\ndata = loadmat('../data/ex8data2.mat')\n\n# Training data\nX = data['X']\n\n# Cross validation data\nXval = data['Xval']\nyval = data['yval']\n\n# Fit guassian distribution on both training and CV examples\nptrain = gaussian_estimate(X, X, gaussian_type='multivariate')\npval = gaussian_estimate(X, Xval, gaussian_type='multivariate')\n\n# Tune epsilon\nepsilon, F1 = select_threshold(yval, pval)\nprint(f'The best epsilon tuned using CV that yielded the best' + \\\n      'F1-score {F1:.3f} is: {epsilon}.')\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/ex8data2.mat'\n\n\nUsing the best-epsilon value we got above, we can then classify any example as anomaly if \\(p(x) < \\epsilon\\); otherwise, it’s normal."
  },
  {
    "objectID": "posts/anomaly-detection/Anomaly-Detection.html#conclusion",
    "href": "posts/anomaly-detection/Anomaly-Detection.html#conclusion",
    "title": "Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe implementation of the variance/covariance in the detection algorithms has \\(m\\) in the denominator not \\((m - 1)\\) because with large datasets this doesn’t make a difference. However, the unbiased estimator of the variance should have \\((m - 1)\\) in the denominator not \\(m\\).\nAnomaly detection vs Supervised learning:\n\nUse Anomaly Detection when you have large number of negative examples and very small number of positive examples. The reason is because the supervised learning algorithm wouldn’t be able to have enough examples to learn about the scene especially if the future anomalies are nothing like training anomalies\nUse Supervised Learning algorithms such as logistic regression if you have enough positive examples that make the learning easy on the algorithm and probably it would outperform Anomaly Detection algorithms.\n\n\nUnivariate PDF performs well most of the times compared to Multivariate PDF and scale really well."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#introduction",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#introduction",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Introduction",
    "text": "Introduction\nA/B testing can be defined as a randomized controlled experiment that allows us to test if there is a causal relationship between a change to a website/app and the user behavior. The change can be visible such as location of a button on the homepage or invisible such as the ranking/recommendation algorithms and backend infrastructure.\nWeb/Mobile developers and business stakeholders always face the following dilemma: Should we try out all ideas and explore all options continuously? Or should we exploit the best available option and stick to it? The answer is, as in most cases, will be a trade-off between the two extremes. If we explore all the time, we’ll collect a lot of data and waste resources in testing inferior ideas and missing sales (e-commerce case). However, if we only exploit the available option and never try new ideas, we would be left behind and loose in the long-term with ever-changing markets.\nIn this series, we’ll explore solutions offered by Multi-armed Bandit Algorithms that have two main advantages over traditional A/B testing:\n\nSmoothly decrease exploration over time instead of sudden jumps.\nFocus resources on better options and not keep evaluating inferior options during the life of the experiment.\n\nWhat is Bandit Algorithms? Bandit Algorithms are algorithms that try to learn a rule of selecting a sequence of options that balance exploring available options and getting enough knowledge about each option and maximize profits by selecting the best option. Note that during the experiment, we only have knowledge about the options we tried. Therefore, every time we select an option that’s not the best one, we incur an opportunity cost of not selecting the best option; however, we also gain a new knowledge (feedback) about the selected option. In other words, we need to have enough feedback about each option to learn the best option. As a result, the best strategy would be to explore more at the beginning of the experiment until we know the best option and then start exploiting that option."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#epsilon-greedy-algorithm",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#epsilon-greedy-algorithm",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "epsilon-Greedy Algorithm",
    "text": "epsilon-Greedy Algorithm\nIn this notebook, we’ll cover epsilon-Greedy Algorithm. Greedy Algorithm can be defined as the algorithm that picks the best currently available option without taking into consideration the long-term effect of that decision, which may happen to be a suboptimal decision. Given that, we can define epsilon-Greedy Algorithm as a Greedy Algorithm that adds some randomness when deciding between options: Instead of picking always the best available option, randomly explore other options with a probability = \\(\\epsilon\\) or pick the best option with a probability = \\(1 - \\epsilon\\). Therefore, we can add randomness to the algorithm by increasing \\(\\epsilon\\), which will make the algorithm explores other options more frequently. Additionally, \\(\\epsilon\\) is a hyper-parameter that needs to be tuned based on the experiment, i.e. there is no value that works best on all experiments. Let’s explore how the algorithm works assuming we have two options: A and B (we can think of them as Control and Treatment groups). For each new user:\n\nAssume we have a coin that has a probability of coming heads = \\(\\epsilon\\) and a probability of coming tails = \\(1 - \\epsilon\\). Therefore,\n\nIf it comes heads, explore randomly the available options (exploration).\n\nThe probability of selecting any option is \\(\\frac{1}{2}\\).\n\nIf it comes tails, select the best option (exploitation).\n\n\nAs a result, the probability of selecting any option randomly if we have \\(N\\) options is \\(\\epsilon \\frac{1}{N}\\); however, the probability of selecting the best option is \\(1 - \\epsilon\\) (see figure 1).\n\n\n\nFigure 1: epsilon-Greedy Algorithm\n\n\nLet’s import the needed packages and implement the algorithm.\n\n\nCode\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Add module path to system path\nsys.path.append(os.path.abspath(\"../\"))\nfrom utils import plot_algorithm, compare_algorithms\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\n\n\n\nCode\nclass EpsilonGreedy:\n    def __init__(self, epsilon, counts=None, values=None):\n        self.epsilon = epsilon\n        self.counts = counts\n        self.values = values\n\n    def initialize(self, n_arms):\n        self.counts = np.zeros(n_arms, dtype=int)\n        self.values = np.zeros(n_arms, dtype=float)\n\n    def select_arm(self):\n        z = np.random.random()\n        if z > self.epsilon:\n            # Pick the best arm\n            return np.argmax(self.values)\n        # Randomly pick any arm with prob 1 / len(self.counts)\n        return np.random.randint(0, len(self.values))\n\n    def update(self, chosen_arm, reward):\n        # Increment chosen arm's count by one\n        self.counts[chosen_arm] += 1\n        n = self.counts[chosen_arm]\n\n        # Recompute the estimated value of chosen arm using new reward\n        value = self.values[chosen_arm]\n        new_value = value * ((n - 1) / n) + reward / n\n        self.values[chosen_arm] = new_value\n\n\nFew things to note from the above implementation: - Initialization of values (rewards) affect the long term performance of the algorithm. - The larger the sample size (N), the less influential the rewards from the recent options since we are using the average of each option in the values array. - Values array will store the estimated values (average) of each option. - Counts is just an internal counter that keeps track of the number of times we selected each option."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#monte-carlo-simulations",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#monte-carlo-simulations",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nIn order to evaluate the algorithm, we will use Monte Carlo simulations. We’ll use 5000 simulations to overcome the randomness generated from the random number generator. Also, we’ll use Bernoulli distribution to get the reward from each option on each run. For each simulation: - Initialize the algorithm with no prior knowledge. - Loop over the time horizon: - Select the option. - Draw the reward for the selected option using Bernoulli distribution and the probability defined. - Update the counts and estimated values of selected arm.\nWe’ll define the % of reward (probability) of each option and test the performance of the algorithm using three different metrics:\n\nProbability of selecting the best option.\nAverage rewards. This metric is a better approximation if the options are similar.\nCumulative rewards. The previous two metrics are not fair metrics for algorithms with large epsilon where they sacrifice by exploring more options; however, cumulative rewards is what we should care about.\n\nMoreover, we’ll evaluate the algorithm using 5 different values of \\(\\epsilon\\): \\(0.1, 0.2, 0.3, 0.4, 0.5\\). Since in the literature they use arm instead of option for historical reasons, we’ll be using arm and option interchangeably.\n\n\nCode\nclass BernoulliArm:\n    def __init__(self, p):\n        self.p = p\n\n    def draw(self):\n        z = np.random.random()\n        if z > self.p:\n            return 0.0\n        return 1.0\n\n\ndef test_algorithm(algo, arms, num_simulations, horizon):\n    # Initialize rewards and chosen_arms with zero 2d arrays\n    chosen_arms = np.zeros((num_simulations, horizon))\n    rewards = np.zeros((num_simulations, horizon))\n\n    # Loop over all simulations\n    for sim in range(num_simulations):\n        # Re-initialize algorithm's counts and values arrays\n        algo.initialize(len(arms))\n\n        # Loop over all time horizon\n        for t in range(horizon):\n            # Select arm\n            chosen_arm = algo.select_arm()\n            chosen_arms[sim, t] = chosen_arm\n\n            # Draw from Bernoulli distribution to get rewards\n            reward = arms[chosen_arm].draw()\n            rewards[sim, t] = reward\n\n            # Update the algorithms' count and estimated values\n            algo.update(chosen_arm, reward)\n\n    # Average rewards across all sims and compute cumulative rewards\n    average_rewards = np.mean(rewards, axis=0)\n    cumulative_rewards = np.cumsum(average_rewards)\n\n    return chosen_arms, average_rewards, cumulative_rewards\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.1, 0.1, 0.1, 0.1, 0.9]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=500, label=\"eps\")\n\n\n\n\n\nFew thing to note from the above graphs: - Regardless of the epsilon values, all algorithms learned the best option. - The algorithm picks options randomly; therefore, it’s not guaranteed to always pick the best option even if it found that option. That’s the main reason why none of the algorithms achieved a probability = 1 of selecting the best option or average rewards = % rewards of the best option even after they learned the best option. - As \\(\\epsilon\\) increases \\(\\rightarrow\\) increase the exploration \\(\\rightarrow\\) increases the chance of picking options randomly instead of the best option. - Algorithms with higher epsilon learn quicker but don’t use that knowledge in exploiting the best option. - Using accuracy in picking the best option and average rewards metrics, the algorithm \\(\\epsilon = 0.1\\) outperforms the rest; however, cumulative rewards metric shows that it takes that algorithm long time to outperform the algorithm with \\(\\epsilon = 0.2\\). - Depends on time planned to run the experiment, different values of epsilons may be more optimal. For example, \\(\\epsilon = 0.2\\) is the best value for almost anything at or below 400.\nLet’s run the experiment again to see how would the algorithm behave under the following settings: - Only two options. - 50 options. - 5 option that are very similar.\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.1, 0.9]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=500, label=\"eps\")\n\n\n\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [i for i in np.random.random(50)]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=250, label=\"eps\")\n\n\n\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.2, 0.18, 0.22, 0.19, 0.21]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=500, label=\"eps\")\n\n\n\n\n\n\nWhen we had lower number of options, all algorithms were faster at learning the best option which can be seen by the steepness of all curves of the first two graphs when time < 100. As a result, all algorithms had higher cumulative rewards than when we had 5 options.\nHaving large number of options made it hard on all algorithms to learn the best option and may need a lot more time to figure it out.\nLastly, when options are very similar (in terms of rewards), the probability of selecting the best option by all algorithms decreases over time. Let’s take the algorithm with \\(\\epsilon = 0.1\\) and see why is this the case. After some investigation, the algorithm was struggling in differentiating between the best option and the second best option since the difference between the % rewards is 1%. Therefore, the probability of selecting the best arm was around 50%."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#annealed-epsilon-greedy-algorithm",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#annealed-epsilon-greedy-algorithm",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Annealed epsilon-Greedy Algorithm",
    "text": "Annealed epsilon-Greedy Algorithm\nEpsilon value plays a major role in the performance of epsilon-Greedy algorithm and has to be tuned to the best of our knowledge in terms of the expectations of the estimated rewards of each option. Nonetheless, this estimation suffers from high uncertainty since most of the times either we have no clue what might work or the results would be against our intuition as user experience research has shown in multiple studies. Therefore, isn’t it nice if we can avoid setting up the epsilon values and make the algorithm parameter-free? That’s what Annealed epsilon-Greedy Algorithm does. We specify the rule of decaying epsilon with time and let the algorithm runs with no hyper-parameter configurations. The rule of we will use here is: \\(\\epsilon = \\frac{1}{log(time + 0.0000001)}\\). As we can see, at the beginning of the experiment, \\(\\epsilon\\) would be close to Inf and that means a lot of exploration; however, as time goes, \\(\\epsilon\\) start approaching zero and the algorithm would exploit more and more by selecting the best option.\nWe will evaluate the Annealed version using the same settings as before and compare it to standard version.\n\n\nCode\nclass AnnealingEpsilonGreedy(EpsilonGreedy):\n    def __init__(self, counts=None, values=None):\n        self.counts = counts\n        self.values = values\n\n    def select_arm(self):\n        # Epsilon decay schedule\n        t = np.sum(self.counts) + 1\n        epsilon = 1 / np.log(t + 0.0000001)\n\n        z = np.random.random()\n        if z > epsilon:\n            # Pick the best arm\n            return np.argmax(self.values)\n        # Randomly pick any arm with prob 1 / len(self.counts)\n        return np.random.randint(0, len(self.values))\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.1, 0.1, 0.1, 0.1, 0.9]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"Annealing epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               num_simulations=5000, horizon=500)\n\n\n\n\n\nEven though the accuracy of selecting the best option and the average rewards of the annealing epsilon-Greedy Algorithm is lower than the standard version, it has higher cumulative rewards. Also, since the real world is uncertain and we may not have any clue about the designed options, it may be preferred to use the annealing version under some scenarios."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#conclusion",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#conclusion",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Conclusion",
    "text": "Conclusion\nepsilon-Greedy Algorithm works by going back and forth between exploration with probability = \\(\\epsilon\\) and exploitation with probability \\(1 - \\epsilon\\). Below are some takeaways:\n\nSetting the value of epsilon:\n\nIf we set \\(\\epsilon = 1\\), we would only explore the available options with a probability = \\(\\frac{1}{N}\\) of selecting any option. This will enable us to explore a lot of ideas at the expense of wasting resources by evaluating inferior options.\nIf we set \\(\\epsilon = 0\\), we would exploit the best option and never explore any new idea. This strategy would leave up behind our competitors given that the markets are so volatile.\n\nExploration should be high at the beginning of the experiment to gain the knowledge about all the available options. It should decrease as a function of time where at some point after having enough data about all options, the algorithm should focus on exploiting the best option.\nAll algorithms with different epsilon values learned the best option; however, they differ by the level of randomness of each algorithm in keep randomly exploring available options.\nTo get the best results of any Bandit algorithm, we should have a lot of data, which means to run the experiment longer in most cases.\nFor experiments that run for short period of time, traditional A/B testing may be better.\nInitialization of estimated rewards can affect the long-term performance of the algorithm. As a result, we may need to use previous experience and intuition to guide our initial values."
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#bias-variance-trade-off",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#bias-variance-trade-off",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\nGeneralization (test) error is the most important metric in Machine/Deep Learning. It gives us an estimate on the performance of the model on unseen data. Test error is decomposed into 3 parts (see above figure): Variance, Squared-Bias, and Irreducible Error. Models with high bias are not complex enough (too simple) for the data and tend to underfit. The simplest model is taking the average (mode) of target variable and assign it to all predictions. On the contrary, models with high variance overfit the training data by closely follow (mimick) the training data where the learning algorithm will follow the signal and the noise. Note that as the complexity (flexibility) of the model increases → the model will become less interpretable such as Neural Networks. Below is the bias-variance decomposition: \\[MSE = E(y - \\widehat{y})^2\\\\{}\\] \\[= E(y - f + f - \\widehat{y})^2\\\\{}\\] \\[= E\\big\\{(y - f)^2 + 2(y - f)(f - \\widehat{y}) + (f - \\widehat{y})^2\\big\\};\\quad substitute\\ y = f + \\epsilon\\\\{}\\] \\[= E\\big\\{(\\epsilon + f - f)^2 + 2(\\epsilon + f - f)(f - \\widehat{y}) + (f - \\widehat{y})^2\\big\\}\\\\{}\\] \\[= E(\\epsilon)^2 + E(\\epsilon)E(f - \\widehat{y}) + E(f - \\widehat{y})^2; \\quad where\\ E(\\epsilon) = 0\\\\{}\\] \\[= E(\\epsilon)^2 + E(f - \\widehat{y})^2;\\quad add\\ and\\ subtract\\ E(\\widehat{y})\\\\{}\\] \\[= E(\\epsilon)^2 + E(f - E(\\widehat{y}) + E(\\widehat{y}) - \\widehat{y})^2\\\\{}\\] \\[= E(\\epsilon)^2 + E(f - E(\\widehat{y}))^2 + E(\\widehat{y} - E(\\widehat{y}))^2\\\\{}\\] \\[\\Rightarrow MSE = var(\\widehat{y}) + (Bias(\\widehat{y}))^2 + var(\\epsilon)\\\\{}\\] Where:\n\n\\(var(\\epsilon)\\): Irreducible error that resulted from omitted features and unmeasured variation with each example.\n\\(Bias(\\widehat{y})\\): Error that is introduced by approximating a real-life problem with a simple model.\n\\(var(\\widehat{y})\\): amount by which \\(\\widehat{y}\\) would change if we estimated it using different data set.\n\nTherefore, we can control only the variance and the bias of the \\(\\widehat{y}\\) BUT NOT irreducible error. As a result, our job is to try to estimate the right level of complexity to achieve the lowest test error."
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#regularization",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#regularization",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "Regularization",
    "text": "Regularization\nRegularization adds stability to the learning algorithm by making it less sensitive to the training data and processes. Since we don’t know and have no access to the true function that we can use to compare our estimated function with it, the best strategy would be to build a very complex model that fits the training data really well (overfitting) and regularize it so that it would have a good generalization (test) error. When using regularization, we try to reduce the generalization error and that may lead to increase the training error in the process which is okay because what we care about is how well the model generalizes. With regularization, we try to bring back the very complex model that suffers from overfitting to a good model by increasing bias and reducing variance. This builds on the assumption that complex model has large parameters and simple model has small parameters.\nBelow are some methods used for regularization:\n\nL2 Parameter Regularization: It’s also known as weight decay. This method adds L2 norm penalty to the objective function to drive the weights towards the origin. Even though this method shrinks all weights by the same proportion towards zero; however, it will never make any weight to be exactly zero.\nL1 Parameter Regularization (Lasso): It can be seen as a feature selection method because; in contrast to L2 regularization, some weights will be actually zero. It shrinks all weights by the same amount by adding L1 norm penalty to the objective function.\nDropout: Dropout can be seen as an approximation to bagging techniques. On each iteration, we randomly shut down some neurons on each layer and don’t use those neurons in both forward propagation and back-propagation. This will force the neural network to spread out weights and not focus on specific neurons because it will never know which neurons will show up on each iteration. Therefore, it can be seen as training different model on each iteration. Also, since we drop some neurons on each iteration, this will lead to smaller network which in turns means simpler network.\nAugmentation: Add fake data by using the training examples and adding distortions to them such as rescaling and rotating the images in the case of image recognition. The idea here is that it’s always better to train the model on more data to achieve better performance. Note that augmented examples don’t add much information to the model as much as independent examples do but still it’s a valid alternative when collecting more data is not feasible.\nEarly Stopping: This method tries to optimize the cost function and regularize it so that it would have lower generalization error. The way it works is that on each iteration we record the validation error. If the validation error improves, we store a copy of the parameters and will continue until the optimization algorithm terminates. It’s a good method if computational time and resources is an issue for us.\n\nIn this post, we’ll cover L2 parameter regularization."
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#l2-parameter-regularization",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#l2-parameter-regularization",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "L2 Parameter Regularization",
    "text": "L2 Parameter Regularization\nWe normally don’t regularize bias and regularize weights only. We can use hessian matrix and it’s eigenvalues and eigenvectors to see the sensitivity of the weights to the weight decay. The weight \\(w_i\\) will be rescaled using \\(\\frac{\\lambda_i}{\\lambda_i + \\alpha}\\) where \\(\\lambda_i\\) (eigenvalue) measures the sensitivity of hessian matrix in that direction (eigenvector) and \\(\\alpha\\) is the regularized hyperparameter. Therefore,\n\nIf \\(\\lambda_i >> \\alpha\\), the cost function is very sensitive in that direction and the corresponding weight reduces the cost significantly \\(\\Rightarrow\\) don’t decay (shrink) much.\nIf \\(\\lambda_i << \\alpha\\), the cost function is not sensitive in that direction and the corresponding weight doesn’t reduce the cost significantly \\(\\Rightarrow\\) decay (shrink) away towards zero.\n\nThe objective function (binary cross-entropy) would then change from: \\[J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}\\] To: \\[J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{binary cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_{l=1}^L\\sum\\limits_{i=1}^{n^l}\\sum\\limits_{j=1}^{n^{l-1}} W_{j,i}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}\\] Also, the new gradients and the update equation would be: \\[\\nabla_w J_{regularized} = \\nabla_w J + \\frac{\\lambda}{m}w\\\\{}\\] \\[w = w - \\alpha\\nabla_w J - \\alpha\\frac{\\lambda}{m}w\\\\{}\\] \\[\\Rightarrow w = w\\underbrace{(1 - \\alpha\\frac{\\lambda}{m})}_\\text{weight decay} - \\nabla J\\] Note that here \\(\\alpha\\) is the learning rate and \\(\\lambda\\) is the regularized hyperparameter. As \\(\\lambda\\) increases, the bias increases (and the model becomes less flexible) with the following extreme cases (see figure 1):\n\n\\(\\lambda = 0\\), no regularization.\n\\(\\lambda \\rightarrow \\infty\\), model becomes very simple where all weights are essentially zero. In the case of regression, we would end-up with the intercept only which is equal to the average of the target variable.\n\n\n\n\nFigure 1: Model complexity (underfitting/overfitting) as a function of regularization parameter\n\n\nIt sometimes maybe helpful to see how L2 parameter regularization works using normal equation. The normal quation is: \\[W = (X^TX + \\lambda I)^{-1}X^TY\\tag{3}\\] This means that:\n\nAdding \\(\\lambda\\) to the variance would decrease the weight since \\(w_i = \\frac{cov_{x, y}}{\\sigma^2_x}\\).\nEven if \\(X^TX\\) is not invertible, adding \\(\\lambda\\) to each feature will make it full rank matrix \\(\\Rightarrow\\) invertible.\n\nTo illustrate how regularization helps us reduce generalization error, we’ll use the cats_vs_dogs dataset. The dataset has images for cats and dogs. We’ll try to build a neural network to classify if the image has a cat or a dog. Each image is 64 x 64 pixels on RGB scale.\nWe’ll be using functions we wrote in “Coding Neural Network - Forward and Backward Propagation” post to initialize parameters, compute forward propagation, cross-entropy cost, gradients, etc.\nLet’s import the data and take a look at the shape as well as a sample of a cat image from the training set.\n\n\nCode\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    initialize_parameters,\n    L_model_forward,\n    compute_cost,\n    relu_gradient,\n    sigmoid_gradient,\n    tanh_gradient,\n    update_parameters,\n    accuracy,\n)\nfrom gradient_checking import dictionary_to_vector\nfrom load_dataset import load_dataset_catvsdog\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"figure.figsize\"] = (12, 6)\n\n\n\n\nCode\n# Import training data\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"])\nY_train = np.array(train_dataset[\"train_set_y\"])\n\n# Plot a sample image\nplt.imshow(X_train[50])\nplt.axis(\"off\")\n\n# Import test data\ntest_dataset = h5py.File(\"../../data/test_catvnoncat.h5\")\nX_test = np.array(test_dataset[\"test_set_x\"])\nY_test = np.array(test_dataset[\"test_set_y\"])\n\n# Transform data\nX_train = X_train.reshape(209, -1).T\nX_train = X_train / 255\nY_train = Y_train.reshape(-1, 209)\n\nX_test = X_test.reshape(50, -1).T\nX_test = X_test / 255\nY_test = Y_test.reshape(-1, 50)\n\n# print the new shape of both training and test datasets\nprint(\"Training data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_train.shape, Y_train.shape))\nprint(\"Test data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_test.shape, Y_test.shape))\n\n\nTraining data dimensions:\nX's dimension: (12288, 209), Y's dimension: (1, 209)\nTest data dimensions:\nX's dimension: (12288, 50), Y's dimension: (1, 50)\n\n\n\n\n\nThe training set has 209 examples and the test set has 50 examples. Let’s first write all the helper functions that would help us write the multi-layer neural network.\n\n\nCode\ndef compute_cost_reg(AL, y, parameters, lambd=0):\n    \"\"\"\n    Computes the Cross-Entropy cost function with L2 regularization.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    y : 2d-array\n        true \"label\" vector.\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    cost : float\n        binary cross-entropy cost.\n    \"\"\"\n    # number of examples\n    m = y.shape[1]\n\n    # compute traditional cross entropy cost\n    cross_entropy_cost = compute_cost(AL, y)\n\n    # convert parameters dictionary to vector\n    parameters_vector = dictionary_to_vector(parameters)\n\n    # compute the regularization penalty\n    L2_regularization_penalty = (lambd / (2 * m)) * np.sum(np.square(parameters_vector))\n\n    # compute the total cost\n    cost = cross_entropy_cost + L2_regularization_penalty\n\n    return cost\n\n\ndef linear_backword_reg(dZ, cache, lambd=0):\n    \"\"\"\n    Computes the gradient of the output w.r.t weight, bias, & post-activation\n    output of (l - 1) layers at layer l.\n\n    Arguments\n    ---------\n    dZ : 2d-array\n        gradient of the cost w.r.t. the linear output (of current layer l).\n    cache : tuple\n        values of (A_prev, W, b) coming from the forward propagation in the\n        current layer.\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of the previous layer l-1).\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l).\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l).\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = (1 / m) * np.dot(dZ, A_prev.T) + (lambd / m) * W\n    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n\n    assert dA_prev.shape == A_prev.shape\n    assert dW.shape == W.shape\n    assert db.shape == b.shape\n\n    return dA_prev, dW, db\n\n\ndef linear_activation_backward_reg(dA, cache, activation_fn=\"relu\", lambd=0):\n    \"\"\"\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient for current layer l.\n    cache : tuple\n        values of (linear_cache, activation_cache).\n    activation : str\n        activation used in this layer: \"sigmoid\", \"tanh\", or \"relu\".\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of previous layer l-1),\n        same shape as A_prev.\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l), same shape as W.\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l), same shape as b.\n    \"\"\"\n    linear_cache, activation_cache = cache\n\n    if activation_fn == \"sigmoid\":\n        dZ = sigmoid_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)\n\n    elif activation_fn == \"tanh\":\n        dZ = tanh_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)\n\n    elif activation_fn == \"relu\":\n        dZ = relu_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)\n\n    return dA_prev, dW, db\n\n\ndef L_model_backward_reg(AL, y, caches, hidden_layers_activation_fn=\"relu\", lambd=0):\n    \"\"\"\n    Computes the gradient of output layer w.r.t weights, biases, etc. starting\n    on the output layer in reverse topological order.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector, output of the forward propagation\n        (L_model_forward()).\n    y : 2d-array\n        true \"label\" vector (containing 0 if non-cat, 1 if cat).\n    caches : list\n        list of caches for all layers.\n    hidden_layers_activation_fn :\n        activation function used on hidden layers: \"tanh\", \"relu\".\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    grads : dict\n        gradients.\n    \"\"\"\n    y = y.reshape(AL.shape)\n    L = len(caches)\n    grads = {}\n\n    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n\n    (\n        grads[\"dA\" + str(L - 1)],\n        grads[\"dW\" + str(L)],\n        grads[\"db\" + str(L)],\n    ) = linear_activation_backward_reg(dAL, caches[L - 1], \"sigmoid\", lambd)\n\n    for l in range(L - 1, 0, -1):\n        current_cache = caches[l - 1]\n        (\n            grads[\"dA\" + str(l - 1)],\n            grads[\"dW\" + str(l)],\n            grads[\"db\" + str(l)],\n        ) = linear_activation_backward_reg(\n            grads[\"dA\" + str(l)], current_cache, hidden_layers_activation_fn, lambd\n        )\n\n    return grads\n\n\ndef model_with_regularization(\n    X,\n    y,\n    layers_dims,\n    learning_rate=0.01,\n    num_epochs=3000,\n    print_cost=False,\n    hidden_layers_activation_fn=\"relu\",\n    lambd=0,\n):\n    \"\"\"\n    Implements L-Layer neural network.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n     num_epochs : int\n        number of times to over the training data.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test\n        examples.\n    \"\"\"\n    # get number of examples\n    m = X.shape[1]\n\n    # to get consistents output\n    np.random.seed(1)\n\n    # initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # intialize cost list\n    cost_list = []\n\n    # implement gradient descent\n    for i in range(num_epochs):\n        # compute forward propagation\n        AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n        # compute regularized cost\n        reg_cost = compute_cost_reg(AL, y, parameters, lambd)\n\n        # compute gradients\n        grads = L_model_backward_reg(AL, y, caches, hidden_layers_activation_fn, lambd)\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # print cost\n        if (i + 1) % 100 == 0 and print_cost:\n            print(\"The cost after {} iterations: {}\".format((i + 1), reg_cost))\n\n        # append cost\n        if i % 100 == 0:\n            cost_list.append(reg_cost)\n\n    # plot the cost curve\n    plt.plot(cost_list)\n    plt.xlabel(\"Iterations (per hundreds)\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Cost curve for the learning rate = {}\".format(learning_rate))\n\n    return parameters\n\n\nNow we’re ready to train the neural network. We’ll first build a neural network with no regularization and then one with regularization to see which one has lower generalization error. Note that \\(\\lambda\\) should be tuned to get the best results but we’ll here choose an arbitrary value to illustrate the concept. Both neural netwotks would have 2 hidden layers where each hidden layer has 5 units.\n\n\nCode\n# set up layers dimensions\nlayers_dims = [X_train.shape[0], 5, 5, 1]\n\n# train NN\nparameters = model_with_regularization(\n    X_train,\n    Y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_epochs=2500,\n    print_cost=True,\n    hidden_layers_activation_fn=\"tanh\",\n    lambd=0,\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"tanh\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"tanh\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations: 0.6555634398145331\nThe cost after 200 iterations: 0.6467746423961933\nThe cost after 300 iterations: 0.6446638811282552\nThe cost after 400 iterations: 0.6441400737542232\nThe cost after 500 iterations: 0.6440063101787575\nThe cost after 600 iterations: 0.6439697872317176\nThe cost after 700 iterations: 0.6439570623358253\nThe cost after 800 iterations: 0.6439491872993496\nThe cost after 900 iterations: 0.6439407592837082\nThe cost after 1000 iterations: 0.6439294591543208\nThe cost after 1100 iterations: 0.6439131091764411\nThe cost after 1200 iterations: 0.6438883396380859\nThe cost after 1300 iterations: 0.6438489715870495\nThe cost after 1400 iterations: 0.6437825798034876\nThe cost after 1500 iterations: 0.6436617691190204\nThe cost after 1600 iterations: 0.6434191397054715\nThe cost after 1700 iterations: 0.642864008138056\nThe cost after 1800 iterations: 0.6413476000796884\nThe cost after 1900 iterations: 0.6360827945885947\nThe cost after 2000 iterations: 0.6124050450908987\nThe cost after 2100 iterations: 0.511236042160854\nThe cost after 2200 iterations: 0.5001328693867211\nThe cost after 2300 iterations: 0.36442904496020256\nThe cost after 2400 iterations: 0.3392740578590327\nThe cost after 2500 iterations: 0.4183570598370425\nThe training accuracy rate: 83.73%.\nThe test accuracy rate: 72.00%.\n\n\n\n\n\nThe training accuracy is 82.30% but the test accuracy is 78%. The difference between training and test accuracy is not that much, i.e. we don’t have a lot of overfitting. Therefore, a little bit of regularization may help such as \\(\\lambda = 0.02\\). Values of \\(\\lambda\\)s that practitioners recommend are: 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24.\n\n\nCode\n# train NN with regularization\nparameters = model_with_regularization(\n    X_train,\n    Y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_epochs=2500,\n    print_cost=True,\n    hidden_layers_activation_fn=\"tanh\",\n    lambd=0.02,\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"tanh\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"tanh\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations: 0.6558634554205135\nThe cost after 200 iterations: 0.6470807090618383\nThe cost after 300 iterations: 0.6449737235917311\nThe cost after 400 iterations: 0.6444519406797673\nThe cost after 500 iterations: 0.6443191828114609\nThe cost after 600 iterations: 0.6442831256251426\nThe cost after 700 iterations: 0.6442705985766486\nThe cost after 800 iterations: 0.6442628048800636\nThe cost after 900 iterations: 0.6442544325786784\nThe cost after 1000 iterations: 0.6442432311807257\nThe cost after 1100 iterations: 0.6442270988055475\nThe cost after 1200 iterations: 0.6442027847231018\nThe cost after 1300 iterations: 0.6441643410411311\nThe cost after 1400 iterations: 0.6440998547029029\nThe cost after 1500 iterations: 0.6439832000181198\nThe cost after 1600 iterations: 0.6437505375793907\nThe cost after 1700 iterations: 0.6432228625403317\nThe cost after 1800 iterations: 0.6417982979158361\nThe cost after 1900 iterations: 0.6369273437378263\nThe cost after 2000 iterations: 0.6152774362019153\nThe cost after 2100 iterations: 0.5207828651496841\nThe cost after 2200 iterations: 0.5209315970380933\nThe cost after 2300 iterations: 0.5769347472395975\nThe cost after 2400 iterations: 0.39380136480047884\nThe cost after 2500 iterations: 0.33629411438613926\nThe training accuracy rate: 88.52%.\nThe test accuracy rate: 58.00%.\n\n\n\n\n\nAs the results above show, we improved the generalization error by increasing the test accuracy from 78% to 80%. On the other hand, training accuracy decreased from 82.30% to 65.55%."
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#conclusion",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#conclusion",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "Conclusion",
    "text": "Conclusion\nRegularization is an effective technique to resolve overfitting. Since we don’t know true distribution of the data, empirical risk, which is based of empirical distribution, is prone to overfitting. Therefore, the best strategy is to fit training data really well and then use a regularization technique so that the model generalizes well. L2 parameter regularization along with Dropout are two of the most widely used regularization technique in machine learning.\n\nOne of the implicit assumptions of regularization techniques such as L2 and L1 parameter regularization is that the value of the parameters should be zero and try to shrink all parameters towards zero. It’s meant to avoid following the training data very well which makes the learning algorithm picks some noise that is not helpful when applied on unseen data.\nThe value of \\(\\lambda\\) should be tuned to get the best generalization error. We typically use validation set when comparing models with values for \\(\\lambda\\)s and pick the one with the lowest validation error.\nOnly use regularization if the model suffers from overfitting, i.e training error << validation error.\nIf after using regularization the validation error is still high, then we’re most likely in the underfitting region. In other words, our model is still too simple and already has high bias. Therefore, add complexity to the model and then use regularization.\nSince the majority of tasks we try to solve don’t have enough data (or expensive to collect more data), overfitting will be more prevalent in Deep Learning than underfitting given the complexity of neural networks."
  },
  {
    "objectID": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html",
    "href": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html",
    "title": "Coding Neural Network Part 2 - Gradient Checking",
    "section": "",
    "text": "In the previous post, Coding Neural Network - Forward and Backward Propagation, we implemented both forward propagation and backpropagation in numpy. However, implementing backpropagation from scratch is usually more prune to bugs/errors. Therefore, it’s necessary before running the neural network on training data to check if our implementation of backpropagation is correct. Before we start, let’s revisit what back-propagation is: We loop over the nodes in reverse topological order starting at the final node to compute the derivative of the cost with respect to each edge’s node tail. In other words, we compute the derivative of cost function with respect to all parameters, i.e \\(\\frac{\\partial J}{\\partial \\theta}\\) where \\(\\theta\\) represents the parameters of the model.\nThe way to test our implementation is by computing numerical gradients and compare it with gradients from backpropagation (analytical). There are two way of computing numerical gradients:\n\nRight-hand form: \\[\\frac{J(\\theta + \\epsilon) - J(\\theta)}{\\epsilon}\\tag{1}\\]\nTwo-sided form (see figure 1): \\[\\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2 \\epsilon}\\tag{2}\\]\n\n\n\n\nFigure 1: Two-sided numerical gradients\n\n\nTwo-sided form of approximating the derivative is closer than the right-hand form. Let’s illustrate that with the following example using the function \\(f(x) = x^2\\) by taking its derivative at \\(x = 3\\). - Analytical derivative: \\[\\nabla_x f(x) = 2x\\ \\Rightarrow\\nabla_x f(3) = 6\\] - Two-sided numerical derivative: \\[\\frac{(3 + 1e-2)^2 - (3 - 1e-2)^2}{2 * 1e-2} = 5.999999999999872\\] - Right-hand numerical derivative: \\[\\frac{(3 + 1e-2)^2 - 3^2}{1e-2} = 6.009999999999849\\] As we see above, the difference between analytical derivative and two-sided numerical gradient is almost zero; however, the difference between analytical derivative and right-sided derivative is 0.01. Therefore, we’ll use two-sided epsilon method to compute the numerical gradients.\nIn addition, we’ll normalize the difference between numerical. gradients and analytical gradients using the following formula: \\[\\frac{\\|grad - grad_{approx}\\|_2}{\\|grad\\|_2 + \\|grad_{approx}\\|_2}\\tag{3}\\] If the difference is \\(\\leq 10^{-7}\\), then our implementation is fine; otherwise, we have a mistake somewhere and have to go back and revisit backpropagation code.\nBelow are the steps needed to implement gradient checking: 1. Pick random number of examples from training data to use it when computing both numerical and analytical gradients. - Don’t use all examples in the training data because gradient checking is very slow. 2. Initialize parameters. 3. Compute forward propagation and the cross-entropy cost. 4. Compute the gradients using our back-propagation implementation. 5. Compute the numerical gradients using the two-sided epsilon method. 6. Compute the difference between numerical and analytical gradients.\nWe’ll be using functions we wrote in “Coding Neural Network - Forward Propagation and Backpropagation” post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\nLet’s first import the data.\n\n\nCode\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.linalg import norm\nimport seaborn as sns\n\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    initialize_parameters,\n    L_model_forward,\n    L_model_backward,\n    compute_cost,\n)\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\n\n\n\n\nCode\n# Import the data\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"]).T\ny_train = np.array(train_dataset[\"train_set_y\"]).T\nX_train = X_train.reshape(-1, 209)\ny_train = y_train.reshape(-1, 209)\n\nX_train.shape, y_train.shape\n\n\n((12288, 209), (1, 209))\n\n\nNext, we’ll write helper functions that faciltate converting parameters and gradients dictionaries into vectors and then re-convert them back to dictionaries.\n\n\nCode\ndef dictionary_to_vector(params_dict):\n    \"\"\"\n    Roll a dictionary into a single vector.\n\n    Arguments\n    ---------\n    params_dict : dict\n        learned parameters.\n\n    Returns\n    -------\n    params_vector : array\n        vector of all parameters concatenated.\n    \"\"\"\n    count = 0\n    for key in params_dict.keys():\n        new_vector = np.reshape(params_dict[key], (-1, 1))\n        if count == 0:\n            theta_vector = new_vector\n        else:\n            theta_vector = np.concatenate((theta_vector, new_vector))\n        count += 1\n\n    return theta_vector\n\n\ndef vector_to_dictionary(vector, layers_dims):\n    \"\"\"\n    Unroll parameters vector to dictionary using layers dimensions.\n\n    Arguments\n    ---------\n    vector : array\n        parameters vector.\n    layers_dims : list or array_like\n        dimensions of each layer in the network.\n\n    Returns\n    -------\n    parameters : dict\n        dictionary storing all parameters.\n    \"\"\"\n    L = len(layers_dims)\n    parameters = {}\n    k = 0\n\n    for l in range(1, L):\n        # Create temp variable to store dimension used on each layer\n        w_dim = layers_dims[l] * layers_dims[l - 1]\n        b_dim = layers_dims[l]\n\n        # Create temp var to be used in slicing parameters vector\n        temp_dim = k + w_dim\n\n        # add parameters to the dictionary\n        parameters[\"W\" + str(l)] = vector[\n            k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1])\n        parameters[\"b\" + str(l)] = vector[\n            temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n\n        k += w_dim + b_dim\n\n    return parameters\n\n\ndef gradients_to_vector(gradients):\n    \"\"\"\n    Roll all gradients into a single vector containing only dW and db.\n\n    Arguments\n    ---------\n    gradients : dict\n        storing gradients of weights and biases for all layers: dA, dW, db.\n\n    Returns\n    -------\n    new_grads : array\n        vector of only dW and db gradients.\n    \"\"\"\n    # Get the number of indices for the gradients to iterate over\n    valid_grads = [key for key in gradients.keys()\n                   if not key.startswith(\"dA\")]\n    L = len(valid_grads)// 2\n    count = 0\n    \n    # Iterate over all gradients and append them to new_grads list\n    for l in range(1, L + 1):\n        if count == 0:\n            new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n        else:\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)))\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n        count += 1\n        \n    return new_grads\n\n\nFinally, we’ll write the gradient checking function that will compute the difference between the analytical and numerical gradients and tell us if our implementation of back-propagation is correct. We’ll randomly choose 1 example to compute the difference.\n\n\nCode\ndef forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn=\"tanh\"):\n    \"\"\"\n    Implements the forward propagation and computes the cost.\n    \n    Arguments\n    ---------\n    X : 2d-array\n        input data, shape: number of features x number of examples.\n    parameters : dict\n        parameters to use in forward prop.\n    Y : array\n        true \"label\", shape: 1 x number of examples.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    cost : float\n        cross-entropy cost.\n    \"\"\"\n    # Compute forward prop\n    AL, _ = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n    # Compute cost\n    cost = compute_cost(AL, Y)\n\n    return cost\n\n\ndef gradient_check(\n        parameters, gradients, X, Y, layers_dims, epsilon=1e-7,\n        hidden_layers_activation_fn=\"tanh\"):\n    \"\"\"\n    Checks if back_prop computes correctly the gradient of the cost output by\n    forward_prop.\n    \n    Arguments\n    ---------\n    parameters : dict\n        storing all parameters to use in forward prop.\n    gradients : dict\n        gradients of weights and biases for all layers: dA, dW, db.\n    X : 2d-array\n        input data, shape: number of features x number of examples.\n    Y : array\n        true \"label\", shape: 1 x number of examples.\n    epsilon : \n        tiny shift to the input to compute approximate gradient.\n    layers_dims : list or array_like\n        dimensions of each layer in the network.\n    \n    Returns\n    -------\n    difference : float\n        difference between approx gradient and back_prop gradient\n    \"\"\"\n    \n    # Roll out parameters and gradients dictionaries\n    parameters_vector = dictionary_to_vector(parameters)\n    gradients_vector = gradients_to_vector(gradients)\n\n    # Create vector of zeros to be used with epsilon\n    grads_approx = np.zeros_like(parameters_vector)\n\n    for i in range(len(parameters_vector)):\n        # Compute cost of theta + epsilon\n        theta_plus = np.copy(parameters_vector)\n        theta_plus[i] = theta_plus[i] + epsilon\n        j_plus = forward_prop_cost(\n            X, vector_to_dictionary(theta_plus, layers_dims), Y,\n            hidden_layers_activation_fn)\n\n        # Compute cost of theta - epsilon\n        theta_minus = np.copy(parameters_vector)\n        theta_minus[i] = theta_minus[i] - epsilon\n        j_minus = forward_prop_cost(\n            X, vector_to_dictionary(theta_minus, layers_dims), Y,\n            hidden_layers_activation_fn)\n\n        # Compute numerical gradients\n        grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n\n    # Compute the difference of numerical and analytical gradients\n    numerator = norm(gradients_vector - grads_approx)\n    denominator = norm(grads_approx) + norm(gradients_vector)\n    difference = numerator / denominator\n\n    if difference > 10e-7:\n        print (\"\\033[31mThere is a mistake in back-propagation \" +\\\n               \"implementation. The difference is: {}\".format(difference))\n    else:\n        print (\"\\033[32mThere implementation of back-propagation is fine! \"+\\\n               \"The difference is: {}\".format(difference))\n\n    return difference\n\n\n\n\nCode\n# Set up neural network architecture\nlayers_dims = [X_train.shape[0], 5, 5, 1]\n\n# Initialize parameters\nparameters = initialize_parameters(layers_dims)\n\n# Randomly selecting 1 example from training data\nperms = np.random.permutation(X_train.shape[1])\nindex = perms[:1]\n\n# Compute forward propagation\nAL, caches = L_model_forward(X_train[:, index], parameters, \"tanh\")\n\n# Compute analytical gradients\ngradients = L_model_backward(AL, y_train[:, index], caches, \"tanh\")\n\n# Compute difference of numerical and analytical gradients\ndifference = gradient_check(parameters, gradients, X_train[:, index], y_train[:, index], layers_dims)\n\n\nThere implementation of back-propagation is fine! The difference is: 3.02205552997035e-09\n\n\nCongratulations! Our implementation is correct :)"
  },
  {
    "objectID": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html#conclusion",
    "href": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html#conclusion",
    "title": "Coding Neural Network Part 2 - Gradient Checking",
    "section": "Conclusion",
    "text": "Conclusion\nBelow are some key takeaways:\n\nTwo-sided numerical gradient approximates the analytical gradients more closely than right-side form.\nSince gradient checking is very slow:\n\nApply it on one or few training examples.\nTurn it off when training neural network after making sure that backpropagation’s implementation is correct.\n\nGradient checking doesn’t work when applying drop-out method. Use keep-prob = 1 to check gradient checking and then change it when training neural network.\nEpsilon = 10e-7 is a common value used for the difference between analytical gradient and numerical gradient. If the difference is less than 10e-7 then the implementation of backpropagation is correct.\nThanks to Deep Learning frameworks such as Tensorflow and Pytorch, we may find ourselves rarely implement backpropagation because such frameworks compute that for us; however, it’s a good practice to understand what happens under the hood to become a good Deep Learning practitioner."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#introduction",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#introduction",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Introduction",
    "text": "Introduction\nOptimization, in Machine Learning/Deep Learning contexts, is the process of changing the model’s weights to improve its performance. In other words, it’s the process of finding the best weights in the predefined hypothesis space to get the best possible performance. There are three kinds of optimization algorithms:\n\nOptimization algorithm that is not iterative and simply solves for one point.\nOptimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.\nOptimization algorithm that is iterative in nature and applied to a set of problems that have non-convex loss functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates.\n\nIn this post, we’ll look at three different cases of parameters’ initialization and see how this affects the error rate:\n\nInitialize all weights to zero.\nInitialize weights to random values from standard normal distribution or uniform distribution and multiply it by a scalar such as 10.\nInitialize weights based on:\n\nXavier recommendation.\nKaiming He recommendation.\n\n\nWe’ll be using functions we wrote in “Coding Neural Network - Forward Propagation and Backpropagation” post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\nTo illustrate the above cases, we’ll use the cats vs dogs dataset which consists of 50 images for cats and 50 images for dogs. Each image is 150 pixels x 150 pixels on RGB color scale. Therefore, we would have 67,500 features where each column in the input matrix would be one image which means our input data would have 67,500 x 100 dimension.\nLet’s first load the data and show a sample of two images before we start the helper functions.\n\n\nCode\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    L_model_forward,\n    compute_cost,\n    L_model_backward,\n    update_parameters,\n    accuracy,\n)\nfrom load_dataset import load_dataset_catvsdog\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\n\n\n\n\nCode\nX, Y = load_dataset_catvsdog(\"../../data\")\n\n# show a sample of of a cat and a dog image\nindex_cat = np.argmax(Y)\nindex_dog = np.argmin(Y)\nplt.subplot(1, 2, 1)\nplt.imshow(X[:, index_cat].reshape(150, 150, 3).astype(int))\nplt.axis(\"off\")\nplt.subplot(1, 2, 2)\nplt.imshow(X[:, index_dog].reshape(150, 150, 3).astype(int))\nplt.axis(\"off\")\n\n# standarize the data\nX = X / 255\n\n\n\n\n\n\n\nCode\ndef model(\n    X,\n    Y,\n    layers_dims,\n    learning_rate=0.01,\n    num_iterations=1000,\n    print_cost=True,\n    hidden_layers_activation_fn=\"relu\",\n    initialization_method=\"he\",\n):\n    \"\"\"\n    Implements multilayer neural network using gradient descent as the\n    learning algorithm.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n    num_iterations : int\n        number of iterations of the optimization loop.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n    initialization_method : str\n        specify the initialization method to be used: \"he\", \"xavier\".\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test\n        examples.\n    \"\"\"\n    np.random.seed(1)\n\n    # initialize cost list\n    cost_list = []\n\n    # initialize parameters\n    if initialization_method == \"zeros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization_method == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    else:\n        parameters = initialize_parameters_he_xavier(layers_dims, initialization_method)\n\n    # iterate over num_iterations\n    for i in range(num_iterations):\n        # iterate over L-layers to get the final output and the cache\n        AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n        # compute cost to plot it\n        cost = compute_cost(AL, Y)\n\n        # iterate over L-layers backward to get gradients\n        grads = L_model_backward(AL, Y, caches, hidden_layers_activation_fn)\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # append each 100th cost to the cost list\n        if (i + 1) % 100 == 0 and print_cost:\n            print(\"The cost after {} iterations is: {}\".format(i + 1, cost))\n\n        if i % 100 == 0:\n            cost_list.append(cost)\n\n    # plot the cost curve\n    plt.figure(figsize=(12, 8))\n    plt.plot(cost_list)\n    plt.xlabel(\"Iterations (per hundreds)\", fontsize=14)\n    plt.ylabel(\"Cost\", fontsize=14)\n    plt.title(\n        \"Cost curve: learning rate = {} and {} initialization method\".format(\n            learning_rate, initialization_method\n        ),\n        y=1.05,\n        fontsize=16,\n    )\n\n    return parameters"
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-all-weights-to-zero",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-all-weights-to-zero",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Initializing all weights to zero",
    "text": "Initializing all weights to zero\nHere, we’ll initialize all weight matrices and biases to zeros and see how this would affect the error rate as well as the learning parameters.\n\n\nCode\ndef initialize_parameters_zeros(layers_dims):\n    \"\"\"\n    Initializes the parameters dictionary to all zeros for both weights and\n    bias.\n\n    Arguments\n    ---------\n    layer_dims : list\n        input size and size of each layer, length: number of layers + 1.\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    for l in range(1, L):\n        parameters[\"W\" + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n    return parameters\n\n\n\n\nCode\n# train NN with zeros initialization weights\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X, Y, layers_dims, hidden_layers_activation_fn=\"tanh\", initialization_method=\"zeros\"\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 0.6931471805599453\nThe cost after 200 iterations is: 0.6931471805599453\nThe cost after 300 iterations is: 0.6931471805599453\nThe cost after 400 iterations is: 0.6931471805599453\nThe cost after 500 iterations is: 0.6931471805599453\nThe cost after 600 iterations is: 0.6931471805599453\nThe cost after 700 iterations is: 0.6931471805599453\nThe cost after 800 iterations is: 0.6931471805599453\nThe cost after 900 iterations is: 0.6931471805599453\nThe cost after 1000 iterations is: 0.6931471805599453\n\n\n'The accuracy rate is: 50.00%.'\n\n\n\n\n\nAs the cost curve shows, the neural network didn’t learn anything! That is because of symmetry between all neurons which leads to all neurons have the same update on every iteration. Therefore, regardless of how many iterations we run the optimization algorithms, all the neurons would still get the same update and no learning would happen. As a result, we must break symmetry when initializing weights so that the model would start learning on each update of the gradient descent."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-with-big-random-values",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-with-big-random-values",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Initializing weights with big random values",
    "text": "Initializing weights with big random values\nThere is no big difference if the random values are initialized from standard normal distribution or uniform distribution so we’ll use standard normal distribution in our examples. Also, we’ll multiply the random values by a big number such as 10 to show that initializing weights to big values may cause our optimization to have higher error rates (and even diverge in some cases). Let’s now train our neural network where all weight matrices have been intitialized using the following formula: np.random.randn() * 10\n\n\nCode\ndef initialize_parameters_random(layers_dims):\n    \"\"\"\n    Initializes the parameters dictionary rabdomly from standard normal\n    distribution multiplied by 10 for weight matrices and zeros for bias\n    vectors.\n\n    Arguments\n    ---------\n    layer_dims : list\n        input size and size of each layer, length: number of layers + 1.\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    for l in range(1, L):\n        parameters[\"W\" + str(l)] = (\n            np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n        )\n        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n    return parameters\n\n\n\n\nCode\n# train NN with random initialization weights\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X,\n    Y,\n    layers_dims,\n    hidden_layers_activation_fn=\"tanh\",\n    initialization_method=\"random\",\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 1.2413142077549013\nThe cost after 200 iterations is: 1.1258751902393416\nThe cost after 300 iterations is: 1.0989052435267657\nThe cost after 400 iterations is: 1.084096647128233\nThe cost after 500 iterations is: 1.070695329210598\nThe cost after 600 iterations is: 1.0574847320236294\nThe cost after 700 iterations is: 1.0443168708889223\nThe cost after 800 iterations is: 1.031157857251139\nThe cost after 900 iterations is: 1.0179838815204905\nThe cost after 1000 iterations is: 1.0047670885153432\n\n\n'The accuracy rate is: 55.00%.'\n\n\n\n\n\nRandom initialization here is helping but still the loss function has high value and may take long time to converge and achieve a significantly low value."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-based-on-he-and-xavier-recommendations",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-based-on-he-and-xavier-recommendations",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Initializing weights based on He and Xavier recommendations",
    "text": "Initializing weights based on He and Xavier recommendations\nWe’ll explore two initialization methods:\n\nKaiming He method is best applied when activation function applied on hidden layers is Rectified Linear Unit (ReLU). so that the weight on each hidden layer would have the following variance: \\[var(W^l) = \\frac{2}{n^{l - 1}}\\] We can achieve this by multiplying the random values from standard normal distribution by \\(\\sqrt{\\frac{2}{number\\ of\\ units\\ in \\ previous\\ layer}}\\)\nXavier method is best applied when activation function applied on hidden layers is Hyperbolic Tangent so that the weight on each hidden layer would have the following variance: \\[var(W^l) = \\frac{1}{n^{l - 1}}\\] We can achieve this by multiplying the random values from standard normal distribution by \\(\\sqrt{\\frac{1}{number\\ of\\ units\\ in \\ previous\\ layer}}\\)\n\nWe’ll train the network using both methods and look at the results.\n\n\nCode\ndef initialize_parameters_he_xavier(layers_dims, initialization_method=\"he\"):\n    \"\"\"\n    Initializes the parameters dictionary for weights based on \"He\" and\n    \"Xavier\" methods and zeros for bias vectors.\n\n    Arguments\n    ---------\n    layer_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    initialization_method : str\n        specify the initialization method to be used: \"he\", \"xavier\".\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    if initialization_method == \"he\":\n        for l in range(1, L):\n            parameters[\"W\" + str(l)] = np.random.randn(\n                layers_dims[l], layers_dims[l - 1]\n            ) * np.sqrt(2 / layers_dims[l - 1])\n            parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n    elif initialization_method == \"xavier\":\n        for l in range(1, L):\n            parameters[\"W\" + str(l)] = np.random.randn(\n                layers_dims[l], layers_dims[l - 1]\n            ) * np.sqrt(1 / layers_dims[l - 1])\n            parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n    return parameters\n\n\n\n\nCode\n# train NN where all weights were initialized based on He recommendation\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X, Y, layers_dims, hidden_layers_activation_fn=\"tanh\", initialization_method=\"he\"\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 0.6203611406013776\nThe cost after 200 iterations is: 0.5123234659821594\nThe cost after 300 iterations is: 0.46677237621933015\nThe cost after 400 iterations is: 0.3187255789472014\nThe cost after 500 iterations is: 0.42826666616322456\nThe cost after 600 iterations is: 0.36422647591323054\nThe cost after 700 iterations is: 0.5706298604437564\nThe cost after 800 iterations is: 0.7309174559673725\nThe cost after 900 iterations is: 0.26898635456817815\nThe cost after 1000 iterations is: 0.46540333116680943\n\n\n'The accuracy rate is: 98.00%.'\n\n\n\n\n\n\n\nCode\n# train NN where all weights were initialized based on Xavier recommendation\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X,\n    Y,\n    layers_dims,\n    hidden_layers_activation_fn=\"tanh\",\n    initialization_method=\"xavier\",\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 0.6336209080471575\nThe cost after 200 iterations is: 0.6427985039433041\nThe cost after 300 iterations is: 0.36738403251144874\nThe cost after 400 iterations is: 0.47375556172838623\nThe cost after 500 iterations is: 0.2851099368160619\nThe cost after 600 iterations is: 0.3806391238429354\nThe cost after 700 iterations is: 0.29261677834759703\nThe cost after 800 iterations is: 0.23118565026967375\nThe cost after 900 iterations is: 0.6721421723051113\nThe cost after 1000 iterations is: 0.11528517185494602\n\n\n'The accuracy rate is: 100.00%.'\n\n\n\n\n\nAs shown from applying the four methods, parameters’ initial values play a huge role in achieving low cost values as well as converging and achieve lower training error rates. The same would apply to test error rate if we had test data."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#conclusion",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#conclusion",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Conclusion",
    "text": "Conclusion\nDeep Learning frameworks make it easier to choose between different initialization methods without worrying about implementing it ourselves. Nonetheless, it’s important to understand the critical role initial values of the parameters in the overall performance of the network. Below are some key takeaways:\n\nWell chosen initialization values of weights leads to:\n\nSpeed up convergence of gradient descent.\nIncrease the likelihood of gradient descent to find lower training and generalization error rates.\n\nBecause we’re dealing with iterative optimization algorithms with non-convex loss function, different initializations lead to different results.\nRandom initialization is used to break symmetry and make sure different hidden units can learn different things.\nDon’t initialize to values that are too large.\nKaiming He (He) initialization works well for neural networks with ReLU activation function.\nXavier initialization works well for neural networks with Hyperbolic Tangent activation function."
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#introduction",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#introduction",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Introduction",
    "text": "Introduction\nDropout is a regularization technique. On each iteration, we randomly shut down some neurons (units) on each layer and don’t use those neurons in both forward propagation and back-propagation. Since the units that will be dropped out on each iteration will be random, the learning algorithm will have no idea which neurons will be shut down on every iteration; therefore, force the learning algorithm to spread out the weights and not focus on some specific feattures (units). Moreover, dropout help improving generalization error by:\n\nSince we drop some units on each iteration, this will lead to smaller network which in turns means simpler network (regularization).\nCan be seen as an approximation to bagging techniques. Each iteration can be viewed as different model since we’re dropping randomly different units on each layer. This means that the error would be the average of errors from all different models (iterations). Therefore, averaging errors from different models especially if those errors are uncorrelated would reduce the overall errors. In the worst case where errors are perfectly correlated, averaging among all models won’t help at all; however, we know that in practice errors have some degree of uncorrelation. As result, it will always improve generalization error.\n\nWe can use different probabilities on each layer; however, the output layer would always have keep_prob = 1 and the input layer has high keep_prob such as 0.9 or 1. If a hidden layer has keep_prob = 0.8, this means that; on each iteration, each unit has 80% probablitity of being included and 20% probability of being dropped out.\nDropout is used a lot in computer vision problems because we have a lot of features and not a lot of data. Also, features (pixels) next to each other usually don’t add a lot of information. Therefore, models always suffer from overfitting.\nTo illustrate how dropout helps us reduce generalization error, we’ll use the same dataset we’ve used in the previous posts. The dataset has images for cats and non-cat. We’ll try to build a neural network to classify if the image has cat or not. Each image is 64 x 64 pixels on RGB scale. Let’s import the data and take a look at the shape as well as a sample of a cat image from the training set.\n\n\nCode\nimport os\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# local modules\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    initialize_parameters,\n    linear_activation_forward,\n    compute_cost,\n    linear_activation_backward,\n    update_parameters,\n    accuracy,\n)\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"figure.figsize\"] = (12, 6)\n\n\n\n\nCode\n# Import training data\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"])\nY_train = np.array(train_dataset[\"train_set_y\"])\n\n# Plot a sample image\nplt.imshow(X_train[50])\nplt.axis(\"off\")\n\n# Import test data\ntest_dataset = h5py.File(\"../../data/test_catvnoncat.h5\")\nX_test = np.array(test_dataset[\"test_set_x\"])\nY_test = np.array(test_dataset[\"test_set_y\"])\n\n# Transform data\nX_train = X_train.reshape(209, -1).T\nX_train = X_train / 255\nY_train = Y_train.reshape(-1, 209)\n\nX_test = X_test.reshape(50, -1).T\nX_test = X_test / 255\nY_test = Y_test.reshape(-1, 50)\n\n# print the new shape of both training and test datasets\nprint(\"Training data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_train.shape, Y_train.shape))\nprint(\"Test data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_test.shape, Y_test.shape))\n\n\nTraining data dimensions:\nX's dimension: (12288, 209), Y's dimension: (1, 209)\nTest data dimensions:\nX's dimension: (12288, 50), Y's dimension: (1, 50)"
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#implementation",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#implementation",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Implementation",
    "text": "Implementation\nNow, we’ll write the functions needed to apply dropout on both forward propagation and back-propagation. Note that we’ll utilize the functions we wrote in previous posts such as initialize_parameters.\n\n\nCode\ndef drop_out_matrices(layers_dims, m, keep_prob):\n    \"\"\"\n    Initializes the dropout matrices that will be used in both forward prop\n    and back-prop on each layer. We'll use random numbers from uniform\n    distribution.\n\n    Arguments\n    ---------\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    m : int\n        number of training examples.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n\n    Returns\n    -------\n    D : dict\n        dropout matrices for each layer l. Each dropout matrix on each layer\n        would have the same dimension as post activation output matrix \"A\".\n        For example: \"D1\" shape: number of units x number of examples.\n    \"\"\"\n    np.random.seed(1)\n    D = {}\n    L = len(layers_dims)\n\n    for l in range(L):\n        # initialize the random values for the dropout matrix\n        D[str(l)] = np.random.rand(layers_dims[l], m)\n        # Convert it to 0/1 to shut down neurons corresponding to each element\n        D[str(l)] = D[str(l)] < keep_prob[l]\n        assert D[str(l)].shape == (layers_dims[l], m)\n    return D\n\n\ndef L_model_forward(X, parameters, D, keep_prob, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the output layer through looping over all units in topological\n    order.\n\n    X : 2d-array\n        input matrix of shape input_size x training_examples.\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    D : dict\n        dropout matrices for each layer l.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\",\"relu\".\n\n\n    Returns\n    -------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    caches : list\n        that contains L tuples where each layer has: A_prev, W, b, Z.\n    \"\"\"\n    A = X  # since input matrix A0\n    A = np.multiply(A, D[str(0)])\n    A /= keep_prob[0]\n    caches = []  # initialize the caches list\n    L = len(parameters) // 2  # number of layer in the network\n\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(\n            A_prev,\n            parameters[\"W\" + str(l)],\n            parameters[\"b\" + str(l)],\n            hidden_layers_activation_fn,\n        )\n        # shut down some units\n        A = np.multiply(A, D[str(l)])\n        # scale that value of units to keep expected value the same\n        A /= keep_prob[l]\n        caches.append(cache)\n\n    AL, cache = linear_activation_forward(\n        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\"\n    )\n    AL = np.multiply(AL, D[str(L)])\n    AL /= keep_prob[L]\n    caches.append(cache)\n    assert AL.shape == (1, X.shape[1])\n\n    return AL, caches\n\n\ndef L_model_backward(AL, Y, caches, D, keep_prob, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the gradient of output layer w.r.t weights, biases, etc. starting\n    on the output layer in reverse topological order.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector, output of the forward propagation\n        (L_model_forward()).\n    y : 2d-array\n        true \"label\" vector (containing 0 if non-cat, 1 if cat).\n    caches : list\n        list of caches for all layers.\n    D : dict\n        dropout matrices for each layer l.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n    hidden_layers_activation_fn :\n        activation function used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    grads : dict\n        gradients.\n    \"\"\"\n    Y = Y.reshape(AL.shape)\n    L = len(caches)\n    grads = {}\n\n    # dA for output layer\n    dAL = np.divide(AL - Y, np.multiply(AL, 1 - AL))\n    dAL = np.multiply(dAL, D[str(L)])\n    dAL /= keep_prob[L]\n\n    (\n        grads[\"dA\" + str(L - 1)],\n        grads[\"dW\" + str(L)],\n        grads[\"db\" + str(L)],\n    ) = linear_activation_backward(dAL, caches[L - 1], \"sigmoid\")\n    grads[\"dA\" + str(L - 1)] = np.multiply(grads[\"dA\" + str(L - 1)], D[str(L - 1)])\n    grads[\"dA\" + str(L - 1)] /= keep_prob[L - 1]\n\n    for l in range(L - 1, 0, -1):\n        current_cache = caches[l - 1]\n        (\n            grads[\"dA\" + str(l - 1)],\n            grads[\"dW\" + str(l)],\n            grads[\"db\" + str(l)],\n        ) = linear_activation_backward(\n            grads[\"dA\" + str(l)], current_cache, hidden_layers_activation_fn\n        )\n\n        grads[\"dA\" + str(l - 1)] = np.multiply(grads[\"dA\" + str(l - 1)], D[str(l - 1)])\n        grads[\"dA\" + str(l - 1)] /= keep_prob[l - 1]\n\n    return grads\n\n\ndef model_with_dropout(\n    X,\n    Y,\n    layers_dims,\n    keep_prob,\n    learning_rate=0.01,\n    num_iterations=3000,\n    print_cost=True,\n    hidden_layers_activation_fn=\"relu\",\n):\n    \"\"\"\n    Implements multilayer neural network with dropout using gradient descent as the\n    learning algorithm.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n    num_iterations : int\n        number of iterations of the optimization loop.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test\n        examples.\n    \"\"\"\n    # get number of examples\n    m = X.shape[1]\n\n    # to get consistents output\n    np.random.seed(1)\n\n    # initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # intialize cost list\n    cost_list = []\n\n    # implement gradient descent\n    for i in range(num_iterations):\n        # Initialize dropout matrices\n        D = drop_out_matrices(layers_dims, m, keep_prob)\n\n        # compute forward propagation\n        AL, caches = L_model_forward(\n            X, parameters, D, keep_prob, hidden_layers_activation_fn\n        )\n\n        # compute regularized cost\n        cost = compute_cost(AL, Y)\n\n        # compute gradients\n        grads = L_model_backward(\n            AL, Y, caches, D, keep_prob, hidden_layers_activation_fn\n        )\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # print cost\n        if (i + 1) % 100 == 0 and print_cost:\n            print(f\"The cost after {i + 1} iterations : {cost:.4f}.\")\n        # append cost\n        if i % 100 == 0:\n            cost_list.append(cost)\n\n    # plot the cost curve\n    plt.plot(cost_list)\n    plt.xlabel(\"Iteration (per hundreds)\")\n    plt.ylabel(\"Cost\")\n    plt.title(f\"Cost curve for the learning rate = {learning_rate}\")\n\n    return parameters"
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#application",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#application",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Application",
    "text": "Application\nFinally, we’re ready to build our neural network. First, we’ll build one fully connected network without dropout. That is to say, keep_prob = 1. Next, we’ll build another network where keep_prob < 1. Lastly, we’ll compare the generalization error of both networks and see how dropout technique can help us in improving our generalization error.\n\n\nCode\n# setup layers dimensions, number of examples, and keep probabilities list\nm = X_train.shape[0]\nkeep_prob = [1, 1, 1, 1]\nlayers_dims = [m, 10, 10, 1]\n\n# train NN with no dropout\nparameters = model_with_dropout(\n    X_train,\n    Y_train,\n    layers_dims,\n    keep_prob=keep_prob,\n    learning_rate=0.03,\n    num_iterations=1000,\n    hidden_layers_activation_fn=\"relu\",\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"relu\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"relu\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations : 0.6555.\nThe cost after 200 iterations : 0.6468.\nThe cost after 300 iterations : 0.6447.\nThe cost after 400 iterations : 0.6442.\nThe cost after 500 iterations : 0.6440.\nThe cost after 600 iterations : 0.6440.\nThe cost after 700 iterations : 0.6440.\nThe cost after 800 iterations : 0.6440.\nThe cost after 900 iterations : 0.6440.\nThe cost after 1000 iterations : 0.6440.\nThe training accuracy rate: 65.55%.\nThe test accuracy rate: 34.00%.\n\n\n\n\n\n\n\nCode\n# setup keep probabilities list\nkeep_prob = [1, 0.5, 0.5, 1]\n\n# train NN with no dropout\nparameters = model_with_dropout(\n    X_train,\n    Y_train,\n    layers_dims,\n    keep_prob=keep_prob,\n    learning_rate=0.03,\n    num_iterations=1000,\n    hidden_layers_activation_fn=\"relu\",\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"relu\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"relu\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations : 0.6555.\nThe cost after 200 iterations : 0.6467.\nThe cost after 300 iterations : 0.6445.\nThe cost after 400 iterations : 0.6437.\nThe cost after 500 iterations : 0.6412.\nThe cost after 600 iterations : 0.6338.\nThe cost after 700 iterations : 0.6108.\nThe cost after 800 iterations : 0.5367.\nThe cost after 900 iterations : 0.4322.\nThe cost after 1000 iterations : 0.3114.\nThe training accuracy rate: 74.16%.\nThe test accuracy rate: 44.00%.\n\n\n\n\n\nAs the results above showed, the network with dropout improved on test accuracy rate by 30%. Note that this is just an illustrative example to show the effectiveness of the dropout technique. We chose an arbitrary probabilities in this example; however, we can tune the dropout probabilities on each layer to yield the best validation loss and accuracy."
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#conclusion",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#conclusion",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Conclusion",
    "text": "Conclusion\nDropout is a very effective regularization technique that is used a lot in Convolutional Neural Networks. Below are some takeaways:\n\nSet keep_prob = 1 when using gradient checking; otherwise, it won’t work.\nDropout is used only during training. Don’t use it when testing/predicting new examples.\nThe lowest the keep_prob \\(\\rightarrow\\) the simpler the neural network. As keep_prob decreases, the bias increases and the variance decreases. Therefore, layers with more neurons are expected to have lower keep_prob to avoid overfitting.\nIt’s computationally a cheap way to improve generalization error and help resolve overfitting.\nOne can tune keep_prob to get the best results out of the task at hand."
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#why-neural-networks",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#why-neural-networks",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "Why Neural Networks?",
    "text": "Why Neural Networks?\nAccording to Universal Approximate Theorem, Neural Networks can approximate as well as learn and represent any function given a large enough layer and desired error margin. The way neural network learns the true function is by building complex representations on top of simple ones. On each hidden layer, the neural network learns new feature space by first compute the affine (linear) transformations of the given inputs and then apply non-linear function which in turn will be the input of the next layer. This process will continue until we reach the output layer. Therefore, we can define neural network as information flows from inputs through hidden layers towards the output. For a 3-layers neural network, the learned function would be: \\(f(x) = f_3(f_2(f_1(x)))\\) where:\n\n\\(f_1(x)\\): Function learned on first hidden layer\n\\(f_2(x)\\): Function learned on second hidden layer\n\\(f_3(x)\\): Function learned on output layer\n\nTherefore, on each layer we learn different representation that gets more complicated with later hidden layers.Below is an example of a 3-layers neural network (we don’t count input layer):\n\n\n\nFigure 1: Neural Network with two hidden layers\n\n\nFor example, computers can’t understand images directly and don’t know what to do with pixels data. However, a neural network can build a simple representation of the image in the early hidden layers that identifies edges. Given the first hidden layer output, it can learn corners and contours. Given the second hidden layer, it can learn parts such as nose. Finally, it can learn the object identity.\nSince truth is never linear and representation is very critical to the performance of a machine learning algorithm, neural network can help us build very complex models and leave it to the algorithm to learn such representations without worrying about feature engineering that takes practitioners very long time and effort to curate a good representation.\nThe post has two parts:\n\nCoding the neural network: This entails writing all the helper functions that would allow us to implement a multi-layer neural network. While doing so, I’ll explain the theoretical parts whenever possible and give some advices on implementations.\nApplication: We’ll use the neural network we coded in the first part on image recognition problem to see if the network we built will be able to detect if the image has a cat or a dog and see it working :)\n\n\n\nCode\nimport os as os\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")"
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#i.-coding-the-neural-network",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#i.-coding-the-neural-network",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "I. Coding The Neural Network",
    "text": "I. Coding The Neural Network\n\nForward Propagation\nThe input \\(X\\) provides the initial information that then propagates to the hidden units at each layer and finally produce the output \\(\\widehat{Y}\\). The architecture of the network entails determining its depth, width, and activation functions used on each layer. Depth is the number of hidden layers. Width is the number of units (nodes) on each hidden layer since we don’t control neither input layer nor output layer dimensions. There are quite a few set of activation functions such Rectified Linear Unit, Sigmoid, Hyperbolic tangent, etc. Research has proven that deeper networks outperform networks with more hidden units. Therefore, it’s always better and won’t hurt to train a deeper network (with diminishing returns).\nLets first introduce some notations that will be used throughout the post:\n\n\\(W^l\\): Weights matrix for the \\(l^{th}\\) layer\n\\(b^l\\): Bias vector for the \\(l^{th}\\) layer\n\\(Z^l\\): Linear (affine) transformations of given inputs for the \\(l^{th}\\) layer\n\\(g^l\\): Activation function applied on the \\(l^{th}\\) layer\n\\(A^l\\): Post-activation output for the \\(l^{th}\\) layer\n\\(dW^l\\): Derivative of the cost function w.r.t \\(W^l\\) (\\(\\frac{\\partial J}{\\partial W^l}\\))\n\\(db^l\\): Derivative of the cost function w.r.t \\(b^l\\) (\\(\\frac{\\partial J}{\\partial b^l})\\))\n\\(dZ^l\\): Derivative of the cost function w.r.t \\(Z^l\\) (\\(\\frac{\\partial J}{\\partial Z^l}\\))\n\\(dA^l\\): Derivative of the cost function w.r.t \\(A^l\\) (\\(\\frac{\\partial J}{\\partial A^l}\\))\n\\(n^l\\): Number of units (nodes) of the \\(l^{th}\\) layer\n\\(m\\): Number of examples\n\\(L\\): Number of layers in the network (not including the input layer)\n\nNext, we’ll write down the dimensions of a multi-layer neural network in the general form to help us in matrix multiplication because one of the major challenges in implementing a neural network is getting the dimensions right.\n\n\\(W^l,\\ dW^l\\): Number of units (nodes) in \\(l^{th}\\) layer x Number of units (nodes) in \\(l - 1\\) layer\n\\(b^l,\\ db^l\\): Number of units (nodes) in \\(l^{th}\\) layer x 1\n\\(Z^l,\\ dZ^l\\): Number of units (nodes) in \\(l^{th}\\) layer x number of examples\n\\(A^l,\\ dA^l\\): Number of units (nodes) in \\(l^{th}\\) layer x number of examples\n\nThe two equations we need to implement forward propagations are: \\[Z^l = W^lA^{l - 1} + b ^l\\tag1\\\\{}\\] \\[A^l = g^l(Z^l) = g^l(W^lA^{l - 1} + b ^l)\\tag2\\] These computations will take place on each layer.\n\n\nParameters Initialization\nWe’ll first initialize the weight matrices and the bias vectors. It’s important to note that we shouldn’t initialize all the parameters to zero because doing so will lead the gradients to be equal and on each iteration the output would be the same and the learning algorithm won’t learn anything. Therefore, it’s important to randomly initialize the parameters to values between 0 and 1. It’s also recommended to multiply the random values by small scalar such as 0.01 to make the activation units active and be on the regions where activation functions’ derivatives are not close to zero.\n\n\nCode\n# Initialize parameters\ndef initialize_parameters(layers_dims):\n    \"\"\"\n    Initialize parameters dictionary.\n\n    Weight matrices will be initialized to random values from uniform normal\n    distribution.\n    bias vectors will be initialized to zeros.\n\n    Arguments\n    ---------\n    layers_dims : list or array-like\n        dimensions of each layer in the network.\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    for l in range(1, L):\n        parameters[\"W\" + str(l)] = (\n            np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01\n        )\n        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n        assert parameters[\"W\" + str(l)].shape == (layers_dims[l], layers_dims[l - 1])\n        assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n\n    return parameters\n\n\n\n\nActivation Functions\nThere is no definitive guide for which activation function works best on specific problems. It’s a trial and error process where one should try different set of functions and see which one works best on the problem at hand. We’ll cover 4 of the most commonly used activation functions:\n\nSigmoid function (\\(\\sigma\\)): \\(g(z) = \\frac{1}{1 + e^{-z}}\\). It’s recommended to be used only on the output layer so that we can easily interpret the output as probabilities since it has restricted output between 0 and 1. One of the main disadvantages for using sigmoid function on hidden layers is that the gradient is very close to zero over a large portion of its domain which makes it slow and harder for the learning algorithm to learn.\nHyperbolic Tangent function: \\(g(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\). It’s superior to sigmoid function in which the mean of its output is very close to zero, which in other words center the output of the activation units around zero and make the range of values very small which means faster to learn. The disadvantage that it shares with sigmoid function is that the gradient is very small on good portion of the domain.\nRectified Linear Unit (ReLU): \\(g(z) = max\\{0, z\\}\\). The models that are close to linear are easy to optimize. Since ReLU shares a lot of the properties of linear functions, it tends to work well on most of the problems. The only issue is that the derivative is not defined at \\(z = 0\\), which we can overcome by assigning the derivative to 0 at \\(z = 0\\). However, this means that for \\(z\\leq 0\\) the gradient is zero and again can’t learn.\nLeaky Rectified Linear Unit: \\(g(z) = max\\{\\alpha*z, z\\}\\). It overcomes the zero gradient issue from ReLU and assigns \\(\\alpha\\) which is a small value for \\(z\\leq 0\\).\n\nIf you’re not sure which activation function to choose, start with ReLU.\nNext, we’ll implement the above activation functions and draw a graph for each one to make it easier to see the domain and range of each function.\n\n\nCode\n# Define activation functions that will be used in forward propagation\ndef sigmoid(Z):\n    \"\"\"\n    Computes the sigmoid of Z element-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = 1 / (1 + np.exp(-Z))\n\n    return A, Z\n\n\ndef tanh(Z):\n    \"\"\"\n    Computes the Hyperbolic Tagent of Z elemnet-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = np.tanh(Z)\n\n    return A, Z\n\n\ndef relu(Z):\n    \"\"\"\n    Computes the Rectified Linear Unit (ReLU) element-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = np.maximum(0, Z)\n\n    return A, Z\n\n\ndef leaky_relu(Z):\n    \"\"\"\n    Computes Leaky Rectified Linear Unit element-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = np.maximum(0.1 * Z, Z)\n\n    return A, Z\n\n\n\n\nCode\n# Plot the 4 activation functions\nz = np.linspace(-10, 10, 100)\n\n# Computes post-activation outputs\nA_sigmoid, z = sigmoid(z)\nA_tanh, z = tanh(z)\nA_relu, z = relu(z)\nA_leaky_relu, z = leaky_relu(z)\n\n# Plot sigmoid\nplt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(z, A_sigmoid, label=\"Function\")\nplt.plot(z, A_sigmoid * (1 - A_sigmoid), label=\"Derivative\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\frac{1}{1 + e^{-z}}$\")\nplt.title(\"Sigmoid Function\", fontsize=16)\n# Plot tanh\nplt.subplot(2, 2, 2)\nplt.plot(z, A_tanh, \"b\", label=\"Function\")\nplt.plot(z, 1 - np.square(A_tanh), \"r\", label=\"Derivative\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\frac{e^z - e^{-z}}{e^z + e^{-z}}$\")\nplt.title(\"Hyperbolic Tangent Function\", fontsize=16)\n# plot relu\nplt.subplot(2, 2, 3)\nplt.plot(z, A_relu, \"g\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$max\\{0, z\\}$\")\nplt.title(\"ReLU Function\", fontsize=16)\n# plot leaky relu\nplt.subplot(2, 2, 4)\nplt.plot(z, A_leaky_relu, \"y\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$max\\{0.1z, z\\}$\")\nplt.title(\"Leaky ReLU Function\", fontsize=16)\nplt.tight_layout()\n\n\n\n\n\n\n\nFeed Forward\nGiven its inputs from previous layer, each unit computes affine transformation \\(z = W^Tx + b\\) and then apply an activation function \\(g(z)\\) such as ReLU element-wise. During the process, we’ll store (cache) all variables computed and used on each layer to be used in back-propagation. We’ll write first two helper functions that will be used in the L-model forward propagation to make it easier to debug. Keep in mind that on each layer, we may have different activation function.\n\n\nCode\n# Define helper functions that will be used in L-model forward prop\ndef linear_forward(A_prev, W, b):\n    \"\"\"\n    Computes affine transformation of the input.\n\n    Arguments\n    ---------\n    A_prev : 2d-array\n        activations output from previous layer.\n    W : 2d-array\n        weight matrix, shape: size of current layer x size of previuos layer.\n    b : 2d-array\n        bias vector, shape: size of current layer x 1.\n\n    Returns\n    -------\n    Z : 2d-array\n        affine transformation output.\n    cache : tuple\n        stores A_prev, W, b to be used in backpropagation.\n    \"\"\"\n    Z = np.dot(W, A_prev) + b\n    cache = (A_prev, W, b)\n\n    return Z, cache\n\n\ndef linear_activation_forward(A_prev, W, b, activation_fn):\n    \"\"\"\n    Computes post-activation output using non-linear activation function.\n\n    Arguments\n    ---------\n    A_prev : 2d-array\n        activations output from previous layer.\n    W : 2d-array\n        weight matrix, shape: size of current layer x size of previuos layer.\n    b : 2d-array\n        bias vector, shape: size of current layer x 1.\n    activation_fn : str\n        non-linear activation function to be used: \"sigmoid\", \"tanh\", \"relu\".\n\n    Returns\n    -------\n    A : 2d-array\n        output of the activation function.\n    cache : tuple\n        stores linear_cache and activation_cache. ((A_prev, W, b), Z) to be used in backpropagation.\n    \"\"\"\n    assert (\n        activation_fn == \"sigmoid\" or activation_fn == \"tanh\" or activation_fn == \"relu\"\n    )\n\n    if activation_fn == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n\n    elif activation_fn == \"tanh\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = tanh(Z)\n\n    elif activation_fn == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n\n    assert A.shape == (W.shape[0], A_prev.shape[1])\n\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\n\n\ndef L_model_forward(X, parameters, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the output layer through looping over all units in topological\n    order.\n\n    Arguments\n    ---------\n    X : 2d-array\n        input matrix of shape input_size x training_examples.\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    caches : list\n        that contains L tuples where each layer has: A_prev, W, b, Z.\n    \"\"\"\n    A = X\n    caches = []\n    L = len(parameters) // 2\n\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(\n            A_prev,\n            parameters[\"W\" + str(l)],\n            parameters[\"b\" + str(l)],\n            activation_fn=hidden_layers_activation_fn,\n        )\n        caches.append(cache)\n\n    AL, cache = linear_activation_forward(\n        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation_fn=\"sigmoid\"\n    )\n    caches.append(cache)\n\n    assert AL.shape == (1, X.shape[1])\n\n    return AL, caches\n\n\n\n\nCost\nWe’ll use the binary Cross-Entropy cost. It uses the log-likelihood method to estimate its error. The cost is: \\[J(W, b) = -\\frac{1}{m}\\sum_{i = 1}^m\\big(y^ilog(\\widehat{y^i}) + (1 - y^i)log(1 - \\widehat{y^i})\\big)\\tag3\\] The above cost function is convex; however, neural network usually stuck on a local minimum and is not guaranteed to find the optimal parameters. We’ll use here gradient-based learning.\n\n\nCode\n# Compute cross-entropy cost\ndef compute_cost(AL, y):\n    \"\"\"\n    Computes the binary Cross-Entropy cost.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    y : 2d-array\n        true \"label\" vector.\n\n    Returns\n    -------\n    cost : float\n        binary cross-entropy cost.\n    \"\"\"\n    m = y.shape[1]\n    cost = -(1 / m) * np.sum(\n        np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL))\n    )\n\n    return cost\n\n\n\n\nBack-Propagation\nBackpropagation allows the information to go back from the cost backward through the network in order to compute the gradient. Therefore, loop over the nodes starting at the final node in reverse topological order to compute the derivative of the final node output with respect to each edge’s node tail. Doing so will help us know who is responsible for the most error and change the parameters in that direction. The following derivatives’ formulas will help us write the back-propagate functions: \\[dA^L = \\frac{A^L - Y}{A^L(1 - A^L)}\\tag4\\\\{}\\] \\[dZ^L = A^L - Y\\tag5\\\\{}\\] \\[dW^l = \\frac{1}{m}dZ^l{A^{l - 1}}^T\\tag6\\\\{}\\] \\[db^l = \\frac{1}{m}\\sum_i(dZ^l)\\tag7\\\\{}\\] \\[dA^{l - 1} = {W^l}^TdZ^l\\tag8\\\\{}\\] \\[dZ^{l} = dA^l*g^{'l}(Z^l)\\tag9\\\\{}\\] Since \\(b^l\\) is always a vector, the sum would be across rows (since each column is an example).\n\n\nCode\n# Define derivative of activation functions w.r.t z that will be used in back-propagation\ndef sigmoid_gradient(dA, Z):\n    \"\"\"\n    Computes the gradient of sigmoid output w.r.t input Z.\n\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient, of any shape.\n    Z : 2d-array\n        input used for the activation fn on this layer.\n\n    Returns\n    -------\n    dZ : 2d-array\n        gradient of the cost with respect to Z.\n    \"\"\"\n    A, Z = sigmoid(Z)\n    dZ = dA * A * (1 - A)\n\n    return dZ\n\n\ndef tanh_gradient(dA, Z):\n    \"\"\"\n    Computes the gradient of hyperbolic tangent output w.r.t input Z.\n\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient, of any shape.\n    Z : 2d-array\n        input used for the activation fn on this layer.\n\n    Returns\n    -------\n    dZ : 2d-array\n        gradient of the cost with respect to Z.\n    \"\"\"\n    A, Z = tanh(Z)\n    dZ = dA * (1 - np.square(A))\n\n    return dZ\n\n\ndef relu_gradient(dA, Z):\n    \"\"\"\n    Computes the gradient of ReLU output w.r.t input Z.\n\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient, of any shape.\n    Z : 2d-array\n        input used for the activation fn on this layer.\n\n    Returns\n    -------\n    dZ : 2d-array\n        gradient of the cost with respect to Z.\n    \"\"\"\n    A, Z = relu(Z)\n    dZ = np.multiply(dA, np.int64(A > 0))\n\n    return dZ\n\n\n# define helper functions that will be used in L-model back-prop\ndef linear_backword(dZ, cache):\n    \"\"\"\n    Computes the gradient of the output w.r.t weight, bias, and post-activation\n    output of (l - 1) layers at layer l.\n\n    Arguments\n    ---------\n    dZ : 2d-array\n        gradient of the cost w.r.t. the linear output (of current layer l).\n    cache : tuple\n        values of (A_prev, W, b) coming from the forward propagation in the current layer.\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of the previous layer l-1).\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l).\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l).\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = (1 / m) * np.dot(dZ, A_prev.T)\n    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n\n    assert dA_prev.shape == A_prev.shape\n    assert dW.shape == W.shape\n    assert db.shape == b.shape\n\n    return dA_prev, dW, db\n\n\ndef linear_activation_backward(dA, cache, activation_fn):\n    \"\"\"\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient for current layer l.\n    cache : tuple\n        values of (linear_cache, activation_cache).\n    activation : str\n        activation used in this layer: \"sigmoid\", \"tanh\", or \"relu\".\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of the previous layer l-1), same shape as A_prev.\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l), same shape as W.\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l), same shape as b.\n    \"\"\"\n    linear_cache, activation_cache = cache\n\n    if activation_fn == \"sigmoid\":\n        dZ = sigmoid_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n\n    elif activation_fn == \"tanh\":\n        dZ = tanh_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n\n    elif activation_fn == \"relu\":\n        dZ = relu_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n\n    return dA_prev, dW, db\n\n\ndef L_model_backward(AL, y, caches, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the gradient of output layer w.r.t weights, biases, etc. starting\n    on the output layer in reverse topological order.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector, output of the forward propagation (L_model_forward()).\n    y : 2d-array\n        true \"label\" vector (containing 0 if non-cat, 1 if cat).\n    caches : list\n        list of caches for all layers.\n    hidden_layers_activation_fn :\n        activation function used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    grads : dict\n        with the gradients.\n    \"\"\"\n    y = y.reshape(AL.shape)\n    L = len(caches)\n    grads = {}\n\n    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n\n    (\n        grads[\"dA\" + str(L - 1)],\n        grads[\"dW\" + str(L)],\n        grads[\"db\" + str(L)],\n    ) = linear_activation_backward(dAL, caches[L - 1], \"sigmoid\")\n\n    for l in range(L - 1, 0, -1):\n        current_cache = caches[l - 1]\n        (\n            grads[\"dA\" + str(l - 1)],\n            grads[\"dW\" + str(l)],\n            grads[\"db\" + str(l)],\n        ) = linear_activation_backward(\n            grads[\"dA\" + str(l)], current_cache, hidden_layers_activation_fn\n        )\n\n    return grads\n\n\n# define the function to update both weight matrices and bias vectors\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update the parameters' values using gradient descent rule.\n\n    Arguments\n    ---------\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    grads : dict\n        stores all gradients (output of L_model_backward).\n\n    Returns\n    -------\n    parameters : dict\n        updated parameters.\n    \"\"\"\n    L = len(parameters) // 2\n\n    for l in range(1, L + 1):\n        parameters[\"W\" + str(l)] = (\n            parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n        )\n        parameters[\"b\" + str(l)] = (\n            parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n        )\n\n    return parameters"
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#application",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#application",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "Application",
    "text": "Application\nThe dataset that we’ll be working on has 209 images. Each image is 64 x 64 pixels on RGB scale. We’ll build a neural network to classify if the image has a cat or not. Therefore, \\(y^i \\in \\{0, 1\\}.\\)\n\nWe’ll first load the images.\nShow sample image for a cat.\nReshape input matrix so that each column would be one example. Also, since each image is 64 x 64 x 3, we’ll end up having 12,288 features for each image. Therefore, the input matrix would be 12,288 x 209.\nStandardize the data so that the gradients don’t go out of control. Also, it will help hidden units have similar range of values. For now, we’ll divide every pixel by 255 which shouldn’t be an issue. However, it’s better to standardize the data to have a mean of 0 and a standard deviation of 1.\n\n\n\nCode\n# Import training dataset\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"])\ny_train = np.array(train_dataset[\"train_set_y\"])\n\ntest_dataset = h5py.File(\"../../data/test_catvnoncat.h5\")\nX_test = np.array(test_dataset[\"test_set_x\"])\ny_test = np.array(test_dataset[\"test_set_y\"])\n\n# print the shape of input data and label vector\nprint(\n    f\"\"\"Original dimensions:\\n{20 * '-'}\\nTraining: {X_train.shape}, {y_train.shape}\nTest: {X_test.shape}, {y_test.shape}\"\"\"\n)\n\n# plot cat image\nplt.figure(figsize=(6, 6))\nplt.imshow(X_train[50])\nplt.axis(\"off\")\n\n# Transform input data and label vector\nX_train = X_train.reshape(209, -1).T\ny_train = y_train.reshape(-1, 209)\n\nX_test = X_test.reshape(50, -1).T\ny_test = y_test.reshape(-1, 50)\n\n# standarize the data\nX_train = X_train / 255\nX_test = X_test / 255\n\nprint(\n    f\"\"\"\\nNew dimensions:\\n{15 * '-'}\\nTraining: {X_train.shape}, {y_train.shape}\nTest: {X_test.shape}, {y_test.shape}\"\"\"\n)\n\n\nOriginal dimensions:\n--------------------\nTraining: (209, 64, 64, 3), (209,)\nTest: (50, 64, 64, 3), (50,)\n\nNew dimensions:\n---------------\nTraining: (12288, 209), (1, 209)\nTest: (12288, 50), (1, 50)\n\n\n\n\n\nNow, our dataset is ready to be used and test our neural network implementation. Let’s first write multi-layer model function to implement gradient-based learning using predefined number of iterations and learning rate.\n\n\nCode\n# Define the multi-layer model using all the helper functions we wrote before\n\n\ndef L_layer_model(\n    X,\n    y,\n    layers_dims,\n    learning_rate=0.01,\n    num_iterations=3000,\n    print_cost=True,\n    hidden_layers_activation_fn=\"relu\",\n):\n    \"\"\"\n    Implements multilayer neural network using gradient descent as the\n    learning algorithm.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n    num_iterations : int\n        number of iterations of the optimization loop.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test examples.\n    \"\"\"\n    np.random.seed(1)\n\n    # initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # intialize cost list\n    cost_list = []\n\n    # iterate over num_iterations\n    for i in range(num_iterations):\n        # iterate over L-layers to get the final output and the cache\n        AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n        # compute cost to plot it\n        cost = compute_cost(AL, y)\n\n        # iterate over L-layers backward to get gradients\n        grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn)\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # append each 100th cost to the cost list\n        if (i + 1) % 100 == 0 and print_cost:\n            print(f\"The cost after {i + 1} iterations is: {cost:.4f}\")\n\n        if i % 100 == 0:\n            cost_list.append(cost)\n\n    # plot the cost curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(cost_list)\n    plt.xlabel(\"Iterations (per hundreds)\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Loss curve for the learning rate = {learning_rate}\")\n\n    return parameters\n\n\ndef accuracy(X, parameters, y, activation_fn=\"relu\"):\n    \"\"\"\n    Computes the average accuracy rate.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    parameters : dict\n        learnt parameters.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    accuracy : float\n        accuracy rate after applying parameters on the input data\n    \"\"\"\n    probs, caches = L_model_forward(X, parameters, activation_fn)\n    labels = (probs >= 0.5) * 1\n    accuracy = np.mean(labels == y) * 100\n\n    return f\"The accuracy rate is: {accuracy:.2f}%.\"\n\n\nNext, we’ll train two versions of the neural network where each one will use different activation function on hidden layers: One will use rectified linear unit (ReLU) and the second one will use hyperbolic tangent function (tanh). Finally we’ll use the parameters we get from both neural networks to classify test examples and compute the test accuracy rates for each version to see which activation function works best on this problem.\n\n\nCode\n# Setting layers dims\nlayers_dims = [X_train.shape[0], 5, 5, 1]\n\n# NN with tanh activation fn\nparameters_tanh = L_layer_model(\n    X_train,\n    y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_iterations=3000,\n    hidden_layers_activation_fn=\"tanh\",\n)\n\n# Print the accuracy\naccuracy(X_test, parameters_tanh, y_test, activation_fn=\"tanh\")\n\n\nThe cost after 100 iterations is: 0.6556\nThe cost after 200 iterations is: 0.6468\nThe cost after 300 iterations is: 0.6447\nThe cost after 400 iterations is: 0.6441\nThe cost after 500 iterations is: 0.6440\nThe cost after 600 iterations is: 0.6440\nThe cost after 700 iterations is: 0.6440\nThe cost after 800 iterations is: 0.6439\nThe cost after 900 iterations is: 0.6439\nThe cost after 1000 iterations is: 0.6439\nThe cost after 1100 iterations is: 0.6439\nThe cost after 1200 iterations is: 0.6439\nThe cost after 1300 iterations is: 0.6438\nThe cost after 1400 iterations is: 0.6438\nThe cost after 1500 iterations is: 0.6437\nThe cost after 1600 iterations is: 0.6434\nThe cost after 1700 iterations is: 0.6429\nThe cost after 1800 iterations is: 0.6413\nThe cost after 1900 iterations is: 0.6361\nThe cost after 2000 iterations is: 0.6124\nThe cost after 2100 iterations is: 0.5112\nThe cost after 2200 iterations is: 0.5001\nThe cost after 2300 iterations is: 0.3644\nThe cost after 2400 iterations is: 0.3393\nThe cost after 2500 iterations is: 0.4184\nThe cost after 2600 iterations is: 0.2372\nThe cost after 2700 iterations is: 0.4299\nThe cost after 2800 iterations is: 0.3064\nThe cost after 2900 iterations is: 0.2842\nThe cost after 3000 iterations is: 0.1902\n\n\n'The accuracy rate is: 70.00%.'\n\n\n\n\n\n\n\nCode\n# NN with relu activation fn\nparameters_relu = L_layer_model(\n    X_train,\n    y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_iterations=3000,\n    hidden_layers_activation_fn=\"relu\",\n)\n\n# Print the accuracy\naccuracy(X_test, parameters_relu, y_test, activation_fn=\"relu\")\n\n\nThe cost after 100 iterations is: 0.6556\nThe cost after 200 iterations is: 0.6468\nThe cost after 300 iterations is: 0.6447\nThe cost after 400 iterations is: 0.6441\nThe cost after 500 iterations is: 0.6440\nThe cost after 600 iterations is: 0.6440\nThe cost after 700 iterations is: 0.6440\nThe cost after 800 iterations is: 0.6440\nThe cost after 900 iterations is: 0.6440\nThe cost after 1000 iterations is: 0.6440\nThe cost after 1100 iterations is: 0.6439\nThe cost after 1200 iterations is: 0.6439\nThe cost after 1300 iterations is: 0.6439\nThe cost after 1400 iterations is: 0.6439\nThe cost after 1500 iterations is: 0.6439\nThe cost after 1600 iterations is: 0.6439\nThe cost after 1700 iterations is: 0.6438\nThe cost after 1800 iterations is: 0.6437\nThe cost after 1900 iterations is: 0.6435\nThe cost after 2000 iterations is: 0.6432\nThe cost after 2100 iterations is: 0.6423\nThe cost after 2200 iterations is: 0.6395\nThe cost after 2300 iterations is: 0.6259\nThe cost after 2400 iterations is: 0.5408\nThe cost after 2500 iterations is: 0.5262\nThe cost after 2600 iterations is: 0.4727\nThe cost after 2700 iterations is: 0.4386\nThe cost after 2800 iterations is: 0.3493\nThe cost after 2900 iterations is: 0.1877\nThe cost after 3000 iterations is: 0.3641\n\n\n'The accuracy rate is: 42.00%.'"
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#conclusion",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#conclusion",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "Conclusion",
    "text": "Conclusion\nThe purpose of this artcile is to code Deep Neural Network step-by-step and explain the important concepts while doing that. We don’t really care about the accuracy rate at this moment since there are tons of things we could’ve done to increase the accuracy which would be the subject of following artciles. Below are some takeaways:\n\nEven if neural network can represent any function, it may fail to learn for two reasons:\n\nThe optimization algorithm may fail to find the best value for the parameters of the desired (true) function. It can stuck in a local optimum.\nThe learning algorithm may find different functional form that is different than the intended function due to overfitting.\n\nEven if neural network rarely converges and always stuck in a local minimum, it is still able to reduce the cost significantly and come up with very complex models with high test accuracy.\nThe neural network we used in this artcile is standard fully connected network. However, there are two other kinds of networks:\n\nConvolutional NN: Where not all nodes are connected. It’s best in class for image recognition.\nRecurrent NN: There is a feedback connections where output of the model is fed back into itself. It’s used mainly in sequence modeling.\n\nThe fully connected neural network also forgets what happened in previous steps and also doesn’t know anything about the output.\nThere are number of hyperparameters that we can tune using cross validation to get the best performance of our network:\n\nLearning rate (\\(\\alpha\\)): Determines how big the step for each update of parameters.\n\nSmall \\(\\alpha\\) leads to slow convergence and may become computationally very expensive.\nLarge \\(\\alpha\\) may lead to overshooting where our learning algorithm may never converge.\n\nNumber of hidden layers (depth): The more hidden layers the better, but comes at a cost computationally.\nNumber of units per hidden layer (width): Research proven that huge number of hidden units per layer doesn’t add to the improvement of the network.\nActivation function: Which function to use on hidden layers differs among applications and domains. It’s a trial and error process to try different functions and see which one works best.\nNumber of iterations.\n\nStandardize data would help activation units have similar range of values and avoid gradients to go out of control."
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#introduction",
    "href": "posts/employee-turnover/Employee-Turnover.html#introduction",
    "title": "Predicting Employee Turnover",
    "section": "Introduction",
    "text": "Introduction\nEmployee turnover refers to the percentage of workers who leave an organization and are replaced by new employees. It is very costly for organizations, where costs include but not limited to: separation, vacancy, recruitment, training and replacement. On average, organizations invest between four weeks and three months training new employees. This investment would be a loss for the company if the new employee decided to leave the first year. Furthermore, organizations such as consulting firms would suffer from deterioration in customer satisfaction due to regular changes in Account Reps and/or consultants that would lead to loss of businesses with clients.\nIn this notebook, we’ll work on simulated HR data from kaggle to build a classifier that helps us predict what kind of employees will be more likely to leave given some attributes. Such classifier would help an organization predict employee turnover and be pro-active in helping to solve such costly matter. We’ll restrict ourselves to use the most common classifiers: Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine.\nThe data has 14,999 examples (samples). Below are the features and the definitions of each one:\n\nsatisfaction_level: Level of satisfaction {0-1}.\nlast_evaluationTime: Time since last performance evaluation (in years).\nnumber_project: Number of projects completed while at work.\naverage_montly_hours: Average monthly hours at workplace.\ntime_spend_company: Number of years spent in the company.\nWork_accident: Whether the employee had a workplace accident.\nleft: Whether the employee left the workplace or not {0, 1}.\npromotion_last_5years: Whether the employee was promoted in the last five years.\nsales: Department the employee works for.\nsalary: Relative level of salary {low, medium, high}.\n\nLet’s first load all the packages.\n\n\nCode\nimport os\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (accuracy_score,\n                             f1_score,\n                             roc_auc_score,\n                             roc_curve,\n                             confusion_matrix)\nfrom sklearn.model_selection import (cross_val_score,\n                                     GridSearchCV,\n                                     RandomizedSearchCV,\n                                     learning_curve,\n                                     validation_curve,\n                                     train_test_split)\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.utils import resample\nfrom warnings import filterwarnings\n\nos.chdir(\"../\")\nfrom scripts.plot_roc import plot_conf_matrix_and_roc, plot_roc\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\nfilterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#data-preprocessing",
    "href": "posts/employee-turnover/Employee-Turnover.html#data-preprocessing",
    "title": "Predicting Employee Turnover",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nLet’s take a look at the data (check if there are missing values and the data type of each features):\n\n\nCode\n# Load the data\ndf = pd.read_csv(\"data/HR_comma_sep.csv\")\n\n# Check both the datatypes and if there is missing values\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Data types:\\n\" + 11 * \"-\")\nprint(\"\\033[30m\" + \"{}\\n\".format(df.dtypes))\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Sum of null values in each column:\\n\" + 35 * \"-\")\nprint(\"\\033[30m\" + \"{}\".format(df.isnull().sum()))\ndf.head()\n\n\nData types:\n-----------\nsatisfaction_level       float64\nlast_evaluation          float64\nnumber_project             int64\naverage_montly_hours       int64\ntime_spend_company         int64\nWork_accident              int64\nleft                       int64\npromotion_last_5years      int64\nsales                     object\nsalary                    object\ndtype: object\n\nSum of null values in each column:\n-----------------------------------\nsatisfaction_level       0\nlast_evaluation          0\nnumber_project           0\naverage_montly_hours     0\ntime_spend_company       0\nWork_accident            0\nleft                     0\npromotion_last_5years    0\nsales                    0\nsalary                   0\ndtype: int64\n\n\n\n\n\n\n  \n    \n      \n      satisfaction_level\n      last_evaluation\n      number_project\n      average_montly_hours\n      time_spend_company\n      Work_accident\n      left\n      promotion_last_5years\n      sales\n      salary\n    \n  \n  \n    \n      0\n      0.38\n      0.53\n      2\n      157\n      3\n      0\n      1\n      0\n      sales\n      low\n    \n    \n      1\n      0.80\n      0.86\n      5\n      262\n      6\n      0\n      1\n      0\n      sales\n      medium\n    \n    \n      2\n      0.11\n      0.88\n      7\n      272\n      4\n      0\n      1\n      0\n      sales\n      medium\n    \n    \n      3\n      0.72\n      0.87\n      5\n      223\n      5\n      0\n      1\n      0\n      sales\n      low\n    \n    \n      4\n      0.37\n      0.52\n      2\n      159\n      3\n      0\n      1\n      0\n      sales\n      low\n    \n  \n\n\n\n\nSince there are no missing values, we do not have to do any imputation. However, there are some data preprocessing needed: 1. Change sales feature name to department. 2. Convert salary into ordinal categorical feature since there is intrinsic order between: low, medium and high. 3. Create dummy features from department feature and drop the first one to avoid linear dependency where some learning algorithms may struggle with.\n\n\nCode\n# Rename sales feature into department\ndf = df.rename(columns={\"sales\": \"department\"})\n\n# Map salary into integers\nsalary_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\ndf[\"salary\"] = df[\"salary\"].map(salary_map)\n\n# Create dummy variables for department feature\ndf = pd.get_dummies(df, columns=[\"department\"], drop_first=True)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      satisfaction_level\n      last_evaluation\n      number_project\n      average_montly_hours\n      time_spend_company\n      Work_accident\n      left\n      promotion_last_5years\n      salary\n      department_RandD\n      department_accounting\n      department_hr\n      department_management\n      department_marketing\n      department_product_mng\n      department_sales\n      department_support\n      department_technical\n    \n  \n  \n    \n      0\n      0.38\n      0.53\n      2\n      157\n      3\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      0.80\n      0.86\n      5\n      262\n      6\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0.11\n      0.88\n      7\n      272\n      4\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0.72\n      0.87\n      5\n      223\n      5\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      4\n      0.37\n      0.52\n      2\n      159\n      3\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n  \n\n\n\n\n\n\nCode\ndf.columns[df.columns != \"left\"].shape\n\n\n(17,)\n\n\nThe data is now ready to be used for modeling. The final number of features are now 17."
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#modeling",
    "href": "posts/employee-turnover/Employee-Turnover.html#modeling",
    "title": "Predicting Employee Turnover",
    "section": "Modeling",
    "text": "Modeling\nLet’s first take a look at the proportion of each class to see if we’re dealing with balanced or imbalanced data since each one has its own set of tools to be used when fitting classifiers.\n\n\nCode\n# Get number of positve and negative examples\npos = df[df[\"left\"] == 1].shape[0]\nneg = df[df[\"left\"] == 0].shape[0]\nprint(\"Positive examples = {}\".format(pos))\nprint(\"Negative examples = {}\".format(neg))\nprint(\"Proportion of positive to negative examples = {:.2f}%\".format((pos / neg) * 100))\nsns.countplot(df[\"left\"])\nplt.xticks((0, 1), [\"Didn't leave\", \"Left\"])\nplt.xlabel(\"Left\")\nplt.ylabel(\"Count\")\nplt.title(\"Class counts\");\n\n\nPositive examples = 3571\nNegative examples = 11428\nProportion of positive to negative examples = 31.25%\n\n\n\n\n\nAs the graph shows, we have an imbalanced dataset. As a result, when we fit classifiers on such datasets, we should use metrics other than accuracy when comparing models such as f1-score or AUC (area under ROC curve). Moreover, class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. There are three ways to deal with this issue: 1. Assign a larger penalty to wrong predictions from the minority class. 2. Upsampling the minority class or downsampling the majority class. 3. Generate synthetic training examples.\nNonetheless, there is no definitive guide or best practices to deal with such situations. Therefore, we have to try them all and see which one works better on the problem at hand. We’ll restrict ourselves to use the first two, i.e assign larger penalty to wrong predictions from the minority class using class_weight in classifiers that allows us do that and evaluate upsampling/downsampling on the training data to see which gives higher performance.\nFirst, split the data into training and test sets using 80/20 split; 80% of the data will be used to train the models and 20% to test the performance of the models. Second, Upsample the minority class and downsample the majority class. For this data set, positive class is the minority class and negative class is the majority class.\n\n\nCode\n# Convert dataframe into numpy objects and split them into\n# train and test sets: 80/20\nX = df.loc[:, df.columns != \"left\"].values\ny = df.loc[:, df.columns == \"left\"].values.flatten()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=1)\n\n# Upsample minority class\nX_train_u, y_train_u = resample(X_train[y_train == 1],\n                                y_train[y_train == 1],\n                                replace=True,\n                                n_samples=X_train[y_train == 0].shape[0],\n                                random_state=1)\nX_train_u = np.concatenate((X_train[y_train == 0], X_train_u))\ny_train_u = np.concatenate((y_train[y_train == 0], y_train_u))\n\n# Downsample majority class\nX_train_d, y_train_d = resample(X_train[y_train == 0],\n                                y_train[y_train == 0],\n                                replace=True,\n                                n_samples=X_train[y_train == 1].shape[0],\n                                random_state=1)\nX_train_d = np.concatenate((X_train[y_train == 1], X_train_d))\ny_train_d = np.concatenate((y_train[y_train == 1], y_train_d))\n\nprint(\"Original shape:\", X_train.shape, y_train.shape)\nprint(\"Upsampled shape:\", X_train_u.shape, y_train_u.shape)\nprint(\"Downsampled shape:\", X_train_d.shape, y_train_d.shape)\n\n\nOriginal shape: (11999, 17) (11999,)\nUpsampled shape: (18284, 17) (18284,)\nDownsampled shape: (5714, 17) (5714,)\n\n\nI don’t think we need to apply dimensionality reduction such as PCA because: 1) We want to know the importance of each feature in determining who will leave vs who won’t (inference). 2) Dimension of the dataset is descent (17 features). However, it’s good to see how many principal components needed to explain 90%, 95% and 99% of the variation in the data.\n\n\nCode\n# Build PCA using standarized trained data\npca = PCA(n_components=None, svd_solver=\"full\")\npca.fit(StandardScaler().fit_transform(X_train))\ncum_var_exp = np.cumsum(pca.explained_variance_ratio_)\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 18), pca.explained_variance_ratio_, align=\"center\",\n        color='red', label=\"Individual explained variance\")\nplt.step(range(1, 18), cum_var_exp, where=\"mid\", label=\"Cumulative explained variance\")\nplt.xticks(range(1, 18))\nplt.legend(loc=\"best\")\nplt.xlabel(\"Principal component index\", {\"fontsize\": 14})\nplt.ylabel(\"Explained variance ratio\", {\"fontsize\": 14})\nplt.title(\"PCA on training data\", {\"fontsize\": 16});\n\n\n\n\n\n\n\nCode\ncum_var_exp\n\n\narray([ 0.1078147 ,  0.18756726,  0.26523205,  0.33604446,  0.4036422 ,\n        0.46807506,  0.53094596,  0.59334034,  0.65535106,  0.71691288,\n        0.77413324,  0.82651546,  0.87672244,  0.92515346,  0.96216602,\n        0.99429813,  1.        ])\n\n\nLooks like it needs 14, 15 and 16 principal components to capture 90%, 95% and 99% of the variation in the data respectively. In other words, this means that the data is already in a good space since eigenvalues are very close to each other and gives further evidence that we don’t need to compress the data.\nThe methodology that we’ll follow when building the classifiers goes as follows: 1. Build a pipeline that handles all the steps when fitting the classifier using scikit-learn’s make_pipeline which will have two steps: 1. Standardizing the data to speed up convergence and make all features on the same scale. 2. The classifier (estimator) we want to use to fit the model. 2. Use GridSearchCV to tune hyperparameters using 10-folds cross validation. We can use RandomizedSearchCV which is faster and may outperform GridSearchCV especially if we have more than two hyperparameters and the range for each one is very big; however, GridSearchCV will work just fine since we have only two hyperparameters and descent range. 3. Fit the model using training data. 5. Plot both confusion matrix and ROC curve for the best estimator using test data.\nRepeat the above steps for Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine. Next, pick the classifier that has the highest cross validation f1 score. Note that some of the hyperparameter ranges will be guided by the paper Data-driven Advice for Applying Machine Learning to Bioinformatics Problems.\n\nRandom Forest\nFirst, we will start by fitting a Random Forest classifier using unsampled, upsampled and downsampled data. Second, we will evaluate each method using cross validation (CV) f1-score and pick the one with the highest CV f1-score. Finally, we will use that method to fit the rest of the classifiers.\nThe only hyperparameters we’ll tune are:\n\nmax_feature: how many features to consider randomly on each split. This will help avoid having few strong features to be picked on each split and let other features have the chance to contribute. Therefore, predictions will be less correlated and the variance of each tree will decrease.\nmin_samples_leaf: how many examples to have for each split to be a final leaf node.\n\nRandom Forest is an ensemble model that has multiple trees (n_estimators), where each tree is a weak learner. The final prediction would be a weighting average or mode of the predictions from all estimators. Note: high number of trees don’t cause overfitting.\n\n\nCode\n# Build random forest classifier\nmethods_data = {\"Original\": (X_train, y_train),\n                \"Upsampled\": (X_train_u, y_train_u),\n                \"Downsampled\": (X_train_d, y_train_d)}\n\nfor method in methods_data.keys():\n    pip_rf = make_pipeline(StandardScaler(),\n                           RandomForestClassifier(n_estimators=500,\n                                                  class_weight=\"balanced\",\n                                                  random_state=123))\n    \n    hyperparam_grid = {\n        \"randomforestclassifier__n_estimators\": [10, 50, 100, 500],\n        \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", 0.4, 0.5],\n        \"randomforestclassifier__min_samples_leaf\": [1, 3, 5],\n        \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"]}\n    \n    gs_rf = GridSearchCV(pip_rf,\n                         hyperparam_grid,\n                         scoring=\"f1\",\n                         cv=10,\n                         n_jobs=-1)\n    \n    gs_rf.fit(methods_data[method][0], methods_data[method][1])\n    \n    print(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters for {} data:\".format(method))\n    for hyperparam in gs_rf.best_params_.keys():\n        print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_rf.best_params_[hyperparam])\n        \n    print(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_rf.best_score_) * 100))\n\n\nThe best hyperparameters for Original data:\ncriterion :  gini\nmax_features :  0.5\nmin_samples_leaf :  1\nn_estimators :  500\nBest 10-folds CV f1-score: 98.19%.\nThe best hyperparameters for Upsampled data:\ncriterion :  entropy\nmax_features :  0.4\nmin_samples_leaf :  1\nn_estimators :  50\nBest 10-folds CV f1-score: 99.80%.\nThe best hyperparameters for Downsampled data:\ncriterion :  entropy\nmax_features :  0.4\nmin_samples_leaf :  1\nn_estimators :  500\nBest 10-folds CV f1-score: 98.44%.\n\n\nUpsampling yielded the highest CV f1-score with 99.80%. Therefore, we’ll be using the upsampled data to fit the rest of the classifiers. The new data now has 18,284 examples with 50% of the examples belong to the positive class and the other 50% belong to the negative example.\n\n\nCode\nX_train_u[y_train_u == 0].shape, X_train_u[y_train_u == 1].shape\n\n\n((9142, 17), (9142, 17))\n\n\nLet’s refit the Random Forest with Upsampled data using best hyperparameters tuned above and plot confusion matrix and ROC curve using test data.\n\n\nCode\n# Reassign original training data to upsampled data\nX_train, y_train = np.copy(X_train_u), np.copy(y_train_u)\n\n# Delete original and downsampled data\ndel X_train_u, y_train_u, X_train_d, y_train_d\n\n# Refit RF classifier using best params\nclf_rf = make_pipeline(StandardScaler(),\n                       RandomForestClassifier(n_estimators=50,\n                                              criterion=\"entropy\",\n                                              max_features=0.4,\n                                              min_samples_leaf=1,\n                                              class_weight=\"balanced\",\n                                              n_jobs=-1,\n                                              random_state=123))\n\n\nclf_rf.fit(X_train, y_train)\n\n# Plot confusion matrix and ROC curve\nplot_conf_matrix_and_roc(clf_rf, X_test, y_test)\n\n\n\n\n\n\n\nGradient Boosting Trees\nGradient Boosting trees are the same as Random Forest except for:\n\nIt starts with small tree and start learning from grown trees by taking into account the residual of grown trees.\nMore trees can lead to overfitting; opposite to Random Forest.\n\nThe two other hyperparameters than max_features and n_estimators that we’re going to tune are:\n\nlearning_rate: rate the tree learns, the slower the better.\nmax_depth: number of split each time a tree is growing which limits the number of nodes in each tree.\n\nLet’s fit GB classifier and plot confusion matrix and ROC curve using test data.\n\n\nCode\n# Build Gradient Boosting classifier\npip_gb = make_pipeline(StandardScaler(),\n                       GradientBoostingClassifier(loss=\"deviance\",\n                                                  random_state=123))\n\nhyperparam_grid = {\"gradientboostingclassifier__max_features\": [\"log2\", 0.5],\n                   \"gradientboostingclassifier__n_estimators\": [100, 300, 500],\n                   \"gradientboostingclassifier__learning_rate\": [0.001, 0.01, 0.1],\n                   \"gradientboostingclassifier__max_depth\": [1, 2, 3]}\n\ngs_gb = GridSearchCV(pip_gb,\n                      param_grid=hyperparam_grid,\n                      scoring=\"f1\",\n                      cv=10,\n                      n_jobs=-1)\n\ngs_gb.fit(X_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_gb.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_gb.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_gb.best_score_) * 100))\n\n\nThe best hyperparameters:\n-------------------------\nlearning_rate :  0.1\nmax_depth :  3\nmax_features :  0.5\nn_estimators :  500\nBest 10-folds CV f1-score: 97.88%.\n\n\n\n\nCode\n# Plot confusion matrix and ROC curve\nplot_conf_matrix_and_roc(gs_gb, X_test, y_test)\n\n\n\n\n\n\n\nK-Nearest Neighbors\nKNN is called a lazy learning algorithm because it doesn’t learn or fit any parameter. It takes n_neighbors points from the training data closest to the point we’re interested to predict it’s class and take the mode (majority vote) of the classes for the neighboring point as its class. The two hyperparameters we’re going to tune are:\n\nn_neighbors: number of neighbors to use in prediction.\nweights: how much weight to assign neighbors based on:\n\n“uniform”: all neighboring points have the same weight.\n“distance”: use the inverse of euclidean distance of each neighboring point used in prediction.\n\n\nLet’s fit KNN classifier and plot confusion matrix and ROC curve.\n\n\nCode\n# Build KNN classifier\npip_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\nhyperparam_range = range(1, 20)\n\ngs_knn = GridSearchCV(pip_knn,\n                      param_grid={\"kneighborsclassifier__n_neighbors\": hyperparam_range,\n                                  \"kneighborsclassifier__weights\": [\"uniform\", \"distance\"]},\n                      scoring=\"f1\",\n                      cv=10,\n                      n_jobs=-1)\n\ngs_knn.fit(X_train, y_train)\n\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_knn.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_knn.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_knn.best_score_) * 100))\n\n\nThe best hyperparameters:\n-------------------------\nn_neighbors :  1\nweights :  uniform\nBest 10-folds CV f1-score: 98.24%.\n\n\n\n\nCode\nplot_conf_matrix_and_roc(gs_knn, X_test, y_test)\n\n\n\n\n\n\n\nLogistic Regression\nFor logistic regression, we’ll tune three hyperparameters:\n\npenalty: type of regularization, L2 or L1 regularization.\nC: the opposite of regularization of parameter \\(\\lambda\\). The higher C the less regularization. We’ll use values that cover the full range between unregularized to fully regularized where model is the mode of the examples’ label.\nfit_intercept: whether to include intercept or not.\n\nWe won’t use any non-linearities such as polynomial features.\n\n\nCode\n# Build logistic model classifier\npip_logmod = make_pipeline(StandardScaler(),\n                           LogisticRegression(class_weight=\"balanced\"))\n\nhyperparam_range = np.arange(0.5, 20.1, 0.5)\n\nhyperparam_grid = {\"logisticregression__penalty\": [\"l1\", \"l2\"],\n                   \"logisticregression__C\":  hyperparam_range,\n                   \"logisticregression__fit_intercept\": [True, False]\n                  }\n\ngs_logmodel = GridSearchCV(pip_logmod,\n                           hyperparam_grid,\n                           scoring=\"accuracy\",\n                           cv=2,\n                           n_jobs=-1)\n\ngs_logmodel.fit(X_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_logmodel.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_logmodel.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_logmodel.best_score_) * 100))\n\n\nThe best hyperparameters:\n-------------------------\nC :  1.0\nfit_intercept :  True\npenalty :  l1\nBest 10-folds CV f1-score: 77.20%.\n\n\n\n\nCode\nplot_conf_matrix_and_roc(gs_logmodel, X_test, y_test)\n\n\n\n\n\n\n\nSupport Vector Machine (SVM)\nSVM is comutationally very expensive to tune it’s hyperparameters for two reasons:\n\nWith big datasets, it becomes very slow.\nIt has good number of hyperparameters to tune that takes very long time to tune on a CPU.\n\nTherefore, we’ll use recommended hyperparameters’ values from the paper we mentioned before that showed to yield the best performane on Penn Machine Learning Benchmark 165 datasets. The hyperparameters that we usually look to tune are: - C, gamma, kernel, degree and coef0\n\n\nCode\n# Build SVM classifier\nclf_svc = make_pipeline(StandardScaler(),\n                        SVC(C=0.01,\n                            gamma=0.1,\n                            kernel=\"poly\",\n                            degree=5,\n                            coef0=10,\n                            probability=True))\n\nclf_svc.fit(X_train, y_train)\n\nsvc_cv_scores = cross_val_score(clf_svc,\n                                X=X_train,\n                                y=y_train,\n                                scoring=\"f1\",\n                                cv=10,\n                                n_jobs=-1)\n\n# Print CV\nprint(\"\\033[1m\" + \"\\033[94m\" + \"The 10-folds CV f1-score is: {:.2f}%\".format(\n       np.mean(svc_cv_scores) * 100))\n\n\nThe 10-folds CV f1-score is: 96.38%\n\n\n\n\nCode\nplot_conf_matrix_and_roc(clf_svc, X_test, y_test)"
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#conclusion",
    "href": "posts/employee-turnover/Employee-Turnover.html#conclusion",
    "title": "Predicting Employee Turnover",
    "section": "Conclusion",
    "text": "Conclusion\nLet’s conclude by printing out the test accuracy rates for all classifiers we’ve trained so far and plot ROC curves. Finally, we’ll pick the classifier that has the highest area under ROC curve.\n\n\nCode\n# Plot ROC curves for all classifiers\nestimators = {\"RF\": clf_rf,\n              \"LR\": gs_logmodel,\n              \"SVC\": clf_svc,\n              \"GBT\": gs_gb,\n              \"KNN\": gs_knn}\nplot_roc(estimators, X_test, y_test, (12, 8))\n\n# Print out accuracy score on test data\nprint(\"The accuracy rate and f1-score on test data are:\")\nfor estimator in estimators.keys():\n    print(\"{}: {:.2f}%, {:.2f}%.\".format(estimator,\n        accuracy_score(y_test, estimators[estimator].predict(X_test)) * 100,\n         f1_score(y_test, estimators[estimator].predict(X_test)) * 100))\n\n\nThe accuracy rate and f1-score on test data are:\nRF: 99.27%, 98.44%.\nLR: 76.33%, 61.29%.\nSVC: 95.90%, 91.69%.\nGBT: 97.97%, 95.74%.\nKNN: 97.23%, 94.33%.\n\n\n\n\n\nEven though Random Forest and Gradient Boosting Trees have almost equal auc, Random Forest has higher accuracy rate and an f1-score with 99.27% and 99.44% respectively. Therefore, we safely say Random Forest outperforms the rest of the classifiers. Let’s have a look of feature importances from Random Forest classifier.\n\n\nCode\n# Refit RF classifier\nclf_rf = RandomForestClassifier(n_estimators=50,\n                                criterion=\"entropy\",\n                                max_features=0.4,\n                                min_samples_leaf=1,\n                                class_weight=\"balanced\",\n                                n_jobs=-1,\n                                random_state=123)\n\n\nclf_rf.fit(StandardScaler().fit_transform(X_train), y_train)\n\n# Plot features importance\nimportances = clf_rf.feature_importances_\nindices = np.argsort(clf_rf.feature_importances_)[::-1]\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 18), importances[indices], align=\"center\")\nplt.xticks(range(1, 18), df.columns[df.columns != \"left\"][indices], rotation=90)\nplt.title(\"Feature Importance\", {\"fontsize\": 16});\n\n\n\n\n\nLooks like the five most important features are: - satisfaction_level - time_spend_company - average_montly_hours - number_project - lats_evaluation\nThe take home message is the following: - When dealing with imbalanced classes, accuracy is not a good method for model evaluation. AUC and f1-score are examples of metrics we can use. - Upsampling/downsampling, data synthetic and using balanced class weights are good strategies to try to improve the accuracy of a classifier for imbalanced classes datasets. - GridSearchCV helps tune hyperparameters for each learning algorithm. RandomizedSearchCV is faster and may outperform GridSearchCV especially when we have more than two hyperparameters to tune. - Principal Component Analysis (PCA) isn’t always recommended especially if the data is in a good feature space and their eigen values are very close to each other. - As expected, ensemble models outperforms other learning algorithms in most cases."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html",
    "href": "posts/clustering/Kmeans-Clustering.html",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#introduction",
    "href": "posts/clustering/Kmeans-Clustering.html#introduction",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Introduction",
    "text": "Introduction\nClustering is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. In other words, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance. The decision of which similarity measure to use is application-specific.\nClustering analysis can be done on the basis of features where we try to find subgroups of samples based on features or on the basis of samples where we try to find subgroups of features based on samples. We’ll cover here clustering based on features. Clustering is used in market segmentation; where we try to fined customers that are similar to each other whether in terms of behaviors or attributes, image segmentation/compression; where we try to group similar regions together, document clustering based on topics, etc.\nUnlike supervised learning, clustering is considered an unsupervised learning method since we don’t have the ground truth to compare the output of the clustering algorithm to the true labels to evaluate its performance. We only want to try to investigate the structure of the data by grouping the data points into distinct subgroups.\nIn this post, we will cover only Kmeans which is considered as one of the most used clustering algorithms due to its simplicity."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#kmeans-algorithm",
    "href": "posts/clustering/Kmeans-Clustering.html#kmeans-algorithm",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Kmeans Algorithm",
    "text": "Kmeans Algorithm\nKmeans algorithm is an iterative algorithm that tries to partition the dataset into \\(K\\) pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the inter-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.\nThe way kmeans algorithm works is as follows: 1. Specify number of clusters \\(K\\). 2. Initialize centroids by first shuffling the dataset and then randomly selecting \\(K\\) data points for the centroids without replacement. 3. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn’t changing. - Compute the sum of the squared distance between data points and all centroids. - Assign each data point to the closest cluster (centroid). - Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster.\nThe approach kmeans follows to solve the problem is called Expectation-Maximization. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it).\nThe objective function is: \\[J = \\sum_{i = 1}^{m}\\sum_{k = 1}^{K}w_{ik}\\|x^i - \\mu_k\\|^2\\\\{}\\] where \\(w_{ik} = 1\\) for data point \\(x^i\\) if it belongs to cluster \\(k\\); otherwise, \\(w_{ik} = 0\\). Also, \\(\\mu_k\\) is the centroid of \\(x^i\\)’s cluster.\nIt’s a minimization problem of two parts. We first minimize J w.r.t. \\(w_{ik}\\) and treat \\(\\mu_k\\) fixed. Then we minimize J w.r.t. \\(\\mu_k\\) and treat \\(w_{ik}\\) fixed. Technically speaking, we differentiate J w.r.t. \\(w_{ik}\\) first and update cluster assignments (E-step). Then we differentiate J w.r.t. \\(\\mu_{k}\\) and recompute the centroids after the cluster assignments from previous step (M-step). Therefore, E-step is: \\[\\frac{\\partial J}{\\partial w_{ik}} = \\sum_{i = 1}^{m}\\sum_{k = 1}^{K}\\|x^i - \\mu_k\\|^2\\\\{}\\] \\[\n\\Rightarrow\n\\begin{equation}\n  w_{ik} = \\begin{cases}\n    1 & \\text{if $k = arg min_j\\ \\|x^i - \\mu_j\\|^2$}\\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\\tag{1}\\\\{}\n\\] In other words, assign the data point \\(x^i\\) to the closest cluster judged by its sum of squared distance from cluster’s centroid.\nAnd M-step is: \\[\\ \\frac{\\partial J}{\\partial \\mu_k} = 2\\sum_{i = 1}^{m}w_{ik}(x^i - \\mu_k) = 0\\\\{}\\] \\[\\Rightarrow \\mu_k = \\frac{\\sum_{i = 1}^{m}w_{ik}x^i}{\\sum_{i = 1}^{m}w_{ik}}\\tag{2}\\\\{}\\] Which translates to recomputing the centroid of each cluster to reflect the new assignments.\nFew things to note here: - Since clustering algorithms including kmeans use distance-based measurements to determine the similarity between data points, it’s recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income. - Given kmeans iterative nature and the random initialization of centroids at the start of the algorithm, different initializations may lead to different clusters since kmeans algorithm may stuck in a local optimum and may not converge to global optimum. Therefore, it’s recommended to run the algorithm using different initializations of centroids and pick the results of the run that that yielded the lower sum of squared distance. - Assignment of examples isn’t changing is the same thing as no change in within-cluster variation: \\[\\frac{1}{m_k}\\sum_{i = 1}^{m_k}\\|x^i - \\mu_{c^k}\\|^2\\]"
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#implementation",
    "href": "posts/clustering/Kmeans-Clustering.html#implementation",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Implementation",
    "text": "Implementation\nWe’ll use simple implementation of kmeans here to just illustrate some concepts. Then we will use sklearn implementation that is more efficient take care of many things for us.\n\n\nCode\nimport numpy as np\nfrom numpy.linalg import norm\n\n\nclass Kmeans:\n    '''Implementing Kmeans algorithm.'''\n\n    def __init__(self, n_clusters, max_iter=100, random_state=123):\n        self.n_clusters = n_clusters\n        self.max_iter = max_iter\n        self.random_state = random_state\n\n    def initializ_centroids(self, X):\n        np.random.RandomState(self.random_state)\n        random_idx = np.random.permutation(X.shape[0])\n        centroids = X[random_idx[:self.n_clusters]]\n        return centroids\n\n    def compute_centroids(self, X, labels):\n        centroids = np.zeros((self.n_clusters, X.shape[1]))\n        for k in range(self.n_clusters):\n            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n        return centroids\n\n    def compute_distance(self, X, centroids):\n        distance = np.zeros((X.shape[0], self.n_clusters))\n        for k in range(self.n_clusters):\n            row_norm = norm(X - centroids[k, :], axis=1)\n            distance[:, k] = np.square(row_norm)\n        return distance\n\n    def find_closest_cluster(self, distance):\n        return np.argmin(distance, axis=1)\n\n    def compute_sse(self, X, labels, centroids):\n        distance = np.zeros(X.shape[0])\n        for k in range(self.n_clusters):\n            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n        return np.sum(np.square(distance))\n    \n    def fit(self, X):\n        self.centroids = self.initializ_centroids(X)\n        for i in range(self.max_iter):\n            old_centroids = self.centroids\n            distance = self.compute_distance(X, old_centroids)\n            self.labels = self.find_closest_cluster(distance)\n            self.centroids = self.compute_centroids(X, self.labels)\n            if np.all(old_centroids == self.centroids):\n                break\n        self.error = self.compute_sse(X, self.labels, self.centroids)\n    \n    def predict(self, X):\n        distance = self.compute_distance(X, old_centroids)\n        return self.find_closest_cluster(distance)"
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#applications",
    "href": "posts/clustering/Kmeans-Clustering.html#applications",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Applications",
    "text": "Applications\nkmeans algorithm is very popular and used in a variety of applications such as market segmentation, document clustering, image segmentation and image compression, etc. The goal usually when we undergo a cluster analysis is either: 1. Get a meaningful intuition of the structure of the data we’re dealing with. 2. Cluster-then-predict where different models will be built for different subgroups if we believe there is a wide variation in the behaviors of different subgroups. An example of that is clustering patients into different subgroups and build a model for each subgroup to predict the probability of the risk of having heart attack.\nIn this post, we’ll apply clustering on two cases: - Geyser eruptions segmentation (2-D dataset). - Image compression.\n\nKmeans on Geyser’s Eruptions Segmentation\nWe’ll first implement the kmeans algorithm on 2D dataset and see how it works. The dataset has 272 observations and 2 features. The data covers the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. We will try to find \\(K\\) subgroups within the data points and group them accordingly. Below is the description of the features: - eruptions (float): Eruption time in minutes. - waiting (int): Waiting time to next eruption.\nLet’s plot the data first:\n\n\nCode\n# Modules\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets.samples_generator import (make_blobs,\n                                                make_circles,\n                                                make_moons)\nfrom sklearn.cluster import KMeans, SpectralClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n%matplotlib inline\nsns.set_context('notebook')\nplt.style.use('fivethirtyeight')\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\n\n\nCode\n# Import the data\ndf = pd.read_csv('../data/old_faithful.csv')\n\n# Plot the data\nplt.figure(figsize=(6, 6))\nplt.scatter(df.iloc[:, 0], df.iloc[:, 1])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of raw data');\n\n\n\n\n\nWe’ll use this data because it’s easy to plot and visually spot the clusters since its a 2-dimension dataset. It’s obvious that we have 2 clusters. Let’s standardize the data first and run the kmeans algorithm on the standardized data with \\(K = 2\\).\n\n\nCode\n# Standardize the data\nX_std = StandardScaler().fit_transform(df)\n\n# Run local implementation of kmeans\nkm = Kmeans(n_clusters=2, max_iter=100)\nkm.fit(X_std)\ncentroids = km.centroids\n\n# Plot the clustered data\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],\n            c='green', label='cluster 1')\nplt.scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],\n            c='blue', label='cluster 2')\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,\n            c='r', label='centroid')\nplt.legend()\nplt.xlim([-2, 2])\nplt.ylim([-2, 2])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of clustered data', fontweight='bold')\nax.set_aspect('equal');\n\n\n\n\n\nThe above graph shows the scatter plot of the data colored by the cluster they belong to. In this example, we chose \\(K = 2\\). The symbol **’*’** is the centroid of each cluster. We can think of those 2 clusters as geyser had different kinds of behaviors under different scenarios.\nNext, we’ll show that different initializations of centroids may yield to different results. I’ll use 9 different random_state to change the initialization of the centroids and plot the results. The title of each plot will be the sum of squared distance of each initialization.\nAs a side note, this dataset is considered very easy and converges in less than 10 iterations. Therefore, to see the effect of random initialization on convergence, I am going to go with 3 iterations to illustrate the concept. However, in real world applications, datasets are not at all that clean and nice!\n\n\nCode\nn_iter = 9\nfig, ax = plt.subplots(3, 3, figsize=(16, 16))\nax = np.ravel(ax)\ncenters = []\nfor i in range(n_iter):\n    # Run local implementation of kmeans\n    km = Kmeans(n_clusters=2,\n                max_iter=3,\n                random_state=np.random.randint(0, 1000, size=1))\n    km.fit(X_std)\n    centroids = km.centroids\n    centers.append(centroids)\n    ax[i].scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],\n                  c='green', label='cluster 1')\n    ax[i].scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],\n                  c='blue', label='cluster 2')\n    ax[i].scatter(centroids[:, 0], centroids[:, 1],\n                  c='r', marker='*', s=300, label='centroid')\n    ax[i].set_xlim([-2, 2])\n    ax[i].set_ylim([-2, 2])\n    ax[i].legend(loc='lower right')\n    ax[i].set_title(f'{km.error:.4f}')\n    ax[i].set_aspect('equal')\nplt.tight_layout();\n\n\n\n\n\nAs the graph above shows that we only ended up with two different ways of clusterings based on different initializations. We would pick the one with the lowest sum of squared distance.\n\n\nImage Compression\nIn this part, we’ll implement kmeans to compress an image. The image that we’ll be working on is 396 x 396 x 3. Therefore, for each pixel location we would have 3 8-bit integers that specify the red, green, and blue intensity values. Our goal is to reduce the number of colors to 30 and represent (compress) the photo using those 30 colors only. To pick which colors to use, we’ll use kmeans algorithm on the image and treat every pixel as a data point. That means reshape the image from height x width x channels to (height * width) x channel, i,e we would have 396 x 396 = 156,816 data points in 3-dimensional space which are the intensity of RGB. Doing so will allow us to represent the image using the 30 centroids for each pixel and would significantly reduce the size of the image by a factor of 6. The original image size was 396 x 396 x 24 = 3,763,584 bits; however, the new compressed image would be 30 x 24 + 396 x 396 x 4 = 627,984 bits. The huge difference comes from the fact that we’ll be using centroids as a lookup for pixels’ colors and that would reduce the size of each pixel location to 4-bit instead of 8-bit.\nFrom now on we will be using sklearn implementation of kmeans. Few thing to note here:\n\nn_init is the number of times of running the kmeans with different centroid’s initialization. The result of the best one will be reported.\ntol is the within-cluster variation metric used to declare convergence.\nThe default of init is k-means++ which is supposed to yield a better results than just random initialization of centroids.\n\n\n\nCode\n# Read the image\nimg = imread('images/my_image.jpg')\nimg_size = img.shape\n\n# Reshape it to be 2-dimension\nX = img.reshape(img_size[0] * img_size[1], img_size[2])\n\n# Run the Kmeans algorithm\nkm = KMeans(n_clusters=30)\nkm.fit(X)\n\n# Use the centroids to compress the image\nX_compressed = km.cluster_centers_[km.labels_]\nX_compressed = np.clip(X_compressed.astype('uint8'), 0, 255)\n\n# Reshape X_recovered to have the same dimension as the original image 128 * 128 * 3\nX_compressed = X_compressed.reshape(img_size[0], img_size[1], img_size[2])\n\n# Plot the original and the compressed image next to each other\nfig, ax = plt.subplots(1, 2, figsize = (12, 8))\nax[0].imshow(img)\nax[0].set_title('Original Image')\nax[1].imshow(X_compressed)\nax[1].set_title('Compressed Image with 30 colors')\nfor ax in fig.axes:\n    ax.axis('off')\nplt.tight_layout();\n\n\n\n\n\nWe can see the comparison between the original image and the compressed one. The compressed image looks close to the original one which means we’re able to retain the majority of the characteristics of the original image. With smaller number of clusters we would have higher compression rate at the expense of image quality. As a side note, this image compression method is called lossy data compression because we can’t reconstruct the original image from the compressed image."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#evaluation-methods",
    "href": "posts/clustering/Kmeans-Clustering.html#evaluation-methods",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Evaluation Methods",
    "text": "Evaluation Methods\nContrary to supervised learning where we have the ground truth to evaluate the model’s performance, clustering analysis doesn’t have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. Moreover, since kmeans requires \\(k\\) as an input and doesn’t learn it from data, there is no right answer in terms of the number of clusters that we should have in any problem. Sometimes domain knowledge and intuition may help but usually that is not the case. In the cluster-predict methodology, we can evaluate how well the models are performing based on different \\(K\\) clusters since clusters are used in the downstream modeling.\nIn this post we’ll cover two metrics that may give us some intuition about \\(k\\):\n\nElbow method\nSilhouette analysis\n\n\nElbow Method\nElbow method gives us an idea on what a good \\(k\\) number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters’ centroids. We pick \\(k\\) at the spot where SSE starts to flatten out and forming an elbow. We’ll use the geyser dataset and evaluate SSE for different values of \\(k\\) and see where the curve might form an elbow and flatten out.\n\n\nCode\n# Run the Kmeans algorithm and get the index of data points clusters\nsse = []\nlist_k = list(range(1, 10))\n\nfor k in list_k:\n    km = KMeans(n_clusters=k)\n    km.fit(X_std)\n    sse.append(km.inertia_)\n\n# Plot sse against k\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, sse, '-o')\nplt.xlabel(r'Number of clusters $k$')\nplt.ylabel('Sum of squared distance');\n\n\n\n\n\nThe graph above shows that \\(k = 2\\) is not a good choice. Sometimes it’s still hard to figure out a good number of clusters to use because the curve is monotonically decreasing and may not show any elbow or has an obvious point where the curve starts flattening out.\n\n\nSilhouette Analysis\nSilhouette analysis can be used to determine the degree of separation between clusters. For each sample:\n\nCompute the average distance from all data points in the same cluster (\\(a^i\\)).\nCompute the average distance from all data points in the closest cluster (\\(b^i\\)).\nCompute the coefficient: \\[\\frac{b^i - a^i}{max(a^i, b^i)}\\] The coefficient can take values in the interval [-1, 1].\n\nIf it is 0 –> the sample is very close to the neighboring clusters.\nIt it is 1 –> the sample is far away from the neighboring clusters.\nIt it is -1 –> the sample is assigned to the wrong clusters.\n\n\nTherefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters. We’ll use here geyser dataset again because its cheaper to run the silhouette analysis and it is actually obvious that there is most likely only two groups of data points.\n\n\nCode\nfor i, k in enumerate([2, 3, 4]):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n    \n    # Run the Kmeans algorithm\n    km = KMeans(n_clusters=k)\n    labels = km.fit_predict(X_std)\n    centroids = km.cluster_centers_\n\n    # Get silhouette samples\n    silhouette_vals = silhouette_samples(X_std, labels)\n\n    # Silhouette plot\n    y_ticks = []\n    y_lower, y_upper = 0, 0\n    for i, cluster in enumerate(np.unique(labels)):\n        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n        cluster_silhouette_vals.sort()\n        y_upper += len(cluster_silhouette_vals)\n        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n        y_lower += len(cluster_silhouette_vals)\n\n    # Get the average silhouette score and plot it\n    avg_score = np.mean(silhouette_vals)\n    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n    ax1.set_yticks([])\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_xlabel('Silhouette coefficient values')\n    ax1.set_ylabel('Cluster labels')\n    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n    \n    # Scatter plot of data colored with labels\n    ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels)\n    ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n    ax2.set_xlim([-2, 2])\n    ax2.set_xlim([-2, 2])\n    ax2.set_xlabel('Eruption time in mins')\n    ax2.set_ylabel('Waiting time to next eruption')\n    ax2.set_title('Visualization of clustered data', y=1.02)\n    ax2.set_aspect('equal')\n    plt.tight_layout()\n    plt.suptitle(f'Silhouette analysis using k = {k}',\n                 fontsize=16, fontweight='semibold', y=1.05);\n\n\n\n\n\n\n\n\n\n\n\nAs the above plots show, n_clusters=2 has the best average silhouette score of around 0.75 and all clusters being above the average shows that it is actually a good choice. Also, the thickness of the silhouette plot gives an indication of how big each cluster is. The plot shows that cluster 1 has almost double the samples than cluster 2. However, as we increased n_clusters to 3 and 4, the average silhouette score decreased dramatically to around 0.48 and 0.39 respectively. Moreover, the thickness of silhouette plot started showing wide fluctuations. The bottom line is: Good n_clusters will have a well above 0.5 silhouette average score as well as all of the clusters have higher than the average score."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#drawbacks",
    "href": "posts/clustering/Kmeans-Clustering.html#drawbacks",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Drawbacks",
    "text": "Drawbacks\nKmeans algorithm is good in capturing structure of the data if clusters have a spherical-like shape. It always try to construct a nice spherical shape around the centroid. That means, the minute the clusters have a complicated geometric shapes, kmeans does a poor job in clustering the data. We’ll illustrate three cases where kmeans will not perform well.\nFirst, kmeans algorithm doesn’t let data points that are far-away from each other share the same cluster even though they obviously belong to the same cluster. Below is an example of data points on two different horizontal lines that illustrates how kmeans tries to group half of the data points of each horizontal lines together.\n\n\nCode\n# Create horizantal data\nX = np.tile(np.linspace(1, 5, 20), 2)\ny = np.repeat(np.array([2, 4]), 20)\ndf = np.c_[X, y]\n\nkm = KMeans(n_clusters=2)\nkm.fit(df)\nlabels = km.predict(df)\ncentroids = km.cluster_centers_\n\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.scatter(X, y, c=labels)\nplt.xlim([0, 6])\nplt.ylim([0, 6])\nplt.text(5.1, 4, 'A', color='red')\nplt.text(5.1, 2, 'B', color='red')\nplt.text(2.8, 4.1, 'C', color='red')\nax.set_aspect('equal')\n\n\n\n\n\nKmeans considers the point ‘B’ closer to point ‘A’ than point ‘C’ since they have non-spherical shape. Therefore, points ‘A’ and ‘B’ will be in the same cluster but point ‘C’ will be in a different cluster. Note the Single Linkage hierarchical clustering method gets this right because it doesn’t separate similar points).\nSecond, we’ll generate data from multivariate normal distributions with different means and standard deviations. So we would have 3 groups of data where each group was generated from different multivariate normal distribution (different mean/standard deviation). One group will have a lot more data points than the other two combined. Next, we’ll run kmeans on the data with \\(K = 3\\) and see if it will be able to cluster the data correctly. To make the comparison easier, I am going to plot first the data colored based on the distribution it came from. Then I will plot the same data but now colored based on the clusters they have been assigned to.\n\n\nCode\n# Create data from three different multivariate distributions\nX_1 = np.random.multivariate_normal(mean=[4, 0], cov=[[1, 0], [0, 1]], size=75)\nX_2 = np.random.multivariate_normal(mean=[6, 6], cov=[[2, 0], [0, 2]], size=250)\nX_3 = np.random.multivariate_normal(mean=[1, 5], cov=[[1, 0], [0, 2]], size=20)\ndf = np.concatenate([X_1, X_2, X_3])\n\n# Run kmeans\nkm = KMeans(n_clusters=3)\nkm.fit(df)\nlabels = km.predict(df)\ncentroids = km.cluster_centers_\n\n# Plot the data\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\nax[0].scatter(X_1[:, 0], X_1[:, 1])\nax[0].scatter(X_2[:, 0], X_2[:, 1])\nax[0].scatter(X_3[:, 0], X_3[:, 1])\nax[0].set_aspect('equal')\nax[1].scatter(df[:, 0], df[:, 1], c=labels)\nax[1].scatter(centroids[:, 0], centroids[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\nfor i, c in enumerate(centroids):\n    ax[1].scatter(c[0], c[1], marker='$%d$' % i, s=50, alpha=1, edgecolor='r')\nax[1].set_aspect('equal')\nplt.tight_layout()\n\n\n\n\n\nLooks like kmeans couldn’t figure out the clusters correctly. Since it tries to minimize the within-cluster variation, it gives more weight to bigger clusters than smaller ones. In other words, data points in smaller clusters may be left away from the centroid in order to focus more on the larger cluster.\nLast, we’ll generate data that have complicated geometric shapes such as moons and circles within each other and test kmeans on both of the datasets.\n\n\nCode\n# Cricles\nX1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n\n# Moons\nX2 = make_moons(n_samples=1500, noise=0.05)\n\nfig, ax = plt.subplots(1, 2)\nfor i, X in enumerate([X1, X2]):\n    fig.set_size_inches(18, 7)\n    km = KMeans(n_clusters=2)\n    km.fit(X[0])\n    labels = km.predict(X[0])\n    centroids = km.cluster_centers_\n\n    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=labels)\n    ax[i].scatter(centroids[0, 0], centroids[0, 1], marker='*', s=400, c='r')\n    ax[i].scatter(centroids[1, 0], centroids[1, 1], marker='+', s=300, c='green')\nplt.suptitle('Simulated data', y=1.05, fontsize=22, fontweight='semibold')\nplt.tight_layout()\n\n\n\n\n\nAs expected, kmeans couldn’t figure out the correct clusters for both datasets. However, we can help kmeans perfectly cluster these kind of datasets if we use kernel methods. The idea is we transform to higher dimensional representation that make the data linearly separable (the same idea that we use in SVMs). Different kinds of algorithms work very well in such scenarios such as SpectralClustering, see below:\n\n\nCode\n# Cricles\nX1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n\n# Moons\nX2 = make_moons(n_samples=1500, noise=0.05)\n\nfig, ax = plt.subplots(1, 2)\nfor i, X in enumerate([X1, X2]):\n    fig.set_size_inches(18, 7)\n    sp = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')\n    sp.fit(X[0])\n    labels = sp.labels_\n    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=labels)\nplt.suptitle('Simulated data', y=1.05, fontsize=22, fontweight='semibold')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#conclusion",
    "href": "posts/clustering/Kmeans-Clustering.html#conclusion",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Conclusion",
    "text": "Conclusion\nKmeans clustering is one of the most popular clustering algorithms and usually the first thing practitioners apply when solving clustering tasks to get an idea of the structure of the dataset. The goal of kmeans is to group data points into distinct non-overlapping subgroups. It does a very good job when the clusters have a kind of spherical shapes. However, it suffers as the geometric shapes of clusters deviates from spherical shapes. Moreover, it also doesn’t learn the number of clusters from the data and requires it to be pre-defined. To be a good practitioner, it’s good to know the assumptions behind algorithms/methods so that you would have a pretty good idea about the strength and weakness of each method. This will help you decide when to use each method and under what circumstances. In this post, we covered both strength, weaknesses, and some evaluation methods related to kmeans.\nBelow are the main takeaways:\n\nScale/standardize the data when applying kmeans algorithm.\nElbow method in selecting number of clusters doesn’t usually work because the error function is monotonically decreasing for all \\(k\\)s.\nKmeans gives more weight to the bigger clusters.\nKmeans assumes spherical shapes of clusters (with radius equal to the distance between the centroid and the furthest data point) and doesn’t work well when clusters are in different shapes such as elliptical clusters.\nIf there is overlapping between clusters, kmeans doesn’t have an intrinsic measure for uncertainty for the examples belong to the overlapping region in order to determine for which cluster to assign each data point.\nKmeans may still cluster the data even if it can’t be clustered such as data that comes from uniform distributions."
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#introduction",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#introduction",
    "title": "Character-Level Language Model",
    "section": "Introduction",
    "text": "Introduction\nHave you ever wondered how Gmail automatic reply works? Or how a Neural Network can generate musical notes? The general way of generating sequence of text is to train a model to predict the next word/character given all previous words/characters. Such model is called Statistical Language Model. So what is a statistical language model? A statistical language model tries to capture the statistical structure (latent space) of training text it’s trained on. Usually Recurrent Neural Network (RNN) models family is used to train the model due to the fact that it’s very powerful and expressive in which they remember and process past information through their high dimensional hidden state units. The main goal of any language model is to learn the joint probability distribution of sequences of characters/words in a training text, i.e. trying to learn the joint probability function. For example, if we’re trying to predict a sequence of \\(T\\) words, we try to get the joint probability \\(P(w_1, w_2, ..., w_T)\\) as big as we can which is equal to the product of all conditional probabilities \\(\\prod_{t = 1}^T P(w_t/w_{t-1})\\) at all time steps (t).\nIn this notebook, we’ll cover Character-level Language Model where almost all the concepts hold for any other language models such as word-language models. The main task of character-level language model is to predict next character given all previous characters in a sequence of data, i.e. generate text character by character. More formally, given a training sequence \\((x^1, ... , x^T)\\), the RNN uses the sequence of its output vectors \\((o^1, ... , o^T)\\) to obtain a sequence of predictive distributions \\(P(x^t|x^{<t}) = softmax(o^t)\\).\nLet’s illustrate how the character-level language model works using my first name (“imad”) as an example (see figure 1 for all the details of this example). 1. We first build a vocabulary dictionary using all the unique letters of the names in the corpus as keys and the index of each letter starting from zero (since python is a zero-index language) in ascending order. For our example, the vocabulary dictionary would be: {“a”: 0, “d”: 1, “i”: 2, “m”: 3}. Therefore, “imad” would become a list of the following integers: [2, 3, 0, 1]. 2. Convert the input and the output characters to lists of integers using the vocabulary dictionary. In this notebook, we’ll assume that \\(x^1 = \\vec{0}\\) for all examples. Therefore, \\(y = \"imad\"\\) and \\(x = \\vec{0}\\ + \"ima\"\\). In other words, \\(x^{t + 1} = y^t\\) which gives us: \\(y = [2, 3, 0, 1]\\) and \\(x = [\\vec{0}, 2, 3, 0]\\). 3. For each character in the input: 1. Convert the input characters into one-hot vectors. Notice how the first character \\(x^1 = \\vec{0}\\). 2. Compute the hidden state layer. 3. Compute the output layer and then pass it through softmax to get the results as probabilities. 4. Feed the target character at time step (t) as the input character at time step \\((t + 1)\\). 5. Go back to step A and repeat until we finish all the letters in the name.\nThe objective is to make the green numbers as big as we can and the red numbers as small as we can in the probability distribution layer. The reason for that is that the true index should have the highest probability by making it as close as we can to 1. The way to do that is to measure the loss using cross-entropy and then compute the gradients of the loss w.r.t. all parameters to update them in the opposite of the gradient direction. Repeating the process over many times where each time we adjust the parameters based on the gradient direction –> model will be able to correctly predict next characters given all previous one using all names in the training text. Notice that hidden state \\(h^4\\) has all past information about all characters.\n\n\n\n\nFigure 1: Illustrative example of character-level language model using RNN"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#training",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#training",
    "title": "Character-Level Language Model",
    "section": "Training",
    "text": "Training\nThe dataset we’ll be using has 5,163 names: 4,275 male names, 1,219 female names, and 331 names that can be both female and male names. The RNN architecture we’ll be using to train the character-level language model is called many to many where time steps of the input \\((T_x)\\) = time steps of the output \\((T_y)\\). In other words, the sequence of the input and output are synced (see figure 2).\n\n\n\n\nFigure 2: RNN architecture: many to many\n\n\n\nThe character-level language model will be trained on names; which means after we’re done with training the model, we’ll be able to generate interesting names :).\nIn this section, we’ll go over four main parts:\n\nForward propagation.\nBackpropagation\nSampling\nFitting the model"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#forward-propagation",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#forward-propagation",
    "title": "Character-Level Language Model",
    "section": "Forward Propagation",
    "text": "Forward Propagation\nWe’ll be using Stochastic Gradient Descent (SGD) where each batch consists of only one example. In other words, the RNN model will learn from each example (name) separately, i.e. run both forward and backward passes on each example and update parameters accordingly. Below are all the steps needed for a forward pass: - Create a vocabulary dictionary using the unique lower case letters. - Create a character to index dictionary that maps each character to its corresponding index in an ascending order. For example, “a” would have index 1 (since python is a zero index language and we’ll reserve 0 index to EOS “”) and “z” would have index 26. We will use this dictionary in converting names into lists of integers where each letter will be represented as one-hot vector. - Create an index to character dictionary that maps indices to characters. This dictionary will be used to convert the output of the RNN model into characters which will be translated into names. - Initialize parameters: weights will be initialized to small random numbers from standard normal distribution to break symmetry and make sure different hidden units learn different things. On the other hand, biases will be initialized to zeros. - \\(W_{hh}\\): weight matrix connecting previous hidden state \\(h^{t - 1}\\) to current hidden state \\(h^t\\). - \\(W_{xh}\\): weight matrix connecting input \\(x^t\\) to hidden state \\(h^t\\). - \\(b\\): hidden state bias vector. - \\(W_{hy}\\): weight matrix connecting hidden state \\(h^t\\) to output \\(o^t\\). - \\(c\\): output bias vector. - Convert input \\(x^t\\) and output \\(y^t\\) into one-hot vector each. The dimension of the one-hot vector is vocab_size x 1. Everything will be zero except for the index of the letter at (t) would be 1. In our case, \\(x^t\\) would be the same as \\(y^t\\) shifted to the left where \\(x^1 = \\vec{0}\\); however, starting from \\(t = 2\\), \\(x^{t + 1} = y^{t}\\). For example, if we use “imad” as the input, then \\(y = [3, 4, 1, 2, 0]\\) while \\(x = [\\vec{0}, 3, 4, 1, 2]\\). Notice that \\(x^1 = \\vec{0}\\) and not the index 0. Moreover, we’re using “” as EOS (end of sentence/name) for each name so that the RNN learns “” as any other character so that it knows when to stop generating characters. Therefore, the last target character for all names will be “” that represents the end of the name. - Compute the hidden state using the following formula: \\[h^t = tanh(W_{hh}h^{t-1} + W_{xh}x^t + b)\\tag{1}\\\\{}\\] Notice that we use hyperbolic tangent \\((\\frac{e^x - e^{-x}}{e^x + e^{-x}})\\) as the non-linear function. One of the main advantages of the hyperbolic tangent function is that it resembles the identity function. - Compute the output layer using the following formula: \\[o^t = W_{hy}h^{t} + c\\tag{2}\\\\{}\\] - Pass the output through softmax layer to normalize the output that allows us to express it as a probability, i.e. all output will be between 0 and 1 and sum up to 1. Below is the softmax formula: \\[y^t = \\frac{e^{o^t}}{\\sum_ie^{o^t}}\\tag{3}\\\\{}\\] The softmax layer has the same dimension as the output layer which is vocab_size x 1. As a result, \\(y^t[i]\\) is the probability of of index \\(i\\) being the next character at time step (t). - As mentioned before, the objective of a character-level language model is to minimize the negative log-likelihood of the training sequence. Therefore, the loss function at time (t) and the total loss across all time steps are: \\[\\mathcal{L}^t = -\\sum_{i = 1}^{T_y}y^tlog\\widehat{y^t}\\tag{4}\\\\{}\\] \\[\\mathcal{L} = \\sum_{t = 1}^{T_y}\\mathcal{L}^t(\\widehat{y^t}, y^t)\\tag{5}\\] Since we’ll be using SGD, the loss will be noisy and have many oscillations, so it’s a good practice to smooth out the loss using exponential weighted average. - Pass the target character \\(y^t\\) as the next input \\(x^{t + 1}\\) until we finish the sequence.\n\n\nCode\n# Load packages\n# | warning: false\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nos.chdir(\"../scripts/\")\nfrom character_level_language_model import (\n    initialize_parameters,\n    initialize_rmsprop,\n    softmax,\n    smooth_loss,\n    update_parameters_with_rmsprop,\n)\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\n\n\n\n\nCode\ndef rnn_forward(x, y, h_prev, parameters):\n    \"\"\"\n    Implement one Forward pass on one name.\n\n    Arguments\n    ---------\n    x : list\n        list of integers for the index of the characters in the example\n        shifted one character to the right.\n    y : list\n        list of integers for the index of the characters in the example.\n    h_prev : array\n        last hidden state from the previous example.\n    parameters : python dict\n        dictionary containing the parameters.\n\n    Returns\n    -------\n    loss : float\n        cross-entropy loss.\n    cache : tuple\n        contains three python dictionaries:\n            xs -- input of all time steps.\n            hs -- hidden state of all time steps.\n            probs -- probability distribution of each character at each time\n                step.\n    \"\"\"\n    # Retrieve parameters\n    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n    Why, c = parameters[\"Why\"], parameters[\"c\"]\n\n    # Initialize inputs, hidden state, output, and probabilities dictionaries\n    xs, hs, os, probs = {}, {}, {}, {}\n\n    # Initialize x0 to zero vector\n    xs[0] = np.zeros((vocab_size, 1))\n\n    # Initialize loss and assigns h_prev to last hidden state in hs\n    loss = 0\n    hs[-1] = np.copy(h_prev)\n\n    # Forward pass: loop over all characters of the name\n    for t in range(len(x)):\n        # Convert to one-hot vector\n        if t > 0:\n            xs[t] = np.zeros((vocab_size, 1))\n            xs[t][x[t]] = 1\n        # Hidden state\n        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n        # Logits\n        os[t] = np.dot(Why, hs[t]) + c\n        # Probs\n        probs[t] = softmax(os[t])\n        # Loss\n        loss -= np.log(probs[t][y[t], 0])\n\n    cache = (xs, hs, probs)\n\n    return loss, cache"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#backpropagation",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#backpropagation",
    "title": "Character-Level Language Model",
    "section": "Backpropagation",
    "text": "Backpropagation\nWith RNN based models, the gradient-based technique that will be used is called Backpropagation Through Time (BPTT). We start at last time step \\(T\\) and backpropagate loss function w.r.t. all parameters across all time steps and sum them up (see figure 3).\n\n\n\n\nFigure 3: Backpropagation Through Time (BPTT)\n\n\n\nIn addition, since recurrent networks are known to have steep cliffs (sudden steep decrease in \\(\\mathcal{L}\\)), gradients may overshoot the minimum and undo a lot of the work that was done even if we are using adaptive learning methods such as RMSProp. The reason for that is that because gradient is a linear approximation of the loss function and may not capture information further than the point it was evaluated on such as the curvature of loss curve. Therefore, it’s a common practice to clip the gradients to be in the interval [-maxValue, maxValue]. For this exercise, we’ll clip the gradients to be in the interval [-5, 5]. That means if the gradient is > 5 or < -5, it would be clipped to 5 and -5 respectively. Below are all the formulas needed to compute the gradients w.r.t. all parameters at all time steps.\n\\[\\nabla_{o^t}\\mathcal{L} = \\widehat{y^t} - y^t\\tag{6}\\\\{}\\] \\[\\nabla_{W_{hy}}\\mathcal{L} = \\sum_t \\nabla_{o^t}\\mathcal{L}\\cdot{h^t}^T\\tag{7}\\\\{}\\] \\[\\nabla_{c}\\mathcal{L} = \\sum_t \\nabla_{o^t}\\mathcal{L} \\tag{8}\\\\{}\\] \\[\\nabla_{h^t}\\mathcal{L} = W_{hy}^T\\cdot\\nabla_{o^t}\\mathcal{L} + \\underbrace { W_{hh}^T\\cdot\\nabla_{h^{t + 1}}\\mathcal{L} * (1 - tanh(W_{hh}h^{t} + W_{xh}x^{t + 1} + b) ^ 2)}_{dh_{next}} \\tag{9}\\\\{}\\] \\[\\nabla_{h^{t - 1}}\\mathcal{L} = W_{hh}^T\\cdot\\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2)\\tag{10}\\\\{}\\] \\[\\nabla_{x^t}\\mathcal{L} = W_{xh}^T\\cdot\\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2)\\tag{11}\\\\{}\\] \\[\\nabla_{W_{hh}}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2)\\cdot{h^{t - 1}}^T\\tag{12}\\\\{}\\] \\[\\nabla_{W_{xh}}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2) . {x^t}^T\\tag{13}\\\\{}\\] \\[\\nabla_{b}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2) \\tag{14}\\\\{}\\]\nNote that at last time step \\(T\\), we’ll initialize \\(dh_{next}\\) to zeros since we can’t get values from future. To stabilize the update at each time step since SGD may have so many oscillations, we’ll be using one of the adaptive learning methods’ optimizer. More specifically, Root Mean Squared Propagation (RMSProp) which tends to have acceptable performance.\n\n\nCode\ndef clip_gradients(gradients, max_value):\n    \"\"\"\n    Implements gradient clipping element-wise on gradients to be between the\n    interval [-max_value, max_value].\n\n    Arguments\n    ----------\n    gradients : python dict\n        dictionary that stores all the gradients.\n    max_value : scalar\n        edge of the interval [-max_value, max_value].\n\n    Returns\n    -------\n    gradients : python dict\n        dictionary where all gradients were clipped.\n    \"\"\"\n    for grad in gradients.keys():\n        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n\n    return gradients\n\n\ndef rnn_backward(y, parameters, cache):\n    \"\"\"\n    Implements Backpropagation on one name.\n\n    Arguments\n    ---------\n    y : list\n        list of integers for the index of the characters in the example.\n    parameters : python dict\n        dictionary containing the parameters.\n    cache : tuple\n            contains three python dictionaries:\n                xs -- input of all time steps.\n                hs -- hidden state of all time steps.\n                probs -- probability distribution of each character at each time\n                    step.\n\n    Returns\n    -------\n    grads : python dict\n        dictionary containing all the gradients.\n    h_prev : array\n        last hidden state from the current example.\n    \"\"\"\n    # Retrieve xs, hs, and probs\n    xs, hs, probs = cache\n\n    # Initialize all gradients to zero\n    dh_next = np.zeros_like(hs[0])\n\n    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n    grads = {}\n    for param_name in parameters_names:\n        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n\n    # Iterate over all time steps in reverse order starting from Tx\n    for t in reversed(range(len(xs))):\n        dy = np.copy(probs[t])\n        dy[y[t]] -= 1\n        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n        grads[\"dc\"] += dy\n        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n        dhraw = (1 - hs[t] ** 2) * dh\n        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n        grads[\"db\"] += dhraw\n        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n        # Clip the gradients using [-5, 5] as the interval\n        grads = clip_gradients(grads, 5)\n    # Get the last hidden state\n    h_prev = hs[len(xs) - 1]\n\n    return grads, h_prev"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#sampling",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#sampling",
    "title": "Character-Level Language Model",
    "section": "Sampling",
    "text": "Sampling\nSampling is what makes the text generated by the RNN at each time step an interesting/creative text. On each time step (t), the RNN output the conditional probability distribution of the next character given all the previous characters, i.e. \\(P(c_t/c_1, c_2, ..., c_{t-1})\\). Let’s assume that we are at time step \\(t = 3\\) and we’re trying to predict the third character, the conditional probability distribution is: \\(P(c_3/c_1, c_2) = (0.2, 0.3, 0.4, 0.1)\\). We’ll have two extremes: 1. Maximum entropy: the character will be picked randomly using uniform probability distribution; which means that all characters in the vocabulary dictionary are equally likely. Therefore, we’ll end up with maximum randomness in picking the next character and the generated text will not be either meaningful or sound real. 2. Minimum entropy: the character with the highest conditional probability will be picked on each time step. That means next character will be what the model estimates to be the right one based on the training text and learned parameters. As a result, the name generated will be both meaningful and sound real. However, it will also be repetitive and interesting since all the parameters were optimized to learn joint probability distribution in predicting the next character.\nAs we increase randomness, text will loose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure. For this exercise, we will sample from the distribution that’s generated by the model which can be seen as an intermediate level of randomness between maximum and minimum entropy (see figure 4). Using this sampling strategy on the above distribution, the index 0 has \\(20\\)% probability of being picked, while index 2 has \\(40\\)% probability to be picked.\n\n\n\n\nFigure 4: Sampling: An example of predicting next character using character-level language model\n\n\n\nTherefore, sampling will be used at test time to generate names character by character.\n\n\nCode\ndef sample(parameters, idx_to_chars, chars_to_idx, n):\n    \"\"\"\n    Implements sampling of a squence of n characters characters length. The\n    sampling will be based on the probability distribution output of RNN.\n\n    Arguments\n    ---------\n    parameters : python dict\n        dictionary storing all the parameters of the model.\n    idx_to_chars : python dict\n        dictionary mapping indices to characters.\n    chars_to_idx : python dict\n        dictionary mapping characters to indices.\n    n : scalar\n        number of characters to output.\n\n    Returns\n    -------\n    sequence : str\n        sequence of characters sampled.\n    \"\"\"\n    # Retrienve parameters, shapes, and vocab size\n    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n    Why, c = parameters[\"Why\"], parameters[\"c\"]\n    n_h, n_x = Wxh.shape\n    vocab_size = c.shape[0]\n\n    # Initialize a0 and x1 to zero vectors\n    h_prev = np.zeros((n_h, 1))\n    x = np.zeros((n_x, 1))\n\n    # Initialize empty sequence\n    indices = []\n    idx = -1\n    counter = 0\n    while counter <= n and idx != chars_to_idx[\"\\n\"]:\n        # Fwd propagation\n        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n        o = np.dot(Why, h) + c\n        probs = softmax(o)\n\n        # Sample the index of the character using generated probs distribution\n        idx = np.random.choice(vocab_size, p=probs.ravel())\n\n        # Get the character of the sampled index\n        char = idx_to_chars[idx]\n\n        # Add the char to the sequence\n        indices.append(idx)\n\n        # Update a_prev and x\n        h_prev = np.copy(h)\n        x = np.zeros((n_x, 1))\n        x[idx] = 1\n\n        counter += 1\n    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n\n    return sequence"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#fitting-the-model",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#fitting-the-model",
    "title": "Character-Level Language Model",
    "section": "Fitting the model",
    "text": "Fitting the model\nAfter covering all the concepts/intuitions behind character-level language model, now we’re ready to fit the model. We’ll use the default settings for RMSProp’s hyperparameters and run the model for 100 iterations. On each iteration, we’ll print out one sampled name and smoothed loss to see how the names generated start to get more interesting with more iterations as well as the loss will start decreasing. When done with fitting the model, we’ll plot the loss function and generate some names.\n\n\nCode\ndef model(\n    file_path,\n    chars_to_idx,\n    idx_to_chars,\n    hidden_layer_size,\n    vocab_size,\n    num_epochs=10,\n    learning_rate=0.01,\n):\n    \"\"\"\n    Implements RNN to generate characters.\n\n    Arguments\n    ---------\n    file_path : str\n        path to the file of the raw data.\n    num_epochs : int\n        number of passes the optimization algorithm to go over the training\n        data.\n    learning_rate : float\n        step size of learning.\n    chars_to_idx : python dict\n        dictionary mapping characters to indices.\n    idx_to_chars : python dict\n        dictionary mapping indices to characters.\n    hidden_layer_size : int\n        number of hidden units in the hidden layer.\n    vocab_size : int\n        size of vocabulary dictionary.\n\n    Returns\n    -------\n    parameters : python dict\n        dictionary storing all the parameters of the model.\n    overall_loss : list\n        list stores smoothed loss per epoch.\n    \"\"\"\n    # Get the data\n    with open(file_path) as f:\n        data = f.readlines()\n    examples = [x.lower().strip() for x in data]\n\n    # Initialize parameters\n    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n\n    # Initialize Adam parameters\n    s = initialize_rmsprop(parameters)\n\n    # Initialize loss\n    smoothed_loss = -np.log(1 / vocab_size) * 7\n\n    # Initialize hidden state h0 and overall loss\n    h_prev = np.zeros((hidden_layer_size, 1))\n    overall_loss = []\n\n    # Iterate over number of epochs\n    for epoch in range(num_epochs):\n        # Shuffle examples\n        np.random.shuffle(examples)\n\n        # Iterate over all examples (SGD)\n        for example in examples:\n            x = [None] + [chars_to_idx[char] for char in example]\n            y = x[1:] + [chars_to_idx[\"\\n\"]]\n            # Fwd pass\n            loss, cache = rnn_forward(x, y, h_prev, parameters)\n            # Compute smooth loss\n            smoothed_loss = smooth_loss(smoothed_loss, loss)\n            # Bwd pass\n            grads, h_prev = rnn_backward(y, parameters, cache)\n            # Update parameters\n            parameters, s = update_parameters_with_rmsprop(parameters, grads, s)\n\n        overall_loss.append(smoothed_loss)\n        if epoch % 10 == 0:\n            print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n            print(f\"\\033[1m\\033[92m=======\")\n            # Sample one name\n            print(\n                f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n                10).capitalize()}\"\"\"\n            )\n            print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n\n    return parameters, overall_loss\n\n\n\n\nCode\n# Load names\ndata = open(\"../data/names.txt\", \"r\").read()\n\n# Convert characters to lower case\ndata = data.lower()\n\n# Construct vocabulary using unique characters, sort it in ascending order,\n# then construct two dictionaries that maps character to index and index to\n# characters.\nchars = list(sorted(set(data)))\nchars_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_chars = {i: ch for ch, i in chars_to_idx.items()}\n\n# Get the size of the data and vocab size\ndata_size = len(data)\nvocab_size = len(chars_to_idx)\nprint(f\"There are {data_size} characters and {vocab_size} unique characters.\")\n\n# Fitting the model\nparameters, loss = model(\n    \"../data/names.txt\", chars_to_idx, idx_to_chars, 10, vocab_size, 50, 0.01\n)\n\n# Plotting the loss\nplt.plot(range(len(loss)), loss)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Smoothed loss\")\n\n\nThere are 36121 characters and 27 unique characters.\nEpoch 0\n=======\nSampled name: Ia\nSmoothed loss: 17.8206\n\nEpoch 10\n=======\nSampled name: Rioee\nSmoothed loss: 15.8061\n\nEpoch 20\n=======\nSampled name: Allise\nSmoothed loss: 15.8609\n\nEpoch 30\n=======\nSampled name: Ininyo\nSmoothed loss: 15.7734\n\nEpoch 40\n=======\nSampled name: Miadoe\nSmoothed loss: 15.7312\n\n\n\nText(0, 0.5, 'Smoothed loss')\n\n\n\n\n\nAs you may notice, the names generated started to get more interesting after 15 epochs. One of the interesting names is “Yasira” which is an Arabic name :)."
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#conclusion",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#conclusion",
    "title": "Character-Level Language Model",
    "section": "Conclusion",
    "text": "Conclusion\nStatistical language models are very crucial in Natural Language Processing (NLP) such as speech recognition and machine translation. We demonstrated in this notebook the main concepts behind statistical language models using character-level language model. The task of this model is generate names character by character using names obtained from census data that were consisted of 5,163 names. Below are the main key takeaways: - If we have more data, bigger model, and train longer we may get more interesting results. However, to get a very interesting results, we should instead use Long Short_Term Memory (LSTM) model with more than one layer deep. People have used 3 layers deep LSTM model with dropout and were able to generate very interesting results when applied on cook books and Shakespeare poems. LSTM models outperform simple RNN due to its ability in capturing longer time dependencies. - With the sampling technique we’re using, don’t expect the RNN to generate meaningful sequence of characters (names). - We used in this notebook each name as its own sequence; however, we may be able to speed up learning and get better results if we increase the batch size lets say from one name to a sequence of 50 characters. - We can control the level of randomness using the sampling strategy. Here, we balanced between what the model thinks its the right character and the level of randomness."
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#introduction",
    "href": "posts/conda-essentials/conda-essentials.html#introduction",
    "title": "Conda Essentials",
    "section": "Introduction",
    "text": "Introduction\nConda in an open source package management system that works on all platforms. It is a tool that helps manage packages and environments for different programming languages. Develop a high level understanding of how Conda works helped me at so many levels especially when it comes to managing environments and make my work more reproducable. Below are the notes that I wrote down during my journey of learning Conda and I always refere back to them:"
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#general",
    "href": "posts/conda-essentials/conda-essentials.html#general",
    "title": "Conda Essentials",
    "section": "General",
    "text": "General\n\nConda packages are files and executables that can in principle contain images, data, noteboeeks, files, etc.\nConda mainly used in Python ecosystem; however, it can be used with other languages such R, Julia, Scala, etc.\nWhen installing a package using Conda, it installs its dependencies with it. Also, Conda is able to figure out the platform you’re using without the need to specify the platform when installing packages.\nWhen installing a package, Conda:\n\nChecks the platform.\nChecks the Python version.\nInstall the latest version of the package that is compatible with Python.\nIf it has dependencies, installs the latest versions of the dependencies that are also compatible with each other.\n\nUnder semantic versioning, software is labeled with a three-part version identifier of the form MAJOR.MINOR.PATCH; the label components are non-negative integers separated by periods. Assuming all software starts at version 0.0.0, the MAJOR version number is increased when significant new functionality is introduced (often with corresponding API changes). Increases in the MINOR version number generally reflect improvements (e.g., new features) that avoid backward-incompatible API changes. For instance, adding an optional argument to a function API (in a way that allows old code to run unchanged) is a change worthy of increasing the MINOR version number. An increment to the PATCH version number is approriate mostly for bug fixes that preserve the same MAJOR and MINOR revision numbers. Software patches do not typically introduce new features or change APIs at all (except sometimes to address security issues).\nWe can specify MAJOR, MAJOR.MINOR, or MAJOR.MINOR.PATCH when installing any package.\nWe can use logical operators to install versions of a package. Examples:\n\nconda install 'python=3.6|3.7'.\nconda install 'python=3.6|3.7*' .\nconda install 'python>=3.6, <=3.7'."
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#common-commands",
    "href": "posts/conda-essentials/conda-essentials.html#common-commands",
    "title": "Conda Essentials",
    "section": "Common Commands",
    "text": "Common Commands\n\nTo update a package, conda update pckg.\nTo uninstall a package, conda remove pckg.\nTo search what available versions of a specific package is available, use conda search pckg.\nconda list will list all installed packages.\nconda list -n env-name will list all packages in the environment env-name.\nconda list pckg will give information about pckg.\nWhen installing a pckg without including a channel, it defaults to the main channel that is maintained by Anaconda Inc.\nThere other channels where people can upload their packages to and we can reach to those channels when looking for installation such fastai. We use conda install -c fastai fastai. Here the channel is fastai and the pckg is also fastai.\nconda search -c conda-forge -c fastai --override-channels --platform osx-64 fastai means:\n\nSearch for fastai in two channels: conda-forge, fastai.\noverride-channels means do not go to default main channel.\nplatform specify which platform.\n\nSometimes we don’t know the channel of the pckg, we can use anaconda search pckg that will return all the channels that the pckg is at and their versions.\nconda-forge is almost as good as the main channels which is led by the community. It has a lot more packages than the main channel.\nThere is no system that rates channels, so be carefel when installing packages from any channel.\nWe can list all packages in a channel such as conda search -c conda-forge --override-channels that will list all packages for the conda-forge channel."
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#environments",
    "href": "posts/conda-essentials/conda-essentials.html#environments",
    "title": "Conda Essentials",
    "section": "Environments",
    "text": "Environments\n\nEnvironments are a good practice of documenting data science/software development work.\nEnvironments are nothing more than a directory that contains all the packages so that when trying to import them, it imports them from this directory only. we can use conda env list to see all the available environments on our machine.\nTo get the packages from a specific environment by name, use conda list -n env-name. Otherwise, we get the packages from the current environment.\nTo activate an environment, use conda activate env-name. To deactivate, conda deactivate.\nEnvironments usually don’t take a lot of space.\nWe can remove environments using conda env remove -n env-name.\nTo create an environment, use conda create -n env-name. We can also add additional package names to install after creation such as conda create -n env-name python=3.6* numpy>=1.1.\nTo export an environment, use conda env export -n env-name. This will return the output to the terminal. We can also export to a file. For that use conda env export -n env-name -f env-name.yml. The ‘.yml’ extension is strongly enouraged. Doing this will assure that all the packages used can be installed by others exactly.\nWe can create also an environment from .yml file using conda env create -f env-name.yml. Note also that if we only use conda env create, it will look for a file that has .yml extension and has the same name as env-name in the current local directory. Moreover, we can create the .yml file with doing the export ourselves and only specify what is important in our environments."
  },
  {
    "objectID": "posts/c/program-startup-notes.html",
    "href": "posts/c/program-startup-notes.html",
    "title": "C Program Startup",
    "section": "",
    "text": "Figure 1: Linux x86 program startup(Source)"
  },
  {
    "objectID": "posts/c/program-startup-notes.html#introduction",
    "href": "posts/c/program-startup-notes.html#introduction",
    "title": "C Program Startup",
    "section": "Introduction",
    "text": "Introduction\nIn this post, I will try to write down the steps of C program execution on x86. I used to believe that all C programs start execution at main, or at least this was my understanding from different books/courses until my best friend gdb debugger showed the symbol for _start. This is how I got curious until I got to the bottom of it. Below are my notes that I took during my learning."
  },
  {
    "objectID": "posts/c/program-startup-notes.html#conclusion",
    "href": "posts/c/program-startup-notes.html#conclusion",
    "title": "C Program Startup",
    "section": "Conclusion",
    "text": "Conclusion\nSo starting program will call execve that starts the loader that at some point pass control to _start, which calls __libc_start_main which calls __libc_csu_init which calls _init."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#introduction",
    "href": "posts/optimization-algorithms/gradient-descent.html#introduction",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Introduction",
    "text": "Introduction\nOptimization refers to the task of minimizing/maximizing an objective function \\(f(x)\\) parameterized by \\(x\\). In machine/deep learning terminology, it’s the task of minimizing the cost/loss function \\(J(w)\\) parameterized by the model’s parameters \\(w \\in \\mathbb{R}^d\\). Optimization algorithms (in case of minimization) have one of the following goals: - Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. - Find the lowest possible value of the objective function within its neighbor. That’s usually the case if the objective function is not convex as the case in most deep learning problems.\nThere are three kinds of optimization algorithms:\n\nOptimization algorithm that is not iterative and simply solves for one point.\nOptimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.\nOptimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates.\n\nGradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function \\(J(w)\\) w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate \\(\\alpha\\). Therefore, we follow the direction of the slope downhill until we reach a local minimum.\nIn this notebook, we’ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent.\nLet’s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let’s assume that the logistic regression model has only two parameters: weight \\(w\\) and bias \\(b\\).\n\nInitialize weight \\(w\\) and bias \\(b\\) to any random numbers.\nPick a value for the learning rate \\(\\alpha\\). The learning rate determines how big the step would be on each iteration.\n\n\nIf \\(\\alpha\\) is very small, it would take long time to converge and become computationally expensive.\nIF \\(\\alpha\\) is large, it may fail to converge and overshoot the minimum.\n\nTherefore, plot the cost function against different values of \\(\\alpha\\) and pick the value of \\(\\alpha\\) that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges (see figure 1).\n\n\n\nFigure 1: Gradient descent with different learning rates Source\n\n\n\nThe most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3.\n\n\nMake sure to scale the data if it’s on very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (see figure 2).\n\n\n\n\nFigure 2: Gradient descent: normalized versus unnormalized level curves\n\n\nScale the data to have \\(\\mu = 0\\) and \\(\\sigma = 1\\). Below is the formula for scaling each example: \\[\\\\{}\\frac{x_i - \\mu}{\\sigma}\\tag{1}\\\\{} \\] 4. On each iteration, take the partial derivative of the cost function \\(J(w)\\) w.r.t each parameter (gradient): \\[\\frac{\\partial}{\\partial w}J(w) = \\nabla_w J\\tag{2}\\\\{}\\] \\[\\frac{\\partial}{\\partial b}J(w) = \\nabla_b J\\tag{3}\\\\{}\\] The update equations are: \\[w = w - \\alpha \\nabla_w J\\tag{4}\\\\{}\\] \\[b = b - \\alpha \\nabla_b J\\tag{5}\\\\{}\\] * For the sake of illustration, assume we don’t have bias. If the slope of the current values of \\(w > 0\\), this means that we are to the right of optimal \\(w^*\\). Therefore, the update will be negative, and will start getting close to the optimal values of \\(w^*\\). However, if it’s negative, the update will be positive and will increase the current values of \\(w\\) to converge to the optimal values of \\(w^*\\) (see figure 3):\n\n\n\nFigure 3: Gradient descent. An illustration of how gradient descent algorithm uses the first derivative of the loss function to follow downhill it’s minimum.\n\n\n\nContinue the process until the cost function converges. That is, until the error curve becomes flat and doesn’t change.\nIn addition, on each iteration, the step would be in the direction that gives the maximum change since it’s perpendicular to level curves at each step.\n\nNow let’s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter’s update (learning step)."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#batch-gradient-descent",
    "href": "posts/optimization-algorithms/gradient-descent.html#batch-gradient-descent",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\nBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: \\[w = w - \\alpha \\nabla_w J(w)\\tag{6}\\]\nfor i in range(num_epochs):\ngrad = compute_gradient(data, params)\nparams = params - learning_rate * grad\nThe main advantages:\n\nWe can use fixed learning rate during training without worrying about learning rate decay.\nIt has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex.\nIt has unbiased estimate of gradients. The more the examples, the lower the standard error.\n\nThe main disadvantages:\n\nEven though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets.\nEach step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#mini-batch-gradient-descent",
    "href": "posts/optimization-algorithms/gradient-descent.html#mini-batch-gradient-descent",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Mini-Batch Gradient Descent",
    "text": "Mini-Batch Gradient Descent\nInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of \\(b\\) examples:\n\\[w = w - \\alpha \\nabla_w J(x^{\\{i:i + b\\}}, y^{\\{i: i + b\\}}; w)\\tag{7}\\\\{}\\]\n\nShuffle the training dataset to avoid pre-existing order of examples.\nPartition the training dataset into \\(b\\) mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch.\n\nfor i in range(num_epochs):\nnp.random.shuffle(data)\nfor batch in radom_minibatches(data, batch_size=32):\n    grad = compute_gradient(batch, params)\n    params = params - learning_rate * grad\nThe batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2.\nThe main advantages:\n\nFaster than Batch version because it goes through a lot less examples than Batch (all examples).\nRandomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning.\nWith batch size < size of training set, it adds noise to the learning process that helps improving generalization error.\nEven though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur.\n\nThe main disadvantages:\n\nIt won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges.\nDue to the noise, the learning steps have more oscillations (see figure 4) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum.\n\n\n\n\nFigure 4: Gradient descent: batch versus mini-batch loss function\n\n\nWith large training datasets, we don’t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size \\(b = m\\), we get the Batch Gradient Descent."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#stochastic-gradient-descent",
    "href": "posts/optimization-algorithms/gradient-descent.html#stochastic-gradient-descent",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example \\((x^i, y^i)\\). Therefore, learning happens on every example:\n\\[w = w - \\alpha \\nabla_w J(x^i, y^i; w)\\tag{7}\\]\n\nShuffle the training dataset to avoid pre-existing order of examples.\nPartition the training dataset into \\(m\\) examples.\n\nfor i in range(num_epochs):\n    np.random.shuffle(data)\n    for example in data:\n        grad = compute_gradient(example, params)\n        params = params - learning_rate * grad\nIt shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD:\n\nIt adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time.\nWe can’t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step.\n\nBelow is a graph that shows the gradient descent’s variants and their direction towards the minimum:\n\n\n\nFigure 5: Gradient descent variants’ trajectory towards minimum\n\n\nAs the figure above shows, SGD direction is very noisy compared to mini-batch."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#challenges",
    "href": "posts/optimization-algorithms/gradient-descent.html#challenges",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Challenges",
    "text": "Challenges\nBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch:\n\nGradient descent is a first-order optimization algorithm, which means it doesn’t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if:\nSecond derivative = 0 \\(\\rightarrow\\) the curvature is linear. Therefore, the step size = the learning rate \\(\\alpha\\).\nSecond derivative > 0 \\(\\rightarrow\\) the curvature is going upward. Therefore, the step size < the learning rate \\(\\alpha\\) and may lead to divergence.\nSecond derivative < 0 \\(\\rightarrow\\) the curvature is going downward. Therefore, the step size > the learning rate \\(\\alpha\\).\n\nAs a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. - If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function (see figure 7).\n\n\n\nFigure 6: Gradient descent fails to exploit the curvature information contained in the Hessian matrix. Source\n\n\n\nThe norm of the gradient \\(g^Tg\\) is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients’ norm is increasing, we’re able to achieve a very low error rates (see figure 8).\n\n\n\n\nFigure 7: Gradient norm. Source\n\n\n\nIn small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive.\n\n\n\n\nFigure 8: Saddle point\n\n\n\nAs discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets.\nAll parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#introduction",
    "href": "projects/data-modeling-with-postgres/index.html#introduction",
    "title": "Data Modeling with Postgres",
    "section": "Introduction",
    "text": "Introduction\nThe goal of this project is to build a PostgreSQL database utilizing the data on users activity and songs metadata. Building the database helps us do complex analytics regarding users activity as well as song play analysis."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#data",
    "href": "projects/data-modeling-with-postgres/index.html#data",
    "title": "Data Modeling with Postgres",
    "section": "Data",
    "text": "Data\nThe songs’ metadata sourse is a subset of the Million Song Dataset. Also, the users’ activities is a simulated data using eventsim. The data resides in two main directories:\n\nSongs metadata: collection of JSON files that describes the songs such as title, artist name, year, etc.\nLogs data: collection of JSON files where each file covers the users activities over a given day."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#methodology",
    "href": "projects/data-modeling-with-postgres/index.html#methodology",
    "title": "Data Modeling with Postgres",
    "section": "Methodology",
    "text": "Methodology\nWe’ll build the database by optimizing the tables around efficient reads for complex queries. To do that, Star schema will be used utilizing dimensional modeling as follows:\n\nFact table: songplays.\nDimensions tables: songs, artist, users, time.\n\nThe three most important advantages of using Star schema are:\n\nDenormalized tables.\nSimplified queries.\nFast aggregation."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#how-to",
    "href": "projects/data-modeling-with-postgres/index.html#how-to",
    "title": "Data Modeling with Postgres",
    "section": "HOW TO",
    "text": "HOW TO\nThe source code is available in three separate Python scripts. Below is a brief description of the main files:\n\nsql_queries.py has all the queries needed to both create/drop tables for the database as well as a SQL query to get song_id and artist_id from other tables since they are not provided in logs dataset.\ncreate_tables.py creates the database, establish the connection and creates/drops all the tables required using sql_queries module.\netl.py build the pipeline that extracts the data from JSON files, does some transformation (such as adding different time attributes from timestamp) and then insert all the data into the corresponding tables.\n\nTherefore, we first run create_tables.py then etl.py to create the database, create tables, and then insert the data using the ETL pipeline."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#examples",
    "href": "projects/data-modeling-with-postgres/index.html#examples",
    "title": "Data Modeling with Postgres",
    "section": "Examples",
    "text": "Examples\n%load_ext sql\n%sql postgresql://student:student@127.0.0.1/sparkifydb\n%%sql\nSELECT COUNT(*) from songplays;\n\n>>> postgresql://student:***@127.0.0.1/sparkifydb\n\n>>> 1 rows affected.\n\n>>> Out[1]: count 6820\nCheck out the project on github."
  },
  {
    "objectID": "projects/transient-ischemic-attack/index.html",
    "href": "projects/transient-ischemic-attack/index.html",
    "title": "Transient Ischemic Attack",
    "section": "",
    "text": "About 1 in 3 people who have a transient ischemic attack will eventually have a stroke, with about half occurring within a year after the transient ischemic attack.\nThe goal of this project is to build a binary classifier to predict Transient Ischemic Attack (TIA) and then deploy the best model as a RESTful API.\nCheck out the project on github."
  },
  {
    "objectID": "mlops/airflow/Dependencies-Between-Tasks.html",
    "href": "mlops/airflow/Dependencies-Between-Tasks.html",
    "title": "Airflow Part 5 - Dependencies Between Tasks",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nDependencies in airflow is specified using the right shift symbol >>. It tells Airflow which tasks should be run first before running other tasks.\nBasic Dependenices:\n\nLinear dependenies: a >> b >> c. This means that a has to run before b which should run before c. If any task fails, the downstream task won’t run and the errors are propagated to them from preceding tasks. They can only run after the errors are fixed for that interval.\nFan-in/Fan-out dependencies: \n\nFan-in: When 1 task is dependent on >= 2 tasks to run. join_datasets is fan-in task. Fan-in tasks can be specified as: [clean_sales, clean_weather] >> join_datasets\nFan-out: When >= 2 tasks are dependent on 1 task to run. start is a fan-out task. Fan-out tasks can be specified as: start >> [fetch_sales, fetch_weather]\nThis is how we can specify dependencies for the DAG in the above picture:\n\nstart >> [fetch_sales, fetch_weather]\nfetch_sales >> clean_sales\nfetch_weather >> clean_weather\n[clean_sales, clean_weather] >> join_datasets\njoin_datasets >> train_model\ntrain_model >> deploy_model\n\nBranching:\n\nWe can take care of conditional execution of code paths inside the task, i.e. inside Python script in the case of PythonOperator. Depending on some condition during execution, different code paths and logic will be followed. The main disadvantages of this approach is that 1) it is hard to figure out with code path is being executed on each run from tree/graph view unless we have logging enabled, 2) Adds more complexity to the code structure, 3) May not let us use specialized operators that abstract aways a lot of the boilerplate code such as PostgresOperator. For example, if we have fetch data from either CSV or SQL database depending on condition at execution.\nWe can add BrachPythonOperatortask that takes a Python callable to determine which tasks to execute next. The Python callable has to return the task_id of the task (or list of task_id) that Airflow should execute next. Example: \n\ndef _pick_erp_system(**context):\n    if context[\"execution_date\"] < ERP_SWITCH_DATE:\n       return \"fetch_sales_old\"\n    else:\n       return \"fetch_sales_new\"\n\npick_erp_system = BranchPythonOperator(\n    task_id=\"pick_erp_system\",\n    python_callable=_pick_erp_system,\n    )\n\nstart >> [pick_erp_system, fetch_weather]\npick_erp_system >> [fetch_sales_old, fetch_sales_new]\nfetch_sales_old >> clean_sales_old\nfetch_sales_new >> clean_sales_new\nfetch_weather >> clean_weather\n[clean_sales_old, clean_sales_new, clean_weather] >> join_datasets\njoin_datasets >> train_model\ntrain_model >> deploy_model\n\nSince downstream tasks only get scheduled & executed if all thier downstream tasks finished successfully, jon_datasets task will never success because with the above dependency either clean_sales_old or clean_sales_new would execute BUT NOT BOTH. We can adjust this using trigger_rule argument (default is \"all_success\" in the operatror by specifying \"non_failed\". This will run downstream task if all downstream tasks haven’t failed even if they never executed. Therefore, we can change trigger_rule for join_datasest task.\nA better approach is to create DummyOperator that does nothing but join both branches and become the upstream task before join_datasets such as below: \n\njoin_branch = DummyOperator(\n   task_id=\"join_erp_branch\",\n   trigger_rule=\"none_failed\"\n    )\n\nstart >> [pick_erp_system, fetch_weather]\npick_erp_system >> [fetch_sales_old, fetch_sales_new]\nfetch_sales_old >> clean_sales_old\nfetch_sales_new >> clean_sales_new\n[clean_sales_old, clean_sales_new] >> join_branch\nfetch_weather >> clean_weather\n[joen_erp_branch, clean_weather] >> join_datasets\njoin_datasets >> train_model\ntrain_model >> deploy_model\nConditional tasks. Sometimes we only want to execute a task if a condition is true, otherwise, the task should be skipped. For example, if we want to only deploy the model on the most recent data and we don’t want deploy_model to always execute if we are doing backfilling -> Create a conditional upstream task that checks the condition and raise Exception if the condition is False so deploy_model will be skipped. \n\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.operators.python import PythonOperator\ndef _latest_only(**context):\n    # execution_time is the first time in the schedule interval\n    # So following_schedule is the next execution_date\n    left_window = context[\"dag\"].following_schedule(context[\"execution_date\"])\n    right_window = context[\"dag\"].following_schedule(left_window)\n    now = pendulum.now(\"utc\")\n    # Since execution of DAG starts after last time point passed of the \n    # schedule interval -> \n    if not left_window < now <= right_window:\n        raise AirflowSkipException(\"Not the most recent run!\")\n\nlatest_only = PythonOperator(task_id=\"latest_only\", python_callable=_latest_only, dag=dag)\nlatest_only >> deplpy_model\n\nTrigger rules: The triggering of Airflow tasks is controlled by the trigger rules which define the behavior of tasks and allow us to configure each task to respond to different situations.\n\nBe default, scheduler picks tasks ready to be executed when all its upstreams tasks were executed successfully and put it in the execute queue. The scheduler always checks downstream tasks if they are ready by checking all their downstream task completion state. Once there is a slot/worker, it will be executed. If any of the upstream tasks failed, it would have failed state and the upstream task won’t be scheduled and have state=upstream_failed. This is called progagation because the error is propagated from upstream to downstream tasks. This is the default trigger_rule which is all_success. If any of the down\nIf any of the upstream task is skipped -> downstream task will be skipped as well (propagation).\nTrigger rules:\n\nall_success: Triggers when all parent tasks have executed successfully\nall_failed: Triggers when all parent tasks have failed or due to failure in their parents\nall_done: Triggers when all parent tasks finished executing regardless of their state. Good to cleanup and shutdown resources regardless of the execution state of the workflow\none_failed: Triggers when at least 1 parent task failed and doesn’t wait for other parent tasks to finish\none_success: Triggers when at least 1 parent task succeeded and doesn’t wait for other parent tasks to finish\nnone_failed: Triggers if no parent task has failed but either completed successfully or skipped\nnone_skipped: Triggers if no parent task has skipped but either completed successfully or failed\ndummy: Triggers regardless of the parent tasks state. Useful for testing"
  },
  {
    "objectID": "mlops/airflow/Triggering-Workflows.html",
    "href": "mlops/airflow/Triggering-Workflows.html",
    "title": "Airflow Part 7 - Triggering Workflows",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nWorkflows are most commonly triggered based on schedule intervals provided using start_date, end_date , schedule_interval. Airflow would calculate when the next schedule would be and start the first task(s) to run at the next data/time.\nHowever, sometimes we want the workflow to run based on the occurance of external events such as a file is available in specific location OR code is changed on git repo etc.\nOne way to execute workflows based on the occurance of external exents is using Airflow’s sensors. Sensor is a subclass of operators that checks if certain condition is true. If true, execute the step (workflow). If false, wait for a given period (default 60 seconds) and tries again. It keeps doing so for timeout period. This is a form of Poking, which is checking for the existence of file in the case of FileSensor.\n\nfrom airflow.sensors.filesystem import FileSensor\nwait_for_file_1 = FileSensor(\n    task_id=\"wait_for_file_1\", filepath=\"/data/file_1.csv\"\n    )\n\nWe can also use globbing with FileSensors by using wildcards to check for the existence of file(s)\nWe can also use PythonSensor which checks for certain condition and must return a Boolean. It is more flexible and easier to read than using globbing within FileSensor. It is the same as PythonOperator in terms of taking a Python callable\n\nfrom pathlib import Path\nfrom airflow.sensors.python import PythonSensor\n\n# Check whether there is any data for a given supermarker\n# and there is _SUCCESS path which indicates whether the \n# data for the given supermarket is all uploaded\ndef _wait_for_supermarket(supermarket):\n    supermarket_path = Path(\"/data\") / supermarket\n    success_path = Path(\"/data\") / \"_SUCCESS\"\n    data_files = supermarketpath.glob(\"*.csv\")\n    return data_files and success_path.exists()\n\nwait_for_supermarket_1 = PythonSensor(\n    task_id=\"wait_for_supermarket_1\",\n    python_callable=_wait_for_supermarket,\n    op_kwargs={\"supermarket\": \"supermarket_1\"},\n    dag=dag\n    )\n\n\nAll sensors take a timeout arguments, which has default value of 7 days\nThere is also a limit on the number of tasks Airflow can run concurrently per DAG (default is 16). DAG takes concurrency argument that can change this number. There is also a limit on the number of tasks per global Airflow and the number DAG runs per DAG\n\nwait_for_supermarket_1 = PythonSensor(\n    task_id=\"wait_for_supermarket_1\",\n    python_callable=_wait_for_supermarket,\n    op_kwargs={\"supermarket\": \"supermarket_1\"},\n    concurreny=20, # Default is 16\n    dag=dag\n    )\n\nThere is snowball effect when sensors don’t succeed. The occupy slots that DAG has (which is determined by the concurrency argument. From the above figure, if only task 1 succeeds and the rest keeps polling and the DAG is scheduled daily with default concurrency of 16 slots and default timeout of 7 days, this is what will happen (sensor deadlock):\n\nDay 1: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 3 tasks.\nDay 2: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 6 tasks.\nDay 3: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 9 tasks.\nDay 4: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 12 tasks.\nDay 5: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 15 tasks.\nDay 6: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 16 tasks; two new tasks cannot run, and any other task trying to run is blocked.\n\n\n\n\nThis also affect the global Airflow limit of maximum number of tasks that can run concurrently, which may lead to whole system get stalled.\nFor sensor task, it pokes to check the condition and block if it is false. So it would run for a little bit and wait for the most part. It keeps poking untel the timeout period is completed, which means it keeps occupying the slot until the condition becomes true or timeout is reached\nmode argument which has two values: {poking, reschedule}. The default is poking. Reschedule can solve the sensor deadlock and snowball effect because it releases the slot the sensor task is occupying after the slot has finished poking. In other words, sensor task would poke, if condition if false, the system will reschedule it and take its slot and make it available to other tasks. It is the same concept as process scheduling that the OS does when a process does a blocking system call.\n\nwait_for_supermarket_1 = PythonSensor(\n    task_id=\"wait_for_supermarket_1\",\n    python_callable=_wait_for_supermarket,\n    op_kwargs={\"supermarket\": \"supermarket_1\"},\n    mode=\"reschedule\",\n    dag=dag\n    )\n\nWe can trigger another DAG to run from inside another DAG using TriggerDagRunOperator. This will cause another DAG to run once the trigger_operator runs which is useful if we want to split DAGs and make some DAGs available to other DAGs instead of repearing functionality. See below for both approaches:  \n\nfrom pathlib import Path\n\nimport airflow.utils.dates\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nfrom airflow.sensors.python import PythonSensor\n\ndag1 = DAG(\n    dag_id=\"ingest_supermarket_data\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=\"0 16 * * *\",\n)\ndag2 = DAG(\n    dag_id=\"create_metrics\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=None, # Since it will be triggered\n)\n\n\ndef _wait_for_supermarket(supermarket_id_):\n    supermarket_path = Path(\"/data/\" + supermarket_id_)\n    data_files = supermarket_path.glob(\"data-*.csv\")\n    success_file = supermarket_path / \"_SUCCESS\"\n    return data_files and success_file.exists()\n\n\nfor supermarket_id in range(1, 5):\n    wait = PythonSensor(\n        task_id=f\"wait_for_supermarket_{supermarket_id}\",\n        python_callable=_wait_for_supermarket,\n        op_kwargs={\"supermarket_id_\": f\"supermarket{supermarket_id}\"},\n        dag=dag1,\n    )\n    copy = DummyOperator(task_id=f\"copy_to_raw_supermarket_{supermarket_id}\", dag=dag1)\n    process = DummyOperator(task_id=f\"process_supermarket_{supermarket_id}\", dag=dag1)\n    trigger_create_metrics_dag = TriggerDagRunOperator(\n        task_id=f\"trigger_create_metrics_dag_supermarket_{supermarket_id}\",\n        trigger_dag_id=\"create_metrics\", # Has to be the same dag_id as dag2\n        dag=dag1,\n    )\n    wait >> copy >> process >> trigger_create_metrics_dag\n\ncompute_differences = DummyOperator(task_id=\"compute_differences\", dag=dag2)\nupdate_dashboard = DummyOperator(task_id=\"update_dashboard\", dag=dag2)\nnotify_new_data = DummyOperator(task_id=\"notify_new_data\", dag=dag2)\ncompute_differences >> update_dashboard\n\nEach DAG run has a run_id that starts with one of the following:\n\nscheduled__ to indicate the DAG run started because of its schedule\nbackfill__ to indicate the DAG run started by a backfill job\nmanual__ to indicate the DAG run started by a manual action (e.g., pressing the Trigger Dag button, or triggered by a TriggerDagRunOperator)\n\nFrom the UI, scheduled DAGs have their task instance in black border while Triggered DAGs don’t\nClearing a task in a DAG will clear the task and all its downstream tasks and trigger a run (backfill)\n\nIt only clears tasks within the same DAG, NOT downstream tasks in another DAG of TriggerDagRunOperator\n\nIf the triggered DAG has dependency on multiple triggering DAGs to be completed before it can run, then we can use ExternalTaskSensor that checks whether the task has been completed successfully (sensor poking the state of tasks in another DAGs). Each ExternalTaskSensor checks for only 1 task by querying the metastore database\n\nBy default, it uses the same execution_date as itself\nIf the task runs on different schedule, we then need to provide timedelta object to execution_delta argument to get what would be the execution_date of the task it tries to sense\n\n\nimport datetime\n\nimport airflow.utils.dates\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\ndag1 = DAG(\n    dag_id=\"ingest_supermarket_data\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=\"0 16 * * *\",\n)\ndag2 = DAG(\n    dag_id=\"create_metrics\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=\"0 18 * * *\",\n)\n\nDummyOperator(task_id=\"copy_to_raw\", dag=dag1) >> DummyOperator(\n    task_id=\"process_supermarket\", dag=dag1\n)\n\nwait = ExternalTaskSensor(\n    task_id=\"wait_for_process_supermarket\",\n    external_dag_id=\"figure_6_20_dag_1\",\n    external_task_id=\"process_supermarket\",\n    # positive # will be subtracted from the execution_date of task sensor\n    # to get the execution_date of the task it is trying to sense\n    execution_delta=datetime.timedelta(hours=6),  \n    dag=dag2,\n)\nreport = DummyOperator(task_id=\"report\", dag=dag2)\nwait >> report\n\nWe can also trigger DAGs from CLI which will have execution_date of the current data and time\n\nairflow dags trigger dag1\nWith configuration; which will be available in the context of each task using context[“dag_run”].conf:\n\nairflow dags trigger -c '{\"supermarket_id\": 1}' dag1\nairflow dags trigger --conf '{\"supermarket_id\": 1}' dag1"
  },
  {
    "objectID": "mlops/airflow/Best-Practices.html",
    "href": "mlops/airflow/Best-Practices.html",
    "title": "Airflow Part 8 - Best Practices",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nStick to coding style conventions by using tools like flake8, pylint, black\nThere are two ways to define DAGs. Stick to one of them:\n\nWith context manager:\n\nwith DAG(...) as dag:\n  task1 = PythonOperator(...)\n  task2 = PythonOperator(...)\n\nTraditional:\n\ndag = DAG(...)\ntask1 = PythonOperator(..., dag=dag)\ntask2 = PythonOperator(..., dag=dag)\nThere are also multiple ways to define dependencies. Stick to one of them:\n\ntask1 >> task2\ntask1 << task2\n[task1] >> task2\ntask1.set_downstream(task2)\ntask2.set_upstream(task1)\n\nWhen loading config files, make sure to understand where the loading happens:\n\nAt the top level on the scheduler\nAt the DAG level when it is parsed\nOr when the DAG is executing -> in the worker\n\nAvoid doing any computation in DAG definition:\n\nAt the top level, it will be computed every time the DAG is loaded\nIn the DAG definition, it will be executed every time the DAG is parsed by the scheduler\nIn the task, it will be computed when the task is executed on the worker machine\n\nFetch credentials within the task, so they are only fetched once the task is executed\nUse factory methods to generate DAGs or set of tasks that are almost typical with few minor changes. Example:\n\ndef generate_tasks(dataset_name, raw_dir, processed_dir, preprocess_script, output_dir, dag):\n    raw_path = os.path.join(raw_dir, dataset_name, \"{ds_nodash}.json\") \n    processed_path = os.path.join(\n    processed_dir, dataset_name, \"{ds_nodash}.json\" )\n    output_path = os.path.join(output_dir, dataset_name, \"{ds_nodash}.json\")\n    fetch_task = BashOperator(\n        task_id=f\"fetch_{dataset_name}\",\n        bash_command=f\"echo 'curl http://example.com/{dataset_name}.json{raw_path}.json'\", dag=dag,\n        )\n    preprocess_task = BashOperator(\n        task_id=f\"preprocess_{dataset_name}\",\n        bash_command=f\"echo '{preprocess_script} {raw_path} {processed_path}'\", dag=dag,\n    )\n    export_task = BashOperator(\n        task_id=f\"export_{dataset_name}\",\n        bash_command=f\"echo 'cp {processed_path} {output_path}'\", dag=dag,\n       )\n        fetch_task >> preprocess_task >> export_task\n    return fetch_task, export_task\n\nwith DAG(\n    dag_id=\"01_task_factory\",\n    start_date=airflow.utils.dates.days_ago(5),\n    schedule_interval=\"@daily\",\n) as dag:\n    for dataset in [\"sales\", \"customers\"]:\n        generate_tasks(\n            dataset_name=dataset,\n            raw_dir=\"/data/raw\", \n            processed_dir=\"/data/processed\", \n            output_dir=\"/data/output\",\n            preprocess_script=f\"preprocess_{dataset}.py\", dag=dag\n        )\n\nWe can use TaskGroup to group related tasks into groups that will help us navigating the DAG in the UI. This is very helpful when DAGs become very complicated\nCreate new DAGs for big changes such as renaming/removing tasks or changing the schedule_date/interval so we can keep the historical info about old DAGs and not confuse the scheduler. Scheduler database has instances of the runs of each DAG\nMake sure that tasks are idempotenet -> Regardless when they run, If given the same input the should produce the same output. Therefore, be careful when writing data. We may want to overwrite or upsert to avoid appending the same data\n\nAlso, tasks should not have side effects\n\nAvoid writing intermediate results on local filesystem because each task runs independently (and mostly on different machines) -> Use cloud shared storage such as Amazon’s S3 bucket where all workers can access it\nWe can use SLAs on each DAG/task where Airflow will notify if they don’t finish within SLA. DAG takes sla argument"
  },
  {
    "objectID": "mlops/airflow/Airflow-DAGs.html",
    "href": "mlops/airflow/Airflow-DAGs.html",
    "title": "Airflow Part 2 - DAGs",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nDAG() class is needed to instantiate a DAG which will be the starting point of any workflow.\n\nThe required arguments are: dag_id which is the name Airflow web UI uses to display workflow. start_date which is when to start running the workflow, it can be in the past\nThere are other arguments such as schedule_interval which determines the schedule to rerun the DAG\n\nOperator is responsible for a piece of work and almost represents a task.\n\nIt has task_id which is the name web UI uses to display the task\nThere are many operators such as BashOperator, PythonOperator … All of them inherits from BaseOperator\nSome operators are generic such as BashOperator and some are specific such as EmailOperator\n\nTask is a wrapper/manager over operator that makes sure the operator gets executed\n>> represents the dependencies between tasks\n\na >> b means a should run before b\n\nAirflow UI offers two views:\n\ntree view that shows the DAG runs over time. Each column is one run. Each row is a task. So we can inspect status of tasks over time\ngraph view that shows the DAG as a graph which helps showing the dependencies of tasks in the workflow\n\nIf any task failed, all successive tasks that depend on it don’t run\n\nWe can rerun the failed tasks (which also would cause successive tasks to rerun) w/o having to rerun the workflow from scratch\nWe can inspect the logs to see what was the reason for the errors\n\nTasks can run in parallel depending on their dependencies\nTo setup Airflow locally inside Python virtual env:\n\npip install apache-airflow\nairflow init db # Initialize metastore locally using SQLite; not recommended for production\nairflow users create –username admin –password admin –firstname Anonymous –lastname Admin –role Admin –email admin@example.org # Create user\nairflow webserver # Start web server to use web UI\nairflow scheduler # Start scheduler, don’t use sequential in production\n\n\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\nf = lambda: print(1)\ndag = DAG(dag_id=\"simple-workflow\", start_date=airflow.utils.dates.days_ago(10))\na = BashOperator(task_id=\"bash\", bash_command=\"echo 'a'\", dag=dag)\nb = PythonOperator(task_id=\"python\", python_callable=f, dag=dag)\na >> b\n\n<Task(PythonOperator): python>"
  },
  {
    "objectID": "mlops/airflow/What-Is-Airflow.html",
    "href": "mlops/airflow/What-Is-Airflow.html",
    "title": "Airflow Part 1 - What is Airflow?",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\nAirflow is workflow orchestration tool that is written in Python at Airbnb. The workflow is also written in Python. It defines the workflow as a DAG so it is easy to determine the dependencies between tasks. If any task failed, we don’t need to rerun the workflow again, we can just run the failed task and all the tasks that depend on it. We can also do backfilling by running the pipeline/tasks for time intervals in the past.\nAirflow consists of mainly three components:\n\nThe Airflow scheduler: Parses DAGs, checks their schedule interval, and (if the DAGs’ schedule has passed) starts scheduling the DAGs’ tasks for execution by passing them to the Airflow workers.\nThe Airflow workers: Pick up tasks that are scheduled for execution and execute them. As such, the workers are responsible for actually “doing the work.”\nThe Airflow webserver: Visualizes the DAGs parsed by the scheduler and provides the main interface for users to monitor DAG runs and their results. It uses the metadata database which has all the logs and other metadata about tasks and workflows.\n\nConceptually, the scheduling algorithm follows the following steps:\n\nFor each open (= uncompleted) task in the graph, do the following: – For each edge pointing toward the task, check if the “upstream” task on the other end of the edge has been completed. – If all upstream tasks have been completed, add the task under consideration to a queue of tasks to be executed.\nExecute the tasks in the execution queue, marking them completed once they finish performing their work.\nJump back to step 1 and repeat until all tasks in the graph have been completed.\n\nThe scheduler in Airflow runs roughly through the following steps:\n\nOnce users have written their workflows as DAGs, the files containing these DAGs are read by the scheduler to extract the corresponding tasks, dependen- cies, and schedule interval of each DAG.\nFor each DAG, the scheduler then checks whether the schedule interval for the DAG has passed since the last time it was read. If so, the tasks in the DAG are scheduled for execution.\nFor each scheduled task, the scheduler then checks whether the dependencies (= upstream tasks) of the task have been completed. If so, the task is added to the execution queue.\nThe scheduler waits for several moments before starting a new loop by jumping back to step 1.\n\n\nAirflow can be run:\n\nIn python virtual environment\nInside Docker containers. In this case, Airflow scheduler, webserver, and metastore would run each in separate containers\n\nThe main disadvantages of Airflow are:\n\nIt can get very messy and hard to understand for complex workflows\nIt is best used for batch/recurring jobs NOT streaming jobs\nMainly support static DAGs and hard to implement dynamic DAGs. Imagine you’re reading from a database and you want to create a step to process each record in the database (e.g. to make a prediction), but you don’t know in advance how many records there are in the database, Airflow won’t be able to handle that.\nIt is monolithic, which means it packages the entire workflow into one container. If two different steps in your workflow have different requirements, you can, in theory, create different containers for them using Airflow’s DockerOperator, but it’s not that easy to do so.\nAirflow’s DAGs are not parameterized, which means you can’t pass parameters into your workflows. So if you want to run the same model with different learning rates, you’ll have to create different workflows.\n\nTo setup Airflow locally inside Python virtual env:\n\npip install apache-airflow\nairflow init db # Initialize metastore locally using SQLite; not recommended for production\nairflow users create –username admin –password admin –firstname Anonymous –lastname Admin –role Admin –email admin@example.org # Create user\nairflow webserver # Start web server to use web UI\nairflow scheduler # Start scheduler, don’t use sequential in production"
  },
  {
    "objectID": "mlops/airflow/Task-Context-And-Templating.html",
    "href": "mlops/airflow/Task-Context-And-Templating.html",
    "title": "Airflow Part 4 - Task Context & Jinja Templating",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nAirflow uses Pendulum library for datetimes. It is a drop-in replacement to the Python standard library datetime but with much nicer API and more features.\nNot all arguments can be templates. By default, all arguments are not made into templates and {{name}} will be read as a literal string name unless it is included in template_fields in the list of attributes that can be templated in the Operator.\n\nElements in the template_fields are names for class attributes. The arguments passed to the __init__ match the class attributes.\n\nAll operators; such as BashOperator, take their argument as string except PythonOperator. It takes its argument as python_callable, which is any callable object in Python. The context and parameters will be available to this callable.\n\nThe context variable is a dictionary that has all the instance variables for this task.\n\nwe can use default **kwargs or make it easier to read using **context\n\nIf we specify argument name in the python_callable, then Airflow will call the python_callable with all the variables in the context.\n\nIf a variable is specified as argument by the callable, then it is passed to the callabe\nOtherwise, it is added to the context dictionary. If we don’t have context dictionary as an argument for the callable, then all other variables in the context that are not specified as arguments will be discarded.\n\n\n\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndag = DAG(dag_id=\"python-operator-context\", start_date=airflow.utils.dates.days_ago(1))\n\n\ndef _print_context(**kwargs):\n    print(kwargs)\nprint_context = PythonOperator(task_id=\"print-context\", python_callable=_print_context, dag=dag)\nprint_context\n\n\nSome arguments of operators can be templated\nTemplating happens at run time\nWe can provide arguments to PythonOperator using:\n\nop_args: list of positional arguments that are passed to the callable\nop_kwargs: dictionary of keyword arguments\n\nWe can inspect the templated arguments either on the UI or using the CLI:\n\nCLI: airflow tasks render [dag id] [task id] [desired execution date]\n\nThere are two ways to pass data between tasks:\n\nread/write to the metastore. It is called XCom\n\nThis is done by pickling the objects we want to share and write it to metastore. After that, tasks can read the pickled objects (and unpickle them)\nThis is only recommended for small objects because the object are stored as blobs in the metastore. Don’t use it for large objects\n\nread/write to persistent storage such as disk or database\n\nTasks are independent and may run on completely different machines -> Can’t share memory -> Sharing has to be through persistent storage.\nMost operators are installed via separate pip install. For example, PostgresOperator allows us to work with PostgreSQL database.\n\nWe can install operators like pip install apache-airflow-providers-*\nWe can import the operator as from airflow.providers.pogstres.operators.postgres import PostgresOperator\nWe can add connections using UI or CLI, which Airflow store them encrypted in metastore, such as:\n\nairflow connections add \\\n--conn-type postgres \\\n--conn-host localhost \\\n--conn-login postgres \\\n--conn-password mysecretpassword \\\nmy_postgres\n  - We can later refer to those credentions by name when connecting to any database\n\nAirflow takes care of setting up the connection and close it once done\n\nPostgres is an external system and Airflow supports connecting to a wide range of external systems with the help of many operators in its ecosystem. This does have an implication: connecting to an external system often requires specific dependencies to be installed, which allow connecting and communicating with the external system. This also holds for Postgres; we must install the package apache-airflow-providers- postgres to install additional Postgres dependencies in our Airflow installation.\n\nUpon execution of the PostgresOperator, a number of things happen. The PostgresOperator will instantiate a so-called hook to communicate with Postgres. The hook deals with creating a connection, sending queries to Postgres and closing the connection afterward. The operator is merely passing through the request from the user to the hook in this situation.\nAn operator determines what has to be done; a hook determines how to do something. When building pipelines like these, you will only deal with operators and have no notion of any hooks, because hooks are used internally in operators.\n\nThere’s a number of things to point out in this last step. The DAG has an additional argument: template_searchpath. Besides a string INSERT INTO …, the content of files can also be templated. Each operator can read and template files with specific extensions by providing the file path to the operator. In the case of the Postgres- Operator, the argument SQL can be templated and thus a path to a file holding a SQL query can also be provided. Any filepath ending in .sql will be read, templates in the file will be rendered, and the queries in the file will be executed by the PostgresOperator. Again, refer to the documentation of the operators and check the field template_ext, which holds the file extensions that can be templated by the operator.\n\nJinja requires you to provide the path to search for files that can be templated. By default, only the path of the DAG file is searched for, but since we’ve stored it in /tmp, Jinja won’t find it. To add paths for Jinja to search, set the argument template_searchpath on the DAG and Jinja will traverse the default path plus additional provided paths to search for."
  },
  {
    "objectID": "mlops/airflow/Sharing-Data-Between-Tasks.html",
    "href": "mlops/airflow/Sharing-Data-Between-Tasks.html",
    "title": "Airflow Part 6 - Sharing Data Between Tasks",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nIt is meant to exchange messages between tasks, which is some form of shared state\nWe can use dag instance to push/pull data between tasks:\n\nconext[\"dag_instance\"].xcom_push(key=\"data_name\", value=\"value\") to push data to metastore. It also store the dag_id, task_id, & execution_date.\nconext[\"dag_instance\"].xcom_pull(key=\"data_name\") which pull the shared data. We can also specify dag_id and execution_date.\nWe can also access push/pull methods in templates using task_instance.xcom_push() or task_instance.xcom_pull()\nWe can view the shared data on the UI by going to Admin -> XComs\n\nLimitations:\n\nXComs data will be pickled and stored in the database -> The objects have to be serializable\nSize limitations:\n\nSQLite—Stored as BLOB type, 2GB limit\nPostgreSQL—Stored as BYTEA type, 1 GB limit\nMySQL—Stored as BLOB type, 64 KB limit\n\nIt create hidden dependency between tasks because now the task the pushes the shared state has to push the data before the task that pulls the data. Airflow won’t manage/respect this dependency the developer has to document this and make sure this is not an issue based on the tasks’ order\n\nDue to its limitations in terms of size, we can create custom backends for XComs by defining a class that inherits from BaseXCom and implements two static methods. Airflow will use this class. It can be added to xcom_backend parameter in the Airflow configWe can use cheap/large storage services on the cloud such as Amazon S3, Azure Blob Storage, or Google GCS.\n\nfrom typing import Any\nfrom airflow.models.xcom import BaseXCom\n\nclass CustomXComBackend(BaseXCom):\n    \n    @staticmethod\n    def serialize(value: Any):\n        ...\n    \n    @staticmethod\n    def deserialize(result):\n        ...\n\nIf most of tasks are PythonOperators, we can use Taskflow API that takes care of passing state between tasks and avoid the boilerplate code that we have to write with regular API. We need to just decorate the function that we use in the PythonOperator with @task and Airflow will take care of the rest by passed XCom data between tasks. Example:\n\n\nfrom airflow.decorators import task\n\n\nwith DAG(...) as dag:\n    start = DummyOperator(task_id=\"start\")\n    start >> fetch_sales\n    start >> fetch_weather\n    fetch_sales >> clean_sales\n    fetch_weather >> clean_weather\n    [clean_sales, clean_weather] >> join_datasets\n    \n    @task\n    def train_model():\n        model_id = str(uuid.uuid4())\n        # Airflow will figure out that the return value is XCom\n        # and would take care of pushing it\n        return model_id\n\n    @task\n    def deploy_model(model_id: str):\n        # Airflow would realize that this task uses XCom so it passes\n        # it from XCom\n        print(f\"Deploying model {model_id}\")\n\nmodel_id = train_model()\ndeploy_model(model_id)\n\n# Now train_model and deploy_model will be new tasks\n# with explicit dependeny. \n# The task type is PythonDecoratedOperator\njoin_datasets >> model_id\n\nAny data passed between Taskflow-style tasks will be stored as XComs and subject to the same limitations of XCom\nThe main limitation of Taskflow API is that it is still only for PythonOperators"
  },
  {
    "objectID": "mlops/airflow/Airflow-Scheduling.html",
    "href": "mlops/airflow/Airflow-Scheduling.html",
    "title": "Airflow Part 3 - DAG Scheduling",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nAirflow will schedule the first execution of DAG at the end of the interval; which means after the last time point in the interval has passed. For example, if we schedule it to run @daily, it will run t midnight of each day starting from the start_date until (optionally) end_date. In other words, as soon as 23:59:59 has passed which means any time after 00:00:00.\n\nExample: if start_date=“2022-01-01” and schedule_interval=“@daily” -> The first time it runs is any time soon after “2022-01-02 00:00” which is midnight of January second.\n\nWe can use convenience string (such as @daily), timedetla objects (such as timedelta(days=3), or cron expressions (such as 0 0 * * 0 which means weekly on Sunday 00:00)\nFrequency scheduling intervals (shorthands):\n\n@once: Schedule once and only once.\n@hourly: Run once an hour at the beginning of the hour.\n@daily: Run once a day at midnight.\n@weekly: Run once a week at midnight on Sunday morning.\n@monthly: Run once a month at midnight on the first day of the month. Run once a year at midnight on January 1.\n\nCron-based intervals:\n\n# ┌─────── minute (0 - 59)\n# │ ┌────── hour (0 - 23)\n# │ │ ┌───── dayofthemonth(1-31)\n# │ │ │ ┌───── month(1-12)\n# │ │ │ │ ┌──── dayoftheweek(0-6)(SundaytoSaturday; \n# │ │ │ │ │ 7 is also Sunday on some systems) \n# * * * * *\n- \"*\" means don't care values.\n- Examples:\n    1. 0**** means hourly\n    2. 00*** means daily at midnight\n    3. 00**0 means weekly at midnight on Sunday\n- Useful link to check meaning of cron-based intervals: https://crontab.guru/\n\nCron expressions have limitations when trying to specify frequency-based intervals such as every three days. The reason for this behavior is that cron expressions are stateless and don’t look at previous runs to determine next run, they only look at the current time to see if it matches the expression.\nAirflow allows us to use frequency-based intervals using timedelta from datetime library. This way we can use previous run to determine the next run.\n\nExample: schedule_interval=“timedelta(days=3)” means to run every 3 days after start_date.\n\nWe can use dynamic time reference that uses execution dates which allows us to do the work incrementally. Airflow will pass those dates to the tasks to determine which schedule interval is being executed.\n\nexecution_date is a timestamp of the start time of the schedule interval\nnext_execution_date is a timestamp of the end time of the schedule interval\nprevious_execution_date is a timestamp of the start time of the previous schedule interval\nAirflow uses Jinja-based templating such as {{variable_name}}:\n\n\nfetch_events = BashOperator(\n    task_id=\"fetch_events\",\n    bash_command=(\n        \"mkdir -p /data && \"\n        \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n        \"start_date={{execution_date.strftime('%Y-%m-%d')}}\" \n        \"&end_date={{next_execution_date.strftime('%Y-%m-%d')}}\"\n    ),\ndag=dag,\n)\n- Or we can use shorthands:\nfetch_events = BashOperator(\n    task_id=\"fetch_events\",\n    bash_command=(\n        \"mkdir -p /data && \"\n        \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n        \"start_date={{ds}}\" \n        \"&end_date={{next_ds}}\"\n    ),\ndag=dag,\n)\n- `ds` has `YYYY-MM-DD` format while `ds_nodash` has `YYYYMMDD` format\n- Shorthands: ds, ds_nodash, next_ds, next_ds_nodash, ps, ps_nodash execution date of the next interval.\n\nWe can also use dates or any dynamic parameters to Python function using templates_dict argument and the python callable will be passed the context that has the templates_dict For example:\n\n    calculate_stats = PythonOperator(\n       task_id=\"calculate_stats\",\n       python_callable=_calculate_stats,\n       templates_dict={\n            \"input_path\": \"/data/events/{{ds}}.json\",\n           \"output_path\": \"/data/stats/{{ds}}.csv\",\n    },\n    dag=dag\n    )\n    def _calculate_stats(**context):\n        \"\"\"Calculates event statistics.\"\"\"\n            input_path = context[\"templates_dict\"][\"input_path\"] \n            output_path = context[\"templates_dict\"][\"output_path\"]\n\nBecause Airlfow follows Interval-Based Scheduling, that means DAGs run only after the last time point of schedule interval passed. If we run the DAG daily starting from 2022-01-01, the first time it runs is soon after 2022-01-02 00:00:00 has passed and the execution_date would be 2022-01-01 even though it is running in 2022-01-02. This is because it is running for the corresponding interval.\n\nThe end of the previous interval is execution_date\nOne caveat for this is that previous_execution_date and next_execution_date are only defined for DAGs that run on schedule interval. This means that those values are undefined when the DAGs are run from the UI or CLI\n\nAirflow allows us to have start_date in the past. This will help us in backfilling. By default, Airflow will run all the schedule intervals from the past until current time once the DAG is activated. We can control this behavior using catchup parameter to the DAG() class. If we set it to False, it won’t run previous schedule intervals.\n\nBackfilling is also helpful if we change the code for the DAG. It would run all previous schedules after we clear them.\n\n\nBest Practices:\n\nTask needs to be atomic which means a single coherent unit of work. This allows us to split work into smaller units where if one fails we know exactly what is it and recover easily.\nTask needs to be idempotent which means it has no side effects on the system when it reruns. If the task is given the same input, it should produce the same output.\n\nIn database systems, we can use upsert, which allows us to overwrite existing row.\nWhen writing to files, make sure that rerunning the same task for the same interval don’t write data again. Append doesn’t let us make the task idempotent."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I learned",
    "section": "",
    "text": "In an effort to learn in public, below is a collection things that I learn day-to-day mainly in technology and languages (repo).\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 11, 2022\n\n\nSequence Slicing\n\n\nImad Dabbura\n\n\n\n\nDec 8, 2022\n\n\nUsing __slots__ to Store Instance’s Attributes\n\n\nImad Dabbura\n\n\n\n\nDec 4, 2022\n\n\nInstance vs Class vs Static Method\n\n\nImad Dabbura\n\n\n\n\nDec 1, 2022\n\n\nObject Destruction\n\n\nImad Dabbura\n\n\n\n\nNov 29, 2022\n\n\n== vs is\n\n\nImad Dabbura\n\n\n\n\nNov 27, 2022\n\n\nClosures\n\n\nImad Dabbura\n\n\n\n\nNov 25, 2022\n\n\nFirst Class Objects\n\n\nImad Dabbura\n\n\n\n\nNov 21, 2022\n\n\nClearing Registers\n\n\nImad Dabbura\n\n\n\n\nNov 20, 2022\n\n\nAborting Git’s Commit & Rebase\n\n\nImad Dabbura\n\n\n\n\nNov 19, 2022\n\n\nParallel vs Series Execution of Macros\n\n\nImad Dabbura\n\n\n\n\nNov 16, 2022\n\n\nTemporary Files & Directories\n\n\nImad Dabbura\n\n\n\n\n\nNov 14, 2022\n\n\nManaged vs External Tables\n\n\nImad Dabbura\n\n\n\n\nNov 6, 2022\n\n\nLists vs Tuples\n\n\nImad Dabbura\n\n\n\n\nNov 3, 2022\n\n\nPython Object Lookup Process\n\n\nImad Dabbura\n\n\n\n\nNov 1, 2022\n\n\nPython Logging\n\n\nImad Dabbura\n\n\n\n\nOct 27, 2022\n\n\nPython Wheel Files\n\n\nImad Dabbura\n\n\n\n\nOct 19, 2022\n\n\nC Program Memory Layout\n\n\nImad Dabbura\n\n\n\n\nOct 17, 2022\n\n\nListing tmux Sessions\n\n\nImad Dabbura\n\n\n\n\nOct 15, 2022\n\n\nOuter Joins & Where Clause\n\n\nImad Dabbura\n\n\n\n\nOct 13, 2022\n\n\nWhat Is Null Actually?\n\n\nImad Dabbura\n\n\n\n\nOct 12, 2022\n\n\nLogical Execution Order of SQL Query\n\n\nImad Dabbura\n\n\n\n\nOct 4, 2022\n\n\nDetach Tensor From Computation Graph\n\n\nImad Dabbura\n\n\n\n\nSep 30, 2022\n\n\nCharacter Array vs String Constant Pointers\n\n\nImad Dabbura\n\n\n\n\n\nSep 28, 2022\n\n\nImmutable Objects and Augmented Assignment Operators\n\n\nImad Dabbura\n\n\n\n\nSep 25, 2022\n\n\nModules and Packages\n\n\nImad Dabbura\n\n\n\n\nSep 24, 2022\n\n\nI/O Redirection\n\n\nImad Dabbura\n\n\n\n\nSep 23, 2022\n\n\nLoading Modules and Packages\n\n\nImad Dabbura\n\n\n\n\nSep 22, 2022\n\n\nNamespace Packages\n\n\nImad Dabbura\n\n\n\n\nSep 21, 2022\n\n\nUseful Tricks with Null Device\n\n\nImad Dabbura\n\n\n\n\nSep 20, 2022\n\n\nDecorators\n\n\nImad Dabbura\n\n\n\n\nSep 19, 2022\n\n\nSave Memory When Operating on Tensors\n\n\nImad Dabbura\n\n\n\n\nSep 18, 2022\n\n\nLazy Layers\n\n\nImad Dabbura\n\n\n\n\nSep 16, 2022\n\n\nList Available GPUs\n\n\nImad Dabbura\n\n\n\n\nSep 15, 2022\n\n\nConditional Iterators\n\n\nImad Dabbura\n\n\n\n\n\nSep 14, 2022\n\n\nMutable Default Function Arguments\n\n\nImad Dabbura\n\n\n\n\nSep 12, 2022\n\n\nMutability and Inplace Operations\n\n\nImad Dabbura\n\n\n\n\nSep 10, 2022\n\n\nUser Defined Classes are Hashable\n\n\nImad Dabbura\n\n\n\n\nSep 6, 2022\n\n\n__getitem__ Makes Object an Iterator\n\n\nImad Dabbura\n\n\n\n\nSep 5, 2022\n\n\nScope of Variables in Comprehensions\n\n\nImad Dabbura\n\n\n\n\nSep 3, 2022\n\n\nItemview and Keyview Attributes of Dictionary are Subclasses of Set\n\n\nImad Dabbura\n\n\n\n\nSep 1, 2022\n\n\nTruthiness of Python Objects\n\n\nImad Dabbura\n\n\n\n\nAug 31, 2022\n\n\nLine Continuation\n\n\nImad Dabbura\n\n\n\n\nAug 30, 2022\n\n\nExecuting vs Sourcing Script\n\n\nImad Dabbura\n\n\n\n\nAug 29, 2022\n\n\nVimium: Vim Key Bindings for Chrome\n\n\nImad Dabbura\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nDoes C program really start at main\n\n\n5 min\n\n\n\nSoftware Development\n\n\nC\n\n\n\n\nImad Dabbura\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\n\n\nImad Dabbura\n\n\nSep 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnough background about Conda to be productive!\n\n\n4 min\n\n\n\nSoftware Development\n\n\nVirtual Environments\n\n\n\n\nImad Dabbura\n\n\nFeb 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep dive into gradient descent algorithm: Batch vs. Mini-batch vs. Stochastic.\n\n\n9 min\n\n\n\nSoftware Development\n\n\nVirtual Environments\n\n\n\n\nImad Dabbura\n\n\nFeb 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep dive into K-means algorithm to find subgroups within data.\n\n\n14 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\nUnsupervised Learning\n\n\nClustering\n\n\n\n\nImad Dabbura\n\n\nSep 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Dropout, its use, and how to implement it?\n\n\n3 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nMay 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is regularization and how it helps NN generalizes better?\n\n\n8 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nMay 8, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe role of parameter initialization in training and different ways to initialize parameters.\n\n\n4 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to check numerically if the implementation of backward propagation is correct?\n\n\n3 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 8, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat it takes to go from input to output? And how to compute the gradients?\n\n\n10 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is epsilon-Greedy Algorithm and how to use it in A/B testing?\n\n\n8 min\n\n\n\nData Science\n\n\nWebsite Optimization\n\n\n\n\nImad Dabbura\n\n\nMar 31, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrying different modeling techniques to deal with imbalanced data, missing values, and ensemble models.\n\n\n16 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\n\n\nImad Dabbura\n\n\nMar 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict the next character given the previous charecter and state.\n\n\n12 min\n\n\n\nNLP\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nFeb 22, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with different models on employee turnover data.\n\n\n8 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\n\n\nImad Dabbura\n\n\nDec 11, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "reading-list.html",
    "href": "reading-list.html",
    "title": "Reading List",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Imad Dabbura is a Staff Data Scientist. He has many years of experience in the field of Data Science and Machine Learning where he worked in a variety of industries such as Consumer Goods, Real Estate, Marketing, and Healthcare.\nAmong other things, Imad is interested in Artificial Intelligence and Machine Learning. He writes articles related to machine learning and AI on Medium and contributes to open source such as publishing his educational notebooks on github."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nThe University of Texas at Dallas | Dallas, TX\nMS in Economics | 2014\nLebanese International University | Beirut, Lebanon\nBA in Economics | 2009"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nArtificial Intelligence\nMachine Learning\nData Science"
  },
  {
    "objectID": "til/vim/executing-macros-parallel-vs-serios.html",
    "href": "til/vim/executing-macros-parallel-vs-serios.html",
    "title": "Parallel vs Series Execution of Macros",
    "section": "",
    "text": "Execute the macro in series such as 5@a. This will queue 5 repititions of the macro in series starting at the current line and continues forward. The execution of the macro on the next line is dependent on the execution of the macro on the current line being successful. If it fails for any reason, the execution of the macro for the remaining lines will abort.\nExecute the macro in parallel such as '<,>':normal @a. This will queue up number of lines in the visual selection repititions of the macro in parallel. This means the execution of the macro on each line is independent of the execution on other lines. Therefore, even if one execution failed, the macro will still be applied to the remaining lines.\n\nWhich one to use depends on the use case. If we want the error to be obvious, then execution in series is better."
  },
  {
    "objectID": "til/vim/clear-register.html",
    "href": "til/vim/clear-register.html",
    "title": "Clearing Registers",
    "section": "",
    "text": "The easiest way to clear a register; in norma mode, is q-register_name-q. The command qa will start recording a macro in the register a, which cleans the regoster before it starts the key strokes that follow. So when it is followed right away by q, it acts as clearing the register a. This is very useful if we’re trying to copy something to a register, such as copy all TODO from all project files into a register so that we can then past them in new file."
  },
  {
    "objectID": "til/vim/abort-git-commit-and-rebase.html",
    "href": "til/vim/abort-git-commit-and-rebase.html",
    "title": "Aborting Git’s Commit & Rebase",
    "section": "",
    "text": "If you want to quit the commit or the rebase w/o letting git perform any action, we can use :cq Ex command. This command will let you exit vim with error code sent to the process/application that called vim. This way, git will recieve the error and won’t proceed with the action.\nI typically use this often when I am in the middle of writing a commit message and realize that I need to check/change some things before committing."
  },
  {
    "objectID": "til/python/object-destruction.html",
    "href": "til/python/object-destruction.html",
    "title": "Object Destruction",
    "section": "",
    "text": "refcount gets incremented when new variable or object points to it\nrefcount gets decremented when either one the variables/objects that used to point to it gets reassigned to point to new object OR we call del   obj\n\nOnce the refcount reaches zero or the object becomes unreachable (determined by mark-sweep algorithm used by the garbage collector)\n\nThe interpreter will call __del__ special method for final cleanup before destorying the object.\nIts memory will be reclaimed by the interpreter through garbage collector"
  },
  {
    "objectID": "til/python/first-class-objects.html",
    "href": "til/python/first-class-objects.html",
    "title": "First Class Objects",
    "section": "",
    "text": "Created at runtime\nAssigned to a variable or element in a data structure\nPassed as an argument to a function\nReturned as a result of a function\n\nExamples of first class objects are intergers, strings, dictionaries, etc."
  },
  {
    "objectID": "til/python/wheel-files.html",
    "href": "til/python/wheel-files.html",
    "title": "Python Wheel Files",
    "section": "",
    "text": "A wheel file is a zip archive with a specially crafted name that tells installers what Python versions and platforms are supported\n\nIt is a ready to install format\nfilename is broken into parts: {dist}-{version}(-{build})?-{python}-{abi}-{platform}.whl\n\nPython Wheels made the installation faster and more efficient\n\nThey are smaller in size than source distributions -> lower network latency\nAvoid building stage -> faster\nAvoids executing setup.py code\nNo need for a compiler for extension modules\npip automatically generates .pyc files\n\nSource Distributions are the source code and extension modules (mostly written in C/C++) bundled together. Those extensions are compiled at the user side NOT developer side\n\nThey also contain metadata directory called <package-name>.egg_info. This metadata helps with the building and isntalling of the package\nIt is the result of python setup.py sdist command\n\nWheel files are sometimes provided by packages on PyPI\n\nEach OS has its own wheel file\nPip prefers wheels over source distributions when trying to install a package\n\nExample wheel filenames:\n\ncryptography-2.9.2-cp35-abi3-macosx_10_9_x86_64.whl\n\ncryptography is the name of the package\n2.9.2 is the package version which is formatted according to PEP 440 recommendation\ncp35 is CPython with Python version 3.5\nabi3 is version 3 of Application Binary Interface\nmacosx_10_9_x86_64:\n\nmacosx MacOs operating system\n10_9 is the version of MacOs\nx86_64 is the x86_64 instruction set architecture\n\n\nchardet-3.0.4-py2.py3-none-any.whl\n\nchardet is the package name\n3.0.4 is the package version\npy2.py3 works on any version of Python 2 & 3\nnone means ABI is not factor\nany works virtually on all platforms\n\n\nWe can view the contents of the wheel file using unzip\nWe can force pip to only install wheel files with --no-binary\n\n:all: would apply this not only to the package we are installing but to all its dependencies\n\nSince there are so many variants of Linux such as CentOS, Debian, etc., package developer may have to provide many wheels for different variations of Linux. This is especially true if the package has module extensions written in C/C++ and may potentially have issues due to compilation error. There are platform tag family:\n\nmanylinux1\nmanylinux2010\nmanylinux2014\n\nTypes of wheels:\n\nUniversal wheels: support both Python 2 & 3 on all platforms\nPure-Python wheels: support either Python 2 or Python 3 on all platforms, but not both\nPlatform wheels: support specific Python version and platform\n\nTo build s pure Python wheel:\n\npython setup.py sdist bdist_wheel. By default, it will be stored in dist directory\npython setup.py sdist -d dir_name bdist_wheel -d dir_name\n\nTo build universal wheel:\n\nUse .cfg file by adding:\n\n[bdist_wheel]\nuniversal = 1\n\nUse python setup.py sdist bdist_wheel --universal\nAdd this line to setup.py file: options={\"bdist_wheel\": {\"universal\": True}}\n\nCheck resources to build platform wheels or manylinux wheels\nWe can use CI pipelines such as GithubActions to test the package on multiple platforms\ncheck-wheel-contents is a tool that helps detecting any problems with the wheel file\nTestPyPI allows us to test the wheel file as if it’s a real thing:\n\nWe first upload it to TestPyPI:\n\npython -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n\nThen try to install it and see if the wheel file works correctly:\n\npython -m pip install --index-url https://test.pypi.org/simple/  <pkg-name>\nWe can use twine tool to upload the package to PyPI:\n\nUpdate twine: python -m pip install -U twine\nUpload package: python -m twine upload dist/* which uploads both source distribution sdist and wheel file bdist_wheel. By default they will be is dist directory"
  },
  {
    "objectID": "til/python/itemview-and-keyview.html",
    "href": "til/python/itemview-and-keyview.html",
    "title": "Itemview and Keyview Attributes of Dictionary are Subclasses of Set",
    "section": "",
    "text": "d1 = dict(zip(\"abcd\", range(1, 5))) #=> {'a': 1, 'b': 2, 'c': 3, 'd': 4}\nd2 = dict(zip(\"cdef\", range(3, 7))) #=> {'c': 3, 'd': 4, 'e': 5, 'f': 6}\nNow we can do any set operations on .items() or .keys() views.\nd1.keys() & d2.keys() #=> {'c', 'd'}\nd1.keys() | d2.keys() #=> {'a', 'b', 'c', 'd', 'e', 'f'}\nd1.items() - d2.items() #=> {('a', 1), ('b', 2)}"
  },
  {
    "objectID": "til/python/modules-and-packages.html",
    "href": "til/python/modules-and-packages.html",
    "title": "Modules and Packages",
    "section": "",
    "text": "Modules are just objects of type ModuleType. They act like a dictionary that holds references to objects it holds; module.__dict__.\n\nWe can set/delete attributes. module.x = 10 is the same as module.__dict__['x'] = 10\n\nWhen importing a module, it executes the module from top to bottom before returning to the caller.\nModule can be namespace, py file, execution environment for statements or container of global variables.\nThe dictionary has preset attributes such as __path__, __loader__, etc. Main attributes:\n\n__name__ : Module name\n__file__ : Associated source file (if any)\n__doc__: Docstring\n__path__ : Package path. It is used to look for package subcomponents\n__package__ : The module’s __package__ attribute must be set. Its value must be a string, but it can be the same value as its __name__. When the module is a package, its __package__ value should be set to its __name__. When the module is not a package, __package__ should be set to the empty string for top-level modules, or for submodules, to the parent package’s name.\n__spec__ : Module spec\n\nThe module’s attributes are set after creation and before execution\nThe main difference between modules and packages is that packages have __path__ and __package__ defined (not None)\nsys.modules serves as a cache for all imported modules/packages\n\nIt is a dictionary so we can delete/set keys\nIf we delete a module, it will force Python to import it when we reimport it\nIf we set module key to None -> result in ModuleNotFoundError\n\nEven if we import one object from a module/package, the module/package will be cached in the sys.modules but not available in the global name space\nThe module created during loading and passed to exec_module() may not be the one returned at the end of the import\n\nThis can happen if the imported module set the sys.modules[__name__] to some other module\n\nExecution of the module is what populates the module’s __dict__ (namespace of the module). This is done by the loader\nWhen a submodule is loaded using any mechanism, a binding is placed in the parent module’s namespace to the submodule object. For example, if we have a package called spam that has a submodule foo and it imports any of its objects like from .foo import x, after importing spam, spam will have an attribute foo which is bound to the submodule -> We can now use spam.foo\nRelative imports use leading dots. A single leading dot indicates a relative import, starting with the current package. Two or more leading dots indicate a relative import to the parent(s) of the current package, one level per dot after the first.\n\nCan only use this form of import: from <> import <>\nIt can’t use import .<> because this is not a valid expression\n\nAbsolute imports have to start from the top level package and go downward to refer to the module: from package.subpackage import module\n\nNot recommended because if we change the name of the package then we need to change all the import statements -> relative imports are more robust and don’t care about namings"
  },
  {
    "objectID": "til/python/slots.html",
    "href": "til/python/slots.html",
    "title": "Using __slots__ to Store Instance’s Attributes",
    "section": "",
    "text": "import sys\n\n\nclass C:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\nclass C_Slots:\n    __slots__ = (\"x\", \"y\")\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\nprint(sys.getsizeof(C))         #=> 904\nprint(sys.getsizeof(C_Slots))   #=> 1,072\nThe main problems with __slots__:\n\nCan’t dynamically add attributes to the instance. We can work around this issue by adding __dict__ to __slots__ but this negates the benefits of using __slots__.\nDoesn’t support __weakref__ by default. We need to add __weakref__ to __slots__ to be able to use it.\nAll sublcasses have to declare __slots__ because attributes can’t be inheritted and will be ignored by the interpreter.\n\nIn conclusion, use __slots__ only if it is justified."
  },
  {
    "objectID": "til/python/cond-iterator.html",
    "href": "til/python/cond-iterator.html",
    "title": "Conditional Iterators",
    "section": "",
    "text": "def gen():\n    return random.randint(1, 10)\n\ncond_iterator = iter(gen, 1)\n[e for e in cond_iterator] #=> [6, 10, 2, 4, 3, 6, 7, 5, 5, 4, 7, 10, 8, 10, 10, 8]"
  },
  {
    "objectID": "til/python/scope-of-variables-in-comprehensions.html",
    "href": "til/python/scope-of-variables-in-comprehensions.html",
    "title": "Scope of Variables in Comprehensions",
    "section": "",
    "text": "x = 10\nl = [x for x in range(100) if x % 2 == 0]\nprint(x) #=> 10"
  },
  {
    "objectID": "til/python/equality-vs-identity.html",
    "href": "til/python/equality-vs-identity.html",
    "title": "== vs is",
    "section": "",
    "text": "To test whether two variables point to the same object, we use is operator which compares the identity of both variables. If the return value is True -> both variables point to the same object. This can also be tested in CPython using id().\nTo test if both variables (objects they point to) have the same content, we use == operator.\n\na = [2]\nb = a\na is b          #=> True\nid(a) == id(b)  #=> True\n\n\na = [2]\nb = [2]\na is b          #=> False\nid(a) == id(b)  #=> True\nIt is recommended to use is operator when checking if a variable is: True, False, or None. If we use ==, it will invoke __eq__ method, which if not implemented, compare the ids of both variables and is slower than is operator because it first has to look up __eq__ method and then invoke id() on both variables."
  },
  {
    "objectID": "til/python/lists-vs-tuples.html",
    "href": "til/python/lists-vs-tuples.html",
    "title": "Lists vs Tuples",
    "section": "",
    "text": "They share some of the same characteristics such as search time which is O(n). However, there are some differences that makes tuples preferred over lists in certain circumstances as is seen in the Python interpretter especially when the object’s size is not expected to change.\n\nLists are mutable but tuples are immutable. This means that we can’t delete or replace elements in tuples or extend/shrink its size.\nTuples are much faster to create because Python cache them at runtime and doesn’t communicate with the OS to allocate memory as it does for the lists.\n\n%timeit l = [0,1,2,3,4,5,6,7,8,9] #=> 60.5 ns ± 0.357 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n%timeit t = (0,1,2,3,4,5,6,7,8,9) #=> 11.9 ns ± 0.255 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)\n\nTuples can be constant folded: Python interpretter generates bytecode for a tuple in one operation, but generates bytecode for each element is the list as a seperate operation then pushes it to data stack.\n\ndis.dis(\"(1, 2)\")\n  1           0 LOAD_CONST               0 ((1, 2))\n              2 RETURN_VALUE\n\ndis.dis(\"[1, 2]\")\n  1           0 LOAD_CONST               0 (1)\n              2 LOAD_CONST               1 (2)\n              4 BUILD_LIST               2\n              6 RETURN_VALUE\n\nTuples can be reused instead of copied: If a tuple is hashable, tuple(t) returns itself because it should stay fixed. On the other side, list(l) would return new copy.\n\nt = (1, 2)\ntuple(t) is t #=>  True\n\nl = (1, 2)\nlist(l) is l #=>  False\n\nTuples are compact and don’t over-allocate.\n\nsys.getsizeof(tuple(iter(range(10)))) #=> 120\nsys.getsizeof(list(iter(range(10)))) #=> 136\n\nTuples directly reference their elements: references to objects are incorporated directly in a tuple object. In contrast, lists have an extra layer of indirection to an external array of pointers. This gives tuples a small speed advantage for indexed lookups and unpacking:\n\nHere is how the tuple (10, 20) stored:\n\ntypedef struct {\n    Py_ssize_t ob_refcnt;\n    struct _typeobject *ob_type;\n    Py_ssize_t ob_size;\n    PyObject *ob_item[2];     /* store a pointer to 10 and a pointer to 20 */\n} PyTupleObject;\n\nHere is how the list [10, 20] stored:\n\nPyObject arr[2];              /* store a pointer to 10 and a pointer to 20 */\n\ntypedef struct {\n    Py_ssize_t ob_refcnt;\n    struct _typeobject *ob_type;\n    Py_ssize_t ob_size;\n    PyObject **ob_item = arr; /* store a pointer to the two-pointer array */\n    Py_ssize_t allocated;\n} PyListObject;\nConclusion: Use tuples as immutable lists to 1) clarify that its length won’t change and 2) get a little of speed up due to some optimizations the Python interpretter may do as a result of some of the above."
  },
  {
    "objectID": "til/python/loading-modules-and-packages.html",
    "href": "til/python/loading-modules-and-packages.html",
    "title": "Loading Modules and Packages",
    "section": "",
    "text": "First checks if it is cached. If not, continue\nIt creates a ModuleType object with that name\nCache the module in sys.modules\nExecutes the source code inside the module (first prefixing it with .py and then assign __file__)\n\nIn the case of the package/subpackage, it assign it the __init__.py file\nIt also executes all the __init__.py on the path\n\nAssign a variable to the module object\n\nBelow is in roughly what it is done in Python code:\nimport sys, types\n\ndef import_module(modname):\n    # Check if it is in the cache first\n    if modname in sys.modules:\n        return sys.modules[modname]\n\n    sourcepath = modname + '.py'\n    with open(sourcepath, 'r') as f:\n        sourcecode = f.read()\n    mod = types.ModuleType(modname)\n    mod.__file__ = sourcepath\n\n    # Cache the module\n    sys.modules[modname] = mod\n\n    # Convert it to Python ByteCode\n    code = compile(sourcecode, sourcepath, 'exec')\n\n    # Execute the code in the module from top to bottom\n    # And update the state (globals) in the module's dictionary\n    exec(code, mod.__dict__)\n\n    # We return the cached one in case there is some patching inside the module\n    return sys.modules[modname]\nFinally, Python puts a lock when importing a module until it is done so that we don’t have multiple threads trying to import the same module at the same time."
  },
  {
    "objectID": "til/python/user-defined-classes-are-hashable.html",
    "href": "til/python/user-defined-classes-are-hashable.html",
    "title": "User Defined Classes are Hashable",
    "section": "",
    "text": "class A:\n    ...\n\na1 = A()\na2 = A()\nid(a1) == hash(id(a1)) #=> True\nid(a2) == hash(id(a2)) #=> True\nIf a user-defined class implements __eq__, then it needs to implement __hash__ and has to guarantee that it is hashable if all its attributes are hashable and if a == b then hash(a) == hash(b)."
  },
  {
    "objectID": "til/python/python-lookup.html",
    "href": "til/python/python-lookup.html",
    "title": "Python Object Lookup Process",
    "section": "",
    "text": "First, Python looks inside the locals() array, which has entries for all local variables. Python works hard to make local variable lookups fast, and this is the only part of the chain that doesn’t require a dictionary lookup.\n\n__local variables__ do not need a dictionary lookup to be found; they are stored in a very slim array that has very fast lookup times. Because of this, finding the function is quite fast!\n\nIf it doesn’t exist there, the globals() dictionary is searched.\nFinally, if the object isn’t found there, the __builtin__ object is searched.\n\nIt is important to note that while locals() and globals() are explicitly dictionaries and __builtin__ is technically a module object, when searching __builtin__ for a given property, we are just doing a dictionary lookup inside its locals() map (this is the case for all module objects and class objects!).\nAs a result of the above:\n\nIt is recommended to import specific objects (variables/functions/classess…) from a module instead of just importing the module and use the module name to refer to the objects it contains because this will slow down the run-time.\nThe performance of the dictionary lookup is largely dependent of the hash function. Therefore, the slower the hash function, the slower the lookup and may not have O(1) lookup anymore."
  },
  {
    "objectID": "til/python/slicing.html",
    "href": "til/python/slicing.html",
    "title": "Sequence Slicing",
    "section": "",
    "text": "When we have commas inside [], __getitem__ gets tuple of slice object.\nWhen we have : inside [], __getitem__ gets a slice object.\n\nslice object has 4 important attributes:\n\nstart\nstep\nstop\nindices"
  },
  {
    "objectID": "til/python/line-continuation.html",
    "href": "til/python/line-continuation.html",
    "title": "Line Continuation",
    "section": "",
    "text": "But the more pythonistic way is to use with {}, [], or () where line breaks will be ignored. For example:\nstring = (\n    \"first \"\n    \"name\"\n    )\nprint(string) #=> \"first name\""
  },
  {
    "objectID": "til/python/closures.html",
    "href": "til/python/closures.html",
    "title": "Closures",
    "section": "",
    "text": "def func(x):\n    y = 100\n    def nested_func(z):\n        return x + y + z\n    return nested_func\n\nf = func(10)\nprint(f.__closure__)            #=> (<cell at 0x7fd10dc26830: int object at 0x7fd108a00210>, \n                                #=>  <cell at 0x7fd10d7abe50: int object at 0x7fd108a00d50>)\nprint(f.__code__.co_freevars)   #=> ('x', 'y')\nf(10)                           #=> 120\nx and y are free variables here. Each object f.__closure__ corresponds to a name in the f.__code__.co_freevars.\nThis is in some way similar to decorators where the decorated function is nothing but the nested function, which can be written in the following form:\ndef func(x):\n    def decorator_func(func):\n        y = 100\n        def wrapper_func(z):\n            return func(z) + x + y\n        return wrapper_func\n    return decorator_func\n\n@func(10)\ndef nested_func(z):\n    return z\n\nnested_func(10)     #=> 120"
  },
  {
    "objectID": "til/python/getitem-makes-object-iterable.html",
    "href": "til/python/getitem-makes-object-iterable.html",
    "title": "__getitem__ Makes Object an Iterator",
    "section": "",
    "text": "class L:\n    def __init__(self, length=10):\n        self.data = list(range(length))\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n    def __repr__(self):\n        res = f\"{self.__class__.__name__}: {self.data[:10]}\"\n        if len(self.data) > 10:\n            res = res[:-1] + \", ...]\"\n        return res\nl = L()\nl[i for i in l] #=> L: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n1 in l          #=> True"
  },
  {
    "objectID": "til/python/logging.html",
    "href": "til/python/logging.html",
    "title": "Python Logging",
    "section": "",
    "text": "logging.getLogger(name) returns an instance of the named logger. If it already exists, return the existing instance. It no name is provided, returns the root logger.\nEach logger have:\n\nLevel (severity)\nZero or more Handlers\nFormatter\nFilter\n\nThe loggers have hierarchy with root being the top level logger.\nIf configuration is not set for a logger, it keeps going up the hierarchy to its parents until it finds it. If nothing is found, use the root logger config since it is the parent of all loggers.\nWe can configure loggers:\n\nDirectly in python scripts\nUsing config file and import it using logging.config.fileConfig(file_name)\nOR using dictionary in YAML file and import it using logging.config.dictConfig()\n\nTo share the same configuration of loggers across different modules, we only configure root logger in the entry module and all other loggers will use the root config since root is their parent."
  },
  {
    "objectID": "til/python/namespace-packages.html",
    "href": "til/python/namespace-packages.html",
    "title": "Namespace Packages",
    "section": "",
    "text": "Python first scans the whole sys.path before deciding that the package is a namespace -> If any name is found with init.py in it, it will give this priority and don’t continue.\n\nIf it finds a regular package with that name -> discard all namespace packages it found and import the regular package.\n\nIf it doesn’t find any regular package with that name -> Use all the namespace packages it found during the search and combine their paths in namespace_path so when we try to import subpackage or modules, it checks all the paths in the namespace_path (which is a list).\n\nThere can be multiple packages of the same name (under different directories) -> They all combined together and the namespace_path list would have the path for all of them. Therefore, the same package can be used to refer to completely different modules in different directories."
  },
  {
    "objectID": "til/python/mutable-default-function-arguments.html",
    "href": "til/python/mutable-default-function-arguments.html",
    "title": "Mutable Default Function Arguments",
    "section": "",
    "text": "This happens because a function gets evaluated once per module load after the program starts up. As a result, default argument value is evaluated only once. This can cause odd behaviors for dynamic values (such as {}, [], or datetime.now()). Therefore:\n\nUse None as the default value for any keyword argument that has a dynamic value. Document the actual default behavior in the function’s docstring.\nUsing None to represent keyword argument default values also works correctly with type annotations.\n\ndef func(y=[]):\n    y.append(\"test\")\n    print(id(y), y)\n\n# The following two call will use the same object for the default arg\nfunc() #=> 140511381181056 ['test']\nfunc() #=> 140511381181056 ['test', 'test']\n\n# Notice how the argument is a different object\nfunc([]) #=> 140511380870592 ['another', 'test']"
  },
  {
    "objectID": "til/python/immutable-objects-and-augmented-assignment-operators.html",
    "href": "til/python/immutable-objects-and-augmented-assignment-operators.html",
    "title": "Immutable Objects and Augmented Assignment Operators",
    "section": "",
    "text": "For mutable objects such as lists:\nl = [1, 2]\nold_id = id(l)\nl += [3]            # the same as l.extend([3])\nprint(l)            #=> [1, 2, 3]\nold_id == id(l)     #=> True\nFor mutable objects such as tuples:\nt = [1, 2]\nold_id = id(t)\nt += (3,)           # the same as t = t + (3,)\nprint(t)            #=> (1, 2, 3)\nold_id == id(t)     #=> False"
  },
  {
    "objectID": "til/python/truthiness-of-objects.html",
    "href": "til/python/truthiness-of-objects.html",
    "title": "Truthiness of Python Objects",
    "section": "",
    "text": "x.__bool__() is called. If __bool__ is not implemented, then\nx.__len__() is called. If __len__() is not implemented, then\nreturn True. As a result, all user defined objects are truthy unless __bool__ or __len__ are implemented. Also, we can use if iterable w/o the need to use if len(iterable).\n\nclass A:\n    ...\n\nclass B:\n    def __bool__(self):\n        print(\"In __bool__\")\n        return False\n\nclass C:\n    def __len__(self):\n        print(\"In __len__\")\n        return False\n\nclass D:\n    def __bool__(self):\n        print(\"In __bool__\")\n        return False\n\n    def __len__(self):\n        print(\"In __len__\")\n        return False\nif A(): print(\"Truthy\")     #=> \"Truthy\"\nif B(): print(\"Truthy\")     #=> \"In __bool__\"\nif C(): print(\"Truthy\")     #=> \"In __len__\"\nif D(): print(\"Truthy\")     #=> \"In __len__\""
  },
  {
    "objectID": "til/python/decorators.html",
    "href": "til/python/decorators.html",
    "title": "Decorators",
    "section": "",
    "text": "@decorator\ndef func():\n    pass\nis the same as\nfunc = decorator(func)\nBoilerplate template for any decorator:\nimport functools\ndef decorator(func):\n    @functools.wraps(func)\n    def wrapper_decorator(*args, **kwargs):\n        # Do something\n        value = func(*args, **kwargs)\n        # Do something\n        return value\n    return wrapper_decorator\nWe can also have decorators that take optional arguments:\n# Here _func is positional-only argument and num_times is keyword-only argument\ndef repeat(_func=None, *, num_times=1):\n    def decorator_repeat(func):\n        @functools.wraps(func)\n        def wrapper_repeat(*args, **kwargs):\n            res = 0\n            for _ in range(num_times):\n                res += func(*args, **kwargs)\n            return res\n        return wrapper_repeat\n    \n    # If the decorator is called with arguments\n    if _func is None:\n        return decorator_repeat\n    else:\n        return decorator_repeat(_func)\n\n@repeat\ndef add(x, y):\n    return x + y\nadd(10, 10)         #=> 20\n\n@repeat(num_times=3)\ndef add(x, y):\n    return x + y\nadd(10, 10)         #=> 60\nIf we want the decorator to be stateful, we can use classes as decorators:\nclass Decorator:\n    def __init__(self, func):\n        # We can't use @functools.wrap(func)\n        functools.update_wrapper(self, func)\n        self.func = func\n        # keep state here\n\n    def __call__(self, *args, **kwargs):\n        # Do something, probably update state\n        value = self.func(*args, **kwargs)\n        # Do something, probably update state\n        return value\nDon’t forget to return the value of decorated function 😀"
  },
  {
    "objectID": "til/python/mutability-and-inplace-operations.html",
    "href": "til/python/mutability-and-inplace-operations.html",
    "title": "Mutability and Inplace Operations",
    "section": "",
    "text": "s = \"test\"\nold_id = id(s)\ns += \" another test\"\nold_id == id(s) #=> False\nBut for mutable objects, it doesn’t create a new object. It extends/updates the existing object.\nl = [1]\nold_id = id(l)\nl += [2]\nold_id == id(l) #=> True"
  },
  {
    "objectID": "til/python/static-vs-class-vs-instance-methods.html",
    "href": "til/python/static-vs-class-vs-instance-methods.html",
    "title": "Instance vs Class vs Static Method",
    "section": "",
    "text": "class C:\n    \n    def instance_method(self):\n        pass\n    \n    @classmethod\n    def class_method(cls):\n        pass\n    \n    @staticmethod\n    def static_method():\n        pass\n\nInstance method is a function that is bound to the instance itself and the first argument will always be the instance object self. It has access to both instance and class variables. Most methods inside the class are typically instance methods.\nClass method is a function that is bound to the class ONLY and the first argument will always be the class object itself cls. It has access ONLY to the class variables. It is typically used as a factory method.\nStatic method is like a plain function that is not bound to neither classs nor instance and doesn’t depend on the state of the object. Therefore, it doesn’t have access to any class or instance variables. It is typically used if it is very close to the class but not exactly belongs to it and is not used from outside the class."
  },
  {
    "objectID": "til/shell/null-device.html",
    "href": "til/shell/null-device.html",
    "title": "Useful Tricks with Null Device",
    "section": "",
    "text": "To discard stderr, which has file descriptor 2, we can use the following redirection:\nls -l dir 2>/dev/null\nTo truncate a file, instead of rm and then touch, we can use:\ncat /dev/null > file-to-truncate"
  },
  {
    "objectID": "til/shell/input-output-redirection.html",
    "href": "til/shell/input-output-redirection.html",
    "title": "I/O Redirection",
    "section": "",
    "text": "Redirect stdin fd to a file such as cat < file\n\nThe file descriptor that we want to redirect has to be infront of <. In the case of redirecting stdin, 0< is the same as <.\n\nRedirect stdout/stderr to a file such as ls dir > tmp or ls dir 2>error\n\nThe file descriptor that we want to redirect has also to be in the front of >. In the case of redirecting stdout, 2> is the same as >.\nIf we want to redirect some file descriptor fd0 to another file descriptor fd1, we can do fd0>fd1>.\n> truncates the file we’re redirecting the output to and writes the new output\n>> appends to the file we’re redirecting the output to, and doesn’t overwrite the old context. Example: cat /etc/passwd >> tmp."
  },
  {
    "objectID": "til/shell/execute-vs-source-script.html",
    "href": "til/shell/execute-vs-source-script.html",
    "title": "Executing vs Sourcing Script",
    "section": "",
    "text": "If we execute a program from the shell (./script.sh), we don’t get the side effects from that program. For example, if the program defines variables or change directories, those variables won’t be defined in the shell and we would still be in the same directory. The reason for that is because the shell spin a new shell instance to execute the program and then terminates once done. Therefore, the changes happens in the child process which can’t affect the parent process (process that invoked the sript which happens to be the shell).\nIf we load the program to the shell (source script.sh), then we get the side effects. Therefore, we can then use some of its functions and if some of those functions change directories it would be reflected in out shell. This happens because the script executes in the current process, which means no child process is created to execute it."
  },
  {
    "objectID": "til/unix/temp-files.html",
    "href": "til/unix/temp-files.html",
    "title": "Temporary Files & Directories",
    "section": "",
    "text": "Creates a file with random name\nMake the file mode “wb+” which means create it for reading/writing and truncate if it exists\nUnlink the file right after creation. This will guarantee that the file will be deleted after the process terminates\nReturn the file object to the caller\n\nOnce we close the file or all the process(es) that has a reference to the file terminates, the file will be removed. The same thing applies to temporary directories.\nExample in Python:\nimport tempfile\n\n# Create temp file \nfp = tempfile.TemporaryFile()\n\n# Write to the file\nfp.write(\"First write!\")\n\n# Read from the beginning\nfp.seek(0)\nfp.read()   #=> \"First write!\"\n\n# Now the file will be closed and removed\nfp.close()"
  },
  {
    "objectID": "til/chrome/navigate-and-control-browser-with-vimium.html",
    "href": "til/chrome/navigate-and-control-browser-with-vimium.html",
    "title": "Vimium: Vim Key Bindings for Chrome",
    "section": "",
    "text": "Vimium is a chrome extension that provides keyboard shortcuts to navigate and control the browser inspired by Vim editor. It comes very handy when navigating between pages as well as on the same page and still be mouse-free. Keyboard shortcut examples: - d to go down half page - u to go up half page - gg to go the top of the page - G to go to the bottom of the page - gt to go to next tab to the right - gT to go to previous tab to the left"
  },
  {
    "objectID": "til/tmux/list-sessions.html",
    "href": "til/tmux/list-sessions.html",
    "title": "Listing tmux Sessions",
    "section": "",
    "text": "If you’re within a tmux session, prefix-w would list the tree of all sessions and the windows/panes withing each session. This is very handy to toggle between sessions/windows."
  },
  {
    "objectID": "til/pytorch/save-memory.html",
    "href": "til/pytorch/save-memory.html",
    "title": "Save Memory When Operating on Tensors",
    "section": "",
    "text": "x = torch.randn(1000, 1000)\nbefore = id(x)\nx.add_(100)\nid(x) == before #=> True\n\nOr change the current object that the tensor is pointing to\n\nx = torch.randn(1000, 1000)\nbefore = id(x)\nx += 100\nid(x) == before #=> True\n\nOr assign the result of the operation to the previously allocated tensor\n\nx = torch.randn(1000, 1000)\nbefore = id(x)\nx[:] = x + 100\nid(x) == before #=> True\nOtherwise, if we do the following, it will create new object and make x point to it. This will put pressure on the host memory.\nx = torch.randn(1000, 1000)\nbefore = id(x)\nx = x + 100\nid(x) == before #=> False"
  },
  {
    "objectID": "til/pytorch/detach-tensor-from-computation-graph.html",
    "href": "til/pytorch/detach-tensor-from-computation-graph.html",
    "title": "Detach Tensor From Computation Graph",
    "section": "",
    "text": "inputs: list of inputs such as tensors and scalars that created the output tensor. Leaf tensors have no inputs\noperation: function that was applied on the inputs to create the output tensor. Leaf tensors have no operations\ndata: the output tensor from applying the operation on the inputs\nrequires_grad: whether we want to track the history of the computations when creating new tensors. If False, inputs and operation attributes will be set to None\n\nTherefore, to detach a tensor from a computation graph Or if we don’t want to track temporary computations done on a tensor (such as during inference), we can do the following:\n\nTensor.detach() returns new tensor sharing the same underlying storage but make it a leaf tensor. Therefore:\n\ninputs will be set to None\noperation will be set to None\nrequires_grad will be set to False\n\nwith torch.no_grad let’s us perform computations on tensors w/o tracking those computations\nOperating directly on Tensor.data avoids recoding the operations. Useful when updating a tensor or for initialization\n\na = torch.tensor([[1., 2.]], requires_grad=True)\na  #=> tensor([[1., 2.]], requires_grad=True)\n\n# The following will record the computation\nb = a + 1\nb.grad_fn  #=> <AddBackward0 at 0x7f81abadc8b0>\n\n# All the following forms allow us to avoid recording the computation on the tensor\nb = a.data + 1\nb.grad_fn           #=> None\nb.requires_grad     #=> False\n\nb = a.detach() + 1\nb.grad_fn           #=> None\nb.requires_grad     #=> False\n\nwith torch.no_grad():\n    b = a + 1\nb.grad_fn           #=> None\nb.requires_grad     #=> False\n\n# Useful for updates/initialization because it keeps requires_grad attribute\na.data = torch.randint(10, size=(1, 2))\na.grad_fn           #=> None\nb.requires_grad     #=> True"
  },
  {
    "objectID": "til/pytorch/list-gpus.html",
    "href": "til/pytorch/list-gpus.html",
    "title": "List Available GPUs",
    "section": "",
    "text": "If you specify cpu as a device such as torch.device(\"cpu\"), this means all available CPUs/cores and memory will be used in the computation of tensors.\nIf you specify cuda as a device such as torch.device(\"cuda\"), it is the same as torch.device(\"cuda:0\") which is the first GPU and memory.\nOtherwise, specify which GPU you want using torch.device(\"cuda:{i}\") for the ith device."
  },
  {
    "objectID": "til/pytorch/lazy-layers.html",
    "href": "til/pytorch/lazy-layers.html",
    "title": "Lazy Layers",
    "section": "",
    "text": "x = torch.randn(1, 20)\nlayer = nn.LazyLinear(1)\nlayer(x) #=> tensor([[-1.3754]], grad_fn=<AddmmBackward0>)"
  },
  {
    "objectID": "til/c/memory-layout.html",
    "href": "til/c/memory-layout.html",
    "title": "C Program Memory Layout",
    "section": "",
    "text": "Env & Args\nStack\nShared memory\nHeap\nUninitialized data (BSS)\nInitialized data\nText\n\nI omitted few segments such as read-only data, init, etc. The code that produced the layout can be found at the endof the page (credit: Advanced Programming in the Unix Environment)\nHigh address (args and env):\n----------------------------\nenvp[55] at                                            : 0x7FF7BE0AD5D0\nenviron[55] at                                         : 0x7FF7BE0AD5D0\nenvp[0] at                                             : 0x7FF7BE0AD418\nenviron[0] at                                          : 0x7FF7BE0AD418\nlast arg at                                            : 0x7FF7BE0AD410\nfirst arg at                                           : 0x7FF7BE0AD408\n\nStack:\n------\nFirst variable inside main at                          : 0x7FF7BE0AD294\nfunc_array[] ends at                                   : 0x7FF7BE0AD2C0\nfunc_array[] (like 'array[]', but on stack) begins at  : 0x7FF7BE0AD2B0\nargc at                                                : 0x7FF7BE0AD2A8\nargv at                                                : 0x7FF7BE0AD2A0\nenvp at                                                : 0x7FF7BE0AD298\nfunc2 (from main): frame at                            : 0x7FF7BE0AD254\nfunc frame at                                          : 0x7FF7BE0AD258\nstatic int n within func at                            : 0x   101E5A06C\nfunc (called     0 times): frame at                    : 0x7FF7BE0AD258\nfunc2 (from func): frame at                            : 0x7FF7BE0AD234\n\nShared memory:\n--------------\nshared memory attachment begins at                     : 0x   101F84000\nshared memory attachment ends at                       : 0x   101F9C6A0\n\nHeap:\n-----\nmalloced area ends at                                  : 0x6000026AD120\nmalloced area begins at                                : 0x6000026AD100\n\nUninitialized Data (BSS):\n-------------------------\nSemaphore at                                           : 0x   101E5A0F4\nCond at                                                : 0x   101E5A080\nLock at                                                : 0x   101E5A0B0\narray[] ends at                                        : 0x   101E5A080\narray[] (uninitialized, fixed-size char * on BSS) from : 0x   101E5A070\nnum2 (uninitialized global int) at                     : 0x   101E5A0F0\nstring2 (uninitialized global char *) at               : 0x   101E5A0F8\nextern **environ at                                    : 0x7FF8497239A0\n\nInitialized Data:\n-----------------\nnum (initialized global int) at                        : 0x   101E5A068\nstring (initialized global char *) at                  : 0x   101E5A060\n\nText Segment:\n-------------\nfunc2 (function) at                                    : 0x   101E55440\nfunc (function) at                                     : 0x   101E55470\nmain (function) at                                     : 0x   101E54F20\n#include <errno.h>\n#include <err.h>\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <pthread.h>\n#include <semaphore.h>\n#include <sys/shm.h>\n\n#define ARRAY_SIZE 16\n#define MALLOC_SIZE 32\n#define SHM_SIZE 100000\n#define SHM_MODE 0600\n\nchar array[ARRAY_SIZE];\nchar *string = \"a string\";\nchar *string2;\nint num = 10;\nint num2;\npthread_mutex_t lock;\npthread_cond_t cond;\nsem_t sem;\nextern char **environ;\n\nvoid func(int);\nvoid func2(const char *);\n\nint main(int argc, char **argv, char **envp) {\n    int vars;\n    int shmid;\n    char *ptr;\n\n    char func_array[ARRAY_SIZE];\n\n    vars = 0;\n    char **tmp = envp;\n    while (*tmp++) {\n        vars++;\n    }\n\n    (void)printf(\"High address (args and env):\\n\");\n    (void)printf(\"----------------------------\\n\");\n    (void)printf(\"envp[%d] at                                            : 0x%12lX\\n\", vars, (unsigned long)&envp[vars]);\n    (void)printf(\"environ[%d] at                                         : 0x%12lX\\n\", vars, (unsigned long)&environ[vars]);\n    (void)printf(\"envp[0] at                                             : 0x%12lX\\n\", (unsigned long)envp);\n    (void)printf(\"environ[0] at                                          : 0x%12lX\\n\", (unsigned long)environ);\n    (void)printf(\"last arg at                                            : 0x%12lX\\n\", (unsigned long)&argv[argc]);\n    (void)printf(\"first arg at                                           : 0x%12lX\\n\", (unsigned long)&argv[0]);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Stack:\\n\");\n    (void)printf(\"------\\n\");\n    (void)printf(\"First variable inside main at                          : 0x%12lX\\n\", (unsigned long)&vars);\n    (void)printf(\"func_array[] ends at                                   : 0x%12lX\\n\", (unsigned long)&func_array[ARRAY_SIZE]);\n    (void)printf(\"func_array[] (like 'array[]', but on stack) begins at  : 0x%12lX\\n\", (unsigned long)&func_array[0]);\n\n    (void)printf(\"argc at                                                : 0x%12lX\\n\", (unsigned long)&argc);\n    (void)printf(\"argv at                                                : 0x%12lX\\n\", (unsigned long)&argv);\n    (void)printf(\"envp at                                                : 0x%12lX\\n\", (unsigned long)&envp);\n\n    func2(\"from main\");\n    func(0);\n\n    (void)printf(\"\\n\");\n\n    printf(\"Shared memory:\\n\");\n    printf(\"--------------\\n\");\n    if ((shmid = shmget(IPC_PRIVATE, SHM_SIZE, SHM_MODE)) < 0) {\n            fprintf(stderr, \"Unable to get shared memory: %s\\n\",\n                strerror(errno));\n            exit(1);\n        }\n\n        if ((ptr = shmat(shmid, 0, 0)) == (void *)-1) {\n            fprintf(stderr, \"Unable to map shared memory: %s\\n\",\n                strerror(errno));\n            exit(1);\n        }\n        printf(\"shared memory attachment begins at                     : 0x%12lX\\n\", (unsigned long)ptr);\n        printf(\"shared memory attachment ends at                       : 0x%12lX\\n\", (unsigned long)ptr+SHM_SIZE);  (void)\n    printf(\"\\n\");\n    printf(\"Heap:\\n\");\n    (void)printf(\"-----\\n\");\n    if ((ptr = malloc(MALLOC_SIZE)) == NULL) {\n        err(EXIT_FAILURE, \"unable to allocate memory\");\n        /* NOTREACHED */\n    }\n\n    (void)printf(\"malloced area ends at                                  : 0x%12lX\\n\", (unsigned long)ptr+MALLOC_SIZE);\n    (void)printf(\"malloced area begins at                                : 0x%12lX\\n\", (unsigned long)ptr);\n    free(ptr);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Uninitialized Data (BSS):\\n\");\n    (void)printf(\"-------------------------\\n\");\n    (void)printf(\"Semaphore at                                           : 0x%12lX\\n\", (unsigned long)&sem);\n    (void)printf(\"Cond at                                                : 0x%12lX\\n\", (unsigned long)&cond);\n    (void)printf(\"Lock at                                                : 0x%12lX\\n\", (unsigned long)&lock);\n    (void)printf(\"array[] ends at                                        : 0x%12lX\\n\", (unsigned long)&array[ARRAY_SIZE]);\n    (void)printf(\"array[] (uninitialized, fixed-size char * on BSS) from : 0x%12lX\\n\", (unsigned long)&array[0]);\n    (void)printf(\"num2 (uninitialized global int) at                     : 0x%12lX\\n\", (unsigned long)&num2);\n    (void)printf(\"string2 (uninitialized global char *) at               : 0x%12lX\\n\", (unsigned long)&string2);\n    (void)printf(\"extern **environ at                                    : 0x%12lX\\n\", (unsigned long)&environ);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Initialized Data:\\n\");\n    (void)printf(\"-----------------\\n\");\n    (void)printf(\"num (initialized global int) at                        : 0x%12lX\\n\", (unsigned long)&num);\n    (void)printf(\"string (initialized global char *) at                  : 0x%12lX\\n\", (unsigned long)&string);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Text Segment:\\n\");\n    (void)printf(\"-------------\\n\");\n    (void)printf(\"func2 (function) at                                    : 0x%12lX\\n\", (unsigned long)&func2);\n    (void)printf(\"func (function) at                                     : 0x%12lX\\n\", (unsigned long)&func);\n    (void)printf(\"main (function) at                                     : 0x%12lX\\n\", (unsigned long)&main);\n    (void)printf(\"\\n\");\n    return EXIT_SUCCESS;\n}\n\n\nvoid func(int recurse) {\n    int fint;\n    char *msg = \"from func\";\n\n    /* Change this value to 0 and note how\n     * the location of where it is stored\n     * changes from the Data to BSS segment. */\n    static int n = 0;\n    (void)printf(\"func frame at                                          : 0x%12lX\\n\", (unsigned long)&fint);\n\n    if (recurse) {\n        msg = \"recursive\";\n    }\n    (void)printf(\"static int n within func at                            : 0x%12lX\\n\", (unsigned long)&n);\n    printf(\"func (called %5d times): frame at                    : 0x%12lX\\n\", n, (unsigned long)&fint);\n\n    n++;\n    func2(msg);\n}\n\nvoid func2(const char *how) {\n    int fint;\n    (void)printf(\"func2 (%s): frame at                            : 0x%12lX\\n\", how, (unsigned long)&fint);\n    func(1);\n}"
  },
  {
    "objectID": "til/c/char-array-vs-string-constant-pointers.html",
    "href": "til/c/char-array-vs-string-constant-pointers.html",
    "title": "Character Array vs String Constant Pointers",
    "section": "",
    "text": "Define it as an array of character such as:\n\nchar aname[] = \"Imad\";\n\nThe array would be of size number of characters plus 1 for the null terminating character.\n\n\nOr define it as a pointer to string constant such as:\n\nchar *pname = \"Imad\";\nThey are almost identical except for 1 subtle difference: - aname will always point to the same storage and can’t be changed to point to something else. The characters themselves can be changes. - pname is a pointer so it can be changed to point to something else. If we try to modify the underlying characters, the result is undefined."
  },
  {
    "objectID": "til/sql/what-is-null.html",
    "href": "til/sql/what-is-null.html",
    "title": "What Is Null Actually?",
    "section": "",
    "text": "and operator:\n\ntrue and Null results in Null\nfalse and Null results in false\nNull and Null results in Null\n\nor operator:\n\ntrue or Null results in true\nfalse or Null results in Null\nNull or Null results in Null\n\nnot operator:\n\nnot Null results in Null\n\nTo test if a value is Null, use IS NULL or IS NOT NULL\nIn where clause, recods that evaluates to either false or Null are excluded\nAggregation functions such as sum and average discard Null records\ncount(*) returns the number of records in a table. But count(field) returns the number of not Null records in the table. This means if the field is empty (Null values only), count(field) will return zero."
  },
  {
    "objectID": "til/sql/outer-join-and-where-clause.html",
    "href": "til/sql/outer-join-and-where-clause.html",
    "title": "Outer Joins & Where Clause",
    "section": "",
    "text": "Records from the left table that don’t find a match in the right table will have NULL values in the fields from the right table\nComparing NULL to any value results in NULL\nBecause WHERE only returns records that evaluate to true and discard records that evaluates to either false or NULL\nTherefore, records with NULL values will be excluded. This means only the join becomes INNER JOIN\n\nThe above works the same for any form of OUTER JOIN.\nTo work around this, add the predicate to join statement after ON clause."
  },
  {
    "objectID": "til/sql/logical-execution-order-of-sql-query.html",
    "href": "til/sql/logical-execution-order-of-sql-query.html",
    "title": "Logical Execution Order of SQL Query",
    "section": "",
    "text": "The FROM clause is first evaluated to get the output relation\nThen WHERE predicate is applied to the records in the output relation. If WHERE is not present, the predicate is True\nRecords satisfies the WHERE clause then placed into groups according to the GROUPBY clause. If GROUPBY clause is not present, all records will be placed into one group\nThen groups will filtered acoording to HAVING clause. If it is not present, the predicate is True\nThen the SELECT clause uses the remaining groups to generate results and, if they exist, apply aggregate functions on the results\nGenerated results will be sorted according to ORDER by clause if it is present. Otherwise, the results will be displayed randomly\nFinally, if LIMIT is present, display the number of records requested in the LIMIT clause"
  },
  {
    "objectID": "til/sql/external-vs-managed-tables.html",
    "href": "til/sql/external-vs-managed-tables.html",
    "title": "Managed vs External Tables",
    "section": "",
    "text": "Managed table is a table that the framework manages both the metadata about the table and its data directories. So when we drop the managed table, both the metadata and the actual data directories will be deleted.\nExternal table, on the other hand, is a table where the framework maintains metadata about it w/o actually managing the data itself. Therefore, when we drop it, only the metadata is deleted but the data directories stay intact."
  },
  {
    "objectID": "mlops-index.html",
    "href": "mlops-index.html",
    "title": "MLOps",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nMar 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nMar 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nFeb 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nFeb 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nJan 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nJan 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nJan 10, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers-summaries.html",
    "href": "papers-summaries.html",
    "title": "Papers’ Implementation & Summaries",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "books-summaries.html",
    "href": "books-summaries.html",
    "title": "Books’ Summaries",
    "section": "",
    "text": "No matching items"
  }
]