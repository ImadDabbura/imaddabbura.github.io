[
  {
    "objectID": "tiny-pytorch.html",
    "href": "tiny-pytorch.html",
    "title": "Tiny-PyTorch",
    "section": "",
    "text": "Tiny-Pytorch (GH repo, Documentation) is a deep learning system that is similar in nature to Pytorch. It involves implementing the core underlying machinery and algorithms behind deep learning systems such as 1) Automatic differentiation, 2) Tensor (multi-dimensional array), 3) Neural network modules such as Linear/BatchNorm/RNN/LSTM, 4) Optimization algorithms such as Stochastic Gradient Boosting (SGD) and Adaptive Momentum (Adam), 5) Hardware acceleration such as GPUs, etc.\nThe main learning and critical part of this project is building everything from the ground up:\n\n\n\n\n\ngraph BT\n    A[Flat Array] --&gt; B[N-Dimensional Array]\n    B[N-Dimensional Array] --&gt; C[Tensor]\n\n\n\n\n\n\nThe main objectives to build this framework:\n\nBuild deep learning systems\n\nContribute to open-source deep learning frameworks.\nWork on developing my own framework for specific tasks. I have been collecting my own implementation of different things in Pytorch such as analyzing gradients of each layer.\n\nUse existing systems more effectively:\n\nUnderstanding how the internals of existing deep learning systems work let you use them much more efficiently.\nThe only way to understand how things really work is to build it from scratch.\n\nUnderstand how operations are carried on both CPU and GPU so I can optimize my customized models/layers to run more efficiently."
  },
  {
    "objectID": "papers-summaries.html",
    "href": "papers-summaries.html",
    "title": "Papers’ Summaries",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 15, 2025\n\n\nMetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n\n\nImad Dabbura\n\n\n\n\nMar 7, 2025\n\n\nOpenHands: An Open Platform for AI Software Developers as Generalist Agents\n\n\nImad Dabbura\n\n\n\n\nFeb 21, 2025\n\n\nSWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering\n\n\nImad Dabbura\n\n\n\n\nAug 15, 2024\n\n\nReflexion: Language Agents with Verbal Reinforcement Learning\n\n\nImad Dabbura\n\n\n\n\nAug 10, 2024\n\n\nReAct: Synergizing Reasoning And Acting In Language Models\n\n\nImad Dabbura\n\n\n\n\nJul 30, 2024\n\n\nWhat Are Tools Anyway? A Survey from the Language Model Perspective\n\n\nImad Dabbura\n\n\n\n\nJul 20, 2024\n\n\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n\n\nImad Dabbura\n\n\n\n\nJul 10, 2024\n\n\nTree of Thoughts: Deliberate Problem Solving with Large Language Models\n\n\nImad Dabbura\n\n\n\n\nJul 7, 2024\n\n\nSearching for Best Practices in Retrieval-Augmented Generation\n\n\nImad Dabbura\n\n\n\n\nJul 5, 2024\n\n\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models\n\n\nImad Dabbura\n\n\n\n\nJul 1, 2024\n\n\nChain-of-Thought Reasoning without Prompting\n\n\nImad Dabbura\n\n\n\n\nJun 10, 2024\n\n\nToolformer: Language Models Can Teach Themselves to Use Tools\n\n\nImad Dabbura\n\n\n\n\nMay 30, 2024\n\n\nJudging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n\n\nImad Dabbura\n\n\n\n\nMay 16, 2024\n\n\nStarCoder 2 and The Stack v2: The Next Generation\n\n\nImad Dabbura\n\n\n\n\nApr 29, 2024\n\n\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\n\n\nImad Dabbura\n\n\n\n\nApr 26, 2024\n\n\nDeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence\n\n\nImad Dabbura\n\n\n\n\nApr 25, 2024\n\n\nDense Passage Retrieval for Open-Domain Question Answering\n\n\nImad Dabbura\n\n\n\n\nApr 19, 2024\n\n\nInternet-augmented language models through few-shot prompting for open-domain question answering\n\n\nImad Dabbura\n\n\n\n\nApr 11, 2024\n\n\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\nImad Dabbura\n\n\n\n\nApr 9, 2024\n\n\nOLMo: Accelerating the Science of Language Models\n\n\nImad Dabbura\n\n\n\n\nApr 1, 2024\n\n\nREALM: Retrieval-Augmented Language Model Pre-Training\n\n\nImad Dabbura\n\n\n\n\nMar 21, 2024\n\n\nSelf-Instruct: Aligning Language Models with Self-Generated Instructions\n\n\nImad Dabbura\n\n\n\n\nMar 17, 2024\n\n\nMixtral of Experts\n\n\nImad Dabbura\n\n\n\n\nMar 10, 2024\n\n\nMistral-7B\n\n\nImad Dabbura\n\n\n\n\nFeb 27, 2024\n\n\nCode Llama: Open Foundation Models for Code\n\n\nImad Dabbura\n\n\n\n\nFeb 20, 2024\n\n\nEfficient Training of Language Models to Fill in the Middle\n\n\nImad Dabbura\n\n\n\n\nFeb 15, 2024\n\n\nChinchilla: Training Compute-Optimal Large Language Models\n\n\nImad Dabbura\n\n\n\n\nJan 5, 2024\n\n\nScaling Laws for Neural Language Models\n\n\nImad Dabbura\n\n\n\n\nDec 9, 2023\n\n\nPythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n\n\nImad Dabbura\n\n\n\n\nNov 16, 2023\n\n\nT5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n\n\nImad Dabbura\n\n\n\n\nOct 5, 2023\n\n\nLlama 2: Open Foundation and Fine-Tuned Chat Models\n\n\nImad Dabbura\n\n\n\n\nSep 7, 2023\n\n\nLLaMA: Open and Efficient Foundation Language Models\n\n\nImad Dabbura\n\n\n\n\nJun 23, 2022\n\n\nInstructGPT: Training language models to follow instructions with human feedback\n\n\nImad Dabbura\n\n\n\n\nJun 11, 2022\n\n\nLoRA: Low-Rank Adaptation of Large Language Models\n\n\nImad Dabbura\n\n\n\n\nFeb 3, 2022\n\n\nGPT3: Language Models are Few-Shot Learners\n\n\nImad Dabbura\n\n\n\n\nNov 10, 2021\n\n\nRoBERTa: A Robustly Optimized BERT Pretraining Approach\n\n\nImad Dabbura\n\n\n\n\nOct 21, 2021\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n\nImad Dabbura\n\n\n\n\nOct 17, 2021\n\n\nPrefix-Tuning: Optimizing Continuous Prompts for Generation\n\n\nImad Dabbura\n\n\n\n\nApr 11, 2021\n\n\nGPT2: Language Models are Unsupervised Multitask Learners\n\n\nImad Dabbura\n\n\n\n\nJan 16, 2021\n\n\nGPT: Improving Language Understanding by Generative Pre-Training\n\n\nImad Dabbura\n\n\n\n\nAug 7, 2020\n\n\nParameter-Efficient Transfer Learning for NLP\n\n\nImad Dabbura\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dl-tips-tricks.html",
    "href": "dl-tips-tricks.html",
    "title": "Deep Learning Tips & Tricks",
    "section": "",
    "text": "Check if your model can overfit 1 example then few examples. The model should easily be able to overfit few examples. If not, there would be something wrong with the optimization or other parts of the training code.\nCheck the worst mistakes that the model make. In the case of binary classification, check the positive examples that were mispredicted with probabilities close to 0 or negative example that were mispredicted with probabilities close to 1. Also the check the examples that model is least confident about such as examples with probabilities close to 0.5. This would highlight issues with data preprocessing and labeling mistakes.\nLook at the output of all transformations before you plug them into the pipeline because you may loose a lot of the characteristics of the input in some transformations.\n32x32 Images have very different characteristics than large images. Once you get below 96x96 they behave differently. This means what works well; for example on CIFAR 10, most likely will not work well on Imagenet. 128 x 128 pixel images do generalize well to bigger images, and conclusions made on such images hold well to large images and much faster to train.\nA big part of a getting good at using deep learning in your domain is knowing how to create small, workable, useful datasets… Try to come up with a toy problem or two that will give you insight into your full problem.\nAugmentation can be done on all kind of data as long as the label would almost not change after the augmentations. Therefore, we need to make sure that the output of augmented data first makes sense and second doesn’t change the label.\nMonitor histogram of activations and gradients at each layer. This would shed some light on whether some activations are saturated and their gradients may be close to 0 and not learning anything.\nMonitor the magnitude of the parameter updates relative to the parameters. Ideally, we want the ratio to be close to 1%, not \\(0.001%\\) (no learning) or \\(50%\\) (changes super fast and may overshoot).\nGradient accumulation: to avoid out-of-memory errors (OOM) while training on GPUs, break larger batches into smaller batches and accumulate gradients by back-propagating after every batch to gather gradients before running optimization step. It should give identical results as if we train with larger batches unless we have layers that kind of depending on the batch size in the forward pass such as BatchNorm.\n\nFor example, in pytorch, instead of calling optimizer.step() for every batch, we call it every few batches.\n\nWhen using mixed precision, it is better to increase batch size to utilize the GPU since every batch will occupy much smaller memory. As a result, there will be less updates for each epoch because the model will see less number of batches in every epoch. This means that 1) we need to increase the number of epochs and 2) increase the learning rate.\nMixup: linear combination of 2 random examples from training dataset using lambda parameter that is drawn from Beta distribution (α, α). The output vector will also be linear combination of two examples labels. This will force model to be more robust and learn linear combination of examples instead of memorizing them. As a result, model becomes less sensitive to corrupted labels and noise in the training data. This method can also be applied to tabular data. However, because it is harder for the model to learn to differentiate between the two examples and the weight of each, we need to train for much longer to get good results."
  },
  {
    "objectID": "dl-tips-tricks.html#general",
    "href": "dl-tips-tricks.html#general",
    "title": "Deep Learning Tips & Tricks",
    "section": "",
    "text": "Check if your model can overfit 1 example then few examples. The model should easily be able to overfit few examples. If not, there would be something wrong with the optimization or other parts of the training code.\nCheck the worst mistakes that the model make. In the case of binary classification, check the positive examples that were mispredicted with probabilities close to 0 or negative example that were mispredicted with probabilities close to 1. Also the check the examples that model is least confident about such as examples with probabilities close to 0.5. This would highlight issues with data preprocessing and labeling mistakes.\nLook at the output of all transformations before you plug them into the pipeline because you may loose a lot of the characteristics of the input in some transformations.\n32x32 Images have very different characteristics than large images. Once you get below 96x96 they behave differently. This means what works well; for example on CIFAR 10, most likely will not work well on Imagenet. 128 x 128 pixel images do generalize well to bigger images, and conclusions made on such images hold well to large images and much faster to train.\nA big part of a getting good at using deep learning in your domain is knowing how to create small, workable, useful datasets… Try to come up with a toy problem or two that will give you insight into your full problem.\nAugmentation can be done on all kind of data as long as the label would almost not change after the augmentations. Therefore, we need to make sure that the output of augmented data first makes sense and second doesn’t change the label.\nMonitor histogram of activations and gradients at each layer. This would shed some light on whether some activations are saturated and their gradients may be close to 0 and not learning anything.\nMonitor the magnitude of the parameter updates relative to the parameters. Ideally, we want the ratio to be close to 1%, not \\(0.001%\\) (no learning) or \\(50%\\) (changes super fast and may overshoot).\nGradient accumulation: to avoid out-of-memory errors (OOM) while training on GPUs, break larger batches into smaller batches and accumulate gradients by back-propagating after every batch to gather gradients before running optimization step. It should give identical results as if we train with larger batches unless we have layers that kind of depending on the batch size in the forward pass such as BatchNorm.\n\nFor example, in pytorch, instead of calling optimizer.step() for every batch, we call it every few batches.\n\nWhen using mixed precision, it is better to increase batch size to utilize the GPU since every batch will occupy much smaller memory. As a result, there will be less updates for each epoch because the model will see less number of batches in every epoch. This means that 1) we need to increase the number of epochs and 2) increase the learning rate.\nMixup: linear combination of 2 random examples from training dataset using lambda parameter that is drawn from Beta distribution (α, α). The output vector will also be linear combination of two examples labels. This will force model to be more robust and learn linear combination of examples instead of memorizing them. As a result, model becomes less sensitive to corrupted labels and noise in the training data. This method can also be applied to tabular data. However, because it is harder for the model to learn to differentiate between the two examples and the weight of each, we need to train for much longer to get good results."
  },
  {
    "objectID": "dl-tips-tricks.html#nlp",
    "href": "dl-tips-tricks.html#nlp",
    "title": "Deep Learning Tips & Tricks",
    "section": "NLP",
    "text": "NLP\n\nWith LLM, generally the more compute the better the results. We can define compute, roughly, as the number of parameters x number of tokens. Therefore, we can make the model bigger and keep the number of tokens fixed OR keep the model size the same and increase the number of tokens which means that we have to train for longer. There is a trade-off that depends on the task.\nImproving logloss for LLMs is correlated with improved performance on downstream tasks\nEven though loss scales smoothly with compute, individual downstream tasks may scale in an emergent fashion. This means some tasks’ loss may be flat, others maybe inversely scaled, etc."
  },
  {
    "objectID": "dl-tips-tricks.html#tabular",
    "href": "dl-tips-tricks.html#tabular",
    "title": "Deep Learning Tips & Tricks",
    "section": "Tabular",
    "text": "Tabular\n\nRule of thumb to pick embedding size for categorical feature (fastai): \\[min(600, round(1.6 * {number\\_categories}^{0.56}))\\]"
  },
  {
    "objectID": "misc-notes/favorite-essays.html",
    "href": "misc-notes/favorite-essays.html",
    "title": "Favorite Essays",
    "section": "",
    "text": "Below are some of the best essays of life that I’ve collected over the years that reasonate with me. I re-read at least one of them weekly."
  },
  {
    "objectID": "misc-notes/useful-notes.html",
    "href": "misc-notes/useful-notes.html",
    "title": "Notes From Random Readings in Personal Growth",
    "section": "",
    "text": "Do not end the week with nothing:\n\nPrefer Working On Things You Can Show. If you can not show your work, then work on side projects.\nPrefer To Work Where People Can See you.\nPrefer To Work On Things You can Keep. This means that sometimes getting your code/project into Github and letting people use it may not be the best strategy. The reason is that nobody will attribute your code that was used in bigger projects that are built by different people. However, when you build a package/website around the project, people have no choice but to give you credit and you will start getting noticed in the industry.\nConsumption Is Sometimes Valuable, But Creation Moves You Forward. JUST DO NOT END THE WEEK WITH NOTHING!\n\nI am in charge of my life, so I better lead it and live it the way I want.\nBegin with the end in mind.\nUse time blocking for scheduling and not the bullet to-do list. This will help us put regular tasks on autopilot scheduling where we just perform the task without thinking about whether we should do it now or later. The non-regular tasks will be scheduled.\nFirst things first. Those are tasks that help us achieve our goals and get where we want to be. Small things such as non-regular tasks can be done in the gaps between the important tasks.\nCommunicate the value you create so people can know what you did.\nDevelop content that can be consumed for multiple purposes. For example, if at work we need to research some ideas, write a blog about them and then refer people to it from the company.\nCreate the content on your domain (website) so you can own it and get recognized. This will also help build my brand.\nBe consistent in developing useful content. This will make people take you more seriously and want to read what you produce.\nIncrease the impact of your value. I can write a blog post or record a video about problems we’re having at work and publish that online. Then I can refer co-workers to that when they have questions. I can also create libraries and open source them that are mainly driven by my job. This way, I will be reaping the benefits of sharing knowledge online, solving my problems at work, and eventually becoming more recognized. All of this can happen while I am on the clock doing my job.\nHave an investment mindset, not an hourly mindset: This means investing in skills that lead to return in the long term and keep growing, DO NOT prefer the fixed return that will pay you the same amount of money in exchange for time. Therefore, prioritize your work and learning to the ones that have a long-term effect even if it may not have a great return in the beginning.\nWealth is about having options, not about having money. With money, we can buy time by paying people to do stuff for us that is not worth our time such as mowing our lawn.\nThree practices to become an outlier:\n\nCommand your time: Build a system that consists of useful habits:\n\nWe have different burners: Family, Religion, Job, Self Development, Health, Recreation, and Civic. So optimize for the ones that add the most value given your current situation. Just keep in mind that it may affect other burners and you should not stress about it because you only have 24 hours a day.\nDo multiple things at the same time that doesn’t require much focus such as a family call or listening to a podcast while working out or driving.\nTry changing meeting schedules so they don’t take the most productive time of day plus put them together so you don’t have much of a context switch because we need a big chunk of time to do development work and have deep work sessions.\n\nHack your image: Perception is reality. It is very important how people perceive you. It is also very important to let people know the value you created and communicate that eloquently.\n\nDeveloper Image: You can strive to have three of them but u can pick only two.\n\nEasy to work with\nDeliver on time\nDo great work\n\n\nOwn your trajectory\n\nThink as if money is not an object.\nThe key to SUCCESS is to learn to teach yourself more efficiently than any institution can.\nScale yourself.\nYour Luck Surface Area is directly proportional to the degree to which you do something you’re passionate about combined with the total number of people to whom this is effectively communicated. Therefore, Luck = Doing * Telling.\n\n\nIF YOU WANT TO BE AN ANOMALY, YOU’VE GOT TO ACT LIKE ONE.\nStay on your lane and DO NOT start dabbling into different technologies/things that may be cool but do not help you achieve your mission. Avoid the tendency of trying everything new and/or looking cool if it does not fit into your plan.\nCaring cycle: Caring -&gt; Dedication -&gt; Mastery -&gt; Trust -&gt; Autonomy.\nFocus on JUST-IN-TIME learning, not JUST-IN-CASE.\nSystems are better than goals:\n\nGoals have an endpoint. Systems don’t.\nGoals require willpower. Systems don ’t.\nGoals are all or nothing. Systems aren’t.\nGoals limit upside. Systems don’t.\n\nWant SUCCESS?\n\nDefine it\nFigure out the price\nPay it\n\n“Your outcomes are a lagging measure of your habits. Your net worth is a lagging measure of your financial habits. Your weight is a lagging measure of your eating habits. Your knowledge is a lagging measure of your learning habits. Your clutter is a lagging measure of your cleaning habits. You get what you repeat.”\nIn brief, sleep produces complex neurochemical baths that improve our brains in various ways. And it “restocks the armory of our immune system, helping fight malignancy, preventing infection, and warding off all manner of sickness.” In other words, sleep greatly enhances our evolutionary fitness—just in ways we can’t see.\nThink of time as money. When deciding between things, think in terms of how much is your hourly rate worth and this will give an idea if the activity will be worth it or not.\nYou have to act as a Senior before you become a Senior. This means you have to have enough required skills and accomplishments and market yourself as meeting those requirements. Main qualities of a Senior:\n\nCommunication\nSeeing the big picture\nImpact on team’s work\nMentoring\nWorking across teams\nSolid technical expertise (especially fundamentals)\n\nMost of the knowledge is tacit knowledge; knowledge that is gained by experience and not written down and can’t be acquired by reading. Therefore, write down all the learnings (such as TIL or blog posts) so they can stick more and be shareable with others which also helps market yourself online.\nDreyfus Model of Skill Acquisition: Novice -&gt; Advanced beginner -&gt; Competent -&gt; Proficient -&gt; Expert.\nIt is very helpful to write down:\n\nThings that I like working on\nWho I like to work with\nWhat problems excite me and get me out of my bed\n\nIt is recommended to position myself 6-12 months before the big move after deciding what I want.\nSpend time learning to be a good storyteller because most senior jobs ask about previous scenarios and want to see your way of thinking and handling complex problems and what value did u add to your team.\nPracticing also allows you to recall from the past when tackling new problems instead of thinking about how to solve every problem because most likely you failed before and learned a few tips along the way that become muscle memory that you can use in the future.\nProductive meditation: Needs to let your brain deep dive into one specific problem. Every time you notice that your brain starts thinking about something else, bring it back to the same problem you want to think about.\nDo not focus on specific jobs when planning your career. Instead, focus on job attributes that make sense and you like and acquire career capital that would let you be in those jobs.\nDon’t do invisible work:\n\nAs time goes by, you will remember less of what you did. Your boss will even remember less than what you remember for the same time period.\nSince performance reviews and bonuses are based on what your boss and others remember of what you did:\n\nYou need to track all the things you do on a weekly basis\nUse a tool that store your work\nTell people about it\n\nTools:\n\nBrag docs: polished accomplishments that can be shared with your manager about all the progress/work you did. Juila Evans template\nSimple text file which has one liner for all activities\nWrite blog posts about all important work\nWritten is better than verbal, and formal is better than informal\n\n\nIt is not what you do, it is what you do consistently that determines your destiny\nknowledge and productivity are like compound interest\nIf what you are doing is not important, and if you don’t think it is going to lead to something important, why are you working on it!!!\n\nWhat are the important problems in my field?\nWhat important problems are you working on?\n\nWhere is my field going? Where were the opportunities? What are the important things to do?\nImplementation Intention: Research has found that motivation is not enough to make people stick to their new habits because motivation is short-lived. Planning out your new behavior doubles your chance of success and this can simply be done by filling in this sentence: “I want to perform [action] at [Day/Time] in [Location]. For example, I want to workout at 4 pm in my backyard every Tuesday and Thursday. Another form of of implementation intention is to stack new habit with old strong habit that became part of of your life. For example, before going to bed I will pray two rak3at keyam.\nAll you need to make you standout from the competition is the ability to show up everyday, stick to the schedule, do the work even if you don’t like it. This is almost all you need :)\nWhat pain do you want in your life? What are you willing to struggle for?\n\n#career-advice #personal-growth"
  },
  {
    "objectID": "til/sql/external-vs-managed-tables.html",
    "href": "til/sql/external-vs-managed-tables.html",
    "title": "Managed vs External Tables",
    "section": "",
    "text": "In big data frameworks such as Hive or Spark, you always hear that this table is a managed table and that is an external table. What is actually the difference:\n\nManaged table is a table that the framework manages both the metadata about the table and its data directories. So when we drop the managed table, both the metadata and the actual data directories will be deleted.\nExternal table, on the other hand, is a table where the framework maintains metadata about it w/o actually managing the data itself. Therefore, when we drop it, only the metadata is deleted but the data directories stay intact."
  },
  {
    "objectID": "til/sql/outer-join-and-where-clause.html",
    "href": "til/sql/outer-join-and-where-clause.html",
    "title": "Outer Joins & Where Clause",
    "section": "",
    "text": "With an outer join such as LEFT OUTER JOIN, we typically want all the records from the left table and records from right table that satisfy the join condition. However, if we include any field from the right table in the where clause, the join becomes an INNER JOIN for the following reason:\n\nRecords from the left table that don’t find a match in the right table will have NULL values in the fields from the right table\nComparing NULL to any value results in NULL\nBecause WHERE only returns records that evaluate to true and discard records that evaluates to either false or NULL\nTherefore, records with NULL values will be excluded. This means only the join becomes INNER JOIN\n\nThe above works the same for any form of OUTER JOIN.\nTo work around this, add the predicate to join statement after ON clause."
  },
  {
    "objectID": "til/sql/rank-denserank-row.html",
    "href": "til/sql/rank-denserank-row.html",
    "title": "RANK vs DENSE_RANK vs ROW",
    "section": "",
    "text": "SELECT ROW() OVER(PARTITION BY attrib_1 ORDER BY attrib_2)\nROW(), RANK(), and DENSE_RANK() all share the same functionality, which is assigning numbers to tuples inside a partition ordered by some attribute(s). However, they are different in terms of assigning those numbers in the case of ties:\n\nROW() assigns numbers incrementally to tuples with the same value(s) in a non-deterministic way. In others words, same tuple that share the same value(s) with other tuples can have different numbers in different executions in a non-deterministic way.\nRANK() assigns same number to tuples with the same value(s). Also, there will be a gap between the number assigned to tuples with same value(s) and the next tuple.\nDENSE_RANK() is similar to RANK() but doesn’t create gaps."
  },
  {
    "objectID": "til/c/data-type-conversion.html",
    "href": "til/c/data-type-conversion.html",
    "title": "Notes on Data Type Conversion",
    "section": "",
    "text": "When casting between signed and unsigned integers of the same data size, the bit representation doesn’t change but the interpretation of the bits change.\nPositive integers have the same bit representation regardless if they are signed or unsigned, but of course the interpretation is different.\nFor arithmetic operation:\n\nIf one of the operands is long double, then the other is converted to long double\nElse if one of the operands is double, then the other is converted to double\nElse if one of the operands is float, then the other is converted to float\nElse if one of the operands is long int, then the other is converted to long int\nElse if one of the operands is int, then the other is converted to int"
  },
  {
    "objectID": "til/c/lexical-scope.html",
    "href": "til/c/lexical-scope.html",
    "title": "Lexical Scope in C",
    "section": "",
    "text": "The lexical scope of an object or function identifier in an external declaration begins at the end of its declarator and persists to the end of the translation unit in which it appears. The scope of a parameter of a function definition begins at the start of theblock defining the function, and persists through the function; the scope of a parameter in a function declaration ends at the end of the declarator. The scope of an identifier declared at the head of a block begins at the end of its declarator, and persists to the end of the block. The scope of a label is the whole of the function in which it appears. The scope of a structure, union, or enumeration tag, or an enumeration constant, beginsat its appearance in a type specifier, and persists to the end of the translation unit (for declarations at the external level) or to the end of the block (for declarations within a function). If an identifier is explicitly declared at the head of a block, including the block constituting a function, any declaration of the identifier outside the block is suspended until the end of the block."
  },
  {
    "objectID": "til/c/signed-op-unsigned.html",
    "href": "til/c/signed-op-unsigned.html",
    "title": "Unintuitive Behavior on Operations with Signed & Unsigned Operands",
    "section": "",
    "text": "When an operation involves signed operand and unsigned operand, C implicitly casts the signed operand to unsigned and perform the operation. This would cause a lot of issues especially with relational operations. Below are some examples that yield unexpected results:\n-1 &lt; 0U                         // The answer is 0 because (unsigned int) -1 yield 4294967295\n2147483647U &gt; -2147483647 - 1  //  The answer is 0 because TMin becomes TMax + 1 when converting to unsigned which is 2147483648"
  },
  {
    "objectID": "til/c/shift-with-more-than-w-bits.html",
    "href": "til/c/shift-with-more-than-w-bits.html",
    "title": "Shift by K >= w",
    "section": "",
    "text": "If we shift an w-bit integer with k, most machines perform k mod w bit shift. For example, for 32-bit integers:\nint i = 1024;\ni &gt;&gt; 32     // Gives 1024 because 32 mod 32 = 0 -&gt; i &gt;&gt; 0 = i\ni &gt;&gt; 36     // Gives 64 bece 36 mode 32 = 4 -&gt; i &gt;&gt; 4 = 64"
  },
  {
    "objectID": "til/pytorch/list-gpus.html",
    "href": "til/pytorch/list-gpus.html",
    "title": "List Available GPUs",
    "section": "",
    "text": "In Pytorch, you can list number of available GPUs using torch.cuda.device_count().\n\nIf you specify cpu as a device such as torch.device(\"cpu\"), this means all available CPUs/cores and memory will be used in the computation of tensors.\nIf you specify cuda as a device such as torch.device(\"cuda\"), it is the same as torch.device(\"cuda:0\") which is the first GPU and memory.\nOtherwise, specify which GPU you want using torch.device(f\"cuda:{i}\") for the ith device."
  },
  {
    "objectID": "til/pytorch/save-memory.html",
    "href": "til/pytorch/save-memory.html",
    "title": "Save Memory When Operating on Tensors",
    "section": "",
    "text": "To avoid allocating new memory when operating on tensors, we can either: - Do inplace operations\nx = torch.randn(1000, 1000)\nbefore = id(x)\nx.add_(100)\nid(x) == before #=&gt; True\n\nOr change the current object that the tensor is pointing to\n\nx = torch.randn(1000, 1000)\nbefore = id(x)\nx += 100\nid(x) == before #=&gt; True\n\nOr assign the result of the operation to the previously allocated tensor\n\nx = torch.randn(1000, 1000)\nbefore = id(x)\nx[:] = x + 100\nid(x) == before #=&gt; True\nOtherwise, if we do the following, it will create new object and make x point to it. This will put pressure on the host memory.\nx = torch.randn(1000, 1000)\nbefore = id(x)\nx = x + 100\nid(x) == before #=&gt; False"
  },
  {
    "objectID": "til/tmux/notes.html",
    "href": "til/tmux/notes.html",
    "title": "Useful Tmux Notes",
    "section": "",
    "text": "tmux new -s session_name -d To create a new session w/o attaching it.\ntmux new -s session_name -n window_name To create a new session and name the first window.\nsend-keys -t session_name:window_index.pane_index 'cmd' C-m\n\nThis command allows us to execute a command in tmux session_name, in pand_index under window_index.\nC-m is carriage return ENTER\nFor example, send-keys -t dev:1.2 'cd ~/Documents' C-m would change the directory to ~/Documents in dev session in pane index 2 under window index 1"
  },
  {
    "objectID": "til/unix/difference-between-effective-real-id.html",
    "href": "til/unix/difference-between-effective-real-id.html",
    "title": "Difference Between Effective & Real UID/GID",
    "section": "",
    "text": "Real UID/GID are attributes of the user that are set at login by login shell. They typyically stay the same.\nEffective UID/GID are attributes of a process that are used when checking access permissions for operations on files or commands.\nReal and effective are typyically the same, but sometimes they change when the executable file have it’s set-user-ID bit on. This happens, for example, when a program needs root or other higher level permissions to perform some operations on behalf of the user such as cron program."
  },
  {
    "objectID": "til/unix/temp-files.html",
    "href": "til/unix/temp-files.html",
    "title": "Temporary Files & Directories",
    "section": "",
    "text": "Sometimes you may want to create temporary file or directories for the life of the process and then you want them deleted once the process terminates or the file is closed. This is very useful to store some metadata or temporary data but you don’t want the data to persist in the filesystem. We can achieve that by creating temporary file which is available in almost all languages such as Python and C. The way temporary file works is as follows:\n\nCreates a file with random name\nMake the file mode “wb+” which means create it for reading/writing and truncate if it exists\nUnlink the file right after creation. This will guarantee that the file will be deleted after the process terminates\nReturn the file object to the caller\n\nOnce we close the file or all the process(es) that has a reference to the file terminates, the file will be removed. The same thing applies to temporary directories.\nExample in Python:\nimport tempfile\n\n# Create temp file \nfp = tempfile.TemporaryFile()\n\n# Write to the file\nfp.write(\"First write!\")\n\n# Read from the beginning\nfp.seek(0)\nfp.read()   #=&gt; \"First write!\"\n\n# Now the file will be closed and removed\nfp.close()"
  },
  {
    "objectID": "til/shell/exclude-command-from-shell-history.html",
    "href": "til/shell/exclude-command-from-shell-history.html",
    "title": "Exclude Shell Command From History",
    "section": "",
    "text": "We can exclude any shell’s command from history by prepending whitepspace ’ ’ before the command. This is useful if we are inserting password or any sensitive commands. In the case that we forgot to do that, we can delete the command from shell’s history file such as .bash_history or .zsh_history."
  },
  {
    "objectID": "til/shell/ssh-config.html",
    "href": "til/shell/ssh-config.html",
    "title": "ssh Configuration",
    "section": "",
    "text": "We can use config file under ~/.ssh/ to host ssh login information for different servers to avoid typing them every time we want to ssh into such servers. Below are few examples of such configs:\nHost vm\n    User foobar\n    HostName 172.16.174.141\n    Port 2222\n    IdentityFile ~/.ssh/id_ed25519\n    RemoteForward 9999 localhost:8888\n\n# Configs can also take wildcards\nHost *.company.com\n    User foobaz"
  },
  {
    "objectID": "til/shell/pipeline-error.html",
    "href": "til/shell/pipeline-error.html",
    "title": "Status Code Returned by Shell’s Pipeline",
    "section": "",
    "text": "By default, shell reports the return status of the last command in the pipeline. Therefore, if any command in the pipeline before the last one failed, the return status code would be 0 which indicates success. To get the return status for all the commands in the pipeline, use echo $pipestatus which returns all the returned codes from all the commands in the most recent pipeline."
  },
  {
    "objectID": "til/shell/null-device.html",
    "href": "til/shell/null-device.html",
    "title": "Useful Tricks with Null Device",
    "section": "",
    "text": "/dev/null is the null device. It is very useful when we want to discard output of any operation or we want truncate a file. This is due to the fact reading/writing to /dev/null has no effect. In the case of reading, it generates end of file right away.\nTo discard stderr, which has file descriptor 2, we can use the following redirection:\nls -l dir 2&gt;/dev/null\nTo truncate a file, instead of rm and then touch, we can use:\ncat /dev/null &gt; file-to-truncate"
  },
  {
    "objectID": "til/python/mutability-and-inplace-operations.html",
    "href": "til/python/mutability-and-inplace-operations.html",
    "title": "Mutability and Inplace Operations",
    "section": "",
    "text": "For immutable objects, inplace operation such as “+=” creates a new object and assign the variable to the newly created object.\ns = \"test\"\nold_id = id(s)\ns += \" another test\"\nold_id == id(s) #=&gt; False\nBut for mutable objects, it doesn’t create a new object. It extends/updates the existing object.\nl = [1]\nold_id = id(l)\nl += [2]\nold_id == id(l) #=&gt; True"
  },
  {
    "objectID": "til/python/when-iterators-used-internally.html",
    "href": "til/python/when-iterators-used-internally.html",
    "title": "When Does iter() Called Internally",
    "section": "",
    "text": "Iterators in Python are objects that implement both __iter__ and __next__ special methods. iter() is called internally to support:\n\nfor loop.\nTuple unpacking, example such as x, y = IterableObject\nUnpacking actual function parameters in function calls such as *args and **kwargs\nlist, dict, & set comprehensions such as [e for e in L]\nLooping over text files line by line such as for line in file: print(line)\nCollection types construction such as list(obj) or tuple(obj)"
  },
  {
    "objectID": "til/python/string-comparison.html",
    "href": "til/python/string-comparison.html",
    "title": "String Comparison",
    "section": "",
    "text": "Strings in Python are compared using lexicographical order of their character code. For ASCII characters, uppercase letters are considered smaller than lowercase letters.\nprint('A' &lt; 'a')  # True"
  },
  {
    "objectID": "til/python/reuse-fixtures.html",
    "href": "til/python/reuse-fixtures.html",
    "title": "Reusing Fixtues By All Tests",
    "section": "",
    "text": "There are some fixtures that we want to use them with every test function/method such as fixtures that report the execution time of each test. So instead of keep using the name of these fixtures in all tests, we can set autouse to True when defining the fixture so it will be called always by all tests in its scope.\n@pytest.fixture(autouse=True)\ndef test_time():\n    start = time.time()\n    yield\n    end = time.time()\n    print(f\"test duration: {end - start:0.3} seconds\")"
  },
  {
    "objectID": "til/python/differences-and-similarities-bw-threads-tasks.html",
    "href": "til/python/differences-and-similarities-bw-threads-tasks.html",
    "title": "Similarities and Differences between Threads and Coroutine Tasks",
    "section": "",
    "text": "Task is very similar to Thread\nTask drives a coroutine object while Thread invokes a callabel\nCoroutine yield control explicitly to other tasks/event loop (cooperative multitasking while Thread is scheduled by the OS (preemptive multitasking)\nTask gets scheduled automatically after creation while Thread must be started explicitly\nTask can be cancelled (task.cancel, which would generate CancelledError) but thread can’t be terminated from outside except using signals"
  },
  {
    "objectID": "til/python/mutable-default-function-arguments.html",
    "href": "til/python/mutable-default-function-arguments.html",
    "title": "Mutable Default Function Arguments",
    "section": "",
    "text": "Using None for default argument values is especially important when the arguments are mutable. If for example we used an empty list as the default value, it would be shared by all the calls to the function.\nThis happens because a function gets evaluated once per module load after the program starts up. As a result, default argument value is evaluated only once. This can cause odd behaviors for dynamic values (such as {}, [], or datetime.now()). Therefore:\n\nUse None as the default value for any keyword argument that has a dynamic value. Document the actual default behavior in the function’s docstring.\nUsing None to represent keyword argument default values also works correctly with type annotations.\n\ndef func(y=[]):\n    y.append(\"test\")\n    print(id(y), y)\n\n# The following two call will use the same object for the default arg\nfunc() #=&gt; 140511381181056 ['test']\nfunc() #=&gt; 140511381181056 ['test', 'test']\n\n# Notice how the argument is a different object\nfunc([]) #=&gt; 140511380870592 ['another', 'test']"
  },
  {
    "objectID": "til/python/logging.html",
    "href": "til/python/logging.html",
    "title": "Python Logging",
    "section": "",
    "text": "Figure 1: Logging Flow (Source)\n\n\n\nlogging.getLogger(name) returns an instance of the named logger. If it already exists, return the existing instance. It no name is provided, returns the root logger.\nEach logger have:\n\nLevel (severity)\nZero or more Handlers\nFormatter\nFilter\n\nThe loggers have hierarchy with root being the top level logger.\nIf configuration is not set for a logger, it keeps going up the hierarchy to its parents until it finds it. If nothing is found, use the root logger config since it is the parent of all loggers.\nWe can configure loggers:\n\nDirectly in python scripts\nUsing config file and import it using logging.config.fileConfig(file_name)\nOR using dictionary in YAML file and import it using logging.config.dictConfig()\n\nTo share the same configuration of loggers across different modules, we only configure root logger in the entry module and all other loggers will use the root config since root is their parent."
  },
  {
    "objectID": "til/python/getitem-makes-object-iterable.html",
    "href": "til/python/getitem-makes-object-iterable.html",
    "title": "__getitem__ Makes Object an Iterator",
    "section": "",
    "text": "Any object that implement __getitem__ special method is iterable and you can use in operater to check if an item is in the object even if the object doesn’t impelement __contain__ special method. - Python generate indexes starts from zero and iterate over all items until the object returns Error -&gt; Stop Iteration - The main assumption is that the object takes integers as indices - To check if an item is in the object, Python iterates over all items (using the above method)\nclass L:\n    def __init__(self, length=10):\n        self.data = list(range(length))\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n    def __repr__(self):\n        res = f\"{self.__class__.__name__}: {self.data[:10]}\"\n        if len(self.data) &gt; 10:\n            res = res[:-1] + \", ...]\"\n        return res\nl = L()\nl[i for i in l] #=&gt; L: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n1 in l          #=&gt; True"
  },
  {
    "objectID": "til/python/class-evaluation-steps.html",
    "href": "til/python/class-evaluation-steps.html",
    "title": "Class Evaluation Steps",
    "section": "",
    "text": "Python follows roughly the following steps when evaluating a class definition:\n\nReads the body of the class from top to bottom\nCreate instances of the descriptors (if any) and bound them to the class attributes\nCalls type.__new__ to create the new object\nCalls __set_name__ on each descriptor (if any)\nCalls __init_subclass__ for all superclasses\nApplies class descorators (if any)\nBind object to namespace"
  },
  {
    "objectID": "til/python/line-continuation.html",
    "href": "til/python/line-continuation.html",
    "title": "Line Continuation",
    "section": "",
    "text": "We can use “\\” for line continuation as long as there is no space after it; otherwise, Python interpreter will treat it as a character.\nBut the more pythonistic way is to use with {}, [], or () where line breaks will be ignored. For example:\nstring = (\n    \"first \"\n    \"name\"\n    )\nprint(string) #=&gt; \"first name\""
  },
  {
    "objectID": "til/python/slicing.html",
    "href": "til/python/slicing.html",
    "title": "Sequence Slicing",
    "section": "",
    "text": "When we provide a slice to any sequence object (object that implements __getitem__ and __len__), we get back an instance of the same type with the elements in the slice. Here is how slicing works:\n\nWhen we have commas inside [], __getitem__ gets tuple of slice object.\nWhen we have : inside [], __getitem__ gets a slice object.\n\nslice object has 4 important attributes:\n\nstart\nstep\nstop\nindices"
  },
  {
    "objectID": "til/python/pseudo-random-number-generator.html",
    "href": "til/python/pseudo-random-number-generator.html",
    "title": "Pseudorandom Number Generator",
    "section": "",
    "text": "Pseudorandom number generator is a proxy for truly random number generator. It is not truly random because the numbers generated are determined by an initial value called seed. Therefore, the numbers generated are deterministic if we know the seed. The randomness comes from the value of the seed. In other words, if we set the seed to a fixed number such as seed = 123, then we can guarantee to generate the exact same sequence any time we set the seed to 123. For example:\n\nrandom.seed(123)\nr1 = random.random()\nrandom.seed(123)\nr2 = random.random()\nr1 == r2\n\nTrue\n\n\nThere is no raltionship between the pseudorandom numbers generated, as the plot below shows:\n\nplt.plot([random.random() for _ in range(50)]);\n\n\n\n\n\n\n\n\nAlso, the pseudorandom numbers generated follow standard uniform distribution.\n\nplt.hist([random.random() for _ in range(10000)]);\n\n\n\n\n\n\n\n\nSimple implementation to set the random_state using the seed as well as generating random numbers that follow the standard uniform distribution:\n\nrnd_state = None # starting value\n\ndef seed(a):\n    \"Set the random state\"\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x) + 1, int(y) + 1, int(z) + 1\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x, y, z\n    return (x / 30269 + y / 30307 + z / 30323) % 1.0\n\nBoth torch and numpy fail at creating different random numbers in both the parent and the child processes because both processes have the same random_state, which is used to generate the random number.. But Python works fine because it takes care of changing the random_state in the child process(es) as the code segments below illustrates:\n\nif os.fork():\n    print(f\"In parent: {torch.rand(1)}\")\nelse:\n    print(f\"In child: {torch.rand(1)}\")\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.3910])\nIn child: tensor([0.3910])\n\n\n\nif os.fork():\n    print(f\"In parent: {np.random.randn(1)}\")\nelse:\n    print(f\"In child: {np.random.randn(1)}\")\n    os._exit(os.EX_OK)\n\nIn parent: [-0.51009584]\nIn child: [-0.51009584]\n\n\n\nif os.fork():\n    print(f\"In parent: {random.random()}\")\nelse:\n    print(f\"In child: {random.random()}\")\n    os._exit(os.EX_OK)\n\nIn parent: 0.6560074137612377\nIn child: 0.3243934113820527\n\n\nAs a result, we need to be careful when using multiprocessing/threading with pytorch and numpy that requires generating different random numbers in different threads/processes."
  },
  {
    "objectID": "til/python/user-defined-classes-are-hashable.html",
    "href": "til/python/user-defined-classes-are-hashable.html",
    "title": "User Defined Classes are Hashable",
    "section": "",
    "text": "All user defined classes are hashable by defualt becuase they inherit __hash__ from object, which is related to their id(). This guarantees that different instances have different hashes since they have different id(). Therefore, hash(id(object)) == id(object) which is different for different objects.\nclass A:\n    ...\n\na1 = A()\na2 = A()\nid(a1) == hash(id(a1)) #=&gt; True\nid(a2) == hash(id(a2)) #=&gt; True\nIf a user-defined class implements __eq__, then it needs to implement __hash__ and has to guarantee that it is hashable if all its attributes are hashable and if a == b then hash(a) == hash(b)."
  },
  {
    "objectID": "til/python/loading-modules-and-packages.html",
    "href": "til/python/loading-modules-and-packages.html",
    "title": "Loading Modules and Packages",
    "section": "",
    "text": "The process of importing a module/package (after locating it):\n\nFirst checks if it is cached. If not, continue\nIt creates a ModuleType object with that name\nCache the module in sys.modules\nExecutes the source code inside the module (first prefixing it with .py and then assign __file__)\n\nIn the case of the package/subpackage, it assign it the __init__.py file\nIt also executes all the __init__.py on the path\n\nAssign a variable to the module object\n\nBelow is in roughly what it is done in Python code:\nimport sys, types\n\ndef import_module(modname):\n    # Check if it is in the cache first\n    if modname in sys.modules:\n        return sys.modules[modname]\n\n    sourcepath = modname + '.py'\n    with open(sourcepath, 'r') as f:\n        sourcecode = f.read()\n    mod = types.ModuleType(modname)\n    mod.__file__ = sourcepath\n\n    # Cache the module\n    sys.modules[modname] = mod\n\n    # Convert it to Python ByteCode\n    code = compile(sourcecode, sourcepath, 'exec')\n\n    # Execute the code in the module from top to bottom\n    # And update the state (globals) in the module's dictionary\n    exec(code, mod.__dict__)\n\n    # We return the cached one in case there is some patching inside the module\n    return sys.modules[modname]\nFinally, Python puts a lock when importing a module until it is done so that we don’t have multiple threads trying to import the same module at the same time.\nAs a result, if a module is imported in different modules in the sample application, it would be imported ONLY once. This is very useful for config files that sets up application’s configuration such as loggers. Importing the configuration module in many places would only lead to executing the module once."
  },
  {
    "objectID": "til/python/equality-vs-identity.html",
    "href": "til/python/equality-vs-identity.html",
    "title": "== vs is",
    "section": "",
    "text": "Every thing in Python is an object. Variables are nothing but pointers to those objects. Therefore, multiple variables can point to the same object. Therefore:\n\nTo test whether two variables point to the same object, we use is operator which compares the identity of both variables. If the return value is True -&gt; both variables point to the same object. This can also be tested in CPython using id().\nTo test if both variables (objects they point to) have the same content, we use == operator.\n\na = [2]\nb = a\na is b          #=&gt; True\nid(a) == id(b)  #=&gt; True\n\n\na = [2]\nb = [2]\na is b          #=&gt; False\nid(a) == id(b)  #=&gt; True\nIt is recommended to use is operator when checking if a variable is: True, False, or None. If we use ==, it will invoke __eq__ method, which if not implemented, compare the ids of both variables and is slower than is operator because it first has to look up __eq__ method and then invoke id() on both variables."
  },
  {
    "objectID": "til/python/cond-iterator.html",
    "href": "til/python/cond-iterator.html",
    "title": "Conditional Iterators",
    "section": "",
    "text": "We can pass a callable that doesn’t take arguments and generate values to iter() and pass marker value (element). It will return a callable iterator that keeps returning values until the returned value == element.\ndef gen():\n    return random.randint(1, 10)\n\ncond_iterator = iter(gen, 1)\n[e for e in cond_iterator] #=&gt; [6, 10, 2, 4, 3, 6, 7, 5, 5, 4, 7, 10, 8, 10, 10, 8]"
  },
  {
    "objectID": "til/python/modules-and-packages.html",
    "href": "til/python/modules-and-packages.html",
    "title": "Modules and Packages",
    "section": "",
    "text": "Modules are just objects of type ModuleType. They act like a dictionary that holds references to objects it holds; module.__dict__.\n\nWe can set/delete attributes. module.x = 10 is the same as module.__dict__['x'] = 10\n\nWhen importing a module, it executes the module from top to bottom before returning to the caller.\nModule can be namespace, py file, execution environment for statements or container of global variables.\nThe dictionary has preset attributes such as __path__, __loader__, etc. Main attributes:\n\n__name__ : Module name\n__file__ : Associated source file (if any)\n__doc__: Docstring\n__path__ : Package path. It is used to look for package subcomponents\n__package__ : The module’s __package__ attribute must be set. Its value must be a string, but it can be the same value as its __name__. When the module is a package, its __package__ value should be set to its __name__. When the module is not a package, __package__ should be set to the empty string for top-level modules, or for submodules, to the parent package’s name.\n__spec__ : Module spec\n\nThe module’s attributes are set after creation and before execution\nThe main difference between modules and packages is that packages have __path__ and __package__ defined (not None)\nsys.modules serves as a cache for all imported modules/packages\n\nIt is a dictionary so we can delete/set keys\nIf we delete a module, it will force Python to import it when we reimport it\nIf we set module key to None -&gt; result in ModuleNotFoundError\n\nEven if we import one object from a module/package, the module/package will be cached in the sys.modules but not available in the global name space\nThe module created during loading and passed to exec_module() may not be the one returned at the end of the import\n\nThis can happen if the imported module set the sys.modules[__name__] to some other module\n\nExecution of the module is what populates the module’s __dict__ (namespace of the module). This is done by the loader\nWhen a submodule is loaded using any mechanism, a binding is placed in the parent module’s namespace to the submodule object. For example, if we have a package called spam that has a submodule foo and it imports any of its objects like from .foo import x, after importing spam, spam will have an attribute foo which is bound to the submodule -&gt; We can now use spam.foo\nRelative imports use leading dots. A single leading dot indicates a relative import, starting with the current package. Two or more leading dots indicate a relative import to the parent(s) of the current package, one level per dot after the first.\n\nCan only use this form of import: from &lt;&gt; import &lt;&gt;\nIt can’t use import .&lt;&gt; because this is not a valid expression\n\nAbsolute imports have to start from the top level package and go downward to refer to the module: from package.subpackage import module\n\nNot recommended because if we change the name of the package then we need to change all the import statements -&gt; relative imports are more robust and don’t care about namings"
  },
  {
    "objectID": "til/python/interactive-code.html",
    "href": "til/python/interactive-code.html",
    "title": "Interactive Code Interpreter",
    "section": "",
    "text": "Python standard library has code library that allows us to run REPL in the middle of our script to experiment with the environment. You can run import code; code.interact(local()). It is very useful for debugging"
  },
  {
    "objectID": "til/python/wheel-files.html",
    "href": "til/python/wheel-files.html",
    "title": "Python Wheel Files",
    "section": "",
    "text": "A wheel file is a zip archive with a specially crafted name that tells installers what Python versions and platforms are supported\n\nIt is a ready to install format\nfilename is broken into parts: {dist}-{version}(-{build})?-{python}-{abi}-{platform}.whl\n\nPython Wheels made the installation faster and more efficient\n\nThey are smaller in size than source distributions -&gt; lower network latency\nAvoid building stage -&gt; faster\nAvoids executing setup.py code\nNo need for a compiler for extension modules\npip automatically generates .pyc files\n\nSource Distributions are the source code and extension modules (mostly written in C/C++) bundled together. Those extensions are compiled at the user side NOT developer side\n\nThey also contain metadata directory called &lt;package-name&gt;.egg_info. This metadata helps with the building and isntalling of the package\nIt is the result of python setup.py sdist command\n\nWheel files are sometimes provided by packages on PyPI\n\nEach OS has its own wheel file\nPip prefers wheels over source distributions when trying to install a package\n\nExample wheel filenames:\n\ncryptography-2.9.2-cp35-abi3-macosx_10_9_x86_64.whl\n\ncryptography is the name of the package\n2.9.2 is the package version which is formatted according to PEP 440 recommendation\ncp35 is CPython with Python version 3.5\nabi3 is version 3 of Application Binary Interface\nmacosx_10_9_x86_64:\n\nmacosx MacOs operating system\n10_9 is the version of MacOs\nx86_64 is the x86_64 instruction set architecture\n\n\nchardet-3.0.4-py2.py3-none-any.whl\n\nchardet is the package name\n3.0.4 is the package version\npy2.py3 works on any version of Python 2 & 3\nnone means ABI is not factor\nany works virtually on all platforms\n\n\nWe can view the contents of the wheel file using unzip\nWe can force pip to only install wheel files with --no-binary\n\n:all: would apply this not only to the package we are installing but to all its dependencies\n\nSince there are so many variants of Linux such as CentOS, Debian, etc., package developer may have to provide many wheels for different variations of Linux. This is especially true if the package has module extensions written in C/C++ and may potentially have issues due to compilation error. There are platform tag family:\n\nmanylinux1\nmanylinux2010\nmanylinux2014\n\nTypes of wheels:\n\nUniversal wheels: support both Python 2 & 3 on all platforms\nPure-Python wheels: support either Python 2 or Python 3 on all platforms, but not both\nPlatform wheels: support specific Python version and platform\n\nTo build s pure Python wheel:\n\npython setup.py sdist bdist_wheel. By default, it will be stored in dist directory\npython setup.py sdist -d dir_name bdist_wheel -d dir_name\n\nTo build universal wheel:\n\nUse .cfg file by adding:\n\n[bdist_wheel]\nuniversal = 1\n\nUse python setup.py sdist bdist_wheel --universal\nAdd this line to setup.py file: options={\"bdist_wheel\": {\"universal\": True}}\n\nCheck resources to build platform wheels or manylinux wheels\nWe can use CI pipelines such as GithubActions to test the package on multiple platforms\ncheck-wheel-contents is a tool that helps detecting any problems with the wheel file\nTestPyPI allows us to test the wheel file as if it’s a real thing:\n\nWe first upload it to TestPyPI:\n\npython -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n\nThen try to install it and see if the wheel file works correctly:\n\npython -m pip install --index-url https://test.pypi.org/simple/  &lt;pkg-name&gt;\nWe can use twine tool to upload the package to PyPI:\n\nUpdate twine: python -m pip install -U twine\nUpload package: python -m twine upload dist/* which uploads both source distribution sdist and wheel file bdist_wheel. By default they will be is dist directory"
  },
  {
    "objectID": "til/python/numpy-array-slice-assignment.html",
    "href": "til/python/numpy-array-slice-assignment.html",
    "title": "Assigning Array to Numpy Slice",
    "section": "",
    "text": "Even when assigning a separate array to a slice, NumPy does not create a persistent copy of the right-hand side array. The values are transferred directly to the original array’s memory:\noriginal = np.zeros(5)\nnew_data = np.array([1, 2, 3])\noriginal[1:4] = new_data  # Values copied directly into original's array memory\nnew_data[0] = 999  # Does NOT affect original array\nprint(original)  # [0, 1, 2, 3, 0]"
  },
  {
    "objectID": "til/python/object-destruction.html",
    "href": "til/python/object-destruction.html",
    "title": "Object Destruction",
    "section": "",
    "text": "Every Python object has a refcount integer field that counts how many objects/variables are pointing towards it within the program.\n\nrefcount gets incremented when new variable or object points to it\nrefcount gets decremented when either one the variables/objects that used to point to it gets reassigned to point to new object OR we call del   obj\n\nOnce the refcount reaches zero or the object becomes unreachable (determined by mark-sweep algorithm used by the garbage collector)\n\nThe interpreter will call __del__ special method for final cleanup before destorying the object.\nIts memory will be reclaimed by the interpreter through garbage collector"
  },
  {
    "objectID": "til/pytest/pytest-init-file.html",
    "href": "til/pytest/pytest-init-file.html",
    "title": "The Role of __init__.py in pytest",
    "section": "",
    "text": "The only role __init__.py file in pytest is that it allows for duplicate test file names in different subdirectories as long as __init__.py exist in those subdirectories. Otherwise, it would return an error."
  },
  {
    "objectID": "til/pytest/pytest-exception-test.html",
    "href": "til/pytest/pytest-exception-test.html",
    "title": "Testing Expected Exceptions",
    "section": "",
    "text": "Tests generally fails for 3 reasons:\n\nThe assetion test fails\npytest.fail() is called with the test\nAn exception is raised in the code that is being tested and not handled\n\nWe can use pytest.fail() to test for exceptions that our code is expected to raise in certain cases.\nWe can test if the code raises expected exception (here ValueError):\ndef test_raises_exc():\n    with pytest.fail(ValueError):\n        fn()\nWe can test if the code raises expected exception and exception message matches correct message:\n# One way using regular expressions to test the message using re.search over\nvalue of the exception\ndef test_raises_exc():\n    msg_regex = \"something .* final\"\n    with pytest.fail(ValueError, match=msg_regex):\n        fn()\n\n# Another way to test the exact message\ndef test_raises_exc():\n    with pytest.fail(ValueError) as exc:\n        fn()\n    expected = \"Some exception message\"\n    assert expected in str(exc.value)"
  },
  {
    "objectID": "til/pytest/selecting-tests-and-markers.html",
    "href": "til/pytest/selecting-tests-and-markers.html",
    "title": "Selecting Tests with -m",
    "section": "",
    "text": "We can use the the flag -m marker_name to run tests that matched the provided marker name(s). We can also use (), and, or, and not to select tests based on markers. However, the name of the markers must be in full and we can’t use substrings or part of the marker. This is different from the -k flag, which lets us use substrings or part of the test name to run those test."
  },
  {
    "objectID": "til/pytest/skip-test.html",
    "href": "til/pytest/skip-test.html",
    "title": "Difference Between pytest’s skip, skipif, & xfail Markers",
    "section": "",
    "text": "Sometimes we may want to skip some tests because either 1) the functionality they test is not supported yet Or 2) some versions of the library didn’t support it. We can use one of the following three pytest markers for this purpose:\n\npytest.mark.skip: Skip the test completely\npytest.mark.skipif(condition, ...): Skip the test if the condition is True\npytest.mark.xfail(..., strict=False): Run the test even though we know it would fail. The strict option determines if the passed test is classified as XPASSED if strict=False OR FAILED if strict=True. If the test failed, it would always return XFAIL regardless of the strict option."
  },
  {
    "objectID": "til/pytest/debugging-pytest-flags.html",
    "href": "til/pytest/debugging-pytest-flags.html",
    "title": "Debugging with pytest Flags",
    "section": "",
    "text": "pytest’s useful command-line flags for debugging (source).\n\n-lf: Runs just the tests that failed last\n-ff: Runs all the tests, starting with the last failed\n-x: Stops the tests session after the first failure\n--maxfail=num: Stops the tests after num failures\n-nf: Runs all the tests, ordered by file modification time\n--sw: Stops the tests at the first failure. Starts the tests at the last failure next time\n--sw-skip: Same as –sw, but skips the first failure\n-v: Displays all the test names, passing or failing\n--tb=[auto/long/short/line/native/no]: Controls the traceback style\n-l: Displays local variables alongside the stacktrace\n--pdb: Starts an interactive debugging session at the point of failure\n--trace: Starts the pdb source-code debugger immediately when running each test\n--pdbcls: Uses alternatives to pdb"
  },
  {
    "objectID": "til/pytest/pytest-fixtures-scope.html",
    "href": "til/pytest/pytest-fixtures-scope.html",
    "title": "Fixtures’ Scopes",
    "section": "",
    "text": "Scopes of fixtures:\n\nFunction (default): The setup/teardown runs before/after every run of the test function or fixture that depends on it\nClass: The setup/teardown runs before/after once for all the test methods inside a class\nModule: The setup/teardown runs before/after once for all the test functions/methods inside a module\nPackage: The setup/teardown runs before/after once for all the test functions/methods inside a package or test directory\nSession: The setup/teardown runs before/after once for all the test functions/methods per session\n\nA fixture can only depend on another fixture that is at the same scope level or wider. Also, for package/session scopes, the fixtures have to be defined inside conftest.py (typically in the root of test directory) that will be read by pytest before running tests in order to be actually at the package/session level; otherwise, they will default to the module."
  },
  {
    "objectID": "til/devops/gitops.html",
    "href": "til/devops/gitops.html",
    "title": "GitOps",
    "section": "",
    "text": "GitOps means that any change to a production system is done through pushes to a Git repository, which is then reflected in the production system through automation. For example, changes to a Kubernetes cluster are mostly done this way since it is percieved as read-only environment."
  },
  {
    "objectID": "til/devops/declarative-vs-imperative-configuration.html",
    "href": "til/devops/declarative-vs-imperative-configuration.html",
    "title": "Declarative vs Imperative Configuration",
    "section": "",
    "text": "Declarative configuration is when the user describes the desired state of the system. This means the state is well understood and doesn’t need to be executed to be understood.\nImperative Configuration is when the configuration states the steps needed to be executed. It wouldn’t usually be clear what would be the desired state. Also, it’s hard to rollback steps.\n\nMost systems, especially distributed systems such as Kubernetes, prefer declarative configuration because the current state of the system can always be compared with the declared/desired state."
  },
  {
    "objectID": "til/vim/abort-git-commit-and-rebase.html",
    "href": "til/vim/abort-git-commit-and-rebase.html",
    "title": "Aborting Git’s Commit & Rebase",
    "section": "",
    "text": "If vim is the editor used by git, git will open vim for things such as amending commits or interactive rebase. Therefore, the buffer will be open according to the git action we are performing. After you’re done, you save the changes and quit the buffer. Finally, git will take it from there and do the necessary actions.\nIf you want to quit the commit or the rebase w/o letting git perform any action, we can use :cq Ex command. This command will let you exit vim with error code sent to the process/application that called vim. This way, git will recieve the error and won’t proceed with the action.\nI typically use this often when I am in the middle of writing a commit message and realize that I need to check/change some things before committing."
  },
  {
    "objectID": "til/vim/executing-macros-parallel-vs-serios.html",
    "href": "til/vim/executing-macros-parallel-vs-serios.html",
    "title": "Parallel vs Series Execution of Macros",
    "section": "",
    "text": "Let’s assume that we recorded a macro to do some work on any given line and we want to repeat that macro n-times on contiguous lines. We have two ways of doing that:\n\nExecute the macro in series such as 5@a. This will queue 5 repititions of the macro in series starting at the current line and continues forward. The execution of the macro on the next line is dependent on the execution of the macro on the current line being successful. If it fails for any reason, the execution of the macro for the remaining lines will abort.\nExecute the macro in parallel such as '&lt;,&gt;':normal @a. This will queue up number of lines in the visual selection repititions of the macro in parallel. This means the execution of the macro on each line is independent of the execution on other lines. Therefore, even if one execution failed, the macro will still be applied to the remaining lines.\n\nWhich one to use depends on the use case. If we want the error to be obvious, then execution in series is better."
  },
  {
    "objectID": "cmn-ai.html",
    "href": "cmn-ai.html",
    "title": "Welcome to cmn_ai",
    "section": "",
    "text": "I am a big believer in Boyd’s Law, which states that speed of iteration beats quality of iteration. Following this principle, I created cmn_ai to incorporate the most common tasks we encounter in ML/AI from data exploration to model development and training. Additionally, I baked into it some of the best practices I learned in my career so I don’t have to repeat myself on every project I work on.\nIt is worth noting that the majority of the DL code such as Learner and callbacks assume that pytorch is used as the underlying DL library. Also, tabular data preprocessors/models are compatible with sklearn so they can be used easily with its Pipeline or ColumnTransformer.\nGiven the nature of the progress in ML/AI, I will always keep adding more functionalities as time passes.\n\nDocumentation: https://imaddabbura.github.io/cmn_ai/\nRepo: https://github.com/ImadDabbura/cmn_ai"
  },
  {
    "objectID": "papers-summaries/rag-for-knowledge-intensive-tasks.html",
    "href": "papers-summaries/rag-for-knowledge-intensive-tasks.html",
    "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "section": "",
    "text": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\nThesis: Train end-to-end retrieval and generator that would provide more specific, diverse, and factual answers. This helps reduce hallucination, continuously update the non-parametric memory of the retriever by just indexing new documents.\nMethod(s): Train end-to-end query encoder and BART (generator) on question/answer pairs. Document encoder is initialized (from DPR) and not trained.\nContribution: End-to-end training of RAG model with generation instead of extraction for answers.\nTakeaways:\n\nHaving up to date memory indexed by document encoder reduces hallucination and overcome cutoff knowledge of generator\nEnd-to-end training of query encoder and generator\nGenerate more specific, diverse, and factual information\n\nImprovements:\n\nNo interaction between query and document\nDocument encoder is not updated and may not be optimal\n\nNotes:\n\nFine-tune only query encoder. No updates for document encoder because it is very expensive\nRetriever is DPR bi-encoder using BERT-base uncased. Index is populated before training using document encoder\nGenerator is BART\nQuestion and top-k documents are concatenated and passed to generator. BART conditions on them and start generating tokens\nIndex can be updated independently\n\n\n#nlp #rag"
  },
  {
    "objectID": "papers-summaries/scaling-laws-for-nlp.html",
    "href": "papers-summaries/scaling-laws-for-nlp.html",
    "title": "Scaling Laws for Neural Language Models",
    "section": "",
    "text": "Scaling Laws for Neural Language Models\n\nThesis: Provide empirical analysis and equations of how to go about the relationship between performance (measured in cross-entropy loss) for LLM and may factors such as model size, dataset size, compute budget, etc.\nMethods:\n\nTrained mainly decoder-only transformers with different variations and hyper-parameters plus LSTMs with context size of 1024 tokens.\nCompute budget = \\(6NBS\\) where \\(N\\) is number of parameters excluding embedding parameters, \\(B\\) is batch size, \\(S\\) is number of steps (optimization update), and \\(6\\) to account for forward and backward passes\n\nContribution: Simple equations and power-law relationships that we can use as a reference when allocating our compute budget.\nTakeaways: The main factors that affect LLMs performance are 1) model size, 2) dataset size, 3) compute budget. Other factors such as architectural hyper-parameters such at best weak effect on the model’s performance. The test loss would continue to decrease as we increase compute in a log-linear fashion.\nNotes:\n\nThere is negative relationship between model size, dataset size, and compute time with cross entropy loss. As we increase one of them -&gt; loss improves. This relationship can be described by power-laws equations.\nLarger models are sample-efficient -&gt; needs much less compute time to get to the same loss as smaller models \\(\\therefore\\) use less data points and less optimization steps.\nFor a fixed compute budget, large models can stop long before convergence and still performs better than small models\nIf we have more compute budget, allocating most of it to larger models yield the most improvement (we may need to increase dataset size to avoid iterating over the same data multiple times). Followed by larger batches to utilize parallelism. Finally, increase training steps (epochs).\nWhen including embeddings parameters, performance is strongly dependent on number of layers and the number of parameters. However, w/o including embedding parameters, any combination of layer/paramaters_size converge to the same trend\nTransformer outperforms LSTM. LSTM plateaus after &lt; 100 tokens while transformer improves through whole context. For the early tokens, LSTM performs similar to Transformer, but for tokens that are later in the context LSTM suffers. As Transformer model size increases -&gt; improves its ability to recognize patters and have lower loss for later tokens\nPerformance improves with model size regardless of what test data is used; however, for a given model size, there is narrow gap between losses for different test data. WebText2 has the lowest loss\n\nThere is a constant penalty between test loss and validation loss regardless of training and test data. In other words, when testing the model on a different distribution, the test loss is a constant factor more than the validation loss. So we can predictably predict test loss from validation loss\n\nTest loss keeps improving as we increase model size and dataset size in almost perfect power law in model size. For smaller datasets, larger models stop improving early and starts to overfit\nFor a fixed compute budget, we have optimal model size that would yield the best test loss\nLarger models require less steps to train compared with smaller models\nThere is very weak dependence on architecture and optimizer hyper-parameters such as number of attention heads or width/depth of the model\nLLM performance depends largely on three factors:\n\nModel size (excluding embedding)\nDataset size\nCompute budget\nWhen not bottlenecked by other two factors, model’s performance follows power-law relationship with these factors. For example, Increasing model size would improve performance in a power-law relationship assuming we’re not bottlenecked by dataset size or compute budget\n\nModel size and dataset size must be changed together to avoid overfitting. If we hold one of them context we hit diminishing returns and enter overfitting. For each \\(8x\\) increase in model size, we need to increase the dataset size by \\(5x\\)\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/starcoder-v2.html",
    "href": "papers-summaries/starcoder-v2.html",
    "title": "StarCoder 2 and The Stack v2: The Next Generation",
    "section": "",
    "text": "StarCoder 2 and The Stack v2: The Next Generation\n\nThesis:\nMethods:\n\nModels: 3B, 7B, 15B\nTwo-stage training:\n\nFirst base models are trained with 4K context windows\nThen fine-tuned with 16K context windows\n\nAt most 5 epochs over the dataset during training, why?\nBPE tokenizer\nRoPE\nGQA\nData:\n\nGH pull requests, Kaggle notebooks, code documentation\nStack v2, which includes 619 programming languages\nIntermediate representations (LLVM). This is useful for low-resource languages\nMath and coding datasets\nTailor training dataset composition of programming languages to model size as smaller models have less capacity and languages compete for model capacity. Therefore, 17 most widely used programming languages are used to train the 3B and 7B models while the full dataset is used to train the 15B model\nModels were trained with repository context, but files within the same repository are in random order\nCode is prepended with some metadata with 50% probability.\n\nMetadata example: &lt;repo_name&gt;reponamefilepath1filepath2 … &lt;|endoftext|&gt;`\nNo metadata example: &lt;file_sep&gt;code1&lt;file_sep&gt;code2 ... &lt;|endoftext|&gt;\n\nFIM 50% at the repo-context file level. So each repository is subjected to be FIM transformed with 50% probability.\n\nExample: &lt;repo_name&gt;reponame&lt;file_sep&gt;filepath0\\ncode0&lt;file_sep&gt;&lt;fim_prefix&gt;filepath1\\n    code1_pre&lt;fim_suffix&gt;code1_suf&lt;fim_middle&gt;code1_mid&lt;file_sep&gt; ...&lt;|endoftext|&gt;\n\n\n\nContribution:\n\nThe Stack v2\nDetailed description and source code of preparing and processing training data\nDetailed description and source code for training\n\nTakeaways:\nImprovements:\nNotes:\n\nStarCoder2-15B outperforms DeepSeekCoder-33B on math and code reasoning but DeepSeekCoder-33B is still the best for code completion on high resource languages\nStarCoder2 beats any open code LLMs of similar size\nStarCoder2-7B doesn’t perform well compared with other open code LLMs of its size. So pick StarCoder2-15B or StarCoder2-3B for deployment\n\n\n#nlp #llm #code-llm"
  },
  {
    "objectID": "papers-summaries/autogen.html",
    "href": "papers-summaries/autogen.html",
    "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
    "section": "",
    "text": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n\nThesis: Utilize conversation design pattern to build Multi-Agent framework by unifying interfaces to be driven by conversation-based interactions between agents that would work together to solve a complex task.\nContribution: AutoGen framework that speed up the development of LLM-based applications through Multi-Agent architectures. This will also modularize components where each agent can be developed, optimized, tested, and deployed independently\nTakeaways: Using conversation as an interface for agents to communicate make a generalizable framework that can be extended/customized/reused across a plethora of applications\nNotes:\n\nConversableAgent is the highest-level abstraction agent\nUserProxyAgent solicits human feedback or execute code/make function calls\nAssistantAgent is an AI assistant agent using LLM\nEach agent has send/receive functions that send and receive messages to/from agents\nEach agent has generate_reply that takes action and generate a response based on the received message\n\nOnce an agent receives a message from another agent, generate_reply is automatically invoked and does some action such as calling LLM/tool/code execution and sends the a message back to the sender agent\n\nAgents can be controlled by natural language, programming language, or both\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/pythia.html",
    "href": "papers-summaries/pythia.html",
    "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
    "section": "",
    "text": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n\nThesis: Provide open-sources models of different sizes and checkpoints as well as training code to study how LLMs capabilities evolve during training such as biases and correlation of term frequencies task accuracy.\nMethods:\n\nModels:\n\n8 models from 70M to 12B parameters with almost identical training setup\nCheckpoint of each model is saved after ~2B tokens\n\nBPE tokenizer\nDatasets:\n\nThe Pile (300B tokens)\nNear de-duplicated version of the Pile (207B tokens)\n\n\nContribution: Released and make available the following:\n\n154 checkpoints for each model\nTraining/model code\n8 model sizes that range from 70M to 12B\nEach model is trained twice:\n\nOne on the Pile dataset (300B tokens)\nOne on the near de-duplicated version of the Pile (207B tokens)\n\n\nImportance: Open sourced models, checkpoints, and training code to help researchers further study the learning dynamics of LLMs as training progress.\nTakeaways:\n\nModels trained on near de-duplicated data didn’t outperform models trained on the full dataset\nChanging corpus statistics 7% and 21% before end of training by replacing masculine pronouns with feminine pronouns led to reduce bias w/o hardly affecting the model’s perplexity. This may be due to the fact that LLM captures tokens’ correlations and distributions and such a change makes the model more robust. The effect is bigger for larger models as they tend to be more biased\nPlacing sequences in the beginning of segments didn’t show any evidence of memorization\nTerm frequencies related to a task affect the few-shot accuracy for the models. The more frequent the term the more accurate the model\n\nSmall models don’t show better performance with more term frequencies, which means this is an emergent ability for large models\n\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/metagpt.html",
    "href": "papers-summaries/metagpt.html",
    "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
    "section": "",
    "text": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework\n\nThesis:\n\nMetaGPT use assembly line paradigm to decompose complex tasks into smaller and simpler subtasks that are assigned to agents. To avoid compounding errors that happen when we chain LLMs together when some of them hallucinate and the errors get compounded as they get cascaded through the chain, it uses specialized agents that act as validators similar to human experts to validate and check intermediate results.\n\nThis is done by take SOPs and generate sequence of prompts for workflows\nUses message pools and subscription for agents to communicate\nDepends heavily on Standardized Operating Procedures (SOP)\nThere is an executable feedback between engineers and other agents during code generation\n\n\nMethods:\n\nUse schema and structured output enforce roles and responsibilities for each agent, which reduces hallucinations when compared to dialogue messages\n\nContribution:\n\nMetaGPT framework with well-defined roles and responsibilities between agents by integrating human-like SOPs and message passing between agents\nExecutable feedback during code generation to debug and test code\n\nTakeaways: Schema and structured output is key to reduce hallucinations for complex and long-running projects. Also, decomposing complex tasks into smaller ones and strictly enforce roles through prompt constraint help with task completion for complex tasks along with assembly line paradigm where agents work sequentially\nImprovements:\n\nNot clear how agents would learn with time from doing different projects\nNot sure how do we validate the output of each agent before moving on to the next agent\nComparisons using LLMs with different sizes as well as codeLLMs\n\nNotes:\n\nUsing SOPs, agents are given specific responsibilities by decomposing complex tasks into smaller and simpler ones. This will create standards for each agent’s output that will be checked by expert agents at each step\nThe standard output of each agent significantly improves the success of code generation such as structured product requirements and artifacts\nMetaGPT is more of SWE agentic framework than just code generation as in StarCoder -&gt; Completion rate is key metric\nAgents work sequentially until reaching the Engineer agent\nEach agent has well-structured and schema for its role/output, mostly documents and diagrams that will be the input for the next agent in the assembly line\nMessage pool is used as a collaboration tool between agents. Instead of sending one-to-one messages or sending irrelevant details to agents, messages will be published to the message pool and agents can subscribe to to the topics (messages) they need to do their work\nExecutable feedback mechanism is mainly used by Engineer agent to check the executability of the code and its correctness (passing test cases). This is done iteratively in collaboration with QA Engineer agent until the code passes all the unit tests and run successfully before moving to next tasks\nAgents learn from previous projects by summarizing feedbacks and update their constraint prompts for next projects. This is done first step for each new project\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/prefix-tuning.html",
    "href": "papers-summaries/prefix-tuning.html",
    "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    "section": "",
    "text": "Prefix-Tuning: Optimizing Continuous Prompts for Generation\n\nThesis: Train continuous task-specific context vectors that will steer the LM based on the task. This is very efficient because only the continuous vectors for the prefix that will be trained and all other pre-trained weights are kept frozen. This approach is driven from the success of prompting and in-context learning that steered the LM to generate tokens based on the examples provided in the prompt. The drawback of in-context learning is that LLMs have limited context window so including examples and other instructions would reduce the number of tokens for the task we want the LLM to perform.\nMethod(s):\n\nTrain continuous task-specific vectors on upstream tasks that will steer the LM for different tasks\nSuch prefix is kind of considered as “virtual tokens” and subsequent (actual) tokens would attend to them\nTraining: Each transformer block will have its own continuous task-specific tensor which is obtained by two linear layers with nonlinearity in-between. Then the output will be concatenated with the input embedding. For inference: we have to supply prefix prompt for the task we’re performing\n\n\n#nlp #llm #fine-tuning"
  },
  {
    "objectID": "papers-summaries/llama.html",
    "href": "papers-summaries/llama.html",
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "section": "",
    "text": "LLaMA: Open and Efficient Foundation Language Models\n\nThesis: It is possible to train LLMs that are competitive with much bigger closed models such as GPT3 on publicly available datasets w/o the need to have proprietary data. Also, inference budget is more important than compute budget, so instead of training very large LMs to achieve high performance, we train smaller models extensively on much bigger datasets. As a result, the tradeoff is between performance and inference.\nMethods:\n\nModels: 7B, 13B, 33B, 65B\nData:\n\nOnly publicly available data that is compatible with open source\nTrained on 1 trillion tokens for 7B and 13B models and 1.4 trillion tokens for 33B and 65B models\n\nTokenizer: BPE algorithm using implementation from Sentence-Piece\nModel:\n\nTransformer architecture (decoder)\nContext length is 2k\nBatch size is 4M\nNumber of heads goes from 32 for 7B model to 64 for 65B model\nHidden dimension goes from 4096 for 7B model to 8192 for 65B model\nNumber of layers goes from 32 for 7B model to 80 for 65B model\nUse RMSNorm which proves to be much more efficient than LayerNorm and is more stable\n\nRMSNorm paper shows that we only need re-scaling invariance to achieve decoupling of layer’s neuron from weight and input (avoid internal covariate shift) and we don’t need re-centering invariance. Therefore, we don’t need to calculate the mean \\[\n\\begin{align}\n\\begin{split} & \\bar{a}_i = \\frac{a_i}{\\text{RMS}(\\mathbf{a})} g_i, \\quad \\text{where}~~ \\text{RMS}(\\mathbf{a}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} a_i^2}. \\end{split}\\nonumber\n\\end{align}\n\\]\nAlso RMSNorm is cheaper to compute because we don’t need to calculate the mean\n\nPre-layer normalization -&gt; normalize the input of multihead attention block and MLP instead of the output. Also, normalize the out of the last transformer block before feeding it into output linear layer (classifier)\nUse SwiGLU instead of ReLU for non-linearity. It was shown that it leads to better performance in LLMs, but no intuition why :)\nReplace absolute positional embedding with rotary embeddings (RoPE), which is a hybrid of absolute and relative positional embeddings.\n\nRelative positional encodings deals with two tokens at a time and it is involved when we calculate the attention (not only when we get the embeddings in the case of absolute positional embedding): since the attention mechanism captures the “intensity” of how much two words are related two each other, relative positional encodings tells the attention mechanism the distance between the two words involved in it. So, given two tokens, we create a vector that represents their distance.\nRoPE gets applied after query and key have been multiplied by the weight matrix \\(W\\). This means after \\(x@W_q\\) and \\(x@W_k\\).\nRoPE is only applied to keys and query NOT values because they determine the attention scores and we want the attention scores to be weighted by how far tokens are from each other. In other words, as tokens get far from each other, they should pay less attention to each other. Therefore, it gets applied to query/key in every transformer block\nThere is nothing to learn in RoPE\n\nRecompute some activations during backward when needed and not store them during the forward pass to reduce memory peak consumption during training\nDon’t store weights or calculate query/key scores that will be masked in causal attention -&gt; scores for future tokens (the ones to the right from each token)\n\n\nContribution:\n\nReleased 4 variants of the model to open-source community that allows for further improvement and experimentation.\n\nThey achieve best possible performance for various inference budgets. The 13B model beats GPT3 on most benchmarks even though it is 10x smaller. The largest model (65B) is competitive with the largest models such as Chinchilla or PaLM-540B\n\nStarted a trend towards focusing on inference budget not only compute budget. Inference budget is very important because it affects the feasibility serving the model at scale.\nIllustrated that smaller models trained extensively on larger and high quality datasets can beat larger proprietary models. Some studies showed we can optimize the model size/dataset size for a given compute budget. For example, 10B model is recommended to be trained on 200B tokens.\n\nTakeaways: High quality large dataset and longer training is all you need to create a decently sized models that are very competitive.\nImprovements: Llama models are base LM and not much effort have been spent on making such models helpful by following user’s instructions. I am guessing this would be corrected with the next version.\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/chain-of-thought-reasoning-wo-prompting.html",
    "href": "papers-summaries/chain-of-thought-reasoning-wo-prompting.html",
    "title": "Chain-of-Thought Reasoning without Prompting",
    "section": "",
    "text": "Chain-of-Thought Reasoning without Prompting\n\nThesis: Provide unsupervised method to elicit reasoning capabilities of LLMs in the decoding space. Instead of relying on CoT prompting that is task-specific and requires humans to keep iterating and optimizing the prompt to yield the intended results, CoT-decoding uses top-k alternative tokens to get the best CoT path. For each token of the top-k tokens in the decoding step 1, we continue with greedy decoding algorithm.\nMethods:\n\nModels: PaLM2, Mistral-7B, and Gemma-7B\n\nContribution:\n\nElicit LLMs inherent reasoning capabilities w/o the need to use prompting by simply changing the decoding process\nAvoid using human priors that are task-specific with prompting to force the LLM how to solve a task through few-shot CoT prompting\nImprove the model’s confidence by traversing top-k alternative paths\n\nTakeaways: We can utilize reasoning capabilities of LLMs; both pre-trained and instruction-tuned, by operating in the decoding space that doesn’t require any human intervention or extensive resources to tune such models for reasoning intensive tasks\nImprovements:\n\nCoT-decoding adds computational complexity to the inference\nThe paper only branches out of the first token to explore possible paths\nIt is harder to use CoT-decoding in the case of open-ended answers\nThe gains from CoT-decoding starts to drop as the tasks gets harder. LLMs still struggle with tasks that require \\(&gt;= 3\\) manipulation steps. One possibility for this behavior is that pre-training data distribution is mostly simple-to-medium difficulty tasks\n\nNotes:\n\nLLM reasoning are typically elicited by prompting that comes in various forms:\n\nCoT with few-shot examples\nZero-shot with detailed instructions of how to perform the task and showing the steps of how the LLM came up with the answer\nTraining of instruction fine-tuning with CoT data\n\nLLMs struggle with reasoning when greedy decoding is used\nThe difference between the top two tokens at the \\(t^{th}\\) decoding step for the \\(k^{th}\\) decoding path is very high, which means the confidence of the LLM in the answer is very high\n\nThe answer’s confidence is the average of the difference of the top two tokens probability for each each time step\nThe model’s own probability is not a reliable indicator for the confidence/correctness of the answer\n\nLLMs typically rush directly into problem solving when asked math/commonsense reasoning questions, which results in the wrong answer most of the times. This can be partially fixed with prompting but don’t yield results as good as CoT-decoding\nBranching at the first-token leads to more diverse paths as opposed to branching at later stages of the decoding process\nIt is recommended to aggregate the top-k paths to get stable results\nThere is a very high correlation between the answer confidence and the existence of CoT paths in the answer\nModel’s inherent reasoning varies according to the task difficulty. Models show less reasoning abilities with harder tasks\ntop-k alternative tokens decoding reveals the existence of CoT reasoning paths which correlates with the model’s confidence in the final answer\nBoth pre-trained models and Instruction-tuned models showed Improvements in accuracy from CoT-decoding\n\nEven thought instruction-tuned models have used a lot of CoT examples during instruction fine-tuning, these models still try to directly jump to problem solving. Therefore, CoT-decoding helped boost performance tremendously\n\nCoT-decoding closes the gap between pre-trained models and Instruction-tuned models in terms of reasoning capabilities\nk has significant effect on pre-trained models but has negligible effect for instruction-tuned models after k = 2. For pre-trained models, the best paths may start at a lower k, but for instruction-tuned models they are already trained to bring the best paths to the top\nCombining CoT prompting with CoT-decoding yields even better performance from eliciting more reasoning capabilities\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/what-are-tools.html",
    "href": "papers-summaries/what-are-tools.html",
    "title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
    "section": "",
    "text": "What Are Tools Anyway? A Survey from the Language Model Perspective\n\nThesis: The paper does a survey regarding tool usage that helps us understand:\n\nWhat is the definition of a tool?\nWhat are the main categories of tools?\nHow to use the tools?\nWhen to use tools?\nWhen and how to make tools?\n\nNotes:\n\nCategories of tools:\n\nPerception: Collect information about the environment such as getting the current date or checking the weather\nAction: Performs actions in the environment that will change state such as positing content on a website or writing to a table in a database\nComputation: Use programs to perform computation such as mathematical computation\n\nA tool can belong to multiple categories such as SQL queries that can perform computation, retrieve data, and update/write to tables\nUsage of tooling:\n\nInference-time prompting: Incorporate instructions inside the prompt that shows few examples of pairs of queries and tools or use tool documentation\nLearning by training: Fine-tune LM through annotated data that illustrates the tool usage (self-supervised)\n\nTool-usage may not be useful in all scenarios such as for summarization or sentiment analysis since the base LM most likely is able to do the job just fine or even outperform specialized smaller models\nComplex tool usage. If we have small fixed tools, we can use them in-context in the prompt; otherwise, we may need to use retrievers to get the top related tools and feed them to the LM\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/tree-of-thought-prompting.html",
    "href": "papers-summaries/tree-of-thought-prompting.html",
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "section": "",
    "text": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n\nThesis: Inspired by cognitive science and AI in terms of how humans and animals approach problem solving, Tree of Thoughts (ToT) make use of system 1 & 2 by using LM to generate different possible paths (thoughts) and evaluate each path in making progress towards solving the problem in the form of of tree. The search for the best possible answer is done through either breadth-first search (BFS) or depth-first search (DFS), which helps in lookahead and backtracking\nMethods:\n\nThought decomposition: Decompose intermediate thought steps. Each step must be big enough and coherent\nThought generator: Use CoT prompt to generate k samples and then propose thoughts sequentially using propose prompt\nState evaluator: Value each state (node in a tree) independently using value prompt via lookahead simulations and commonsense. Finally vote across states which one would lead to the most promising solution using vote prompt\nSearch: Use any of the search algorithms depending on the nature of the tree: BFS or DFS\n\nContribution:\n\nToT framework\n\nImportance: ToT framework has many benefits such as 1) generality to any problem solving, 2) modularity where each stage of the framework can be changed independently, 3) interpretability/debugging, 4) no need to fine-tuning for different tasks\nTakeaways: ToT significantly improves performance of LLMs over CoT or self-consistency with CoT due to the fact that it mimics how problem-solving is carried in humans/animals using insights from cognitive science. Utilizing exploration and lookahead, ToT can generate and evaluate different paths at any given step.\nImprovements:\n\nIt is tested on three simple tasks, so we need more diverse set of tasks to check for applicability and generalizability\nIt is not needed and an overkill for tasks that current LLMs excel at\nIt is much more compute intensive than other sampling approaches\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/deep-seek.html",
    "href": "papers-summaries/deep-seek.html",
    "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence",
    "section": "",
    "text": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence\n\nThesis:\nMethods:\n\nModel sizes: 1.3B, 6.7B, 33B\n2T tokens of project-level code data from 87 programming languages\n\nEnhance understanding across files\n\n16K context window\nNext token prediction and [[fim]]\nUse repository-level data to improve generation across files\nData:\n\n87% source code\n10% english code-related natural language corpus\n3% chinese code-unrelated natural language corpus\n\nBPE tokenizer\nFIM rate is 50% (PSM)\nRoPE embedding\nGroup Query Attention\nFlash Attention v2\n\nContribution:\n\nDeepSeek-Coder Base and DeepSeek-Coder Instruct models\nLeetCode contest data benchmark\n\nTakeaways: Effective Code LLMs must have a deep understanding of natural language to be able to interpret and execute programming tasks. Also, high quality training data is key to achieve great performance. Finally, continued pre-training on methematical and natural language data improves performance on natural language tasks and tasks that require reasoning but may slightly reduce the accuracy on coding benchmarks such as HumanEval\nImprovements:\n\n16K context is still not enough for real-world code completion tasks that require context across files in the repo.\nSPM wasn’t tried but [[fim]] showed it yields better performance\n\nNotes:\n\nBase models are fine-tuned using high quality instruction data to augment the zero-shot capabilities of base models\nBase models are also fine-tuned on 2B high quality code, natural language, and mathematical data to improve their NLU capabilities. This result with DeepSeek-Coder-v1.5\nDeepSeek-Coder Instruct outperforms GPT-3.5-Turbo and very close to GPT-4\nDeepSeek-Coder Base outperforms all open-sourced code LLMs\nDependency parsing helps sort files in the order they are needed. This means placing dependencies before files that use them which simulate real-world applications where completion/generation happens at the project-level\nDeduplication at the repo level by concatenating files then perform near-deduplication\nFilter out low quality source files based on some heuristics\nAdding Chain-of-thought to the prompt is shown to significantly improve DeepSeek-Coder Instruct models\nDeepSeek-Coder 7B is recommended to use for completion tasks as it balances efficiency and accuracy. The bigger model (33B) is slightly better but has much more memory footprint and huge computation\ncross-file code completion utilizes keyword search (BM25) to provide contexts from other files in the repo\nDeepSeek-Coder v1.5 uses 7B model to continue pre-training on 2T tokens using next token prediction task. Data includes source code (70%) and other natural language code related corpus. This model is slightly less accurate on coding tasks but is much better at reasonong and natural language understanding\n\n\n#nlp #llm #code-llm"
  },
  {
    "objectID": "papers-summaries/olmo.html",
    "href": "papers-summaries/olmo.html",
    "title": "OLMo: Accelerating the Science of Language Models",
    "section": "",
    "text": "OLMo: Accelerating the Science of Language Models\n\nThesis: Following the same mission as [[pythia]], this paper open sourced every aspect of the model’s training and inference code. This includes training and inference codes as well as training data and model weights and checkpoints. OLMo model has similar performance to models such as Llama and much more performant than [[pythia]] models.\nMethods:\n\nModels:\n\n4 variants of 7B models and 1 model of 1B parameters\nRotary positional embedding\nSwiGLUE\nBPE tokenizer with 50280 vocabulary size\n\nDataset: Dolmo\nAll models train on ~2T tokens\n\nContribution: Released all the details of the framework such as training and inference code, data pipeline to reproduce training data, checkpoints, model weights, evaluation framework\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/t5.html",
    "href": "papers-summaries/t5.html",
    "title": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "section": "",
    "text": "T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n\nThesis: T5 aims to come up with unified framework for transfer learning using transformer’s Encoder-Decoder architecture and convert any NLP task into text-to-text tasks. As a result, we can use the same model, objective, and decoding for any downstream tasks\nMethods:\n\nModel:\n\nEncoder-Decoder transformer that follows original transformer paper (Attention is all you need) with few exceptions such as:\n\nUse relative position embedding instead of sin/cosine positional embeddings. Each head learns its own positional embedding which is scalar or 32 relative positions that scales logarithmically to reach 128 of offset. Anything beyond 128 would be assigned 128 positional embedding. The positional embeddings are shared among all transformer layers\nUse pre-layer normalization w/o bias term\nThe authors didn’t look into Encoder-only transformer such as BERT because they are more meant for token/sequence classification\nThe authors didn’t look into Decoder-only transformer such as GPT because they use causal self-attention that only considers previous tokens and don’t have access to future tokens, which is necessary for some tasks such as summarization\nTo reduce computational cost, only masked/denoised input’s output is computed\nThe authors also found that sharing the parameters between the encoder and decoder performs quite well and reduce the number of parameters by half\n\nContext window size is 512\nBatch size is 128\nAll other transformer hyperparameters are similar to [[bert]]\nThe model is trained on much less tokens that BERT\nThe model will be also fine-tuned separately on different downstream tasks\nSentencePiece tokenizer\nDenoising objective is used where a span of tokens is masked/corrupted and the model has to predict these tokens\n\nRandomly selects 15% of tokens in the input sequence\nReplaces all consecutive spans of selected tokens with a single sentinel token\nGives each sentinel token an ID that is unique to the current input sequence\nConstructs a target using all selected tokens, separated by the sentinel tokens\nAdd the sentinel and its ID to the vocabulary learned by the tokenizer\nSpan corruption is used with an average span length of 3\n\nThe authors tested multiple denoising strategies and all performed similarly. So they opted for the one that reduces the target sequence length as it is computationally more efficient, which is replace \\(15\\%\\) tokens strategy\n\nDifferent corruption rates performed similarly except for very large rates, for example \\(50\\%\\)\n\n\nData:\n\nInput and output of the model is just text. To differentiate between different tasks, each task has its own prefix that is prepended to the input text before is fed to the model. The choice of prefixes has some impact on the model’s performance. Below are some examples for different prefixes:\n\nFor classification tasks, the predicted token would be the class. The input would be something like: mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. for natural language inference.\nFor translation tasks, Translate from English to German: That is good.\n\n\n\nContribution:\n\nThe paper evaluated different model’s architecture and objectives to gain insights on what affect pre-training and fine-tuning performance. Different variants of transformer architecture were compared to the Encoder-Decoder baseline of T5:\n\nDecoder-only\nPrefix LM that uses self-attention for given prefix of the input and masked attention when generating the output\n\nT5 model that uses unified framework for transfer learning by casting all NLP tasks as text-to-text problems. The unified framework allows us to use the same model, objective, training procedure, and decoding method on any downstream task\n\nTakeaways: Having a unified framework where all NLP tasks can be casted to take text as input and produce text as output with denoising objective for pre-training and fine-tuning on downstream tasks is all you need!\nImprovements:\n\nT5 is fine-tuned separately for each downstream task, which might not be feasible or yield good performance for low-resource tasks\nOnly first-order effects were studied, but a lot of the hyperparameters and other architecture variations have second-order effects\n\nNotes:\n\nT5 stands for Text-to-Text Transfer Transformer\n\n\n#llm #nlp"
  },
  {
    "objectID": "papers-summaries/searching-for-best-practices-rag.html",
    "href": "papers-summaries/searching-for-best-practices-rag.html",
    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
    "section": "",
    "text": "Searching for Best Practices in Retrieval-Augmented Generation\n\nThesis: RAG workflows have various processing steps and modules where each step/module affect other module(s). In order to determine the best combinations, we have to test all possible combinations which is not feasible. The paper provides systematic process and recommendations of how to approach RAG workflow optimizations and how to pick the mix of modules to get the best possible performance with an acceptable latency.\nMethods:\n\nFor each module/processing step:\n\nEvaluate all methods and pick at most top 3 most performant methods\nEvaluate the impact of each method on the overall RAG performance\nPick the best method and use it in further experiments\n\nExperiments were evaluated on the following:\n\nNLP task:\n\nAccuracy score for Commonsense Reasoning, Fact Checking, and Medical QA\nF1-score and Exact-Match scores for Open-Domain QA and Multihop QA\n\n\nRAG capabilities:\n\nAverage of the following scores:\n\nfaithfulness, context relevancy, answer relevancy, and answer correctness.\n\nAlso, cosine similarity between retrieved documents and gold documents\n\n\nContribution:\n\nEvaluation framework and datasets. This includes metrics that are more general to LLMs, specific to some domains, and RAG-related capabilities\nEvaluating different RAG approaches to provide recommended combinations\n\nQuery Classification Module: Whether query needs to pas through retrieval process or go to LLM directly. This helps reduce latency\nRetrieval Module: Hybrid search is key. HyDE plus Hybrid search perform yielded best performance at a huge latency increase\nReranking Module: MonoT5 achieved the highest average score, affirming its efficacy in augmenting the relevance of retrieved documents.\nRepacking Module: Putting retrieved documents in reverse order of their scores yielded the best results\nSummarization Module: Didn’t provide enough performance boost\n\n\nImportance: Provides a set of recommendations for different RAG modules and processing steps that can be used as a starting point when building RAG-based applications.\nTakeaways: The paper recommends the following after thorough experimentations even though I would still suggest to take such recommendations with a grain of salt and still experiment with use-case specific datasets to evaluate and iterate over all RAG components quickly:\n\nQuery Classification: Classifies if retrieval is necessary for a given query.\nChunking: Advanced techniques Sliding window chunking or small2big improve relevancy and faithfulness.\nRetrieval: Hybrid with HyDE achieves the highest retrieval performance.\nReranking: MonoT5 (LLM) provides the best relevance improvement.\nRepacking: Reverse (most relevant at the end) shows best performance.\nSummarization: Recomp method balances performance and latency effectively.\nGenerator Fine-tuning: Mixed contexts (relevant and random documents) enhance robustness and accuracy.\n\nImprovements:\n\nExperiments were done on academic datasets, which are completely different than real production data and user queries are messier\nEncoders (retriever) were not Fine-tuned along with generator\nMetadata wasn’t part of the study\n\nNotes:\n\nQuery and retrieval transformation:\n\nDecompose query into sub-queries and aggregate the documents retrieved\nGenerate pseudo-queries for retrieved documents\nTrain embedding model through contrastive loss using triplet of query and positive and negative documents. This helps bring the query and document embeddings to be closer in semantic space\nUse abstractive and extractive summarization of retrieved documents to remove redundancy\n\nRetrieval:\n\nReranker to remove irrelevant retrieved documents\nChunk size\n\nQuery classification: Not all queries need to go through retrieval process and can just be answered using the generator. Train a classifier to determine if the query needs retrieval or not\nChunking:\n\nSentence-level chunking is a middle ground between token-level chunking that suffers from splitting sentences and context and semantic-level chunking that suffers from long latencies\nChunk size is key to get good quality retrieved documents. Large chunk size provides more context but slower and may lead to lost in the middle issue that generator faces with long contexts. Small chunk size is faster but may not provide the full context\nChunk techniques such as small2big and sliding window leads to better retrieval of documents\n\nsmall2big is a technique used to split documents into small/child chunks that would refer to parent/bigger chunks. For retrieving, we use smaller chunks. Once we identify relevant small chunks, we use the longer chunks that are the parent of the small chunks which will be used as the context for the generator\nsliding window refers to embed sentences instead of larger chunks. For retrieved sentences, we fetch n sentences around the retrieved sentences. For example, retrieve 5 sentences left and right of the relevant sentences to provide more context to the generator\n\nAdd metadata to chunks\n\nRetrival methods:\n\nQuery rewriting to improve ambiguity of user queries\nQuery decomposition by retrieving documents based on sub-questions derived from original user query\nPseudo-documents generation involves generating hypothetical documents from user queries and use them to compare with retrieved documents using their embeddings\nHybrid search of sparse and dense retrievals such as BM25\n\nHybrid search and Pseudo-documents generation with HyDE yielded the best results while query rewriting and query decomposition didn’t improve results. Single hypothetical document is a good trade-off between performance and latency for HyDE. For Hybrid search, \\(\\alpha\\), which measures the weighting factor for sparse score, of \\(0.3\\) yields the best performance\nReranking retrieved documents is key to get good performance. The paper experimented with two approaches:\n\nUse deep-learning-based methods, mainly transformer architecture, to rank documents based on user query.\nTILDE Reranking which calculates the likelihood of each query term independently by predicting the next token in the vocabulary and sum all log probabilities. It is very fast compared to deep-learning-based methods but not performant\n\nDocument repacking involves rearranging reranked documents before passing them to the generator. We can arrange them in ascending order of their scores (forward), or descending order (backward).\nSummarizing retrieved documents is key to avoid redundant and unnecessary information and long contexts that may lead to problems such as lost in the middle that LLMs suffer from. Abstractive summarization is important\nFine-tuning generators with mix of relevant and random documents yields the best performance\n\n\n#nlp #rag"
  },
  {
    "objectID": "papers-summaries/chinchilla.html",
    "href": "papers-summaries/chinchilla.html",
    "title": "Chinchilla: Training Compute-Optimal Large Language Models",
    "section": "",
    "text": "Chinchilla: Training Compute-Optimal Large Language Models\n\nThesis: Most LLMs are significantly undertrained because they mostly increase model size without increasing the number of training tokens. Based on the empirical analysis, for a \\(10x\\) increase in compute budget, we should increase the model size and the number of training tokens almost EQUALLY.\nMethods:\n\nApproach 1:\n\nFor fixed family of models that range from 70M to 10B in size, each model is trained on 4 different number of training tokens.\nLearning rate is decayed by \\(10x\\) over number of training tokens.\nEach model would have 4 runs. Plot loss vs FLOPS for each model and smooth the loss curve. Therefore, we end up with a curve for each model that would give us a mapping from FLOPS to training loss.\nFor each FLOP, we determine the lowest loss achieved by each model (which run achieves the best loss for each model)\n\nApproach 2:\n\nFor 9 fixed compute budget, train every model size 9 times on varying training tokens s.t. the FLOP counts is ~ predetermined compute budget.\nSo we end up with 9 different curves, one for each compute budget\nThe curves are U shaped and tell us which model (and consequently training tokens) yield the lowest loss.\nAfter fitting power-laws between FLOPs and loss-optimal model size and number of training tokens, we can estimate the model size and training tokens for any compute budget\n\nApproach 3:\n\nFit parametric loss function on the results of all experiments from approach 1 & 2 as a parametric function of model size and number of training tokens\n\nThe three approaches yielded very similar results in the scaling of parameter counts and number of training tokens. As compute budget increases, model size and number of training tokens should increase almost equally\nChinchilla Model:\n\nNumber of parameters is 70B, which was determined using the scaling function(s) from above with the same compute budget that Gopher LLM used. Therefore, it is \\(4x\\) smaller than Gopher\nNumber of training tokens is 1.4T\nNumber of layers = 80\nNumber of heads = 64\nBatch size = 1.5M -&gt; 3M\nDimension of model is 8192\nDimension of head is 128\n\n\nContribution:\n\nEmpirically estimated function(s) to figure out the optimal model size and number of training tokens for any compute budget.\nChinchilla model that is very similar to Gopher model that is \\(4x\\) smaller and outperforms Gopher model on almost all tasks.\n\nTakeaways: To get the optimal performance for a given compute budget, increase the number of training tokens by the same rate of increasing the model size for any additional compute budget.\nImprovements:\n\nThe paper assumes that the relationship between loss and model size and number of training tokens follows a power-law. That is may not be true for all scales. This may suggest that the author may have overestimated the model size for a given compute budget.\nAmong all the runs, there is only 2 large-scale models (Gopher and Chinchilla) and almost no intermediate-scale models\nAll models in all runs have been trained in less than 1 epoch of data. Could we have different trends if we train for many epochs?\n\nQuestions:\n\nDive deeper into approach 1 as not clear what is the role of decaying learning rate has to do with scaling laws\n\nNotes:\n\nChinchilla is LLM that uses the same compute budget as Gopher but with \\(4x\\) less parameters (70B instead of 280B) and trained on \\(4x\\) more tokens (1.4T instead of 400B). Gopher was significantly undertrained. Chinchilla outperforms Gopher on almost all downstream task and the inference cost is much smaller (since the model is \\(4x\\) smaller) which makes it more feasible to use by downstream tasks.\nThe paper is trying to compute the optimal model size and the number of training tokens (training dataset size) for a given compute budget. This is traditional optimization problem where we’re trying to find the minimum number of model’s parameters (N) and number of training tokens (D) such that the compute budget \\(FLOPS(N, D) = C\\).\n\nThe authors trained 400 models that range in size between 70M to 16B trained on 5B to over 400B tokens.\n\n[[scaling-laws-for-nlp]] predicted that for \\(10x\\) increase in compute budget, the model size should increase by \\(5.5x\\) while training dataset size should increase by \\(1.8x\\).\nDense transformer models are models like GPT/BERT family of models while sparse transformer models are models like Mistral and Mixtral that are base on mixture of experts\nLarge and high quality training data plays a critical role in training LLMs\n[[scaling-laws-for-nlp]] overestimate the effect of model size on loss because the authors kept the learning rate schedule and training tokens fixed for all runs. Therefore, they arrived at the conclusion that model should be scaled much faster than the size of the training data. In this paper, the authors changed learning rate schedule to match training tokens and changed training tokens."
  },
  {
    "objectID": "papers-summaries/mistral-7b.html",
    "href": "papers-summaries/mistral-7b.html",
    "title": "Mistral-7B",
    "section": "",
    "text": "Mistral-7B\n\nThesis: Mistral-7B shows that it is possible to train high performant LLM w/o sacrificing the efficiency in terms of memory usage and computational cost. The model outperforms Llama2-13B on all tasks and Llama2-34B in tasks such math and code generation\nMethods:\n\nModel:\n\ndim = 4096\nn_layers = 32\nhead_dim = 128\nhidden_dim = 14336; dimension of FFN\nn_heads = 32; n_kv_heads = 8. Because we’re using GQA -&gt; 1 head is shared between 4 keys/values\nwindow_size 4096\ncontext_len = 8192\nvocab_size = 32000\nTokenizer is SentencePiece\nSliding Window Attention (SWA). On every layer, each hidden state at position \\(i\\), it attends to tokens from previous layer from position \\(i -\nwindow_{size}\\) to \\(i\\). So a token attends to tokens in its local context (window size). This means that we theoretically would have attention span in the last layer = \\(n*{layers} x window\\_{size}\\) due to receptive field that is similar to CNNs.\n\nThis reduces the dot products performed to get the attention scores which speeds up training and inference\nIt may affect model’s performance since a token may not have direct access to attend directly to all tokens in the context size, but if the window size is big enough and through the indirect attention of tokens’ receptive fields -&gt; performance may not suffer\n\nRolling Buffer Cache. Because tokens only attend to up to \\(window_{size}\\) tokens, we can have fixed cache where tokens at positions \\(i &gt; window_{size}\\) will be stored at position \\(i % window_{size}\\), which means overwrites past values. Therefore, the KV cache would be of size \\(window_size\\) instead of \\(context_len\\). In other words, we use entries in KV cache that are after \\(i % window_size\\) followed by entries from \\(0\\) index to \\(i % window_size\\) for all \\(i &gt; window_size\\)\nUses Grouped Multi Query Attention (GMQA) [[llama2]] that reduces memory usage and speed up inference speed especially during decoding -&gt; reduces latency and increases throughput through larger batch sizes\nCache Pre-fill and Chunking: During sequence generation (inference), we already know the prompt -&gt; we can pre-fill the KV cache with the prompt. If the prompt is very large, it would be chunked into \\(window_{size}\\) chunks and each chunk will be processed separately where the current chunk will attend to the cache from previous chunk and itself. The keys and values will come from the KV cache and the tokens from the current chunk, but the query comes from the tokens in the current chunk. This means that the attention mask would be bigger than the KV cache starting from 2nd chunk where it would be of dimension: size of current chunk (for query) x size of (current chunk + previous chunk) for keys. Same logic applies to values metric. This allows the current chunk to attend to previous chunks. This process stops when we start generation where we use typical KV cache that appends K&V for each token separately\nMistral-7B-Instruct is fine-tuned on instruction dataset from HF. The model is comparable with Llama2-13B chat model\n\n\nContribution: Open-sourced weights and training code for a very performant model that outperforms Llama2-13B on all tasks. This helps in democratizing adoption of LLMs and deploy smaller models that don’t compromise on quality\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/internet-augmented-lm.html",
    "href": "papers-summaries/internet-augmented-lm.html",
    "title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
    "section": "",
    "text": "Internet-augmented language models through few-shot prompting for open-domain question answering\n\nThesis: Improve the LM performance on ODQA w/o any fine tuning or learnable parameters by 1) Use google search as the retriever that return an up-to-date information, 2) Prompt the LM using the returned results separately utilizing few-shot prompting for each returned document, 3) Rerank the answers returned by LM. This helps reduce the hallucination and improve the factuality of the answers plus close the gap between small and large LMs using the web as a source for information.\nMethod(s):\n\nFor every question, get the top-20 results returned from Google API\nSplit each document into paragraphs where each paragraph contains at most 6 sentences\nUse TF-IDF as ranker to rank relevant paragraphs to the query\nUse few-shot prompt on each of the top paragraphs to condition the LM and get answers\nRerank the answers according to their score using same LM as scorer\n\nContributions:\n\nUse of up to date information to condition LM\n\n\n#nlp #rag"
  },
  {
    "objectID": "papers-summaries/adapters.html",
    "href": "papers-summaries/adapters.html",
    "title": "Parameter-Efficient Transfer Learning for NLP",
    "section": "",
    "text": "Parameter-Efficient Transfer Learning for NLP\n\nThesis: To support continual/online learning w/o incurring too much cost for fine-tuning task-specific model, the proposed solution keep the pre-trained weights frozen and adds two adapter layers for each transformer layer. This reduces the number of trainable parameters to 3-8% of the original model. With this setup, each downstream task would have its own weights in the adapter layers but all of the tasks share the pre-trained weights. The performance is almost similar to the full fine-tuning.\nMethod(s):\n\nEach transformer layer has two adapter layers:\n\nOne after projection layer of multi-head attention but before skip connection\nOne after feedforward layer but before skip connection\n\nEach adapter layer has:\n\nFeedforward layer to down-project input -&gt; creates bottleneck. The smaller the bottleneck -&gt; less parameters at cost of less performance\nFollowed by non-linearity\nFeedforward layer to up-project input to original size\nSkip connection of input and output of Feedforward up-project\n\nTraining:\n\nWeights in adapter layers\nLayernorms in each transformer layer\n\n\n\n#nlp #llm #fine-tuning"
  },
  {
    "objectID": "papers-summaries/retrieval-augmented-lm-realm.html",
    "href": "papers-summaries/retrieval-augmented-lm-realm.html",
    "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
    "section": "",
    "text": "REALM: Retrieval-Augmented Language Model Pre-Training\n\nThesis: Pretrain end-to-end retriever and generator and then fine-tune on downstream tasks. The external knowledge would augment the LM and increase its performance. This will lead to back propagate through the retriever.\nContribution: End-to-end training of both retriever and LM.\nMethod(s):\n\nPretraining:\n\n[CLS] question masked some tokens [SEP] document\nThe retriever retrieves the document that is most similar to the question. Embedding is obtained from BERT encoder.\nThe LM encoder (BERT) predicts the masked token given the question and the answer\n\nFine-tune\n\n[CLS] question [SEP] answer\nLM encoder predicts the start and end of answer\n\n\n\n#nlp #rag"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nNotes on Learning\n\n\n7 min\n\n\n\nPersonal Growth\n\n\n\n\nImad Dabbura\n\n\nJan 3, 2025\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Having the Last Layer in a Neural Network as Raw Logits is Better Than Softmax\n\n\n5 min\n\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nJun 9, 2024\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding GPT2/3 (124M) From Scratch\n\n\n2 min\n\n\n\nNLP\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 10, 2024\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nByte-level Byte-Pair Encoding (BPE)\n\n\n1 min\n\n\n\nNLP\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 10, 2024\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdeas to Improve RAG Applications\n\n\n4 min\n\n\n\nNLP\n\n\nRAG\n\n\n\n\nImad Dabbura\n\n\nMar 5, 2024\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Deep Dive into Python’s Modules and Packages\n\n\n20 min\n\n\n\nPython\n\n\nSWE\n\n\n\n\nImad Dabbura\n\n\nFeb 9, 2024\n\n\n\n\n\nModified\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Forward and Reverse-mode Automatic Differentiation in Deep Learning\n\n\n4 min\n\n\n\nMLSys\n\n\n\n\nImad Dabbura\n\n\nFeb 3, 2024\n\n\n\n\n\nModified\n\n\nFeb 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning ML Systems\n\n\n22 min\n\n\n\nMLSys\n\n\n\n\nImad Dabbura\n\n\nJan 29, 2024\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit Confident\n\n\n38 min\n\n\n\nSWE\n\n\n\n\nImad Dabbura\n\n\nDec 22, 2023\n\n\n\n\n\nModified\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for Deep Learning Systems\n\n\n15 min\n\n\n\nMLSys\n\n\n\n\nImad Dabbura\n\n\nDec 20, 2023\n\n\n\n\n\nModified\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer Architecture Explained\n\n\n13 min\n\n\n\nNLP\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nFeb 14, 2023\n\n\n\n\n\nModified\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization Strategies\n\n\n7 min\n\n\n\nNLP\n\n\n\n\nImad Dabbura\n\n\nJan 14, 2023\n\n\n\n\n\nModified\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLSTM Implementation\n\n\n6 min\n\n\n\nNLP\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nDec 10, 2022\n\n\n\n\n\nModified\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC Program Startup\n\n\nDoes C program really start at main\n\n\n6 min\n\n\n\nSWE\n\n\nC\n\n\n\n\nImad Dabbura\n\n\nOct 21, 2022\n\n\n\n\n\nModified\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 8 - Best Practices\n\n\n4 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nMar 28, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 7 - Triggering Workflows\n\n\n8 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nMar 14, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 6 - Sharing Data Between Tasks\n\n\n4 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nFeb 28, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 5 - Dependencies Between Tasks\n\n\n7 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nFeb 14, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 4 - Task Context & Jinja Templating\n\n\n5 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nFeb 7, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 3 - DAG Scheduling\n\n\n6 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nJan 24, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 2 - DAGs\n\n\n3 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nJan 17, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAirflow Part 1 - What is Airflow?\n\n\n4 min\n\n\n\nData Engineering\n\n\nMLOps\n\n\nAirflow\n\n\n\n\nImad Dabbura\n\n\nJan 10, 2022\n\n\n\n\n\nModified\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n6 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\n\n\nImad Dabbura\n\n\nSep 11, 2019\n\n\n\n\n\nModified\n\n\nSep 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConda Essentials\n\n\nEnough background about Conda to be productive!\n\n\n5 min\n\n\n\nSWE\n\n\n\n\nImad Dabbura\n\n\nFeb 18, 2019\n\n\n\n\n\nModified\n\n\nFeb 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent Algorithm and Its Variants\n\n\nDeep dive into gradient descent algorithm: Batch vs. Mini-batch vs. Stochastic.\n\n\n10 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nFeb 18, 2019\n\n\n\n\n\nModified\n\n\nFeb 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks\n\n\nDeep dive into K-means algorithm to find subgroups within data.\n\n\n15 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\nUnsupervised Learning\n\n\n\n\nImad Dabbura\n\n\nSep 11, 2018\n\n\n\n\n\nModified\n\n\nSep 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Neural Network Part 5 - Dropout\n\n\nWhat is Dropout, its use, and how to implement it?\n\n\n4 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nMay 20, 2018\n\n\n\n\n\nModified\n\n\nMay 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Neural Network Part 4 - Regularization\n\n\nWhat is regularization and how it helps NN generalizes better?\n\n\n9 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nMay 8, 2018\n\n\n\n\n\nModified\n\n\nMay 8, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Neural Network Part 3 - Parameters’ Initialization\n\n\nThe role of parameter initialization in training and different ways to initialize parameters.\n\n\n5 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 20, 2018\n\n\n\n\n\nModified\n\n\nApr 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Neural Network Part 2 - Gradient Checking\n\n\nHow to check numerically if the implementation of backward propagation is correct?\n\n\n4 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 8, 2018\n\n\n\n\n\nModified\n\n\nApr 8, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Neural Network Part 1 - Forward & Backward Propagation\n\n\nWhat it takes to go from input to output? And how to compute the gradients?\n\n\n11 min\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nApr 1, 2018\n\n\n\n\n\nModified\n\n\nApr 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBandit Algorithms: epsilon-Greedy Algorithm\n\n\nWhat is epsilon-Greedy Algorithm and how to use it in A/B testing?\n\n\n9 min\n\n\n\nData Science\n\n\nWebsite Optimization\n\n\n\n\nImad Dabbura\n\n\nMar 31, 2018\n\n\n\n\n\nModified\n\n\nMar 31, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Loan Repayment\n\n\nTrying different modeling techniques to deal with imbalanced data, missing values, and ensemble models.\n\n\n17 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\n\n\nImad Dabbura\n\n\nMar 15, 2018\n\n\n\n\n\nModified\n\n\nMar 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacter-Level Language Model\n\n\nPredict the next character given the previous charecter and state.\n\n\n13 min\n\n\n\nNLP\n\n\nDeep Learning\n\n\n\n\nImad Dabbura\n\n\nFeb 22, 2018\n\n\n\n\n\nModified\n\n\nFeb 22, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Employee Turnover\n\n\nExperimenting with different models on employee turnover data.\n\n\n9 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\n\n\nImad Dabbura\n\n\nDec 11, 2017\n\n\n\n\n\nModified\n\n\nDec 11, 2017\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/transient-ischemic-attack/index.html",
    "href": "projects/transient-ischemic-attack/index.html",
    "title": "Transient Ischemic Attack",
    "section": "",
    "text": "A transient ischemic attack (TIA) is like a stroke, producing similar symptoms, but usually lasting only a few minutes and causing no permanent damage. Often called a ministroke, a transient ischemic attack may be a warning.\nAbout 1 in 3 people who have a transient ischemic attack will eventually have a stroke, with about half occurring within a year after the transient ischemic attack.\nThe goal of this project is to build a binary classifier to predict Transient Ischemic Attack (TIA) and then deploy the best model as a RESTful API.\nCheck out the project on github."
  },
  {
    "objectID": "books-summaries/deep-work/index.html#introduction",
    "href": "books-summaries/deep-work/index.html#introduction",
    "title": "Deep Work: Rules for Focused Success in A Distracted Life",
    "section": "Introduction",
    "text": "Introduction\nWe’re living in an information/knowledge economy which means people are rewarded for the valuable things they produce. Therefore, the more knowledgeable a person is, the more valuable he’ll be in such economies. As result, a person needs to continuously invest in himself/herself since technology is changing so rapidly. People who don’t invest in acquiring such knowledge quickly will be losing their value in this economy and lose their competitiveness.\nTo thrive in this economy, a person should:\n\nBe able to learn hard things. Since technology is changing rapidly, this means that the person needs to learn quickly and such learning never ends.\nProduce at an elite level in terms of both quality and speed. This means in addition to mastering the foundational (relevant) stuff, put the extra time and effort to produce something novel and complex.\n\nTo learn hard things quickly, we must focus intensely and without distraction."
  },
  {
    "objectID": "books-summaries/deep-work/index.html#current-state",
    "href": "books-summaries/deep-work/index.html#current-state",
    "title": "Deep Work: Rules for Focused Success in A Distracted Life",
    "section": "Current State",
    "text": "Current State\nThe trends in the corporate world in the past decade or so tend to move in the opposite direction of what deep work requires.\n\nOpen Spaces. More and more companies are moving towards open space offices which means a lot more distractions and makes it very hard for the employees to focus and concentrate on deep work. The rationale behind open spaces is to enforce more collaboration between teams and foster a better learning environment where employees learn faster by engaging with other employees continuously. However, multiple recent studies including a study from Google; which was almost the first to market for open spaces, showed that productivity doesn’t necessarily increase and employees are using a lot more emails/IMs than before even though they sit very close to each other. My experience is that this is just another illusion. The more cognitively demanding your job is, the harder it will be to work in open-space offices especially when multiple employees are on the phone at any given time during the day.\nEmployees should always be connected and responsive. Most companies/managers especially in the west expect employees to always respond to emails even after-hours and on weekends. Such expectations would affect the employee’s ability to focus and do deep work. The sender sometimes creates or pushes the work to the receiver so that he/she can clear the inbox even though he/she could invest a little more time and figure it out himself. Meetings are another story by themselves. Most of these meetings don’t lead to anything: they don’t solve problems and don’t lead to actions.\n\nOne of the main reasons that corporates don’t adopt deep work strategies is that it is hard to show the effect of deep work on the bottom line of the business. It’s the same concept that it is hard to show the effects of habits on people’s lives. Plus, adopting deep work strategies means a lot more work in the short-run for managers and others since now they just can’t request information all the time from their employees and they need to do some work to figure it out."
  },
  {
    "objectID": "books-summaries/deep-work/index.html#rules",
    "href": "books-summaries/deep-work/index.html#rules",
    "title": "Deep Work: Rules for Focused Success in A Distracted Life",
    "section": "Rules",
    "text": "Rules\nBelow are some of the most important rules to help us achieve deep work.\n\nRULE I: Deep Work\nhigh-quality work = time spent x intensity of focus\nWhen switching between tasks, there is an overhead cost called “attention residue” where it takes time to completely switch the brain and clear it off from the previous task and start focusing on the new task —&gt; Another reason for intense and concentrated focus.\nSeeing an email or a message that you can’t deal with now dampens the performance because now we have attention residue about the message and have a secondary task left unfinished on top of the primary task we are performing.\nTo produce at peak level, we need to work for extended periods with full concentration on a single task free from any interruption/distraction.\n\nAttention\nThe role of attention - what we choose to focus on and what to ignore - plays a huge role in the quality of our lives. Who you are, what you think, feel, and do, and what you love - are the sum of what you focus on. Concentration leads to no attention left over to think about anything irrelevant or worry about problems. When we lose focus, our minds tend to focus on trying to fix what’s wrong in our lives instead of what’s right.\nThe best moments usually occur when a person’s mind or body is stretched to its limit in a voluntarily effort to accomplish something difficult and worthwhile. Free time is harder to enjoy because it is highly unstructured and requires much greater effort to be shaped to make it enjoyable.\n\n\nDeep Work\nDeep work is required to hone skills that allow us to consider our jobs as crafts and be able to apply those skills to the job so that we start having meaning in our professional jobs.\nWe have a finite amount of willpower that becomes depleted as we use it. For example, we can fight the desire to check our inbox or social media; however, with time, our willpower gets drained and could no longer resist the desire.\nThe chain method is a way where we cross each day we do a defined task with a red X. The chain will grow and gets longer every day. The challenge would be to not break the chain —&gt; Do that task every day. In other words, be consistent.\nModes of Deep Work Scheduling:\n\nMonastic - Focus almost all of your time on the important and valuable tasks that require deep work by minimizing and maybe getting rid of shallow tasks. Very few people can afford such a style and mostly those people who are very influential in their fields and have created unquestionable value using their talent such as Donald Knuth (father of modern algorithms).\nBimodal - The best example is when you take an extended period such as a month or so and isolate yourself and do only deep work while the rest of the time you do the normal work that includes shallow tasks that can’t be avoided.\nRhythmic - Schedule a set time for deep work such as between 5-7 am every weekday. This works with most people due to the reality of human nature.\nJournalist - Fit the deep work whenever you can into your schedule.\n\nEffective rituals help us engage in long deep work sessions such as:\n\nWhere you’ll work and for how long\nHow you will work once you start such as restricting the use of the internet or metrics to evaluate progress\nHow you’ll support your work such as stuff needed (coffee, food, resources, books, walks, etc.)\n\nChanging the environment seeking quietness and inspiring surroundings helps in achieving going deep. It first helps the brain to overcome procrastination and other mental obstacles after all the investments in effort and money have been made toward that goal. It also helps the deep goal to be at a higher level of mental priority.\nEven though it may look like a contradiction to the deep work, collaboration is a complement to deep work to get invaluable outcomes. We should optimize for both collaboration and deep work separately not together. We still use deep work to get the best out of our capabilities; however, collaboration helps in learning from others and getting feedback as well in the form of discussions, white-boarding, and co-authoring. Etc.\nExecuting a deep work plan as a business plan makes deep work more meaningful:\n\nHave an identified goal that is tangible\nLag measures deal with the long-term goal and Lead measures deals with short-term behaviors that we can control daily that also lead towards achieving the lag measure. We should focus on the Lead measure because it is more relevant and can easily be controlled and changed by getting feedback on how we’re doing.\nKeep a compelling Scorecard that tracks your performance. This will reinforce attention and drives more motivation towards the wild goal.\nCreate Accountability. This can be done by a weekly review of the scorecard to celebrate achievements and investigate bad results to avoid in the future. Then plan for the next week to achieve a lead measure.\n\n\n\nBrain Downtime\nResearch has shown that some decisions are better left off to the unconscious. Such decisions are complex and require processing a lot of information to come up with solutions. The unconscious is known to have more neuronal bandwidth which means faster moving information around and sorting through complex tasks. On the other hand, consciousness is better left to tasks that have more strict rules such as mathematics. Therefore, it is recommended to have brain downtime to help diversify the work between conscious and unconscious.\nAttention is finite. This means if we exhaust this resource we would struggle to concentrate. Walking in nature leads us to use less directed attention than walking in the city. The same can be said when we watch TV where we use some of the directed attention that will reduce our ability to focus for long hours. Also, walking through nature has enough stimuli that help replenish directed attention resources and boost concentration. Any activity that frees us from directed attention would help boost concentration such as prayers/meditation/conversation with a friend, etc. We can conclude to make sure to not engage in work-related stuff in the evenings because it will prevent us from restoring directed attention and may not help us focus the next day.\nResearch has also shown that there is a limit to how long a person can spend in deep work. It can start from an hour for beginners to 4 hours as the limit for the experts. This means evening time work is beyond the point where we’re alert to perform cognitively demanding work (and deep work as well). It would usually be a slow pace work. Therefore, we may not be missing much in the evening downtime\nDevelop a habit to help with brain shutdown that should be done by the end of the day. It can be something like this:\n\nCheck email and make sure nothing urgent hasn’t been addressed.\nLook at the to-do list and go over all items to make sure every item has a plan in place. Helps in avoiding surprises when meetings/deadlines are approaching.\nPlan (roughly) for the next day.\n\nThis will help brain idleness and be ready to Shutdown\n\n\n\nRULE II: Embrace Boredom\nJust like any other habit/muscle, the ability to focus and goes into deep work requires practice and training that gets better with time.\nConstant attention switching has a lasting negative effect on our brains. Our working memory will always have irrelevant stuff for the current task at hand and it becomes harder to filter out the irrelevancy. Therefore, even if we try to focus it will not work because our brain is trained to handle any minute of boredom. For example, if we are in line waiting for something, the brain feels relieved when we glance at our smartphones. This prevents us from being laser-focused even if we set time to practice concentration.\nSince deep work is a skill (not a habit) that needs training, we need to address two goals:\n\nImproving our ability to concentrate intensely\nOvercoming the desire for distraction\n\nTo succeed in deep work, we must train our brains to be comfortable resisting distracting stimuli and tolerate boredom. The way to help us achieve that is by scheduling Internet Blocks where we are allowed to use the internet only when the time arrives for each block. Whatever the temptation is, we should not use the internet outside of the scheduled blocks. This will help the brain regain attention autonomy.\nTo push the level of intensity of concentration, every week identify a deep task that needs to be completed and is a high priority on the list. Then commit to finishing the task in a time less than what usually takes you on average. So, for example, if the task requires 10 hours to be done, commit to finishing it in 9 hours. This will push you to work intensely to get it done - no emails, no social media. When approaching the task, attack it with every neuron to get it done before the deadline. As weeks go by, the intensity would go up (to 8 hours then 7 then 6 etc.) and that helps the brain to work more intensely. Try this experiment at least once a week.\n\nProductive Meditation\nUse the time that you are physically occupied but mentally free to practice productive meditation. For example, use the commute time to think about one of the problems you’re facing professionally and think about it during the commute time. Every time you get distracted, you must get back to the main problem you defined at first. This will help you think deeply by strengthening the distraction-resisting muscle and pushing your focus deeper and deeper on the problem which will in turn sharpen your concentration. We can schedule a walk during lunch to help achieve the same results. At the beginning of adopting such a strategy, you may not see valuable results but surely after a week or so it would kick in and should start reaping the benefits of this strategy.\n\n\nAvoid\n\nDistractions that come in the form of doing something also important but not related to the problem you defined to dig deep into. Always return to the main problem.\nLooping is where the brain tries to avoid thinking about the hard part of the problem and instead focuses on what you already know about the problem which is not helpful. The brain does that to avoid spending too much energy on the hard part and wants the easy way.\n\nStructure your deep thinking by first introducing and storing the variables of the problem. Then identify the next question that you need to answer. Finally, consolidate the gains by reviewing the process that you followed to solve the question. This will improve the ability to go deep in later sessions.\n\n\nMemory Training\nResearch has shown that mental athletes who train their memories to memorize as much as they can develop their cognitive ability of attention control, i.e. maintain their focus on essential information. This will have a side effect on the general ability to concentrate. Memorizing Quran can act as a tool for memory training where you will get the best of both words - memorizing the Quran (good deeds) and improving the ability to go deep and concentrate.\n\n\n\nRULE III: Quit Social Media\nNetwork tools such as Facebook and Instagram are addictive and the algorithms that curate the incoming contents that come your way are designed to be optimized for engagement to increase returns by making you exposed to more stuff and spend more time on the platform. These tools compete with other tasks on your plate to acquire your attention and thus reducing your ability to focus and concentrate on the important tasks.\nYou can find a benefit from using such tools as connecting to old friends on Facebook. The majority of people who use such tools name very few benefits to use them; however, that is not enough for you to use. The any-benefit approach where a person uses any tool that has any benefit is not good because we should weigh the pros and cons of these tools and then decide if we should use them or not. And definitely, the cons outweigh the pros for the majority of us.\nTo help figure out which tools are important to you, list very few important goals both personally and professionally. Second, list the activities that help you achieve those goals. Then evaluate each tool based on its contributions to those activities. Finally, only adopt a tool if it has a huge positive impact that outweighs its negative impact. Most of the time, the 80/20 rule applies here. In other words, 20% of the activities/tools lead to 80% of the positive impact and contribution in achieving your goals.\n\n\nRULE IV: Drain the Shallows\nPerformance psychologists found that a person’s limit of deep work varies between 1 hour (novice) to 4 hours (familiar with the field) but rarely more.\nShallow work is unavoidable in many situations; otherwise, you would lose your job if you cut it out completely. The objective here is to try to keep it to a minimal level that keeps you going but not consuming much of the energy that you should spend on deep work.\nPlan every minute of your workday. This can be done either in the morning or the previous night. Divide the workday into blocks where each block is at least 30 minutes. Be flexible for anything that comes up that leads to changes in the schedule because this will happen daily. Even if a task took longer than planned or you came up with a creative idea/insight, you can adjust the schedule. Keep refining the schedule during the day and don’t feel that you have to follow the schedule because the essence of planning out the day is to have control over your day and don’t let shallow works dominate your day.\nShallow work - non-cognitively demanding, logistical-style tasks, usually performed while distracted.\nTo determine the scale of each task on the shallow-to-depth scale, determine how long a recent smart college graduate needs (in months) to train to do the task. The lower the number of months the shallower the task. Therefore, try to stay away from tasks that are shallower and prioritize the deeper tasks.\nGood rules of thumb for using email:\n\nMake people who send you e-mails do more work.\nDon’t respond - it is the sender’s responsibility to convince the receiver that a reply is worthwhile.\nDo more work when sending or replying to emails."
  },
  {
    "objectID": "books-summaries/deep-work/index.html#conclusion",
    "href": "books-summaries/deep-work/index.html#conclusion",
    "title": "Deep Work: Rules for Focused Success in A Distracted Life",
    "section": "Conclusion",
    "text": "Conclusion\nDeep work is a skill that gets valuable things done. It helps prioritize the important and cognitively demanding tasks over shallow tasks that are not valuable both intellectually and professionally. As with any other skill, it requires training and careful attention for continuous improvement. Deep life, of course, is not for everyone. It requires hard work and leaving stuff behind to live a more productive and meaningful life that can contribute to changing the world for the better and pushes you to the limit of your cognitive abilities."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#introduction",
    "href": "posts/git/Advanced-Git.html#introduction",
    "title": "Git Confident",
    "section": "Introduction",
    "text": "Introduction\nGit is a distributed version control system that thinks/stores its data as a series of snapshots (not delta). Each commit is a snapshot for the state of the system at the time of the commit. For files that haven’t changed, Git doesn’t store the file again but uses a pointer to the previous identical file that it stored before. It also lets us do almost all operations locally.\nEverything in Git is checksummed before it is stored in its object store using SHA-1 hash. SHA-1 hash returns 40 hexadecimal characters. All objects are referred to by their checksummed because Git is content addressable filesystem. This means that Git notice any changes to the files it tracks by comparing the checksummed of the stored version vs the current version.\nAll actions in Git only add data to the object store (Git database). Therefore, it is almost impossible to not undo any operation especially if we regularly push our Git database to other repository such as Github.\nGit has three states:\n\nModified: file changed but not yet committed.\nStaged: marked changed file to go to next commit snapshot. Staging area is a single file that is typically called “index”, which stores information about what will go into our next commit snapshot. When we run git add file, Git does the following:\n\nComputes checksum of the file and store the SHA-1 value in index file\nCompress the contents of the file and store it in .git directory under objects where the first two characters of the checkum would be the name of the directory and the next 38 characters would be the name of the file\nAdd the checksum to the index file (staging area)\n\nCommitted: store data (snapshot) in the database. The snapshot is represented as tree for root directory of the Git project. When we run git commit, Git does the following:\n\nIt computes checksum of each subdirectory until we end up with the root directory.\nStores them as tree objects in Git repository\nFinally, Git create a commit object and store it in the Git repository with the following metadata:\n\nDate\nAuthor name\nCommitter name\nCommit message\nParent(s) commit. First commit would have no parents. Following commits may have 1 parent or more parents in the case of merges\nPointer to the root project tree\n\n\n\n.git directory which is at the root directory of the project has all the metadata for Git project such as the database (object store).\nFiles can be in two states:\n\nUnTracked: files that Git doesn’t know about. They are files that are neither in any snapshot nor in staging area. Therefore, they don’t have modified/unmodified states.\nTracked: files that were in last snapshot or in staging area. They have all states mentioned above."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#git-object-model",
    "href": "posts/git/Advanced-Git.html#git-object-model",
    "title": "Git Confident",
    "section": "Git Object Model",
    "text": "Git Object Model\n\nGit stores everything in .git directory. So deleting this directory will basically delete the whole history and can’t be recovered.\nGit stores all of its representations using objects directory. Object can be: blob or tree or commit.\nGit use sha1sum to get the hash value of each object. It is 40 hexadecimal characters (160 bits).\n\nGit uses the first two characters for the name of directory for the object and the other 38 characters for the object itself.\nGit stores objects based on their hash values (content addressable storage).\nGit compresses the contents using zlib\n\n// a file is a bunch of bytes\ntype object = blob | tree | commit\nobjects = map&lt;sha1sum(object), object&gt;\ndef store(obj):\n  id = sha1sum(obj)\n  objects[id] = obj\n  return\n// a directory contains named files and directories\ntype tree = map&lt;string, tree | file&gt;\ndef load(id):\n  return objects[id]\n\n\n!ls -al ../../.git\n\ntotal 48\ndrwxr-xr-x  15 imad  staff   480 Nov  5 09:08 .\ndrwxr-xr-x@ 12 imad  staff   384 Nov  5 06:56 ..\n-rw-r--r--   1 imad  staff    15 Mar 17  2020 COMMIT_EDITMSG\n-rw-r--r--   1 imad  staff    23 Feb 12  2020 HEAD\ndrwxr-xr-x   2 imad  staff    64 Feb 12  2020 branches\n-rw-r--r--   1 imad  staff   455 Mar 17  2020 config\n-rw-r--r--   1 imad  staff    73 Feb 12  2020 description\ndrwxr-xr-x  13 imad  staff   416 Feb 12  2020 hooks\n-rw-r--r--   1 imad  staff  3913 Nov  5 09:08 index\ndrwxr-xr-x   3 imad  staff    96 Feb 12  2020 info\ndrwxr-xr-x   4 imad  staff   128 Feb 12  2020 logs\ndrwxr-xr-x   3 imad  staff    96 Mar 17  2020 modules\ndrwxr-xr-x  94 imad  staff  3008 Mar 17  2020 objects\n-rw-r--r--   1 imad  staff   114 Feb 12  2020 packed-refs\ndrwxr-xr-x   5 imad  staff   160 Feb 12  2020 refs\n\n\n\n!ls -a ../../.git/objects\n\n.    0c   1a   26   37   43   50   62   72   8b   9e   b1   c5   d1   e0   fc\n..   0f   1d   29   38   45   52   63   73   8e   a2   b2   c7   d2   e1   ff\n00   12   1f   2a   39   49   58   64   75   91   a5   b3   ca   d3   eb   info\n03   13   20   2b   3f   4a   5a   67   77   96   a9   b4   cc   d8   ee   pack\n07   17   21   2c   40   4b   5f   68   7f   98   aa   b6   ce   dc   f0\n08   19   22   33   42   4d   61   6f   8a   9d   b0   b8   d0   dd   f8\n\n\n\n!ls -Ral ../../.git/objects/ee\n\ntotal 8\ndrwxr-xr-x   3 imad  staff    96 Mar  4  2020 .\ndrwxr-xr-x  94 imad  staff  3008 Mar 17  2020 ..\n-r--r--r--   1 imad  staff   166 Mar  4  2020 5941ab3c125a3a669370d96cd5cb8496f8acde\n\n\n\n!git cat-file -p ee5941ab3c125a3a669370d96cd5cb8496f8acde\n\n100644 blob b6e47617de110dea7ca47e087ff1347cc2646eda    .gitignore\n100644 blob 261eeb9e9f8b2b4b0d119366dda99c6fd7d35c64    LICENSE\n100644 blob 58da9de606d62625c379fea5ca020d19d958fb18    README.md\n040000 tree 6fea6c3802fd1cf83bf19bfc2302da6b79638ab5    missing-cs-semester\n\n\n\nBlobs\n\nblobs are binary large objects which stores only the context of the file; not its name (array of bytes).\n\ntype blob = array&lt;byte&gt;\n\nThe type of file which is “blob”, the number of characters in it, the separator character, and the actual content are passed to the sha1sum to get the hash value.\nSince Git does not store the name of the file or any of its metadata, if you have two files with the same content then Git only stores it once.\n\n\n!git cat-file -p 58da9de606d62625c379fea5ca020d19d958fb18\n\n# software-engineering\nMaterials for software engineering.\n\n\n\n!wc ../../README.md\n\n       2       6      59 ../../README.md\n\n\n\n%%bash\ncat &lt;(echo -e \"blob 60\\0\") ../../README.md\n\nblob 60\n# software-engineering\nMaterials for software engineering.\n\n\n\n# # Since todo.md and todo2.md are identical, Git saves ONLY one copy\n# 100644 blob b3dfa8b0b7c73f2c7156dfc69c737d05f2f900c3    file.txt\n# 100644 blob c1ee9d5404109b66f21fa193da635aa8c4f04c47    todo.md\n# 100644 blob c1ee9d5404109b66f21fa193da635aa8c4f04c47    todo2.md\n\n\n%%bash\ncat &lt;(echo -e \"blob 58\\0\") ../../README.md | shasum\n\n5ee782528aa7bc3d388c33339962f6fce514b39e  -\n\n\n\n\nTrees\n\nTree is a recursive data structure that contains other trees/blobs; i.e. it contains a list of pointers to other trees/blobs. In this context, tree is a directory. Therefore, the root directory is the main directory that has .git as its subdirectory. Each line in the tree object’s file contains a pointer (the object’s hash) to one such object (tree or blob), while also providing the mode, object type, and a name for the file or directory.\n\n// a directory contains named files and directories\ntype tree = map&lt;sha1sum(tree | file), tree | file&gt;;\n\nIt maps strings (hash values) to objects. So if a directory is empty, Git does not add it as untracked change until we add a file or a directory to it because empty directory has nothing to map stuff to. Therefore, to track empty directories, we can add .gitkeep to the directory if it is empty to be able to track it.\nWe pass all objects (not their contents) to get the hash value.\nTree objects themselves do not have names, much like blobs. Parent trees associate names for subtrees, and the root tree, referred to as the “working tree” of a repository, in fact has no name. This has two fun characteristics:\n\nThe repo doesn’t care what you call it. You can rename your local directory that contains your repository to anything you’d like. Git is blissfully unaware of the name of the directory that contains the .git repo directory.\nWe can rename subtrees as much as we want, and only parent objects need to update. The subtree object itself and everything below remain untouched.\n\nTrees summary:\n\nTrees list out the contents of a directory (blobs and subtrees)\nFor each object, the mode, permissions, type, hash, and name is listed\nTree objects must contain at least one blob or tree; otherwise, it won’t be tracked\nTrees can be nested to any depth\nTrees, like blobs, don’t store names. The names are stored in parent trees. Therefore, changing names of subtrees only change the names in the parent tree. Therefore, since root directory has no parent, changing its name doesn’t have any effect on git\nTrees are named and stored in the objects directory by hashing their contents (the list of objects described above)\n\n\n\n# master is a branch that points to a commit which also points to a tree\n!git ls-tree master\n\n100644 blob ced9612a7d927cdb23d0ba2de47679504b0c9fc3    Command-Line-Environment.ipynb\n100644 blob 68fe60e479d316b7f40f194a8d3400e7f5c8af60    Data-Wrangling.ipynb\n100644 blob dcee12ed0ae6096339dc40a70c5ff67a2afdec31    Debugging-And-Profiling.ipynb\n100644 blob 40582a6d5d23028acc251c54f6e124ce9f2ec5ba    Petpouri.ipynb\n100644 blob 6405b8d2b26e17020b12f98639148615d6c9baea    Plan.ipynb\n100644 blob f0aca55804924d43eb3d687ebc9a780f3b8baff3    Security-And-Cryptography.ipynb\n100644 blob 035f52d11f6126cac575b899f6ff0011060aeddd    Shell-Scripting.ipynb\n100644 blob a2235c5c9a32920e7f15c3bf63afde41e60c4e52    Version-Control(Git).ipynb\n100644 blob 395d086c29d15560f0b3eee28c0489afe1b6de8e    Vim-Tutor-Summaries.ipynb\n100644 blob 9d756b15f398735d1bd414fd97afa5f709db06f5    basic.png\n100644 blob 50daf1bb695821f251fbc44880413ca17ca8a8b6    commit_history.png\n100644 blob 43d8b8f1031c6c81f0153e0616be7576de6401f9    pycallgraph.png\n100644 blob 5a0fa1a6cb3918e9c2d316433edfd5ccd275bd59    vim-tutorial.md\n\n\n\n\nCommits\n\n\n\n\ncommits contain parent, message, author, commiter, and current tree. Therefore, it is a file like any other object.\n\n// a commit has parents, metadata, and the top-level tree\ntype commit = struct {\n    parent: array&lt;commit&gt;;\n    author: string\n    message: string\n    snapshot: tree\n}\n\nIt’s worth noting that the commit object only contains a single reference to a working directory; Git doesn’t store diffs. When diffing between two commits, it compares the working trees of the commits, computing the diff on demand.\n\nGit only stores the delta changes between commits and not everything. It also point to blobs/trees that have not been changed using old commits and don’t store them again for new commits. Therefore, if a file has not been changed from previous commit, the hash value for that commit is the same so its address is still the same -&gt; keep the same pointer.\n\nReferences are nothing but pointers to commits. They are stored under .git/refs directory as files where each file contains the hash value of some commit. Since it is a hassle to always refer to objects by their 40 hexadecimal string, we can use references to refer to objects. Contrary to objects, references are mutable. For example, master always refers to the latest commit in the main branch. HEAD refers to where we currently are in the history which will be used when creating new snapshot by making the parent for this commit the HEAD and then update HEAD.\nreferences = map&lt;string, commit&gt;;\n\ndef update_reference(name, id):\n    references[name] = id\n\ndef read_reference(name):\n    return references[name]\n\ndef load_reference(name_or_id):\n    if name_or_id in references:\n        return load(references[name_or_id])\n    else:\n        return load(name_or_id)\n\n!tree -C -L 1 ../../.git\n\n../../.git\n├── COMMIT_EDITMSG\n├── HEAD\n├── branches\n├── config\n├── description\n├── hooks\n├── index\n├── info\n├── logs\n├── modules\n├── objects\n├── packed-refs\n└── refs\n\n7 directories, 6 files\n\n\n\n!tree -C -L 1 ../../.git/refs/\n\n../../.git/refs/\n├── heads\n├── remotes\n└── tags\n\n3 directories, 0 files\n\n\n\n!tree -C -L 1 ../../.git/refs/heads/\n\n../../.git/refs/heads/\n└── master\n\n0 directories, 1 file\n\n\n\n%cat ../../.git/refs/heads/master\n\ne1d95ecb4c02e0c30a8635a37631c523d2041299\n\n\n\n!git cat-file -t e1d95ecb4c02e0c30a8635a37631c523d2041299\n\ncommit\n\n\n\n!git cat-file -p e1d95ecb4c02e0c30a8635a37631c523d2041299\n\ntree 75407c234b245d258c809de234e030f57dd98148\nparent 429137bbf1334dcea2719458bcc3a323cd829ecd\nauthor Imad &lt;imad.dabbura@hotmail.com&gt; 1584462760 -0500\ncommitter Imad &lt;imad.dabbura@hotmail.com&gt; 1584462760 -0500\n\nReview all nbs\n\n\nHEAD, unlike the other objects we’ve discussed, is a singleton, meaning that there is only ever one HEAD. It identifies the currently checked out object. Typically, this is a branch (with that branch pointing to a commit), but it is possible to check out a commit directly, in which case HEAD would be pointing at that commit.\nHEAD is a file just like our branch objects. It lives at the root of the .git directory and its contents are similarly simple.\n\n%cat /Users/imad/Desktop/git-repo/.git/HEAD\n\nref: refs/heads/master\n\n\n\n%%bash\ncd ~/Desktop/git-repo/\ngit graph2\n\n* 6e9c688    (HEAD -&gt; master, tag: v.0.1, test) Renaming (Imad)\n* ff9e667    Add test1 dir (Imad)\n* e503b6e    Add test dir (Imad)\n* 8610c78    Add copied file (Imad)\n* ed738c9    (feature) Rebased all commits (Imad)\n* 2050b90    Add host to file (Imad)\n* 91eacf5    Add host (Imad)\n* ed27259    patch commit (Imad)\n* ff2d260    third commit (Imad)\n* 6dd0c14    Change second commit (Imad)\n* c2b7166    first commit (Imad)\n\n\n\n%%bash\ncd ~/Desktop/git-repo/\ngit checkout 8610c78\n\nNote: checking out '8610c78'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b &lt;new-branch-name&gt;\n\nHEAD is now at 8610c78 Add copied file\n\n\n\n%cat /Users/imad/Desktop/git-repo/.git/HEAD\n\n8610c78113fe423b20a9f84d485b49af5ad089b0\n\n\n\n%%bash\ncd ~/Desktop/git-repo/\ngit graph2\n\n* 6e9c688    (tag: v.0.1, test, master) Renaming (Imad)\n* ff9e667    Add test1 dir (Imad)\n* e503b6e    Add test dir (Imad)\n* 8610c78    (HEAD) Add copied file (Imad)\n* ed738c9    (feature) Rebased all commits (Imad)\n* 2050b90    Add host to file (Imad)\n* 91eacf5    Add host (Imad)\n* ed27259    patch commit (Imad)\n* ff2d260    third commit (Imad)\n* 6dd0c14    Change second commit (Imad)\n* c2b7166    first commit (Imad)\n\n\n\n\nSummary\n\nObjects: blobs, trees, and commits\nRefs: branches, tags, and remote branches\nHEAD: The single pointer to rule them all"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#branches",
    "href": "posts/git/Advanced-Git.html#branches",
    "title": "Git Confident",
    "section": "Branches",
    "text": "Branches\n\nheads aka branches (because it is a collection of HEADs for each branch in Git repo) are nothing but pointers to commits. They are very simple objects, they only contain hash value of the commit they are pointing to. Therefore, creating a branch is just creating a file in refs/heads with the name of the branch that has the commit of the HEAD of that branch. At the beginning, this file will have the commit the HEAD points to from the branch you were on when created the branch.\nWhen you switch branches, Git resets your working directory to look like it did the last time you committed on that branch. It adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit.\nMerging:\n\nIf we are merging a feature branch into master branch and the feature branch is directly ahead of master where master’s last commit can be reached following feature branch commit’s history, Git will do fast-forward merge, which means it just updates the pointer to point forward.\nOtherwise, if head of master branch isn’t direct ancestor of feature branch, Git does three-way merge by using 3 commits:\n\nCommon ancestor commit\nLast commit from master branch and feature branch\nCreates a new snapshot with new commit object (merge commit) that points to two parents: last commit from master and last commit from feature branches\n\nIf we have merge conflict, we can either abort the merge or resolve the merge conflist ourselves. Once we resolve the conflicts in all files, we should stage those files and then commit the changes. This would be the merge commit. We can use mergetool to resolve merge conflicts such as vimdiff.\n\ngit branch -v will show last commit of all branches\ngit branch --merged show all branches that were merged with the current branch we are on. git branch --merged master show all branches that were merged with master branch.\ngit branch --no-merged does the opposite.\nWe can’t delete a branch if it has work that we haven’t merged with master branch. We can force delete using -D flag.\nWe can rename a branch, but we should do it both locally and on the remote server. It is recommended to avoid renaming master branch because it would break integrations/scripts/etc. and requires a lot more work.\n\nLocally:\n\ngit branch --move oldname newname\n\nRemote:\n\ngit push --set-upstream origin newname\ngit push origin -d oldname\n\n\nheads are for local branches."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#remote-branches",
    "href": "posts/git/Advanced-Git.html#remote-branches",
    "title": "Git Confident",
    "section": "Remote Branches",
    "text": "Remote Branches\nRemote Branches are the same as local branches. They are again files that point to commits.\n\nWe can have multiple remotes where each one has its own branches. origin is the (default) main one typically used for the upstream (we can change it to other names when cloning a repo such as git clone URL -o anothername. git remote -v would list all the remotes for the repository. We can add remotes git remote add remote_name remote_url\nAll the remote branches under remotes/origin/ will be updated ONLY when communicating with the remote server. Such branches act more as bookmarks and can’t be changed by any Git commands to point to different commits directly.\nLocal branch is called Tracking Branch if it tracks a remote branch (called Upstream Branch)\n\ngit checkout branchname would create tracking branch that tracks default remotename/branchname if branchname doesn’t exist and exactly matches one upstream branch names.\nWe can have local branches track branches from different remotes: git checkout -b remotename/remotebranch which would create local branch named remotebranch that tracks remotebranch on remotename server. We can have different name for our local branch as git checkout -b localbranchname remotename/remotebranch.\nIf we already have a local branch, we can use git branch --set-upstream-to=remotename/remotebranch to make current branch track remotebranch on remotename server\nIf I am on a tracking branch and run git pull, it knows which server to fetch from and which branch to merge in\n\ngit fetch download the changes from all branches from remote to local repository without merging them. We should do the merge ourselves such as git merge origin/branchname\ngit pull download and merge the changes from remote to local branches.\ngit remote show remote_name will show everything in details about the remote_name such as URL, local/remote branches, etc.\nWe can rename/delete remotes as git remote rename/remove remote_name. If we delete remote, it deletes all config/settings related to the deleted remote. Renaming would rename branches.\nRemote references are read-only, which means we will never update them using git commit but Git manages them as bookmarks.\nBy default, Git fetches all references from remote to heads -&gt; All branches. We can change this behavior on the command line when running git fetch remote_name remote_branch:refs/remotes/remote_name/branch_name\nPushing local branch to remote can be done in different forms:\n\ngit push origin branchname\ngit push origin localbranchname:remotebranchname which lets us have a different name on the remote server for our local branch\n\nWe can delete remote branch git push origin --delete branchname\n\n\n!tree -C ../../.git/refs/remotes/\n\n../../.git/refs/remotes/\n└── origin\n    ├── HEAD\n    └── master\n\n1 directory, 2 files\n\n\n\n%cat ../../.git/refs/remotes/origin/*\n\nref: refs/remotes/origin/master\ne1d95ecb4c02e0c30a8635a37631c523d2041299"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#tags",
    "href": "posts/git/Advanced-Git.html#tags",
    "title": "Git Confident",
    "section": "Tags",
    "text": "Tags\n\nTags are like branches, they too point to a commit and stored under .git/refs dir in tags dir. They are basically files that have the commits they are pointing to.\nTags can be created simply by git tag version_no. We can also create more complex tags by adding annotations, PGP signature, and other metadata. In this case, they will be stored in refs/objects dir and the tag will simply be the hash value of the tag object (which will also contain the hash of the commit that was tagged).\n\nAnnotated tags, however, are stored as full objects in the Git database. They’re checksummed; contain the tagger name, email, and date; have a tagging message; and can be signed and verified with GNU Privacy Guard (GPG). It’s generally recommended that you create annotated tags so you can have all this information.\n\nWe can also tag previous commits by specifying their hash abbreviation: git tag -a v1.0 ca21323\ngit push doesn’t transfer tags to remote server, we have to explicityly push tags: git push origin v1.0\ngit tag to list all tags\ngit tag -l pattern to look for tags that match specific patters\nWe can checkout tags to inspect files from that version: git checkout tagname. Any changes that are made and committed wouldn’t belong to any branch and be unreachable unless we use exact commit hash. Therefore, to fix issues, create new branch from tag and do the changes.\nThe difference between tags and branches is that branches evolve over time; however, tags point to fixed commit in repo’s history.\nWe can delete tags:\n\nlocally: git tag -d tagname\nremote: git push remote_name -d tagname OR git push remote_name :refs/tags/tagname\n\n\n\n!tree -C /Users/imad/Desktop/git-repo/.git/refs/\n\n/Users/imad/Desktop/git-repo/.git/refs/\n├── heads\n│   ├── feature\n│   ├── master\n│   └── test\n└── tags\n    └── v.0.1\n\n2 directories, 4 files\n\n\n\n!cat /Users/imad/Desktop/git-repo/.git/refs/tags/*\n\n6e9c6886a2180fdfde291a130aa9e10a52bac679\n\n\n\n%%bash\ncd ~/Desktop/git-repo/\ngit graph2\n\n* 6e9c688    (HEAD -&gt; master, tag: v.0.1, test) Renaming (Imad)\n* ff9e667    Add test1 dir (Imad)\n* e503b6e    Add test dir (Imad)\n* 8610c78    Add copied file (Imad)\n* ed738c9    (feature) Rebased all commits (Imad)\n* 2050b90    Add host to file (Imad)\n* 91eacf5    Add host (Imad)\n* ed27259    patch commit (Imad)\n* ff2d260    third commit (Imad)\n* 6dd0c14    Change second commit (Imad)\n* c2b7166    first commit (Imad)"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#cloning-repository",
    "href": "posts/git/Advanced-Git.html#cloning-repository",
    "title": "Git Confident",
    "section": "Cloning Repository",
    "text": "Cloning Repository\ngit clone https://github.com/UserName/RepoName would do the following:\n\nCreate a directory called RepoName\nCreate a directory called .git inside RepoName\nPull down all versions for every file for the history of the project\nCheck out the latest version\n\nAs a result, if initially a huge file was committed but then deleted years ago, cloning will pull down the huge file even if such file is never needed again. Therefore, if a project has a long history, we may not need to clone all history and restrict to last N days."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#ignoring-files",
    "href": "posts/git/Advanced-Git.html#ignoring-files",
    "title": "Git Confident",
    "section": "Ignoring Files",
    "text": "Ignoring Files\n.gitignore hosts all patterns that Git should ignore and not track. It is typically located at the root directory of the project and applies recursively to all subsdirectories; however, we can have .gitignore in subdirectories that only gets applied specifically to those subdirectories.\nThe rules for the patterns you can put in the .gitignore file are as follows:\n\nBlank lines or lines starting with # are ignored.\nStandard glob patterns work, and will be applied recursively throughout the entire working tree. Example:\n\n*.log ignores all files that end with log recursively\ndoc/*.txt ignores all .txt files under doc\ndoc/**/*.pdf igores all pdf files in the doc directory and all its subdirectoris\n\nWe can start patterns with a forward slash (/) to avoid recursivity. Example: /TODO ignores TODO in the current directory.\nYou can end patterns with a forward slash (/) to specify a directory. Example: build/ ignores all files under build in all directories.\nYou can negate a pattern by starting it with an exclamation point (!). Example: !test.log tracks test.log"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#general",
    "href": "posts/git/Advanced-Git.html#general",
    "title": "Git Confident",
    "section": "General",
    "text": "General\n\ngit rm would remove a file from working tree and stage it. If we ever staged files by mistake, we could run git rm --cached filename to remove it from staging area and keep it on hard drive especially if we don’t want Git to track it.\ngit mv both change the file name and stage it\ngit reflog remember all actions taken in a repository (even intermediate steps such as creating branches, clone, pull, etc.) and not just commits. It is local to your copy of the respository and others who have the same copy of the repoository would have their own version of reflog. It also starts empty after we clone the repository. Therefore, it is more like shell history. We can always get back to some state.\n\ngit reflog show hash_value will show all the actions happened for the hash.\n\ngit diff to see the changes in the working tree compared to the index\ngit diff --cached to see the changes compared to the last commit\ngit difftool shows the changes in external tools such as vimdiff\ngit add -i for interactive staging to control which files to stage and which parts of the files (patches) to stage all interactively. This is very helpful if we have done a lot of work on many files without staging anything. We can add/checkout/restore/stash patches (parts of files) by adding --patch or -p flag to their corresponding git command.\ngit commit -m \"test\" When we commit, It involves at least a change to one blob. This will lead to a creation of new tree with current state of the code that reflect the changes. Git then creates commit object that will point to the new tree. Finally, it will update the current branch to point to the newly created branch.\ngit merge --ff-only branch-name This kind of merge creates no objects, It just updates the current branch to a different commit.\ngit merge branch-name In contrast to the fast forward merge, Git creates a new tree by trying its best to combine two divergent branches. It then creates a new commit that point to the newly created tree and the parent would be the two commits; latest commit from each branch. This is what merging using pull requests does on Github. It may not be preferable because the actual code changes during merge when Git tries to combine all of them and we may end up with conflict."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#stashing",
    "href": "posts/git/Advanced-Git.html#stashing",
    "title": "Git Confident",
    "section": "Stashing",
    "text": "Stashing\nIt is very helpful when we staged some work and/or have modified tracked files and want to jump to different branch to work on something else. By default, Git stashes only modified and staged tracked files but not untracked files. We can add -u to add also untracked files.\n\nWe can run git stash to stash the work\ngit stash list to list all the stashes\ngit stash apply to apply last stash OR git stash apply stashname. This keeps the stash on the stack\ngit stash drop to remove a stash\ngit stash pop to apply and remove last stash in one command\n\nWe can apply stashes from one branch on another branches.\nTo avoid issues/merge conflicts when trying to apply stashes, it may be helpful to create new branch and apply stash in the new branch. This can be done by git stash branch newbranchname. This will create new branch, checkout last commit you were on, apply stash, and then drop the stash."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#managing-history",
    "href": "posts/git/Advanced-Git.html#managing-history",
    "title": "Git Confident",
    "section": "Managing History",
    "text": "Managing History\n\ngit log has all the information we need to the repo’s history.\n\ngit log --all --decorate --graph --oneline is great to get an overview and see the divergence of branches\ngit log -n will limit the log to the top n\ngit log --oneline file is useful to get an overview of the log of one file\ngit log --pretty=format:'%C(yellow)%h%C(reset) - %an [%C(green)%ar%C(reset)] %s to change the format of the log output\ngit log -E -i --grep regexp will do extended search the logs for the regexp phrase; case insensitive\ngit log -S term will search for changes related to that term in the code base (addition/deletion). Check this post\ngit log -G regexp will search for changes related to the regexp in the code base; but looks for patterns not literal string.\n\ngit show commit will show everything that happened with that commit including diff\ngit blame file is useful to know who did what to the file and when especially if we want to trace who introduced some bug/logic to the codebase. Use -L to restrict to specific lines. Use -C to detect if block of lines were copied from other files that were in the same commit."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#bisect",
    "href": "posts/git/Advanced-Git.html#bisect",
    "title": "Git Confident",
    "section": "Bisect",
    "text": "Bisect\nGit Bisect is useful to trace when a bug is introduced to get the commit that introduced the bug especially if the commit was pretty far in the history. It does binary search between the commit that you believe was good (no bug) and the current commit or any commit that we know has the bug. Below are a typical workflow:\n\ngit bisect start to start the binary search\ngit bisect bad which means current HEAD is the bad commit which would be last commit in the range of commits of the binary search\ngit bisect good commit which tells Git that the provided commit didn’t have the bug and would be the first commit in the range of commits of the binary search\nWe can use the three commands in one command git bisect start badcommit goodcommit\nFrom here, we interactively run either git bisect good to tell Git the given commit is good so it does binary search from next commit to the last commit OR git bisect bad to tell Git that the given commit is bad and the next binary search stops at the commit before it. We keep doing this until we arrive at the commit that introduced the bug.\n\nWe can also use a script that runs tests for us to check whether a commit is good or bad and automate the whole process:\n\ngit bisect start badcommit goodcommit\ngit bisect run test-script.sh OR git bisect run make rule OR git bisect run pytest. For each commit, git bisect runs the script or command on the checked out commit. If it returns 0 -&gt; good; otherwise, bad."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#submodules",
    "href": "posts/git/Advanced-Git.html#submodules",
    "title": "Git Confident",
    "section": "Submodules",
    "text": "Submodules\nGit submodules are Git repositories inside Git repository that allows us to track them and keep commit histories separate. Each submodule would be in different directory inside the project git repository.\n\nWe can add submodule by git submodule add URL. This will create a directory with the name of the Git repository (we can have different names using git submodule add URL name). If we run git status, we see that Git added the directory as special type of file as well as add a file named .gitmodules that has the path and the URL for each submodule. We need to commit those two files to include them in our main project history.\nIf we clone a project that has submodules, we can either pass --recurse-submodules to initialize and pull all contents of all submodules OR go in each submodule directory and run git submodule update --init (add --recursive if there are any nested submodules.\nTo pull out changes made to submodules, run git submodule update --remote submodule_name\ngit diff --submodule to get a nice diff for submodules"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#hooks",
    "href": "posts/git/Advanced-Git.html#hooks",
    "title": "Git Confident",
    "section": "Hooks",
    "text": "Hooks\nGit hooks are scripts that can either be client-side hooks that run for operations such as committing/merging or system-side hooks that run on network operations such as receiving pushed commits. All hooks are stored in .git/hooks directory. Git prepopulates any new Git repository with example hooks that end with .sample. To use such hooks, remove the extrension. We can write hooks in many languages such as Python but they have to be executable and can’t have any extension. Also, client-side hooks aren’t copied when the repository is cloned.\nBelow are the most common client-side hooks:\n\npre-commit: Runs before we type the commit message and abort if the return code is not zero. This can be used to run tests, check code style, check for documentation or whitespaces, etc.\nprepare-commit-msg: Runs before the commit message editor but after the default message is created.\ncommit-msg: Typically used to check if a commit message conforms to some predefined patterns.\npost-commit: Runs after the commit proccess is completed.\n\nThere are other client-side hooks such as pre-rebase, pre-merge, post-merge, etc.\nBelow are the most common system-side hooks:\n\npre-receive: Runs when handling a push from client. It can be used to check for things such as rejecting non-fast-forwards or access control.\nupdate: Similar to pre-receive but runs once for each branch the pusher is trying to update.\npost-receive: Runs after the entire push process is completed. It can be used to notify users or update services."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#resetting",
    "href": "posts/git/Advanced-Git.html#resetting",
    "title": "Git Confident",
    "section": "Resetting",
    "text": "Resetting\ngit reset HEAD|commit command allows us to:\n\nMove what the branch HEAD points to (stops if --soft and everything will be in the staging).\nMake the index look like HEAD (stops here if not --hard)\nMake the working directory look like the index\n\nIf we provide a path such as git reset filepath, it is a shorthand for git reset --mixed HEAD filepath and does the following:\n\nMove what the branch HEAD points to (skipped)\nMake the index look like HEAD; i.e. has the effect of unstaging the file\n\nIf we run git reset commit -- filepath, it will act as if we reverted the content of the file to what was in the commit and then ran git add on the file without changing working directory. The HEAD and the working directory would have the same version of the file. Therefore, running git commit will commit the changes back to what was in the commit leaving both index file and HEAD point to the same changes.\ngit checkout without paths is similar to git reset with two differences:\n\nreset moves the branch HEAD points to while checkout moves HEAD itself. For example, git checkout branch would change what HEAD is pointing to while git reset commit would change what branch points to.\ncheckout is working-directory safe where it tries to do a trivial merge but reset --hard will overwrite working-directory.\n\ngit checkout filepath is similar to `git reset –hard filepath -&gt; overwrite working directory."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#inspecting-commit-ranges",
    "href": "posts/git/Advanced-Git.html#inspecting-commit-ranges",
    "title": "Git Confident",
    "section": "Inspecting Commit Ranges",
    "text": "Inspecting Commit Ranges\n\n^ refers to the parent. HEAD^ means the parent of last commit in the current branch.\n~ refers to the first parent. HEAD~ means the first parent of last commit in the current branch. It will be different than ^ in the case a commit has multiple parents as is the case of merge commits that have multiple parents.\nHEAD~5 is equivalent in some sense to HEAD^^^^^.\nDouble dots (..): If we want to see the commits that are reachable from target branch (commit) but not the source branch (commit), we use git log sourcecommit..targetcommit.\nTriple dots (...): If we want to see the commits that are reachable by either of the branches (commits) but not from both of them, we use git log sourcecommit...targetcommit. This will return commits unique to sourcecommit and targetcommit but not common commit.\nMultiple points: If we want to see the commits for multiple points such as git log refA refB ^refC which means commits reachable from refA and refB but not C. Therefore:\n\ngit log refA..refB is equivalent to git log refB ^refA"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#grep",
    "href": "posts/git/Advanced-Git.html#grep",
    "title": "Git Confident",
    "section": "Grep",
    "text": "Grep\nGit grep allows us to search for a pattern in working directory, index, and committed tree. We can also search in older versions of the code such as using old tags/commits, which grep/ack tools can’t.\nThe most useful flags to use with grep is git grep -n -p --break --heading pattern optional_path optionalcommit."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#undoing",
    "href": "posts/git/Advanced-Git.html#undoing",
    "title": "Git Confident",
    "section": "Undoing",
    "text": "Undoing\nCommits are immutable. This means that even though we can fix some stuff related to commits, we can’t change the commits themselves. They will still be in the history. Therefore, anything that is committed in Git can almost always be recovered. Even commits that were on branches that were deleted or commits that were overwritten with an –amend commit can be recovered. However, anything you lose that was never committed is likely never to be seen again.\n\ngit commit --amend will open an editor to write a new commit message to the already committed changes.\n\ngit commit --amend -m \"message\" is a shorthand\ngit commit --amend --no-edit will add new files to the last commit; in case we forgot to add some files to that belong to the same commit\n\ngit reset HEAD file OR git restore --staged file will undo the staging of the file. This is helpful if we staged a file and then we need to change some things before committing.\nget checkout -- file OR git restore file will delete all the changes made to a file. We will never be able to get back the deleted changes.\nget reset --soft HEAD~2 This will remove the commits from the history and point HEAD to its grand parent. --soft here means to keep the changes in the current working directory and index file. Therefore, running git commit would commit the latest changes and make grand parent as the parent of changes (Squashing Commits).\nTo cancel the commit while writing the message, we can exit vim with :cquit which exits vim with error and git will get that error -&gt; won’t proceed in creating the commit."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#rebasing-history",
    "href": "posts/git/Advanced-Git.html#rebasing-history",
    "title": "Git Confident",
    "section": "Rebasing History",
    "text": "Rebasing History\n\ngit add file or git add --all or git add directory. This will add all changes made to a specific file/directory.\ngit add --patch Allows us to cherry pick the changes that we want to stage. This is useful if we want to split the changes we made to a specific file into different commits. When we run the command, we will interactively choose what we want to stage using shortcuts.\ngit diff/log HEAD..HEAD~2 will give us the diff/log for the range between two commits in history. We can either choose hash_values of commits or their references such as HEAD/master.\ngit reset --hard HEAD~1 will make HEAD point to its parent and remove last commit from log history. Note that the last commit is not completely removed, we see that with git reflog.\ngit cherry-pick origin/master..master will replay the commits with this range in another branch. This is useful when we commit to the wrong branch and we want to make those commits in another branch. We can use this command after we checkout the correct branch and run the above command. To remove the commits from the branch we first commit, we can use git reset --hard (even though the removed commits are still in our history).\ngit rebase master We want to take the work we’ve done on our feature branch, and reapply it as if it was done on top of the additional commits in our master branch. When performing the rebase, Git finds the commits unique to our branch and computes the diff off the changes they introduced, then moves to the target branch, master in this case, and one by one applies the diffs, creating new commits reusing the commit messages from our branch. Once done, it updates our branch to point at the newest of these commits created by reapplying the diffs.\nWhile we would never revise published history, specifically the master branch, we almost always revise our commits on feature branches before merging them in. We value a clean history, and the majority of the time, the commits in a feature branch contain many rounds of refactoring and PR reviews which we don’t want in the permanent history. Instead, we want the most direct and concise form of the history that fully captures the change we settled on in our feature branch after completing any refactoring or updates. Use git rebase -i master will allow us to do just that.\n\nWe can remove, reorder, squash, edit, and split commits using interactive rebase.\nGit applies and rewrite the changed commits and all the commits that follow the changed ones.\nIt is highly recommended to not change history if you already pushed it to the remote server unless we’re working on feature branch and are doing it to clean up history before merging and close the pull request.\nReording is simply reordering the commits shown in the editor.\nBe careful that the order of commits is reverse order. This means last commit will be last."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#packfiles",
    "href": "posts/git/Advanced-Git.html#packfiles",
    "title": "Git Confident",
    "section": "Packfiles",
    "text": "Packfiles\nThese are files that Git uses to combine files into single file to save space instead of having different versions of the same file taking all the space and only saves the original version with deltas where pack index file will have offsets that point to the object in the pack file. Git automatically runs this when we have too many loose files or when run git gc command or when we push to remote server."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#github-and-remotes",
    "href": "posts/git/Advanced-Git.html#github-and-remotes",
    "title": "Git Confident",
    "section": "Github and Remotes",
    "text": "Github and Remotes\n\nHub and Github CLI tool gh make it easy to interact with Github from the command line and integrate well with Git. Useful commands are compare, browse, and pull-request.\nTo share the code on a given branch using a URL that always point to the same code, we can press y to change the name of the branch with its hash that will always point to the same version of code even if we make changes to the branch. We can also select lines from the code file that will be highlighted when we open the URL.\nIf we are creating a new branch locally and want to have an upstream version for that branch:\n\ngit branch --remote origin/new-branch-name will create upstream version of the branch so we can easily push.\ngit push -u origin new-branch-name will create the new branch while pushing to Github\nIf we want the upstream name to have different name than the local branch name, git push -u origin local-branch-name:upstream-branch-name\n\nIf we want to delete a branch:\n\nLocally: git branch -d branch-name\nUpstream: git remote --delete branch-name\n\nWe can force to push local branch to another existing upstream branch. This is risky and we may not need to use it git push --force origin local-branch:upstream-branch"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#typical-workflow",
    "href": "posts/git/Advanced-Git.html#typical-workflow",
    "title": "Git Confident",
    "section": "Typical Workflow",
    "text": "Typical Workflow\n\nAlways start by creating new branch for new features. Almost always strive to not commit directly to master branch even for small changes. The workflow is:\n\n\ncreate new branch -&gt; make small changes -&gt; create pull request -&gt; pass code reviews and other stuff like CI/CD -&gt; Rebase master into feature branch -&gt; Interactive rebase to squash all commits from feature branch into one commit message -&gt; Fast forward merge with master -&gt; push master -&gt; delete feature branch locally and on upstream.\n\n\nAlways commit small changes and don’t wait for large changes to commit. It will be harder to figure out what changes have been made and make it difficult for code reviewers to understand. We can always refine commits with interactive rebase.\nPull Requests:\n\nWe first need to push the feature branch into Github using git push -u origin feature-branch\nWe then have two choices to open PRs: Either through Github UI or though command line tools like hub and gh. The advantage of Github UI is that it lets you review the code one more time through compare view before submitting it.\nProvide as much context as possible when drafting your PR description. Try to provide as much useful detail as you can. Answering the following questions is a great start:\n\nWhy is this change needed?\nWere other solutions considered?\nWere any assumptions made?\n\nFor work that can’t be broken down into small changes, we can use Github Task lists that shows all the items that need to be worked on and the methodology so that people would know not to do in depth code reviews. So every time we push changes we mark items that were already done.\nCode reviews resources:\n\nDerek Prior’s talk on Code Review Culture\nthoughtbot guide to code review\n\nAfter getting the feedback from the team on the code reviews as well as the CI comments, we can incorporate the changes that team recommended. Then push the new commits to the feature branch and those will automatically be included in the PR.\nWe prefer a clean history built using fast-forward merges. In order to ensure this, before merging our PR we always pull master and rebase our feature branch onto master to ensure that our commits are ahead of master. One nice helper for this is the mup alias which checks out master, pulls, then checks back out our feature branch: mup = !git checkout master && git pull && git checkout -. Finally, git rebase master. If we’ve done any rebase, we need to force push changes to remote git push -f\nOnce we’re ahead of master, we can perform an interactive rebase to revise our commits and craft our history. In particular, we can use this time to squash down cleanup and WIP commits, ensuring that each commit we keep is useful and has a solid commit message.\nThis is the time to ensure that we’ve captured as much context as possible in our commit message to describe the “why” of the change. Two great resources on this topic are:\n\nFive Rules for A Good Git Commit Message\nStephen Ball’s Deliberate Git talk\n\nIf we’ve performed any form of rebase, then we’ll have created new commits and will want to push those up to GitHub in order to get everything in sync. To do this we can force push (git push -f) our branch.\nFinal steps:\n\nIf we’ve force pushed after rebasing as described above, we should be all set, but never hurts to give one last git push just to confirm that our local and remote feature branches are in sync.\nMerge fast-forward: git co master & git merge - --ff-only\nPush master: Now that we’ve merged master, we can push it up to GitHub with git push. As a reminder, with a fast-forward merge we are simply moving our master branch pointer to point at our feature branches tip commit, not actually creating any new commits. This is one of the main benefits of using fast-forward merges, namely that all commits are created and can be reviewed on our feature branch before merging into master. With “Big Green Button on GitHub” merges and other non-fast-forward merges, the merge commit is created directly on master based on Git’s merging algorithm.\nDelete local branch: git branch -d decks-ordering\nDelete remote branch: git push origin --delete &lt;branchName&gt;. We can also delete the branch via the GitHub PR page, and then git pull on master, letting the fetch prune setting automatically clean up our local reference to the remote branch.\nPull request auto closing. Assuming we’ve performed the steps outlined above, GitHub will have automatically closed the PR based on the fact that master now contains our branch’s commits."
  },
  {
    "objectID": "posts/git/Advanced-Git.html#configuration",
    "href": "posts/git/Advanced-Git.html#configuration",
    "title": "Git Confident",
    "section": "Configuration",
    "text": "Configuration\nGit looks for configurations in the following places:\n\nFirst look for/inside /etc/gitconfig. Any time we use git config --system, it reads/writes this file\nSecond look for/inside ~/.gitconfig for each user. Any time we use git config --global, it reads/writes this file\nFinaly look for/inside .gitconfig inside the Git directory. Any time we use git config --local, it reads/writes this file\n\ngitconfig file is read automatically before any Git command is run. That turns out to be very handy as it means you never have to reload or experience out-of-sync commands. Additionally, git automatically writes to it when we run commands like git config –global alias.ga.\n\nThe config file is split into sections such as color, alias, core, push, etc. For example:\n\n[push]\n    default = upstream\nis the same as git config --global push.default upstream.\nFew useful configurations:\n\npush.default upstream this instructs Git how to respond when you run git push with no arguments. With the upstream configuration, it will push the configured upstream tracking branch (set up with git push -u).\nmerge.ff only this configuration tells Git to reject merges that are non-fastforward. With fast-forward merges, no new commits are created, but instead the merging branch (typically master) is only moved to point at the commits on the target branch (typically our feature branch).\nfetch.prune true this instructs Git to clear local references to remote branches which have been deleted when you pull.\n\nBy default, we can only execute one git command when aliasing. To execute more than one command, we can start the command with ! and then we can execute multiple shell commands using pipes, &&, and ||. For example, !git checkout master && git pull && git checkout -.\nGit subcommands allow us to write scripts in any language we want; not necessarily bash, and make Git executes it. The script subcommand has to be:\n\nOn our $PATH\nMarked as executable\nThe file name has to be prefixed with git and then dash and then the name of the command. For example, git-subcommand-name. Actually, all git commands are files that share all those criteria such as git-add. Below is an example of subcommand:\n\n#!/bin/bash\n#\n# Small wrapper around git commit. Bare 'cm' will enter normal git commit\n# editor, but with args it will do a direct `commit -m`\n\nif [[ $# &gt; 0 ]]; then\n    git commit -m \"$@\"\nelse\n    git commit -v\nfi"
  },
  {
    "objectID": "posts/git/Advanced-Git.html#resources",
    "href": "posts/git/Advanced-Git.html#resources",
    "title": "Git Confident",
    "section": "Resources",
    "text": "Resources\n\nGit Ready: Practical how-to pages on topics like “get a file from a specific revision.”\nPro Git: A great in-depth resource I find myself continually coming back to.\nGit Internals: A deep dive into the Git object model, with more detail and nuance than we could cover in the this course’s video on the topic\nThoughtbot Guides\nGithub CLI\nAdd this to Vim autocmd Filetype gitcommit setlocal spell textwidth=72\nFugitive Plugin\n\nfive part Fugitive series on Vimcasts\n\nConflicted Optimizing Fugitive for merge and rebase conflicts"
  },
  {
    "objectID": "posts/personal-growth/notes-on-learning.html",
    "href": "posts/personal-growth/notes-on-learning.html",
    "title": "Notes on Learning",
    "section": "",
    "text": "Introduction\nIn today’s fast-paced world, the ability to learn effectively and retain information has become more crucial than ever. Whether you’re a student preparing for exams, a professional mastering new skills, or simply someone seeking personal growth, finding ways to optimize learning and improve memory recall can make a significant difference in achieving your goals.\nFortunately, research in neuroscience and cognitive psychology has shed light on strategies that enhance how we absorb and retain knowledge. There are numerous methods to make learning not only more efficient but also more enjoyable. This article explores a variety of evidence-based approaches to optimize learning and strengthen recall and hopefully you’ll find actionable insights to elevate your learning journey.\n\n\nLearning Techniques\n\nEffective instructions should match the content not the learning styles. For example, cooking instruction should use hands-on practices even if the student is a visual learner.\nLearning means that a change made to long-term memory.\nHuman memory is not as precise/reliable as computer memory. It has read-and-update. Reading memory will lead to strengthen and modify the fetched information especially if the information is recently learned.\nStored information is stored in interconnected neural pathways. If we try to access targeted information, we activate a pathway of neurons to access the information which leads to spread the activation to other connected pathways that may not be related to the target information. This spreading activation leave related pathways primed for activation for hours . As a result:\n\nSpreading activation leads to related but imprecise information to be conflated with target information, which leads to unreliable recall of information.\nBecause pathways stay primed for hours, it helps us with problem solving when we step away to work on something else, go for a walk, or take a shower, and the two unrelated areas connect in the middle.\n\nThere are two types of memory:\n\nLong-term memory where information is permanently stored and is unlimited. It is analogous to disk storage.\nWorking (short-term) memory is used to solve problems. It is analogous to CPU’s registers. The bigger the working memory, the faster we can learn. It is roughly fixed at birth.\n\nChunking is when we relate information together as one piece. The more we combine information as one piece (chunk), the easier it is to reason about and solve problems related to it. This is due to the fact that we can store pointers to such chunks in the working memory and access such chunks in long-term memory if needed. Therefore, it is critical to decompose difficult tasks into smaller pieces (chunks) when learning, which later will be chunked together as we practice.\nThe difference between experts and beginners is that experts remember and recognize patterns to help them solve problems. However, beginners read code line by line to understand what it is doing or how to approach solving problems. Therefore, to achieve proficiency in programming, you need to read/write and work with a lot of code to be exposed to more patterns as well as programming using different programming paradigms/languages.\nTo understand a concept, we may need to go from abstract to diverse set of concrete examples and back to abstract. This helps us with chunking and treating all the concrete examples as different views of the abstract concept. Once we understand the concrete examples, we can connect it back to the abstract concept.\nSpacing and Repetition are keys for learning. We learn problem-solving concepts best by spacing out their practice across multiple sessions, multiple days, and ideally, multiple weeks. Practice helps us connect the text in the problem to the concept and applying the concept to solve the problem.\nConcentration after 90 minutes is hard due to neuro-chemical balance in the brain. It is recommended to rest/sleep/walk after the 90 minutes so the information gets consolidated. Don’t work on other tasks, talk to others, or browse the internet.\nEven if we can access information on the internet, it is advisable to understand concepts we deal with frequently so the brain can form connections and help us understand deeper concepts. Also, it is much better to try to recall information from long-term memory than search for it on the internet especially if we are not experts.\nProblem-solving is not a generic skill. It is domain-specific skill. This means that a good chess player may not be a good problem-solver in programming or other domains. As a result, to get better at programming problem-solving, learn to solve programming problems.\nThere is no clear predictor in programming ability other than experience.\nGrowth mindset, learning to overcome setbacks and failures, and practice is all you need to be successful in your career. You will have to always evaluate your learning strategies to get the best outcome.\n\n\n\nRecommendations\n\nFor recruiting:\n\nThere are no good proxies for programming ability, look at their previous work or test them on authentic programming tasks.\nAt least among young developers, years of experience may not be a very reliable measure of ability.\n\nFor learning and training:\n\nReading a lot of code will help become a more efficient programmer.\nExperts are not always the best at training beginners.\nLearning takes time, including time between learning sessions. Intense cramming is not effective, but spaced repetition is.\nSimilarly, spending time away from a problem can help to solve it.\nJust because you can find it through an Internet search or generative AI tool does not mean learning has become obsolete.\nUse examples to go between abstract concepts and concrete learnable facts.\nSeeking to succeed (rather than avoid failure) and believing that ability is changeable are important factors in resilience and learning.\n\n\n\n\nConclusion\nAs we’ve explored various approaches to optimize learning and enhance recall, it’s clear that the journey to effective learning is both dynamic and multifaceted. By incorporating evidence-based strategies such as spaced repetition, and active engagement, individuals can significantly improve their ability to absorb and retain information.\nEmbracing a growth mindset and being open to adapting your strategies will further empower you to navigate the complexities of acquiring knowledge. In conclusion, by implementing these innovative approaches, you can transform your learning experience into a more productive and rewarding endeavor. Start applying these insights today and watch as your ability to learn and recall information flourishes!\n\n\nFurther Reading\n\nWhy Don’t Students Like School? by Daniel T. Willingham provides a short and readable explanation of many of the principles of memory and how the brain works.\nThe Programmer’s Brain by Felienne Hermans et al.c relates these concepts to programming and describes how techniques for learning and revision that are used at school can still apply to professional development.\nHow Learning Happens: Seminal Works in Educational Psychology and What They Mean in Practice by Paul A. Kirschner and Carl Hendrick provides a tour through influential papers, explaining them in plain language and the implications and linkages between them.\n10 Things Software Developers Should Learn about Learning by Neil C. C. Brown, Felienne F. J. Hermans, and Lauren E. Margulieux.\n\n#personal-growth #career-advice"
  },
  {
    "objectID": "posts/nlp/LSTM-Annotated-Implementation.html#lstm-cell",
    "href": "posts/nlp/LSTM-Annotated-Implementation.html#lstm-cell",
    "title": "LSTM Implementation",
    "section": "LSTM Cell",
    "text": "LSTM Cell\nLet’s take a look at the equations for an LSTMCell (each gate has the same dimension as hidden state):\n\\[\\begin{array}{ll} \\\\\ni_t = \\sigma(W_{ii} x_t + b_{ii} + W_{ih} h_{t-1} + b_{hi}) \\\\\nf_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\ng_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\no_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\nh_t = o_t \\odot \\tanh(c_t) \\\\\n\\end{array}\\]\nWhere:\n\n\\(i_t\\) is the input gate. It looks at \\(x_t\\) and \\(h_t\\) and determines what information to keep and what to throw away. The output is between 0 & 1 where 1 means keep all the information and 0 means get rid of this information.\n\\(f_t\\) is the forget gate. This gate is responsible to determine which information from the old cell state needs to be forgotten in order to be replaced with new information when updating the new cell state based on the input gate.\n\\(g_t\\) is the cell gate. This gate determines which cell elements to update with new input data.\n\\(o_t\\) is the output gate. This is the last gate which determines which information from cell state to use to output to the new hidden state.\n\nEven though we have 4 gates, we actually implement them using one matrix to speed up the computation. Then later we will split the output to compute the corresponding gates.\nLet’s implement LSTMCell and check its correctness with pytorch.\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n# Long version\nclass LSTMCellNew(nn.Module):\n    def __init__(self, input_sz, hidden_sz, bias=True):\n        super().__init__()\n        self.weight_ih = nn.Parameter(torch.randn((input_sz, hidden_sz * 4)))\n        self.weight_hh = nn.Parameter(torch.randn((hidden_sz, hidden_sz * 4)))\n        self.bias_ih = nn.Parameter(torch.zeros(hidden_sz * 4))\n        self.bias_hh = nn.Parameter(torch.zeros(hidden_sz * 4))\n\n    def forward(self, x, h, c):\n        # B x hidden_sz\n        out = x @ self.weight_ih + h @ self.weight_hh + self.bias_ih + self.bias_hh\n        i, f, g, o = torch.split(out, 100, dim=-1)\n        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n        g = torch.tanh(g)\n        c_t = f * c + i * g\n        h_t = o * torch.tanh(c_t)\n        return h_t, c_t\n\n\n# Short version utilizing linear layer module\nclass LSTMCellNew(nn.Module):\n    def __init__(self, input_sz, hidden_sz, bias=True):\n        super().__init__()\n        self.ih = nn.Linear(input_sz, hidden_sz * 4, bias=bias)\n        self.hh = nn.Linear(hidden_sz, hidden_sz * 4, bias=bias)\n\n    def forward(self, x, h, c):\n        out = self.ih(x) + self.hh(h)\n        i, f, g, o = torch.split(out, 100, dim=-1)\n        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n        g = torch.tanh(g)\n        c_t = f * c + i * g\n        h_t = o * torch.tanh(c_t)\n        return h_t, c_t\n\n\nbatch_sz = 64\nseq_len = 8\ninput_sz = 20\nhidden_sz = 100\nnum_layers = 2\n\n\nX = torch.randn(seq_len, batch_sz, input_sz, dtype=torch.float32)\nc_0 = torch.randn(num_layers, batch_sz, hidden_sz, dtype=torch.float32)\nh_0 = torch.randn(num_layers, batch_sz, hidden_sz, dtype=torch.float32)\n\n\npytorch_cell = nn.LSTMCell(input_sz, hidden_sz, bias=True)\n(\n    pytorch_cell.weight_hh.shape,\n    pytorch_cell.weight_ih.shape,\n    pytorch_cell.bias_ih.shape,\n    pytorch_cell.bias_hh.shape,\n)\n\n(torch.Size([400, 100]),\n torch.Size([400, 20]),\n torch.Size([400]),\n torch.Size([400]))\n\n\n\n# h: B x hidden_sz\n# c: B x hidden_sz\npytorch_h, pytorch_c = pytorch_cell(X[0], (h_0[0], c_0[0]))\n\n\ncell = LSTMCellNew(input_sz, hidden_sz)\n\n# To make sure pytorch and our implementation both\n# have the same weights so we can compare them\ncell.ih.weight.data = pytorch_cell.weight_ih.data\ncell.hh.weight.data = pytorch_cell.weight_hh.data\ncell.ih.bias.data = pytorch_cell.bias_ih.data\ncell.hh.bias.data = pytorch_cell.bias_hh.data\n\n\nh_t, c_t = cell(X[0], h_0[0], c_0[0])\n\n\nprint(\n    np.linalg.norm(pytorch_h.detach().numpy() - h_t.detach().numpy()),\n    np.linalg.norm(pytorch_c.detach().numpy() - c_t.detach().numpy()),\n)\n\n0.0 0.0"
  },
  {
    "objectID": "posts/nlp/LSTM-Annotated-Implementation.html#lstm",
    "href": "posts/nlp/LSTM-Annotated-Implementation.html#lstm",
    "title": "LSTM Implementation",
    "section": "LSTM",
    "text": "LSTM\nThere are few things worth mentioning about our LSTM implementations as well as other implementations in common libraries:\n\nWe use sequence length as the first dimension instead of the batch first. This would give us better performance since we iterate over timesteps and we want to avoid copying memory for each operation which would be the case if the matrix is not contiguous when first dimension is the batch. Therefore, we use T x B x input_sz.\nBackpropagation Through Time (BPTT): This essentially means we backpropagate through all the history for each example when we calculate the gradient of the loss w.r.t. weights. Since for each layer, the weights are shared among all timesteps, long sequences will suffer greatly from vanishing/exploding gradients. Therefore, we typically truncate history by detaching hidden and cell states from computation graph after every batch so gradients stop at \\(t_0\\) for each bach for each sequence. We only have access to the hidden/cell states from previous batch for the same sequence but can’t propagate beyond the first timestep of each batch.\nWe can stack LSTMs (and RNNs) on top of each other using num_layers argument. This would build multiple LSTM layers, each has its own LSTMCell that is shared across all timesteps within each layer. This would increase the capacity of the model.\nWhen we have multilpe layers, we can either 1) iterate first over all timesteps for each layer before moving to the next layer Or 2) iterate over number of layers first for a given timestep before moving to the next timestep.\nWhen we have long sequences, it is common that we divide the sequences into shorter segments using predefined block_size.\nSince not all sequences have the same length, we need to make them of the same length to utilize matrix-matrix multiplication. There are two approaches to handle this issue:\n\nMake the sequence length the length of the longest sequence. Pad shorter sequences with zeros, using either pre-padding (zeros at the beginning) or post-padding (zeros after last token at the end).\nPadding leads to wasteful computation. To avoid this issue, we can use packed sequences where we combine all sequences together and have indices of where each sequence starts and ends.\n\n\n\nclass LSTMNew(nn.Module):\n    def __init__(self, input_sz, hidden_sz, num_layers=1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_sz = hidden_sz\n        self.cells = nn.ModuleList(\n            [\n                LSTMCellNew(input_sz, hidden_sz)\n                if i == 0\n                else LSTMCellNew(hidden_sz, hidden_sz)\n                for i in range(self.num_layers)\n            ]\n        )\n\n    def forward(self, x, h_t, c_t):\n        # x  :      T     x B x hidden_sz\n        # h_t: num_layers x B x hidden_sz\n        # c_t: num_layers x B x hidden_sz\n        T, B, _ = x.shape\n        H = torch.zeros(T, B, self.hidden_sz)\n        for i, cell in enumerate(self.cells):\n            h, c = h_t[i], c_t[i]\n            if i &gt; 0:\n                x = H\n            for t in range(T):\n                h, c = cell(x[t], h, c)\n                H[t] = h\n            # last hidden state for each layer\n            h_t[i], c_t[i] = h, c\n        # Truncated BPTT\n        return H, (h_t.detach(), c_t.detach())\n\n\npytorch_lstm = nn.LSTM(input_sz, hidden_sz, num_layers=num_layers)\npytorch_H, (pytorch_h, pytorch_c) = pytorch_lstm(X, (h_0, c_0))\n\n\nlstm = LSTMNew(input_sz, hidden_sz, num_layers=num_layers)\n\nfor i in range(num_layers):\n    lstm.cells[i].ih.weight.data = getattr(pytorch_lstm, f\"weight_ih_l{i}\").data\n    lstm.cells[i].hh.weight.data = getattr(pytorch_lstm, f\"weight_hh_l{i}\").data\n    lstm.cells[i].ih.bias.data = getattr(pytorch_lstm, f\"bias_ih_l{i}\").data\n    lstm.cells[i].hh.bias.data = getattr(pytorch_lstm, f\"bias_hh_l{i}\").data\n\nH, (h_t, c_t) = lstm(X, h_0, c_0)\n\n\nprint(\n    np.linalg.norm(pytorch_H.detach().numpy() - H.detach().numpy()),\n    np.linalg.norm(pytorch_h.detach().numpy() - h_t.detach().numpy()),\n    np.linalg.norm(pytorch_c.detach().numpy() - c_t.detach().numpy()),\n)\n\n0.0 0.0 0.0"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#embedding-layer",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#embedding-layer",
    "title": "Transformer Architecture Explained",
    "section": "Embedding Layer",
    "text": "Embedding Layer\nAfter the input sequence is tokenized and numericalized, we need to project each token into lower dimension space. Such projection is called embedding and it captures the semantic representation of tokens based on the context the token mostly occurs in.\nAttention operation is a permutation equivariant, this means that if we permute the input then the corresponding output will be permuted in exactly the same way. In other words, attention mechanism is not aware of the relative ordering of the tokens. Therefore, we need some way to encode the positions of the tokens in each sequence. This is where positional encoding comes into play. There are two types of encodings:\n\nAbsolute Positional Encoding: Use token absolute position. Can use either static patterns such as sign function, or learned parameters\nRelative Positional Encoding: Encode the relative position of tokens. We need to adjust the attention mechanism itself by adding new terms to be used when dot-products are used to encode the relative position between tokens up to maximum relative position.\nRotary Encoding: Combine both absolute and relative position of tokens to achieve great results. This can be done by encoding the absolute positions with a rotation matrix that will be multiplied with key and value matrices of each attenetion layer to add the relative position information at every layer.\n\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n\n\n\nCode\nconfig = {\n    \"vocab_sz\": 1000,\n    \"block_sz\": 8,\n    \"intermediare_sz\": 4 * 64,  # 4x hidden_dim\n    \"hidden_dropout_prob\": 0.2,\n    \"num_attention_heads\": 12,\n    \"hidden_sz\": 64,           # embed_dim / num_attention_head = 768 / 12 = 64\n    \"num_hidden_layers\": 6,\n    \"embed_dim\": 768,\n    \"num_classes\": 2,\n    \"layer_norm_eps\": 1e-12,\n}\n\n\n\nclass Embeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.token_embedding = nn.Embedding(config.vocab_sz, config.embed_dim)\n        self.position_embedding = nn.Embedding(\n            config.block_sz, config.embed_dim\n        )\n        self.layer_norm = nn.LayerNorm(\n            config.embed_dim, eps=config.layer_norm_eps\n        )\n        self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, x):\n        # X:                   B x T\n        # token_embeddings:    B x T x embed_dim\n        # position_embeddings: T x embed_dim\n        embeddings = self.token_embedding(x) + self.position_embedding(\n            torch.arange(x.shape[1])\n        )\n        embeddings = self.layer_norm(embeddings)\n        return self.dropout(embeddings)"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#attention",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#attention",
    "title": "Transformer Architecture Explained",
    "section": "Attention",
    "text": "Attention\nAttention is a communication mechanism that is used by NN model to learn to make predictions by attending to some tokens in the context window (only current and previous tokens for decoder-only architectute). The attention weights, which are learned, are used to construct the weighted average of all the tokens attended to by each token. This will help each token focus on what is important in the context. As a reminder, with attention, there is no notion of space. This means it operates on a set of vectors. This is why we need positional encoding for tokens.\nThe results of the attention layer would be contextualized embeddings, since the output of the embedding layer is contextless embeddings. This is very useful because we know that the meaning of a word changes according to the context, and embeddings from the embedding layer for a token is the same regardless of its context. For example, the word “bear” has the same embedding vector whether it comes in “teddy bear” or “to bear”.\nSelf-attention is a type of attention mechanism where the keys and values come from the same source as the queries, which is the input \\(x\\). Whereas in cross-attention, the queries still get produced from the input \\(x\\), but the keys and values come from some other external source (encoder module in the case of encoder-decoder architecture).\n\n\n\n\nFigure 2: Scaled Dot-Product Attention (source)\n\n\n\nFor self-attention, we have:\n\nQuery matrix \\(Q\\) (hidden_sz x head_dim): what each token is looking for\nKey matrix \\(K\\) (hidden_sz x head_dim): what each token contains\nValue matrix \\(V\\) (hidden_sz x head_dim): what each token communicate with\n\nThen,\n\nThe dot-product of query with all the keys of the tokens give us the affinities. Dot-product is just used as a form of computing similarities. Other form of attention include additive attention.\n\nIf query and key vectors are aligned -&gt; very high value -&gt; get to know more about that token as opposed to other tokens\nAll the tokens in all positions in B x T matrix produce query/key/value vectors in parallel and independently from each other and no communication is happening\nThen all queries will be dot-product with all the keys\nWe scale attention by dividing it with \\(sqrt(head\\_sz)\\). This makes it so when input Q,K are unit variance, weights will be unit variance too and softmax will stay diffuse and not saturate too much\n\nFinally, we multiply the attention weights with the value matrix \\(V\\) to get the contextualized embeddings\n\nIn equations: \\[attn(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt d_k})V\\]\n\nclass AttentionHead(nn.Module):\n    def __init__(self, config, head_dim, is_decoder=False) -&gt; None:\n        super().__init__()\n        self.k = nn.Linear(config.embed_dim, head_dim, bias=False)\n        self.q = nn.Linear(config.embed_dim, head_dim, bias=False)\n        self.v = nn.Linear(config.embed_dim, head_dim, bias=False)\n        self.is_decoder = is_decoder\n        if self.is_decoder:\n            self.register_buffer(\n                \"mask\", torch.tril(torch.ones(config.block_sz, config.block_sz))\n        )\n\n    def forward(self, query, key, value):\n        # query, key, value are each B x T x embed_dim\n        q = self.q(query)\n        k = self.k(key)\n        v = self.v(value)\n        # w is B x T x T\n        w = q @ k.transpose(2, 1) / (k.shape[-1] ** 0.5)\n        if self.is_decoder:\n            w = w.masked_fill(self.mask == 0, -float(\"inf\"))\n        w = F.softmax(w, dim=-1)\n        # output is B x T x head_dim\n        return w @ v"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#multi-head-attention",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#multi-head-attention",
    "title": "Transformer Architecture Explained",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\nWhat we described in the previous section was self-attention mechanism with one-head. Since each attention head focuses on one specific characteristic of the data in terms of similarity such as subject-verb interaction, other heads are needed to focus on other aspects such as adjectives. We can also think of having multiple heads as if each head focuses on one or few other tokens. Remember that all of this is done in parallel and there is no communication between heads. This means that each head has no idea what other heads are doing.\n\n\n\n\nFigure 3: Mutli-Head Attention with several attention layers running in parallel (source)\n\n\n\nIn multi-head layer, we typically have the head_sz be the result of dividing the hidden_sz (or the embeddind_dim if it is the first layer) by the number of heads.\nOnce we get all contextualized embeddings from all heads, we concatenate them. Then we pass the output through a projection layer with the same dimension as the input.\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config, is_decoder=False) -&gt; None:\n        super().__init__()\n        head_dim = config.embed_dim // config.num_attention_heads\n        self.heads = nn.ModuleList(\n            [\n                AttentionHead(head_dim, config, is_decoder)\n                for _ in range(config.num_attention_heads)\n            ]\n        )\n        self.output = nn.Linear(config.embed_dim, config.embed_dim)\n\n    def forward(self, x):\n        x = torch.cat([head(x) for head in self.heads], dim=-1)\n        return self.output(x)"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#feed-forward-layer",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#feed-forward-layer",
    "title": "Transformer Architecture Explained",
    "section": "Feed-Forward Layer",
    "text": "Feed-Forward Layer\nBecause there are no elementwise nonlinearities involved in the calculation of the attention, stacking multiple layers of attention wouldn’t help much because the output would still be linear transformation of the input. As a result, feed-forward NN is added to add such nonlinearities to post-process each output vector from the attention layer. Therefore, each embedding vector is processed independently in the batched sequence, which leads to the position-wise feed-forward layer.\nWe typically first project the output vector into new space 4x the hidden_sz. Therefore, most of the capacity and memorization is expected to happen in the first layer, which is what gets scaled when the model is scaled up. Then we project it back to the original dimension. We use GELU as the activation function, which is a Gaussian Error Linear Units.\n\nclass FeedForwardNN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # intermediate_sz is typically 4 x embed_dim\n        self.l1 = nn.Linear(config.embed_dim, config.intermediate_sz)\n        self.l2 = nn.Linear(config.intermediate_sz, config.embed_dim)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, x):\n        return self.dropout(self.l2(F.gelu(self.l1(x))))"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#layer-normalization",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#layer-normalization",
    "title": "Transformer Architecture Explained",
    "section": "Layer Normalization",
    "text": "Layer Normalization\nLayer normalization was introduced in this paper to overcome the main challenges of Batch normalization, which are 1) how do we handles batches with 1 or few examples because we would have infinite variance or unstable training and 2) how do we handle RNNs. The main differences with batch normalization are 1) we don’t have moving averages/standard deviations and 2) we average over the hidden dimesnion(s), so it is indepenedent of the batch size. It has two learnable parameters (scalars): \\(\\beta\\) and \\(\\gamma\\) (see the equation below):\n\\[y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\\]\nIt is used as a trick to train complex models, such as Transformer, faster. In our case, we would normalize the hidden vectors to zero mean and unit standard deviation. This trick helps maintain consistent distribution of signals by cutting down uninformative variations in hidden vector values.\nThere are two arrangements for the layer normalization as illustrated in Figure-4:\n\n\n\n\nFigure 4: Different LayerNorm arrangement (source)\n\n\n\n\nPrelayer normalization: Places the layer normalization within the span of skip connections. This arrangement is easier to train.\nPostlayer normalization: Places the layer normalization in between skip connections. This arrangement is used in the Transformer paper."
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#skip-connections",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#skip-connections",
    "title": "Transformer Architecture Explained",
    "section": "Skip Connections",
    "text": "Skip Connections\nSkip connections help train deeper and more complex models faster as well as avoid the issue of vanishing gradients that deeper networks face. It provides paths for the gradient to flow through back to the input. In our case, we are using skip connections with addition, which means we take a copy of the inputs and added it to the output of a block (involves some computations). If we assume \\(y = x + F(x)\\), then it is as if we are asking the block to predict \\(y - x\\). In other words, it means to backpropagate through the identity function, which leads to multiply the gradient of \\(y\\) by one and retain its value in the earlier layers.\nSkip connections help also smooth out the loss landscape (see Figure-5), and make it easier for the gradients to flow back as addition operator split the gradients equally. This means that small changes in the input can still find their way to the output. Additionally, it preserves the original input sequence, which means there is no way for the current word to forget to attend to its position because we always add it back.\n\n\n\n\nFigure 5: The loss surfaces of ResNet-56 with/without skip connections (source)"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#dropout",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#dropout",
    "title": "Transformer Architecture Explained",
    "section": "Dropout",
    "text": "Dropout\n\n\n\n\nFigure 6: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped. (source)\n\n\n\nDropout is a regularization technique that was introduced by Geoffrey Hinton et al. in this paper. On each iteration, we randomly shut down some outputs from the previous layer and don’t use those outputs in both forward propagation and back-propagation. Since the outputs that will be dropped out on each iteration will be random, the learning algorithm will have no idea which neurons will be shut down on every iteration; therefore, force the learning algorithm to spread out the weights and not focus on some specific feattures. Moreover, dropout help improving generalization error by:\n\nSince we drop some units on each iteration, this will lead to smaller network which in turns means simpler network (regularization).\nCan be seen as an approximation to bagging techniques. Each iteration can be viewed as different model since we’re dropping randomly different units on each layer. This means that the error would be the average of errors from all different models (iterations). Therefore, averaging errors from different models especially if those errors are uncorrelated would reduce the overall errors. In the worst case where errors are perfectly correlated, averaging among all models won’t help at all; however, we know that in practice errors have some degree of uncorrelation. As result, it will always improve generalization error.\n\nDropout is used in the Transformer in embeddings layer after adding the token and positional embeddings as well as after each multi-head/feed-forward layers in both the encoder and decoder layers.\nFor more information on dropout, check out my previous post."
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#encoder-only-architecture",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#encoder-only-architecture",
    "title": "Transformer Architecture Explained",
    "section": "Encoder-only Architecture",
    "text": "Encoder-only Architecture\nEncoder-only architecture are well suited for classification tasks. The most common model that uses encoder-only branch of the Transformer architecture is BERT and all its variants such as RoBERTa. In this architecture, we would have:\n\nbody: Stacked encoder layers. The output would be B x T x hidden_sz.\nhead: A classification head which consists of linear layer that project the hidden_sz into num_classes. We take the hidden vector of the first token, which is the special token [CLS] in the case of BERT (indicates the beginning of sequence), and pass it through the linear layer to get the logits.\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attn = MultiHeadAttention(config)\n        self.layer_norm_1 = nn.LayerNorm(config.embed_dim)\n        self.layer_norm_2 = nn.LayerNorm(config.embed_dim)\n        self.ff = FeedForwardNN(config)\n\n    def forward(self, x):\n        # There are two arrangements for layer_norm:\n        # Prelayer normalization & Postlayer normalization\n        # we are using postlayer normalization arrangement\n        x = self.layer_norm_1(x + self.attn(x))\n        x = self.layer_norm_2(x + self.ff(x))\n        # Prelayer normalization\n        # x = self.layer_norm_1(x)\n        # x = x + self.attn(x)\n        # x = x + self.ff(self.layer_norm_2(x))\n        return x\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        self.embeddings = Embeddings(config)\n        self.encoder_blocks = nn.Sequential(\n            *[EncoderLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n\n    def forward(self, x):\n        x = self.embeddings(x)\n        return self.encoder_blocks(x)\n\n\nclass TransformerForSequenceClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = TransformerEncoder(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.embed_dim, config.num_classes)\n\n    def forward(self, x):\n        # We take the hidden state of the [CLS] token as\n        # input to the classifier\n        x = self.encoder(x)[:, 0, :]\n        x = self.dropout(x)\n        return self.classifier(x)"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#decoder-only-architecture",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#decoder-only-architecture",
    "title": "Transformer Architecture Explained",
    "section": "Decoder-only Architecture",
    "text": "Decoder-only Architecture\nThese models are typically used as language models such as GPT and all its variants. In this architecture, as opposed to the encoder-only architecture, the token can only see past tokens but not future tokens because this would be a kind of cheating since we are trying to predict the next token. Therefore, we need to mask all future tokens in the attention layer. In this architecture, we would have:\n\nbody: Stacked decoder layers. The output would be B x T x hidden_sz.\nhead: A classification head which consists of linear layer that project the hidden_sz into vocab_sz. The output would then be passed through softmax to get the probability distribution over all tokens in the vocabulary. The token with the highest probability would be chosen during training.\n\nAt inference, we can use many sampling algorithms such as the greedy algorithm or top-k algorithm using the probability distribution obtained from the classification head.\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attn = MultiHeadAttention(config, is_decoder=True)\n        self.layer_norm_1 = nn.LayerNorm(config.head_dim)\n        self.layer_norm_2 = nn.LayerNorm(config.head_dim)\n        self.ff = FeedForwardNN(config)\n\n    def forward(self, x):\n        x = self.layer_norm_1(x + self.attn(x))\n        x = self.layer_norm_2(x + self.ff(x))\n        return x\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        self.embeddings = Embeddings(config)\n        self.decoder_blocks = nn.Sequential(\n            *[DecoderLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n\n    def forward(self, x):\n        x = self.embeddings(x)\n        return self.decoder_blocks(x)\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.decoder = TransformerDecoder(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.lm_head = nn.Linear(config.head_dim, config.vacab_sz)\n\n    def forward(self, x):\n        x = self.decoder(x)\n        x = self.dropout(x)\n        return self.lm_head(x)"
  },
  {
    "objectID": "posts/nlp/Transformer-Architecture-Explained.html#encoder-decoder-architecture",
    "href": "posts/nlp/Transformer-Architecture-Explained.html#encoder-decoder-architecture",
    "title": "Transformer Architecture Explained",
    "section": "Encoder-Decoder Architecture",
    "text": "Encoder-Decoder Architecture\nThe encoder-decoder architecture is the first Transformer architecture used in the Attention Is All You Need paper for Neural Machine Translation Task. It is typically used for tasks that have both their input and output as text such as summarization. T5 and BART are the most common models that use such architecture.\nFor each decoder layer, we add masked multi-head attention layer in the middle that 1) takes the hidden state from the last encoder layer to compute the keys and values and 2) takes the hidden state from layer norm to compute the query. This means, this additional middle multi-head attention layer attends to the all tokens in the input sequence. This is a kind of cross-attention that we defined earlier where keys and values come from different source (input sequence) while the query comes from other source.\nIt is very easy to extend or modify our implementation of DecoderLayer to use it in this architecture, so I will leave it for you as an exercise!"
  },
  {
    "objectID": "posts/nlp/improving-rag.html",
    "href": "posts/nlp/improving-rag.html",
    "title": "Ideas to Improve RAG Applications",
    "section": "",
    "text": "Introduction\nRAG-based applications have so many components and moving parts that seems impossible to optimize or know where to start. Add to that the fact that the field changes so fast, which makes it super hard to keep up. So I’ve gathered few ideas over time to improve RAG-based applications from reading research papers and implementations I’ve deployed in the past.\n\n\n\n\n\n\nNote\n\n\n\nThe list will keep changing as I learn/implement new things\n\n\n\n\nIdeas\n\nMetadata filtering is key for good RAG apps\nFor an MVP, it is a good idea to use both bi-encoders and full-text search such as Tf-Idf and BM25 and combine them\nColBERT reranker is great and less sensitive to chunking\nHave at least 50 chars overlapping in chunks when splitting to not cut-off context\nIf you have data and compute, always fine-tune both encoders if you can\nUse sentence-transformer to fine-tune embedding models\n\nWe typically use triplet loss where for each query we would have positive and negative examples. We want the negative examples to be hard negatives -&gt; very close to positive examples so the model can learn to differentiate between them\n\nLarge/New LLM not necessarily are good embedding models and mayn’t be worth the latency. LLMs with ~1B is enough for most cases\nChallenges with embedding models:\n\nMayn’t transfer to your domain\nFixed vocabulary used when model was trained\nBecause chunk/doc is represented in one vector which is combination of all tokens in the chunk/doc, the output vector may dilute the meaning especially for long texts -&gt; Be careful about chunking strategy\n\nAlways start with a baseline such as BM25 (Best Match 25)\nBuild your own gold dataset and check its correlation with synthetic dataset generated from LLMs\nChunking beyond 256 tokens will affect high precision search because it will dilute the vector representation because embedding models were not trained on long contexts such as BERT-based encoders\nFeedback of how users are liking the app is key to guide us where we should focus our efforts to improve the app\n\nSatisfaction ratings such as “How did we do today” or “Did we answer your question”\n\nMonitor cosine similarity between embeddings of query and retrieved docs and reranking scores that come from reranker (cohere)\nUse clustering of questions using tools such as LDA or BERT-Topic to cluster questions into topics and focus on largest topics (by count) that have lowest means of cosine and feedback\nWe have two kinds of topics:\n\nContent topics: Topics that we don’t have inventory of documents about such topics\nCapability topics: Topics that reader will never be able to generate if we don’t capture them in our docs and docs metadata and include them in the prompt. For example, “Who last updated the pricing document” is asking about last modified date/person\n\nBuild classifier to classify questions real-time for better observability and better react to sudden changes in usage\nGenerate synthetic data (questions) for topics we’re not doing great job at and evaluate new improvements on the generated questions\n\nThis can be done by providing random chunk from docs that belong to topics we’re trying to improve to decent LLM and ask to generate questions\n\nWe can use LLM to get metadata about docs/objects\nLancedb is a good vector database to use for small/scale workloads\nBM25 (full text search) outperforms similarity search when questions are just searching for file names. They may have similar performance with similarity search baseline.\n\nIt is always helpful to include BM25\n\nWe can do citation through prompting and attaching IDs to chunks\nFine-tune embedding model is key for domain-specific RAGs\n\nWith recent increase in context window sizes, chunk size of 800 and 30% overlap is recommended\n\n\n\n\nResources\n\nLarge language models can accurately predict searcher preferences\nUMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor"
  },
  {
    "objectID": "posts/airflow/Airflow-Scheduling.html",
    "href": "posts/airflow/Airflow-Scheduling.html",
    "title": "Airflow Part 3 - DAG Scheduling",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nAirflow will schedule the first execution of DAG at the end of the interval; which means after the last time point in the interval has passed. For example, if we schedule it to run @daily, it will run t midnight of each day starting from the start_date until (optionally) end_date. In other words, as soon as 23:59:59 has passed which means any time after 00:00:00.\n\nExample: if start_date=“2022-01-01” and schedule_interval=“@daily” -&gt; The first time it runs is any time soon after “2022-01-02 00:00” which is midnight of January second.\n\nWe can use convenience string (such as @daily), timedetla objects (such as timedelta(days=3), or cron expressions (such as 0 0 * * 0 which means weekly on Sunday 00:00)\nFrequency scheduling intervals (shorthands):\n\n@once: Schedule once and only once.\n@hourly: Run once an hour at the beginning of the hour.\n@daily: Run once a day at midnight.\n@weekly: Run once a week at midnight on Sunday morning.\n@monthly: Run once a month at midnight on the first day of the month. Run once a year at midnight on January 1.\n\nCron-based intervals:\n\n# ┌─────── minute (0 - 59)\n# │ ┌────── hour (0 - 23)\n# │ │ ┌───── dayofthemonth(1-31)\n# │ │ │ ┌───── month(1-12)\n# │ │ │ │ ┌──── dayoftheweek(0-6)(SundaytoSaturday; \n# │ │ │ │ │ 7 is also Sunday on some systems) \n# * * * * *\n- \"*\" means don't care values.\n- Examples:\n    1. 0**** means hourly\n    2. 00*** means daily at midnight\n    3. 00**0 means weekly at midnight on Sunday\n- Useful link to check meaning of cron-based intervals: https://crontab.guru/\n\nCron expressions have limitations when trying to specify frequency-based intervals such as every three days. The reason for this behavior is that cron expressions are stateless and don’t look at previous runs to determine next run, they only look at the current time to see if it matches the expression.\nAirflow allows us to use frequency-based intervals using timedelta from datetime library. This way we can use previous run to determine the next run.\n\nExample: schedule_interval=“timedelta(days=3)” means to run every 3 days after start_date.\n\nWe can use dynamic time reference that uses execution dates which allows us to do the work incrementally. Airflow will pass those dates to the tasks to determine which schedule interval is being executed.\n\nexecution_date is a timestamp of the start time of the schedule interval\nnext_execution_date is a timestamp of the end time of the schedule interval\nprevious_execution_date is a timestamp of the start time of the previous schedule interval\nAirflow uses Jinja-based templating such as {{variable_name}}:\n\n\nfetch_events = BashOperator(\n    task_id=\"fetch_events\",\n    bash_command=(\n        \"mkdir -p /data && \"\n        \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n        \"start_date={{execution_date.strftime('%Y-%m-%d')}}\" \n        \"&end_date={{next_execution_date.strftime('%Y-%m-%d')}}\"\n    ),\ndag=dag,\n)\n- Or we can use shorthands:\nfetch_events = BashOperator(\n    task_id=\"fetch_events\",\n    bash_command=(\n        \"mkdir -p /data && \"\n        \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n        \"start_date={{ds}}\" \n        \"&end_date={{next_ds}}\"\n    ),\ndag=dag,\n)\n- `ds` has `YYYY-MM-DD` format while `ds_nodash` has `YYYYMMDD` format\n- Shorthands: ds, ds_nodash, next_ds, next_ds_nodash, ps, ps_nodash execution date of the next interval.\n\nWe can also use dates or any dynamic parameters to Python function using templates_dict argument and the python callable will be passed the context that has the templates_dict For example:\n\n    calculate_stats = PythonOperator(\n       task_id=\"calculate_stats\",\n       python_callable=_calculate_stats,\n       templates_dict={\n            \"input_path\": \"/data/events/{{ds}}.json\",\n           \"output_path\": \"/data/stats/{{ds}}.csv\",\n    },\n    dag=dag\n    )\n    def _calculate_stats(**context):\n        \"\"\"Calculates event statistics.\"\"\"\n            input_path = context[\"templates_dict\"][\"input_path\"] \n            output_path = context[\"templates_dict\"][\"output_path\"]\n\nBecause Airlfow follows Interval-Based Scheduling, that means DAGs run only after the last time point of schedule interval passed. If we run the DAG daily starting from 2022-01-01, the first time it runs is soon after 2022-01-02 00:00:00 has passed and the execution_date would be 2022-01-01 even though it is running in 2022-01-02. This is because it is running for the corresponding interval.\n\nThe end of the previous interval is execution_date\nOne caveat for this is that previous_execution_date and next_execution_date are only defined for DAGs that run on schedule interval. This means that those values are undefined when the DAGs are run from the UI or CLI\n\nAirflow allows us to have start_date in the past. This will help us in backfilling. By default, Airflow will run all the schedule intervals from the past until current time once the DAG is activated. We can control this behavior using catchup parameter to the DAG() class. If we set it to False, it won’t run previous schedule intervals.\n\nBackfilling is also helpful if we change the code for the DAG. It would run all previous schedules after we clear them.\n\n\nBest Practices:\n\nTask needs to be atomic which means a single coherent unit of work. This allows us to split work into smaller units where if one fails we know exactly what is it and recover easily.\nTask needs to be idempotent which means it has no side effects on the system when it reruns. If the task is given the same input, it should produce the same output.\n\nIn database systems, we can use upsert, which allows us to overwrite existing row.\nWhen writing to files, make sure that rerunning the same task for the same interval don’t write data again. Append doesn’t let us make the task idempotent."
  },
  {
    "objectID": "posts/airflow/Task-Context-And-Templating.html",
    "href": "posts/airflow/Task-Context-And-Templating.html",
    "title": "Airflow Part 4 - Task Context & Jinja Templating",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nAirflow uses Pendulum library for datetimes. It is a drop-in replacement to the Python standard library datetime but with much nicer API and more features.\nNot all arguments can be templates. By default, all arguments are not made into templates and {name} will be read as a literal string name unless it is included in template_fields in the list of attributes that can be templated in the Operator.\n\nElements in the template_fields are names for class attributes. The arguments passed to the __init__ match the class attributes.\n\nAll operators; such as BashOperator, take their argument as string except PythonOperator. It takes its argument as python_callable, which is any callable object in Python. The context and parameters will be available to this callable.\n\nThe context variable is a dictionary that has all the instance variables for this task.\n\nwe can use default **kwargs or make it easier to read using **context\n\nIf we specify argument name in the python_callable, then Airflow will call the python_callable with all the variables in the context.\n\nIf a variable is specified as argument by the callable, then it is passed to the callabe\nOtherwise, it is added to the context dictionary. If we don’t have context dictionary as an argument for the callable, then all other variables in the context that are not specified as arguments will be discarded.\n\n\n\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndag = DAG(dag_id=\"python-operator-context\", start_date=airflow.utils.dates.days_ago(1))\n\n\ndef _print_context(**kwargs):\n    print(kwargs)\nprint_context = PythonOperator(task_id=\"print-context\", python_callable=_print_context, dag=dag)\nprint_context\n\n\nSome arguments of operators can be templated\nTemplating happens at run time\nWe can provide arguments to PythonOperator using:\n\nop_args: list of positional arguments that are passed to the callable\nop_kwargs: dictionary of keyword arguments\n\nWe can inspect the templated arguments either on the UI or using the CLI:\n\nCLI: airflow tasks render [dag id] [task id] [desired execution date]\n\nThere are two ways to pass data between tasks:\n\nread/write to the metastore. It is called XCom\n\nThis is done by pickling the objects we want to share and write it to metastore. After that, tasks can read the pickled objects (and unpickle them)\nThis is only recommended for small objects because the object are stored as blobs in the metastore. Don’t use it for large objects\n\nread/write to persistent storage such as disk or database\n\nTasks are independent and may run on completely different machines -&gt; Can’t share memory -&gt; Sharing has to be through persistent storage.\nMost operators are installed via separate pip install. For example, PostgresOperator allows us to work with PostgreSQL database.\n\nWe can install operators like pip install apache-airflow-providers-*\nWe can import the operator as from airflow.providers.pogstres.operators.postgres import PostgresOperator\nWe can add connections using UI or CLI, which Airflow store them encrypted in metastore, such as:\n\nairflow connections add \\\n--conn-type postgres \\\n--conn-host localhost \\\n--conn-login postgres \\\n--conn-password mysecretpassword \\\nmy_postgres\n  - We can later refer to those credentions by name when connecting to any database\n\nAirflow takes care of setting up the connection and close it once done\n\nPostgres is an external system and Airflow supports connecting to a wide range of external systems with the help of many operators in its ecosystem. This does have an implication: connecting to an external system often requires specific dependencies to be installed, which allow connecting and communicating with the external system. This also holds for Postgres; we must install the package apache-airflow-providers- postgres to install additional Postgres dependencies in our Airflow installation.\n\nUpon execution of the PostgresOperator, a number of things happen. The PostgresOperator will instantiate a so-called hook to communicate with Postgres. The hook deals with creating a connection, sending queries to Postgres and closing the connection afterward. The operator is merely passing through the request from the user to the hook in this situation.\nAn operator determines what has to be done; a hook determines how to do something. When building pipelines like these, you will only deal with operators and have no notion of any hooks, because hooks are used internally in operators.\n\nThere’s a number of things to point out in this last step. The DAG has an additional argument: template_searchpath. Besides a string INSERT INTO …, the content of files can also be templated. Each operator can read and template files with specific extensions by providing the file path to the operator. In the case of the Postgres- Operator, the argument SQL can be templated and thus a path to a file holding a SQL query can also be provided. Any filepath ending in .sql will be read, templates in the file will be rendered, and the queries in the file will be executed by the PostgresOperator. Again, refer to the documentation of the operators and check the field template_ext, which holds the file extensions that can be templated by the operator.\n\nJinja requires you to provide the path to search for files that can be templated. By default, only the path of the DAG file is searched for, but since we’ve stored it in /tmp, Jinja won’t find it. To add paths for Jinja to search, set the argument template_searchpath on the DAG and Jinja will traverse the default path plus additional provided paths to search for."
  },
  {
    "objectID": "posts/airflow/Airflow-DAGs.html",
    "href": "posts/airflow/Airflow-DAGs.html",
    "title": "Airflow Part 2 - DAGs",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nDAG() class is needed to instantiate a DAG which will be the starting point of any workflow.\n\nThe required arguments are: dag_id which is the name Airflow web UI uses to display workflow. start_date which is when to start running the workflow, it can be in the past\nThere are other arguments such as schedule_interval which determines the schedule to rerun the DAG\n\nOperator is responsible for a piece of work and almost represents a task.\n\nIt has task_id which is the name web UI uses to display the task\nThere are many operators such as BashOperator, PythonOperator … All of them inherits from BaseOperator\nSome operators are generic such as BashOperator and some are specific such as EmailOperator\n\nTask is a wrapper/manager over operator that makes sure the operator gets executed\n&gt;&gt; represents the dependencies between tasks\n\na &gt;&gt; b means a should run before b\n\nAirflow UI offers two views:\n\ntree view that shows the DAG runs over time. Each column is one run. Each row is a task. So we can inspect status of tasks over time\ngraph view that shows the DAG as a graph which helps showing the dependencies of tasks in the workflow\n\nIf any task failed, all successive tasks that depend on it don’t run\n\nWe can rerun the failed tasks (which also would cause successive tasks to rerun) w/o having to rerun the workflow from scratch\nWe can inspect the logs to see what was the reason for the errors\n\nTasks can run in parallel depending on their dependencies\nTo setup Airflow locally inside Python virtual env:\n\npip install apache-airflow\nairflow init db # Initialize metastore locally using SQLite; not recommended for production\nairflow users create –username admin –password admin –firstname Anonymous –lastname Admin –role Admin –email admin@example.org # Create user\nairflow webserver # Start web server to use web UI\nairflow scheduler # Start scheduler, don’t use sequential in production\n\n\n\nimport airflow\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\nf = lambda: print(1)\ndag = DAG(dag_id=\"simple-workflow\", start_date=airflow.utils.dates.days_ago(10))\na = BashOperator(task_id=\"bash\", bash_command=\"echo 'a'\", dag=dag)\nb = PythonOperator(task_id=\"python\", python_callable=f, dag=dag)\na &gt;&gt; b\n\n&lt;Task(PythonOperator): python&gt;"
  },
  {
    "objectID": "posts/airflow/Triggering-Workflows.html",
    "href": "posts/airflow/Triggering-Workflows.html",
    "title": "Airflow Part 7 - Triggering Workflows",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nWorkflows are most commonly triggered based on schedule intervals provided using start_date, end_date , schedule_interval. Airflow would calculate when the next schedule would be and start the first task(s) to run at the next data/time.\nHowever, sometimes we want the workflow to run based on the occurance of external events such as a file is available in specific location OR code is changed on git repo etc.\nOne way to execute workflows based on the occurance of external exents is using Airflow’s sensors. Sensor is a subclass of operators that checks if certain condition is true. If true, execute the step (workflow). If false, wait for a given period (default 60 seconds) and tries again. It keeps doing so for timeout period. This is a form of Poking, which is checking for the existence of file in the case of FileSensor.\n\nfrom airflow.sensors.filesystem import FileSensor\nwait_for_file_1 = FileSensor(\n    task_id=\"wait_for_file_1\", filepath=\"/data/file_1.csv\"\n    )\n\nWe can also use globbing with FileSensors by using wildcards to check for the existence of file(s)\nWe can also use PythonSensor which checks for certain condition and must return a Boolean. It is more flexible and easier to read than using globbing within FileSensor. It is the same as PythonOperator in terms of taking a Python callable\n\nfrom pathlib import Path\nfrom airflow.sensors.python import PythonSensor\n\n# Check whether there is any data for a given supermarker\n# and there is _SUCCESS path which indicates whether the \n# data for the given supermarket is all uploaded\ndef _wait_for_supermarket(supermarket):\n    supermarket_path = Path(\"/data\") / supermarket\n    success_path = Path(\"/data\") / \"_SUCCESS\"\n    data_files = supermarketpath.glob(\"*.csv\")\n    return data_files and success_path.exists()\n\nwait_for_supermarket_1 = PythonSensor(\n    task_id=\"wait_for_supermarket_1\",\n    python_callable=_wait_for_supermarket,\n    op_kwargs={\"supermarket\": \"supermarket_1\"},\n    dag=dag\n    )\n\n\nAll sensors take a timeout arguments, which has default value of 7 days\nThere is also a limit on the number of tasks Airflow can run concurrently per DAG (default is 16). DAG takes concurrency argument that can change this number. There is also a limit on the number of tasks per global Airflow and the number DAG runs per DAG\n\nwait_for_supermarket_1 = PythonSensor(\n    task_id=\"wait_for_supermarket_1\",\n    python_callable=_wait_for_supermarket,\n    op_kwargs={\"supermarket\": \"supermarket_1\"},\n    concurreny=20, # Default is 16\n    dag=dag\n    )\n\nThere is snowball effect when sensors don’t succeed. The occupy slots that DAG has (which is determined by the concurrency argument. From the above figure, if only task 1 succeeds and the rest keeps polling and the DAG is scheduled daily with default concurrency of 16 slots and default timeout of 7 days, this is what will happen (sensor deadlock):\n\nDay 1: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 3 tasks.\nDay 2: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 6 tasks.\nDay 3: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 9 tasks.\nDay 4: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 12 tasks.\nDay 5: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 15 tasks.\nDay 6: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 16 tasks; two new tasks cannot run, and any other task trying to run is blocked.\n\n\n\n\nThis also affect the global Airflow limit of maximum number of tasks that can run concurrently, which may lead to whole system get stalled.\nFor sensor task, it pokes to check the condition and block if it is false. So it would run for a little bit and wait for the most part. It keeps poking untel the timeout period is completed, which means it keeps occupying the slot until the condition becomes true or timeout is reached\nmode argument which has two values: {poking, reschedule}. The default is poking. Reschedule can solve the sensor deadlock and snowball effect because it releases the slot the sensor task is occupying after the slot has finished poking. In other words, sensor task would poke, if condition if false, the system will reschedule it and take its slot and make it available to other tasks. It is the same concept as process scheduling that the OS does when a process does a blocking system call.\n\nwait_for_supermarket_1 = PythonSensor(\n    task_id=\"wait_for_supermarket_1\",\n    python_callable=_wait_for_supermarket,\n    op_kwargs={\"supermarket\": \"supermarket_1\"},\n    mode=\"reschedule\",\n    dag=dag\n    )\n\nWe can trigger another DAG to run from inside another DAG using TriggerDagRunOperator. This will cause another DAG to run once the trigger_operator runs which is useful if we want to split DAGs and make some DAGs available to other DAGs instead of repearing functionality. See below for both approaches:  \n\nfrom pathlib import Path\n\nimport airflow.utils.dates\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nfrom airflow.sensors.python import PythonSensor\n\ndag1 = DAG(\n    dag_id=\"ingest_supermarket_data\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=\"0 16 * * *\",\n)\ndag2 = DAG(\n    dag_id=\"create_metrics\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=None, # Since it will be triggered\n)\n\n\ndef _wait_for_supermarket(supermarket_id_):\n    supermarket_path = Path(\"/data/\" + supermarket_id_)\n    data_files = supermarket_path.glob(\"data-*.csv\")\n    success_file = supermarket_path / \"_SUCCESS\"\n    return data_files and success_file.exists()\n\n\nfor supermarket_id in range(1, 5):\n    wait = PythonSensor(\n        task_id=f\"wait_for_supermarket_{supermarket_id}\",\n        python_callable=_wait_for_supermarket,\n        op_kwargs={\"supermarket_id_\": f\"supermarket{supermarket_id}\"},\n        dag=dag1,\n    )\n    copy = DummyOperator(task_id=f\"copy_to_raw_supermarket_{supermarket_id}\", dag=dag1)\n    process = DummyOperator(task_id=f\"process_supermarket_{supermarket_id}\", dag=dag1)\n    trigger_create_metrics_dag = TriggerDagRunOperator(\n        task_id=f\"trigger_create_metrics_dag_supermarket_{supermarket_id}\",\n        trigger_dag_id=\"create_metrics\", # Has to be the same dag_id as dag2\n        dag=dag1,\n    )\n    wait &gt;&gt; copy &gt;&gt; process &gt;&gt; trigger_create_metrics_dag\n\ncompute_differences = DummyOperator(task_id=\"compute_differences\", dag=dag2)\nupdate_dashboard = DummyOperator(task_id=\"update_dashboard\", dag=dag2)\nnotify_new_data = DummyOperator(task_id=\"notify_new_data\", dag=dag2)\ncompute_differences &gt;&gt; update_dashboard\n\nEach DAG run has a run_id that starts with one of the following:\n\nscheduled__ to indicate the DAG run started because of its schedule\nbackfill__ to indicate the DAG run started by a backfill job\nmanual__ to indicate the DAG run started by a manual action (e.g., pressing the Trigger Dag button, or triggered by a TriggerDagRunOperator)\n\nFrom the UI, scheduled DAGs have their task instance in black border while Triggered DAGs don’t\nClearing a task in a DAG will clear the task and all its downstream tasks and trigger a run (backfill)\n\nIt only clears tasks within the same DAG, NOT downstream tasks in another DAG of TriggerDagRunOperator\n\nIf the triggered DAG has dependency on multiple triggering DAGs to be completed before it can run, then we can use ExternalTaskSensor that checks whether the task has been completed successfully (sensor poking the state of tasks in another DAGs). Each ExternalTaskSensor checks for only 1 task by querying the metastore database\n\nBy default, it uses the same execution_date as itself\nIf the task runs on different schedule, we then need to provide timedelta object to execution_delta argument to get what would be the execution_date of the task it tries to sense\n\n\nimport datetime\n\nimport airflow.utils.dates\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\ndag1 = DAG(\n    dag_id=\"ingest_supermarket_data\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=\"0 16 * * *\",\n)\ndag2 = DAG(\n    dag_id=\"create_metrics\",\n    start_date=airflow.utils.dates.days_ago(3),\n    schedule_interval=\"0 18 * * *\",\n)\n\nDummyOperator(task_id=\"copy_to_raw\", dag=dag1) &gt;&gt; DummyOperator(\n    task_id=\"process_supermarket\", dag=dag1\n)\n\nwait = ExternalTaskSensor(\n    task_id=\"wait_for_process_supermarket\",\n    external_dag_id=\"figure_6_20_dag_1\",\n    external_task_id=\"process_supermarket\",\n    # positive # will be subtracted from the execution_date of task sensor\n    # to get the execution_date of the task it is trying to sense\n    execution_delta=datetime.timedelta(hours=6),  \n    dag=dag2,\n)\nreport = DummyOperator(task_id=\"report\", dag=dag2)\nwait &gt;&gt; report\n\nWe can also trigger DAGs from CLI which will have execution_date of the current data and time\n\nairflow dags trigger dag1\nWith configuration; which will be available in the context of each task using context[“dag_run”].conf:\n\nairflow dags trigger -c '{\"supermarket_id\": 1}' dag1\nairflow dags trigger --conf '{\"supermarket_id\": 1}' dag1"
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#introduction",
    "href": "posts/conda-essentials/conda-essentials.html#introduction",
    "title": "Conda Essentials",
    "section": "Introduction",
    "text": "Introduction\nConda in an open source package management system that works on all platforms. It is a tool that helps manage packages and environments for different programming languages. Develop a high level understanding of how Conda works helped me at so many levels especially when it comes to managing environments and make my work more reproducable. Below are the notes that I wrote down during my journey of learning Conda and I always refere back to them:"
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#general",
    "href": "posts/conda-essentials/conda-essentials.html#general",
    "title": "Conda Essentials",
    "section": "General",
    "text": "General\n\nConda packages are files and executables that can in principle contain images, data, noteboeeks, files, etc.\nConda mainly used in Python ecosystem; however, it can be used with other languages such R, Julia, Scala, etc.\nWhen installing a package using Conda, it installs its dependencies with it. Also, Conda is able to figure out the platform you’re using without the need to specify the platform when installing packages.\nWhen installing a package, Conda:\n\nChecks the platform.\nChecks the Python version.\nInstall the latest version of the package that is compatible with Python.\nIf it has dependencies, installs the latest versions of the dependencies that are also compatible with each other.\n\nUnder semantic versioning, software is labeled with a three-part version identifier of the form MAJOR.MINOR.PATCH; the label components are non-negative integers separated by periods. Assuming all software starts at version 0.0.0, the MAJOR version number is increased when significant new functionality is introduced (often with corresponding API changes). Increases in the MINOR version number generally reflect improvements (e.g., new features) that avoid backward-incompatible API changes. For instance, adding an optional argument to a function API (in a way that allows old code to run unchanged) is a change worthy of increasing the MINOR version number. An increment to the PATCH version number is approriate mostly for bug fixes that preserve the same MAJOR and MINOR revision numbers. Software patches do not typically introduce new features or change APIs at all (except sometimes to address security issues).\nWe can specify MAJOR, MAJOR.MINOR, or MAJOR.MINOR.PATCH when installing any package.\nWe can use logical operators to install versions of a package. Examples:\n\nconda install 'python=3.6|3.7'.\nconda install 'python=3.6|3.7*' .\nconda install 'python&gt;=3.6, &lt;=3.7'."
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#common-commands",
    "href": "posts/conda-essentials/conda-essentials.html#common-commands",
    "title": "Conda Essentials",
    "section": "Common Commands",
    "text": "Common Commands\n\nTo update a package, conda update pckg.\nTo uninstall a package, conda remove pckg.\nTo search what available versions of a specific package is available, use conda search pckg.\nconda list will list all installed packages.\nconda list -n env-name will list all packages in the environment env-name.\nconda list pckg will give information about pckg.\nWhen installing a pckg without including a channel, it defaults to the main channel that is maintained by Anaconda Inc.\nThere other channels where people can upload their packages to and we can reach to those channels when looking for installation such fastai. We use conda install -c fastai fastai. Here the channel is fastai and the pckg is also fastai.\nconda search -c conda-forge -c fastai --override-channels --platform osx-64 fastai means:\n\nSearch for fastai in two channels: conda-forge, fastai.\noverride-channels means do not go to default main channel.\nplatform specify which platform.\n\nSometimes we don’t know the channel of the pckg, we can use anaconda search pckg that will return all the channels that the pckg is at and their versions.\nconda-forge is almost as good as the main channels which is led by the community. It has a lot more packages than the main channel.\nThere is no system that rates channels, so be carefel when installing packages from any channel.\nWe can list all packages in a channel such as conda search -c conda-forge --override-channels that will list all packages for the conda-forge channel."
  },
  {
    "objectID": "posts/conda-essentials/conda-essentials.html#environments",
    "href": "posts/conda-essentials/conda-essentials.html#environments",
    "title": "Conda Essentials",
    "section": "Environments",
    "text": "Environments\n\nEnvironments are a good practice of documenting data science/software development work.\nEnvironments are nothing more than a directory that contains all the packages so that when trying to import them, it imports them from this directory only. we can use conda env list to see all the available environments on our machine.\nTo get the packages from a specific environment by name, use conda list -n env-name. Otherwise, we get the packages from the current environment.\nTo activate an environment, use conda activate env-name. To deactivate, conda deactivate.\nEnvironments usually don’t take a lot of space.\nWe can remove environments using conda env remove -n env-name.\nTo create an environment, use conda create -n env-name. We can also add additional package names to install after creation such as conda create -n env-name python=3.6* numpy&gt;=1.1.\nTo export an environment, use conda env export -n env-name. This will return the output to the terminal. We can also export to a file. For that use conda env export -n env-name -f env-name.yml. The ‘.yml’ extension is strongly enouraged. Doing this will assure that all the packages used can be installed by others exactly.\nWe can create also an environment from .yml file using conda env create -f env-name.yml. Note also that if we only use conda env create, it will look for a file that has .yml extension and has the same name as env-name in the current local directory. Moreover, we can create the .yml file with doing the export ourselves and only specify what is important in our environments."
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#introduction",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#introduction",
    "title": "Character-Level Language Model",
    "section": "Introduction",
    "text": "Introduction\nHave you ever wondered how Gmail automatic reply works? Or how a Neural Network can generate musical notes? The general way of generating sequence of text is to train a model to predict the next word/character given all previous words/characters. Such model is called Statistical Language Model. So what is a statistical language model? A statistical language model tries to capture the statistical structure (latent space) of training text it’s trained on. Usually Recurrent Neural Network (RNN) models family is used to train the model due to the fact that it’s very powerful and expressive in which they remember and process past information through their high dimensional hidden state units. The main goal of any language model is to learn the joint probability distribution of sequences of characters/words in a training text, i.e. trying to learn the joint probability function. For example, if we’re trying to predict a sequence of \\(T\\) words, we try to get the joint probability \\(P(w_1, w_2, ..., w_T)\\) as big as we can which is equal to the product of all conditional probabilities \\(\\prod_{t = 1}^T P(w_t/w_{t-1})\\) at all time steps (t).\nIn this post, we’ll cover Character-level Language Model where almost all the concepts hold for any other language models such as word-language models. The main task of character-level language model is to predict next character given all previous characters in a sequence of data, i.e. generate text character by character. More formally, given a training sequence \\((x^1, ... , x^T)\\), the RNN uses the sequence of its output vectors \\((o^1, ... , o^T)\\) to obtain a sequence of predictive distributions \\(P(x^t|x^{&lt;t}) = softmax(o^t)\\).\nLet’s illustrate how the character-level language model works using my first name (“imad”) as an example (see figure 1 for all the details of this example).\n\nWe first build a vocabulary dictionary using all the unique letters of the names in the corpus as keys and the index of each letter starting from zero (since python is a zero-index language) in ascending order. For our example, the vocabulary dictionary would be: {“a”: 0, “d”: 1, “i”: 2, “m”: 3}. Therefore, “imad” would become a list of the following integers: [2, 3, 0, 1].\nConvert the input and the output characters to lists of integers using the vocabulary dictionary. In this post, we’ll assume that \\(x^1 = \\vec{0}\\) for all examples. Therefore, \\(y = \"imad\"\\) and \\(x = \\vec{0}\\ + \"ima\"\\). In other words, \\(x^{t + 1} = y^t\\) which gives us: \\(y = [2, 3, 0, 1]\\) and \\(x = [\\vec{0}, 2, 3, 0]\\).\nFor each character in the input:\n\nConvert the input characters into one-hot vectors. Notice how the first character \\(x^1 = \\vec{0}\\).\nCompute the hidden state layer.\nCompute the output layer and then pass it through softmax to get the results as probabilities.\nFeed the target character at time step (t) as the input character at time step \\((t + 1)\\).\nGo back to step A and repeat until we finish all the letters in the name.\n\n\nThe objective is to make the green numbers as big as we can and the red numbers as small as we can in the probability distribution layer. The reason for that is that the true index should have the highest probability by making it as close as we can to 1. The way to do that is to measure the loss using cross-entropy and then compute the gradients of the loss w.r.t. all parameters to update them in the opposite of the gradient direction. Repeating the process over many times where each time we adjust the parameters based on the gradient direction –&gt; model will be able to correctly predict next characters given all previous one using all names in the training text. Notice that hidden state \\(h^4\\) has all past information about all characters.\n\n\n\n\nFigure 1: Illustrative example of character-level language model using RNN"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#training",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#training",
    "title": "Character-Level Language Model",
    "section": "Training",
    "text": "Training\nThe dataset we’ll be using has 5,163 names: 4,275 male names, 1,219 female names, and 331 names that can be both female and male names. The RNN architecture we’ll be using to train the character-level language model is called many to many where time steps of the input \\((T_x)\\) = time steps of the output \\((T_y)\\). In other words, the sequence of the input and output are synced (see figure 2).\n\n\n\n\nFigure 2: RNN architecture: many to many\n\n\n\nThe character-level language model will be trained on names; which means after we’re done with training the model, we’ll be able to generate interesting names :).\nIn this section, we’ll go over four main parts:\n\nForward propagation.\nBackpropagation\nSampling\nFitting the model"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#forward-propagation",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#forward-propagation",
    "title": "Character-Level Language Model",
    "section": "Forward Propagation",
    "text": "Forward Propagation\nWe’ll be using Stochastic Gradient Descent (SGD) where each batch consists of only one example. In other words, the RNN model will learn from each example (name) separately, i.e. run both forward and backward passes on each example and update parameters accordingly. Below are all the steps needed for a forward pass:\n\nCreate a vocabulary dictionary using the unique lower case letters.\n\nCreate a character to index dictionary that maps each character to its corresponding index in an ascending order. For example, “a” would have index 1 (since python is a zero index language and we’ll reserve 0 index to EOS “”) and “z” would have index 26. We will use this dictionary in converting names into lists of integers where each letter will be represented as one-hot vector.\nCreate an index to character dictionary that maps indices to characters. This dictionary will be used to convert the output of the RNN model into characters which will be translated into names.\n\nInitialize parameters: weights will be initialized to small random numbers from standard normal distribution to break symmetry and make sure different hidden units learn different things. On the other hand, biases will be initialized to zeros.\n\n\\(W_{hh}\\): weight matrix connecting previous hidden state \\(h^{t - 1}\\) to current hidden state \\(h^t\\).\n\\(W_{xh}\\): weight matrix connecting input \\(x^t\\) to hidden state \\(h^t\\).\n\\(b\\): hidden state bias vector.\n\\(W_{hy}\\): weight matrix connecting hidden state \\(h^t\\) to output \\(o^t\\).\n\\(c\\): output bias vector.\n\nConvert input \\(x^t\\) and output \\(y^t\\) into one-hot vector each. The dimension of the one-hot vector is vocab_size x 1. Everything will be zero except for the index of the letter at (t) would be 1. In our case, \\(x^t\\) would be the same as \\(y^t\\) shifted to the left where \\(x^1 = \\vec{0}\\); however, starting from \\(t = 2\\), \\(x^{t + 1} = y^{t}\\). For example, if we use “imad” as the input, then \\(y = [3, 4, 1, 2, 0]\\) while \\(x = [\\vec{0}, 3, 4, 1, 2]\\). Notice that \\(x^1 = \\vec{0}\\) and not the index 0. Moreover, we’re using “” as EOS (end of sentence/name) for each name so that the RNN learns “” as any other character so that it knows when to stop generating characters. Therefore, the last target character for all names will be “” that represents the end of the name.\nCompute the hidden state using the following formula: \\[h^t = tanh(W_{hh}h^{t-1} + W_{xh}x^t + b)\\tag{1}\\\\{}\\] Notice that we use hyperbolic tangent \\((\\frac{e^x - e^{-x}}{e^x + e^{-x}})\\) as the non-linear function. One of the main advantages of the hyperbolic tangent function is that it resembles the identity function.\nCompute the output layer using the following formula: \\[o^t = W_{hy}h^{t} + c\\tag{2}\\\\{}\\]\nPass the output through softmax layer to normalize the output that allows us to express it as a probability, i.e. all output will be between 0 and 1 and sum up to 1. Below is the softmax formula: \\[y^t = \\frac{e^{o^t}}{\\sum_ie^{o^t}}\\tag{3}\\\\{}\\] The softmax layer has the same dimension as the output layer which is vocab_size x 1. As a result, \\(y^t[i]\\) is the probability of of index \\(i\\) being the next character at time step (t).\nAs mentioned before, the objective of a character-level language model is to minimize the negative log-likelihood of the training sequence. Therefore, the loss function at time (t) and the total loss across all time steps are: \\[\\mathcal{L}^t = -\\sum_{i = 1}^{T_y}y^tlog\\widehat{y^t}\\tag{4}\\\\{}\\] \\[\\mathcal{L} = \\sum_{t = 1}^{T_y}\\mathcal{L}^t(\\widehat{y^t}, y^t)\\tag{5}\\] Since we’ll be using SGD, the loss will be noisy and have many oscillations, so it’s a good practice to smooth out the loss using exponential weighted average.\nPass the target character \\(y^t\\) as the next input \\(x^{t + 1}\\) until we finish the sequence.\n\n\n\nCode\n# Load packages\n# | warning: false\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nos.chdir(\"../scripts/\")\nfrom character_level_language_model import (\n    initialize_parameters,\n    initialize_rmsprop,\n    softmax,\n    smooth_loss,\n    update_parameters_with_rmsprop,\n)\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\n\n\n\n\nCode\ndef rnn_forward(x, y, h_prev, parameters):\n    \"\"\"\n    Implement one Forward pass on one name.\n\n    Arguments\n    ---------\n    x : list\n        list of integers for the index of the characters in the example\n        shifted one character to the right.\n    y : list\n        list of integers for the index of the characters in the example.\n    h_prev : array\n        last hidden state from the previous example.\n    parameters : python dict\n        dictionary containing the parameters.\n\n    Returns\n    -------\n    loss : float\n        cross-entropy loss.\n    cache : tuple\n        contains three python dictionaries:\n            xs -- input of all time steps.\n            hs -- hidden state of all time steps.\n            probs -- probability distribution of each character at each time\n                step.\n    \"\"\"\n    # Retrieve parameters\n    Wxh, Whh, b = parameters[\"Wxh\"], parameters[\"Whh\"], parameters[\"b\"]\n    Why, c = parameters[\"Why\"], parameters[\"c\"]\n\n    # Initialize inputs, hidden state, output, and probabilities dictionaries\n    xs, hs, os, probs = {}, {}, {}, {}\n\n    # Initialize x0 to zero vector\n    xs[0] = np.zeros((vocab_size, 1))\n\n    # Initialize loss and assigns h_prev to last hidden state in hs\n    loss = 0\n    hs[-1] = np.copy(h_prev)\n\n    # Forward pass: loop over all characters of the name\n    for t in range(len(x)):\n        # Convert to one-hot vector\n        if t &gt; 0:\n            xs[t] = np.zeros((vocab_size, 1))\n            xs[t][x[t]] = 1\n        # Hidden state\n        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)\n        # Logits\n        os[t] = np.dot(Why, hs[t]) + c\n        # Probs\n        probs[t] = softmax(os[t])\n        # Loss\n        loss -= np.log(probs[t][y[t], 0])\n\n    cache = (xs, hs, probs)\n\n    return loss, cache"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#backpropagation",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#backpropagation",
    "title": "Character-Level Language Model",
    "section": "Backpropagation",
    "text": "Backpropagation\nWith RNN based models, the gradient-based technique that will be used is called Backpropagation Through Time (BPTT). We start at last time step \\(T\\) and backpropagate loss function w.r.t. all parameters across all time steps and sum them up (see figure 3).\n\n\n\n\nFigure 3: Backpropagation Through Time (BPTT)\n\n\n\nIn addition, since recurrent networks are known to have steep cliffs (sudden steep decrease in \\(\\mathcal{L}\\)), gradients may overshoot the minimum and undo a lot of the work that was done even if we are using adaptive learning methods such as RMSProp. The reason for that is that because gradient is a linear approximation of the loss function and may not capture information further than the point it was evaluated on such as the curvature of loss curve. Therefore, it’s a common practice to clip the gradients to be in the interval [-maxValue, maxValue]. For this exercise, we’ll clip the gradients to be in the interval [-5, 5]. That means if the gradient is &gt; 5 or &lt; -5, it would be clipped to 5 and -5 respectively. Below are all the formulas needed to compute the gradients w.r.t. all parameters at all time steps.\n\\[\\nabla_{o^t}\\mathcal{L} = \\widehat{y^t} - y^t\\tag{6}\\\\{}\\] \\[\\nabla_{W_{hy}}\\mathcal{L} = \\sum_t \\nabla_{o^t}\\mathcal{L}\\cdot{h^t}^T\\tag{7}\\\\{}\\] \\[\\nabla_{c}\\mathcal{L} = \\sum_t \\nabla_{o^t}\\mathcal{L} \\tag{8}\\\\{}\\] \\[\\nabla_{h^t}\\mathcal{L} = W_{hy}^T\\cdot\\nabla_{o^t}\\mathcal{L} + \\underbrace { W_{hh}^T\\cdot\\nabla_{h^{t + 1}}\\mathcal{L} * (1 - tanh(W_{hh}h^{t} + W_{xh}x^{t + 1} + b) ^ 2)}_{dh_{next}} \\tag{9}\\\\{}\\] \\[\\nabla_{h^{t - 1}}\\mathcal{L} = W_{hh}^T\\cdot\\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2)\\tag{10}\\\\{}\\] \\[\\nabla_{x^t}\\mathcal{L} = W_{xh}^T\\cdot\\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2)\\tag{11}\\\\{}\\] \\[\\nabla_{W_{hh}}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2)\\cdot{h^{t - 1}}^T\\tag{12}\\\\{}\\] \\[\\nabla_{W_{xh}}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(W_{hh}\\cdot h^{t-1} + W_{xh}\\cdot x^t + b) ^ 2) . {x^t}^T\\tag{13}\\\\{}\\] \\[\\nabla_{b}\\mathcal{L} = \\sum_t \\nabla_{h^t}\\mathcal{L} * (1 - tanh(h^t) ^ 2) \\tag{14}\\\\{}\\]\nNote that at last time step \\(T\\), we’ll initialize \\(dh_{next}\\) to zeros since we can’t get values from future. To stabilize the update at each time step since SGD may have so many oscillations, we’ll be using one of the adaptive learning methods’ optimizer. More specifically, Root Mean Squared Propagation (RMSProp) which tends to have acceptable performance.\n\n\nCode\ndef clip_gradients(gradients, max_value):\n    \"\"\"\n    Implements gradient clipping element-wise on gradients to be between the\n    interval [-max_value, max_value].\n\n    Arguments\n    ----------\n    gradients : python dict\n        dictionary that stores all the gradients.\n    max_value : scalar\n        edge of the interval [-max_value, max_value].\n\n    Returns\n    -------\n    gradients : python dict\n        dictionary where all gradients were clipped.\n    \"\"\"\n    for grad in gradients.keys():\n        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])\n\n    return gradients\n\n\ndef rnn_backward(y, parameters, cache):\n    \"\"\"\n    Implements Backpropagation on one name.\n\n    Arguments\n    ---------\n    y : list\n        list of integers for the index of the characters in the example.\n    parameters : python dict\n        dictionary containing the parameters.\n    cache : tuple\n            contains three python dictionaries:\n                xs -- input of all time steps.\n                hs -- hidden state of all time steps.\n                probs -- probability distribution of each character at each time\n                    step.\n\n    Returns\n    -------\n    grads : python dict\n        dictionary containing all the gradients.\n    h_prev : array\n        last hidden state from the current example.\n    \"\"\"\n    # Retrieve xs, hs, and probs\n    xs, hs, probs = cache\n\n    # Initialize all gradients to zero\n    dh_next = np.zeros_like(hs[0])\n\n    parameters_names = [\"Whh\", \"Wxh\", \"b\", \"Why\", \"c\"]\n    grads = {}\n    for param_name in parameters_names:\n        grads[\"d\" + param_name] = np.zeros_like(parameters[param_name])\n\n    # Iterate over all time steps in reverse order starting from Tx\n    for t in reversed(range(len(xs))):\n        dy = np.copy(probs[t])\n        dy[y[t]] -= 1\n        grads[\"dWhy\"] += np.dot(dy, hs[t].T)\n        grads[\"dc\"] += dy\n        dh = np.dot(parameters[\"Why\"].T, dy) + dh_next\n        dhraw = (1 - hs[t] ** 2) * dh\n        grads[\"dWhh\"] += np.dot(dhraw, hs[t - 1].T)\n        grads[\"dWxh\"] += np.dot(dhraw, xs[t].T)\n        grads[\"db\"] += dhraw\n        dh_next = np.dot(parameters[\"Whh\"].T, dhraw)\n        # Clip the gradients using [-5, 5] as the interval\n        grads = clip_gradients(grads, 5)\n    # Get the last hidden state\n    h_prev = hs[len(xs) - 1]\n\n    return grads, h_prev"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#sampling",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#sampling",
    "title": "Character-Level Language Model",
    "section": "Sampling",
    "text": "Sampling\nSampling is what makes the text generated by the RNN at each time step an interesting/creative text. On each time step (t), the RNN output the conditional probability distribution of the next character given all the previous characters, i.e. \\(P(c_t/c_1, c_2, ..., c_{t-1})\\). Let’s assume that we are at time step \\(t = 3\\) and we’re trying to predict the third character, the conditional probability distribution is: \\(P(c_3/c_1, c_2) = (0.2, 0.3, 0.4, 0.1)\\). We’ll have two extremes:\n\nMaximum entropy: the character will be picked randomly using uniform probability distribution; which means that all characters in the vocabulary dictionary are equally likely. Therefore, we’ll end up with maximum randomness in picking the next character and the generated text will not be either meaningful or sound real.\nMinimum entropy: the character with the highest conditional probability will be picked on each time step. That means next character will be what the model estimates to be the right one based on the training text and learned parameters. As a result, the name generated will be both meaningful and sound real. However, it will also be repetitive and interesting since all the parameters were optimized to learn joint probability distribution in predicting the next character.\n\nAs we increase randomness, text will loose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure. For this exercise, we will sample from the distribution that’s generated by the model which can be seen as an intermediate level of randomness between maximum and minimum entropy (see figure 4). Using this sampling strategy on the above distribution, the index 0 has \\(20\\)% probability of being picked, while index 2 has \\(40\\)% probability to be picked.\n\n\n\n\nFigure 4: Sampling: An example of predicting next character using character-level language model\n\n\n\nTherefore, sampling will be used at test time to generate names character by character.\n\n\nCode\ndef sample(parameters, idx_to_chars, chars_to_idx, n):\n    \"\"\"\n    Implements sampling of a squence of n characters characters length. The\n    sampling will be based on the probability distribution output of RNN.\n\n    Arguments\n    ---------\n    parameters : python dict\n        dictionary storing all the parameters of the model.\n    idx_to_chars : python dict\n        dictionary mapping indices to characters.\n    chars_to_idx : python dict\n        dictionary mapping characters to indices.\n    n : scalar\n        number of characters to output.\n\n    Returns\n    -------\n    sequence : str\n        sequence of characters sampled.\n    \"\"\"\n    # Retrienve parameters, shapes, and vocab size\n    Whh, Wxh, b = parameters[\"Whh\"], parameters[\"Wxh\"], parameters[\"b\"]\n    Why, c = parameters[\"Why\"], parameters[\"c\"]\n    n_h, n_x = Wxh.shape\n    vocab_size = c.shape[0]\n\n    # Initialize a0 and x1 to zero vectors\n    h_prev = np.zeros((n_h, 1))\n    x = np.zeros((n_x, 1))\n\n    # Initialize empty sequence\n    indices = []\n    idx = -1\n    counter = 0\n    while counter &lt;= n and idx != chars_to_idx[\"\\n\"]:\n        # Fwd propagation\n        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)\n        o = np.dot(Why, h) + c\n        probs = softmax(o)\n\n        # Sample the index of the character using generated probs distribution\n        idx = np.random.choice(vocab_size, p=probs.ravel())\n\n        # Get the character of the sampled index\n        char = idx_to_chars[idx]\n\n        # Add the char to the sequence\n        indices.append(idx)\n\n        # Update a_prev and x\n        h_prev = np.copy(h)\n        x = np.zeros((n_x, 1))\n        x[idx] = 1\n\n        counter += 1\n    sequence = \"\".join([idx_to_chars[idx] for idx in indices if idx != 0])\n\n    return sequence"
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#fitting-the-model",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#fitting-the-model",
    "title": "Character-Level Language Model",
    "section": "Fitting the model",
    "text": "Fitting the model\nAfter covering all the concepts/intuitions behind character-level language model, now we’re ready to fit the model. We’ll use the default settings for RMSProp’s hyperparameters and run the model for 100 iterations. On each iteration, we’ll print out one sampled name and smoothed loss to see how the names generated start to get more interesting with more iterations as well as the loss will start decreasing. When done with fitting the model, we’ll plot the loss function and generate some names.\n\n\nCode\ndef model(\n    file_path,\n    chars_to_idx,\n    idx_to_chars,\n    hidden_layer_size,\n    vocab_size,\n    num_epochs=10,\n    learning_rate=0.01,\n):\n    \"\"\"\n    Implements RNN to generate characters.\n\n    Arguments\n    ---------\n    file_path : str\n        path to the file of the raw data.\n    num_epochs : int\n        number of passes the optimization algorithm to go over the training\n        data.\n    learning_rate : float\n        step size of learning.\n    chars_to_idx : python dict\n        dictionary mapping characters to indices.\n    idx_to_chars : python dict\n        dictionary mapping indices to characters.\n    hidden_layer_size : int\n        number of hidden units in the hidden layer.\n    vocab_size : int\n        size of vocabulary dictionary.\n\n    Returns\n    -------\n    parameters : python dict\n        dictionary storing all the parameters of the model.\n    overall_loss : list\n        list stores smoothed loss per epoch.\n    \"\"\"\n    # Get the data\n    with open(file_path) as f:\n        data = f.readlines()\n    examples = [x.lower().strip() for x in data]\n\n    # Initialize parameters\n    parameters = initialize_parameters(vocab_size, hidden_layer_size)\n\n    # Initialize Adam parameters\n    s = initialize_rmsprop(parameters)\n\n    # Initialize loss\n    smoothed_loss = -np.log(1 / vocab_size) * 7\n\n    # Initialize hidden state h0 and overall loss\n    h_prev = np.zeros((hidden_layer_size, 1))\n    overall_loss = []\n\n    # Iterate over number of epochs\n    for epoch in range(num_epochs):\n        # Shuffle examples\n        np.random.shuffle(examples)\n\n        # Iterate over all examples (SGD)\n        for example in examples:\n            x = [None] + [chars_to_idx[char] for char in example]\n            y = x[1:] + [chars_to_idx[\"\\n\"]]\n            # Fwd pass\n            loss, cache = rnn_forward(x, y, h_prev, parameters)\n            # Compute smooth loss\n            smoothed_loss = smooth_loss(smoothed_loss, loss)\n            # Bwd pass\n            grads, h_prev = rnn_backward(y, parameters, cache)\n            # Update parameters\n            parameters, s = update_parameters_with_rmsprop(parameters, grads, s)\n\n        overall_loss.append(smoothed_loss)\n        if epoch % 10 == 0:\n            print(f\"\\033[1m\\033[94mEpoch {epoch}\")\n            print(f\"\\033[1m\\033[92m=======\")\n            # Sample one name\n            print(\n                f\"\"\"Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,\n                10).capitalize()}\"\"\"\n            )\n            print(f\"Smoothed loss: {smoothed_loss:.4f}\\n\")\n\n    return parameters, overall_loss\n\n\n\n\nCode\n# Load names\ndata = open(\"../data/names.txt\", \"r\").read()\n\n# Convert characters to lower case\ndata = data.lower()\n\n# Construct vocabulary using unique characters, sort it in ascending order,\n# then construct two dictionaries that maps character to index and index to\n# characters.\nchars = list(sorted(set(data)))\nchars_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_chars = {i: ch for ch, i in chars_to_idx.items()}\n\n# Get the size of the data and vocab size\ndata_size = len(data)\nvocab_size = len(chars_to_idx)\nprint(f\"There are {data_size} characters and {vocab_size} unique characters.\")\n\n# Fitting the model\nparameters, loss = model(\n    \"../data/names.txt\", chars_to_idx, idx_to_chars, 10, vocab_size, 50, 0.01\n)\n\n# Plotting the loss\nplt.plot(range(len(loss)), loss)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Smoothed loss\")\n\n\nThere are 36121 characters and 27 unique characters.\nEpoch 0\n=======\nSampled name: Ia\nSmoothed loss: 17.8206\n\nEpoch 10\n=======\nSampled name: Rioee\nSmoothed loss: 15.8061\n\nEpoch 20\n=======\nSampled name: Allise\nSmoothed loss: 15.8609\n\nEpoch 30\n=======\nSampled name: Ininyo\nSmoothed loss: 15.7734\n\nEpoch 40\n=======\nSampled name: Miadoe\nSmoothed loss: 15.7312\n\n\n\nText(0, 0.5, 'Smoothed loss')\n\n\n\n\n\n\n\n\n\nAs you may notice, the names generated started to get more interesting after 15 epochs. One of the interesting names is “Yasira” which is an Arabic name :)."
  },
  {
    "objectID": "posts/character-language-model/Character-LeveL-Language-Model.html#conclusion",
    "href": "posts/character-language-model/Character-LeveL-Language-Model.html#conclusion",
    "title": "Character-Level Language Model",
    "section": "Conclusion",
    "text": "Conclusion\nStatistical language models are very crucial in Natural Language Processing (NLP) such as speech recognition and machine translation. We demonstrated in this post the main concepts behind statistical language models using character-level language model. The task of this model is generate names character by character using names obtained from census data that were consisted of 5,163 names. Below are the main key takeaways:\n\nIf we have more data, bigger model, and train longer we may get more interesting results. However, to get a very interesting results, we should instead use Long Short_Term Memory (LSTM) model with more than one layer deep. People have used 3 layers deep LSTM model with dropout and were able to generate very interesting results when applied on cook books and Shakespeare poems. LSTM models outperform simple RNN due to its ability in capturing longer time dependencies.\nWith the sampling technique we’re using, don’t expect the RNN to generate meaningful sequence of characters (names).\nWe used in this post each name as its own sequence; however, we may be able to speed up learning and get better results if we increase the batch size lets say from one name to a sequence of 50 characters.\nWe can control the level of randomness using the sampling strategy. Here, we balanced between what the model thinks its the right character and the level of randomness."
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#introduction",
    "href": "posts/employee-turnover/Employee-Turnover.html#introduction",
    "title": "Predicting Employee Turnover",
    "section": "Introduction",
    "text": "Introduction\nEmployee turnover refers to the percentage of workers who leave an organization and are replaced by new employees. It is very costly for organizations, where costs include but not limited to: separation, vacancy, recruitment, training and replacement. On average, organizations invest between four weeks and three months training new employees. This investment would be a loss for the company if the new employee decided to leave the first year. Furthermore, organizations such as consulting firms would suffer from deterioration in customer satisfaction due to regular changes in Account Reps and/or consultants that would lead to loss of businesses with clients.\nIn this notebook, we’ll work on simulated HR data from kaggle to build a classifier that helps us predict what kind of employees will be more likely to leave given some attributes. Such classifier would help an organization predict employee turnover and be pro-active in helping to solve such costly matter. We’ll restrict ourselves to use the most common classifiers: Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine.\nThe data has 14,999 examples (samples). Below are the features and the definitions of each one:\n\nsatisfaction_level: Level of satisfaction {0-1}.\nlast_evaluationTime: Time since last performance evaluation (in years).\nnumber_project: Number of projects completed while at work.\naverage_montly_hours: Average monthly hours at workplace.\ntime_spend_company: Number of years spent in the company.\nWork_accident: Whether the employee had a workplace accident.\nleft: Whether the employee left the workplace or not {0, 1}.\npromotion_last_5years: Whether the employee was promoted in the last five years.\nsales: Department the employee works for.\nsalary: Relative level of salary {low, medium, high}.\n\nLet’s first load all the packages.\n\n\nCode\nimport os\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (accuracy_score,\n                             f1_score,\n                             roc_auc_score,\n                             roc_curve,\n                             confusion_matrix)\nfrom sklearn.model_selection import (cross_val_score,\n                                     GridSearchCV,\n                                     RandomizedSearchCV,\n                                     learning_curve,\n                                     validation_curve,\n                                     train_test_split)\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.utils import resample\nfrom warnings import filterwarnings\n\nos.chdir(\"../\")\nfrom scripts.plot_roc import plot_conf_matrix_and_roc, plot_roc\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\nfilterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#data-preprocessing",
    "href": "posts/employee-turnover/Employee-Turnover.html#data-preprocessing",
    "title": "Predicting Employee Turnover",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nLet’s take a look at the data (check if there are missing values and the data type of each features):\n\n\nCode\n# Load the data\ndf = pd.read_csv(\"data/HR_comma_sep.csv\")\n\n# Check both the datatypes and if there is missing values\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Data types:\\n\" + 11 * \"-\")\nprint(\"\\033[30m\" + \"{}\\n\".format(df.dtypes))\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Sum of null values in each column:\\n\" + 35 * \"-\")\nprint(\"\\033[30m\" + \"{}\".format(df.isnull().sum()))\ndf.head()\n\n\nData types:\n-----------\nsatisfaction_level       float64\nlast_evaluation          float64\nnumber_project             int64\naverage_montly_hours       int64\ntime_spend_company         int64\nWork_accident              int64\nleft                       int64\npromotion_last_5years      int64\nsales                     object\nsalary                    object\ndtype: object\n\nSum of null values in each column:\n-----------------------------------\nsatisfaction_level       0\nlast_evaluation          0\nnumber_project           0\naverage_montly_hours     0\ntime_spend_company       0\nWork_accident            0\nleft                     0\npromotion_last_5years    0\nsales                    0\nsalary                   0\ndtype: int64\n\n\n\n\n\n\n\n\n\n\nsatisfaction_level\nlast_evaluation\nnumber_project\naverage_montly_hours\ntime_spend_company\nWork_accident\nleft\npromotion_last_5years\nsales\nsalary\n\n\n\n\n0\n0.38\n0.53\n2\n157\n3\n0\n1\n0\nsales\nlow\n\n\n1\n0.80\n0.86\n5\n262\n6\n0\n1\n0\nsales\nmedium\n\n\n2\n0.11\n0.88\n7\n272\n4\n0\n1\n0\nsales\nmedium\n\n\n3\n0.72\n0.87\n5\n223\n5\n0\n1\n0\nsales\nlow\n\n\n4\n0.37\n0.52\n2\n159\n3\n0\n1\n0\nsales\nlow\n\n\n\n\n\n\n\n\nSince there are no missing values, we do not have to do any imputation. However, there are some data preprocessing needed: 1. Change sales feature name to department. 2. Convert salary into ordinal categorical feature since there is intrinsic order between: low, medium and high. 3. Create dummy features from department feature and drop the first one to avoid linear dependency where some learning algorithms may struggle with.\n\n\nCode\n# Rename sales feature into department\ndf = df.rename(columns={\"sales\": \"department\"})\n\n# Map salary into integers\nsalary_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\ndf[\"salary\"] = df[\"salary\"].map(salary_map)\n\n# Create dummy variables for department feature\ndf = pd.get_dummies(df, columns=[\"department\"], drop_first=True)\ndf.head()\n\n\n\n\n\n\n\n\n\n\nsatisfaction_level\nlast_evaluation\nnumber_project\naverage_montly_hours\ntime_spend_company\nWork_accident\nleft\npromotion_last_5years\nsalary\ndepartment_RandD\ndepartment_accounting\ndepartment_hr\ndepartment_management\ndepartment_marketing\ndepartment_product_mng\ndepartment_sales\ndepartment_support\ndepartment_technical\n\n\n\n\n0\n0.38\n0.53\n2\n157\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n1\n0.80\n0.86\n5\n262\n6\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n0.11\n0.88\n7\n272\n4\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n0.72\n0.87\n5\n223\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n0.37\n0.52\n2\n159\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\nCode\ndf.columns[df.columns != \"left\"].shape\n\n\n(17,)\n\n\nThe data is now ready to be used for modeling. The final number of features are now 17."
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#modeling",
    "href": "posts/employee-turnover/Employee-Turnover.html#modeling",
    "title": "Predicting Employee Turnover",
    "section": "Modeling",
    "text": "Modeling\nLet’s first take a look at the proportion of each class to see if we’re dealing with balanced or imbalanced data since each one has its own set of tools to be used when fitting classifiers.\n\n\nCode\n# Get number of positve and negative examples\npos = df[df[\"left\"] == 1].shape[0]\nneg = df[df[\"left\"] == 0].shape[0]\nprint(\"Positive examples = {}\".format(pos))\nprint(\"Negative examples = {}\".format(neg))\nprint(\"Proportion of positive to negative examples = {:.2f}%\".format((pos / neg) * 100))\nsns.countplot(df[\"left\"])\nplt.xticks((0, 1), [\"Didn't leave\", \"Left\"])\nplt.xlabel(\"Left\")\nplt.ylabel(\"Count\")\nplt.title(\"Class counts\");\n\n\nPositive examples = 3571\nNegative examples = 11428\nProportion of positive to negative examples = 31.25%\n\n\n\n\n\n\n\n\n\nAs the graph shows, we have an imbalanced dataset. As a result, when we fit classifiers on such datasets, we should use metrics other than accuracy when comparing models such as f1-score or AUC (area under ROC curve). Moreover, class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. There are three ways to deal with this issue: 1. Assign a larger penalty to wrong predictions from the minority class. 2. Upsampling the minority class or downsampling the majority class. 3. Generate synthetic training examples.\nNonetheless, there is no definitive guide or best practices to deal with such situations. Therefore, we have to try them all and see which one works better on the problem at hand. We’ll restrict ourselves to use the first two, i.e assign larger penalty to wrong predictions from the minority class using class_weight in classifiers that allows us do that and evaluate upsampling/downsampling on the training data to see which gives higher performance.\nFirst, split the data into training and test sets using 80/20 split; 80% of the data will be used to train the models and 20% to test the performance of the models. Second, Upsample the minority class and downsample the majority class. For this data set, positive class is the minority class and negative class is the majority class.\n\n\nCode\n# Convert dataframe into numpy objects and split them into\n# train and test sets: 80/20\nX = df.loc[:, df.columns != \"left\"].values\ny = df.loc[:, df.columns == \"left\"].values.flatten()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=1)\n\n# Upsample minority class\nX_train_u, y_train_u = resample(X_train[y_train == 1],\n                                y_train[y_train == 1],\n                                replace=True,\n                                n_samples=X_train[y_train == 0].shape[0],\n                                random_state=1)\nX_train_u = np.concatenate((X_train[y_train == 0], X_train_u))\ny_train_u = np.concatenate((y_train[y_train == 0], y_train_u))\n\n# Downsample majority class\nX_train_d, y_train_d = resample(X_train[y_train == 0],\n                                y_train[y_train == 0],\n                                replace=True,\n                                n_samples=X_train[y_train == 1].shape[0],\n                                random_state=1)\nX_train_d = np.concatenate((X_train[y_train == 1], X_train_d))\ny_train_d = np.concatenate((y_train[y_train == 1], y_train_d))\n\nprint(\"Original shape:\", X_train.shape, y_train.shape)\nprint(\"Upsampled shape:\", X_train_u.shape, y_train_u.shape)\nprint(\"Downsampled shape:\", X_train_d.shape, y_train_d.shape)\n\n\nOriginal shape: (11999, 17) (11999,)\nUpsampled shape: (18284, 17) (18284,)\nDownsampled shape: (5714, 17) (5714,)\n\n\nI don’t think we need to apply dimensionality reduction such as PCA because: 1) We want to know the importance of each feature in determining who will leave vs who won’t (inference). 2) Dimension of the dataset is descent (17 features). However, it’s good to see how many principal components needed to explain 90%, 95% and 99% of the variation in the data.\n\n\nCode\n# Build PCA using standarized trained data\npca = PCA(n_components=None, svd_solver=\"full\")\npca.fit(StandardScaler().fit_transform(X_train))\ncum_var_exp = np.cumsum(pca.explained_variance_ratio_)\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 18), pca.explained_variance_ratio_, align=\"center\",\n        color='red', label=\"Individual explained variance\")\nplt.step(range(1, 18), cum_var_exp, where=\"mid\", label=\"Cumulative explained variance\")\nplt.xticks(range(1, 18))\nplt.legend(loc=\"best\")\nplt.xlabel(\"Principal component index\", {\"fontsize\": 14})\nplt.ylabel(\"Explained variance ratio\", {\"fontsize\": 14})\nplt.title(\"PCA on training data\", {\"fontsize\": 16});\n\n\n\n\n\n\n\n\n\n\n\nCode\ncum_var_exp\n\n\narray([ 0.1078147 ,  0.18756726,  0.26523205,  0.33604446,  0.4036422 ,\n        0.46807506,  0.53094596,  0.59334034,  0.65535106,  0.71691288,\n        0.77413324,  0.82651546,  0.87672244,  0.92515346,  0.96216602,\n        0.99429813,  1.        ])\n\n\nLooks like it needs 14, 15 and 16 principal components to capture 90%, 95% and 99% of the variation in the data respectively. In other words, this means that the data is already in a good space since eigenvalues are very close to each other and gives further evidence that we don’t need to compress the data.\nThe methodology that we’ll follow when building the classifiers goes as follows: 1. Build a pipeline that handles all the steps when fitting the classifier using scikit-learn’s make_pipeline which will have two steps: 1. Standardizing the data to speed up convergence and make all features on the same scale. 2. The classifier (estimator) we want to use to fit the model. 2. Use GridSearchCV to tune hyperparameters using 10-folds cross validation. We can use RandomizedSearchCV which is faster and may outperform GridSearchCV especially if we have more than two hyperparameters and the range for each one is very big; however, GridSearchCV will work just fine since we have only two hyperparameters and descent range. 3. Fit the model using training data. 5. Plot both confusion matrix and ROC curve for the best estimator using test data.\nRepeat the above steps for Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine. Next, pick the classifier that has the highest cross validation f1 score. Note that some of the hyperparameter ranges will be guided by the paper Data-driven Advice for Applying Machine Learning to Bioinformatics Problems.\n\nRandom Forest\nFirst, we will start by fitting a Random Forest classifier using unsampled, upsampled and downsampled data. Second, we will evaluate each method using cross validation (CV) f1-score and pick the one with the highest CV f1-score. Finally, we will use that method to fit the rest of the classifiers.\nThe only hyperparameters we’ll tune are:\n\nmax_feature: how many features to consider randomly on each split. This will help avoid having few strong features to be picked on each split and let other features have the chance to contribute. Therefore, predictions will be less correlated and the variance of each tree will decrease.\nmin_samples_leaf: how many examples to have for each split to be a final leaf node.\n\nRandom Forest is an ensemble model that has multiple trees (n_estimators), where each tree is a weak learner. The final prediction would be a weighting average or mode of the predictions from all estimators. Note: high number of trees don’t cause overfitting.\n\n\nCode\n# Build random forest classifier\nmethods_data = {\"Original\": (X_train, y_train),\n                \"Upsampled\": (X_train_u, y_train_u),\n                \"Downsampled\": (X_train_d, y_train_d)}\n\nfor method in methods_data.keys():\n    pip_rf = make_pipeline(StandardScaler(),\n                           RandomForestClassifier(n_estimators=500,\n                                                  class_weight=\"balanced\",\n                                                  random_state=123))\n    \n    hyperparam_grid = {\n        \"randomforestclassifier__n_estimators\": [10, 50, 100, 500],\n        \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", 0.4, 0.5],\n        \"randomforestclassifier__min_samples_leaf\": [1, 3, 5],\n        \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"]}\n    \n    gs_rf = GridSearchCV(pip_rf,\n                         hyperparam_grid,\n                         scoring=\"f1\",\n                         cv=10,\n                         n_jobs=-1)\n    \n    gs_rf.fit(methods_data[method][0], methods_data[method][1])\n    \n    print(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters for {} data:\".format(method))\n    for hyperparam in gs_rf.best_params_.keys():\n        print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_rf.best_params_[hyperparam])\n        \n    print(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_rf.best_score_) * 100))\n\n\nThe best hyperparameters for Original data:\ncriterion :  gini\nmax_features :  0.5\nmin_samples_leaf :  1\nn_estimators :  500\nBest 10-folds CV f1-score: 98.19%.\nThe best hyperparameters for Upsampled data:\ncriterion :  entropy\nmax_features :  0.4\nmin_samples_leaf :  1\nn_estimators :  50\nBest 10-folds CV f1-score: 99.80%.\nThe best hyperparameters for Downsampled data:\ncriterion :  entropy\nmax_features :  0.4\nmin_samples_leaf :  1\nn_estimators :  500\nBest 10-folds CV f1-score: 98.44%.\n\n\nUpsampling yielded the highest CV f1-score with 99.80%. Therefore, we’ll be using the upsampled data to fit the rest of the classifiers. The new data now has 18,284 examples with 50% of the examples belong to the positive class and the other 50% belong to the negative example.\n\n\nCode\nX_train_u[y_train_u == 0].shape, X_train_u[y_train_u == 1].shape\n\n\n((9142, 17), (9142, 17))\n\n\nLet’s refit the Random Forest with Upsampled data using best hyperparameters tuned above and plot confusion matrix and ROC curve using test data.\n\n\nCode\n# Reassign original training data to upsampled data\nX_train, y_train = np.copy(X_train_u), np.copy(y_train_u)\n\n# Delete original and downsampled data\ndel X_train_u, y_train_u, X_train_d, y_train_d\n\n# Refit RF classifier using best params\nclf_rf = make_pipeline(StandardScaler(),\n                       RandomForestClassifier(n_estimators=50,\n                                              criterion=\"entropy\",\n                                              max_features=0.4,\n                                              min_samples_leaf=1,\n                                              class_weight=\"balanced\",\n                                              n_jobs=-1,\n                                              random_state=123))\n\n\nclf_rf.fit(X_train, y_train)\n\n# Plot confusion matrix and ROC curve\nplot_conf_matrix_and_roc(clf_rf, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting Trees\nGradient Boosting trees are the same as Random Forest except for:\n\nIt starts with small tree and start learning from grown trees by taking into account the residual of grown trees.\nMore trees can lead to overfitting; opposite to Random Forest.\n\nThe two other hyperparameters than max_features and n_estimators that we’re going to tune are:\n\nlearning_rate: rate the tree learns, the slower the better.\nmax_depth: number of split each time a tree is growing which limits the number of nodes in each tree.\n\nLet’s fit GB classifier and plot confusion matrix and ROC curve using test data.\n\n\nCode\n# Build Gradient Boosting classifier\npip_gb = make_pipeline(StandardScaler(),\n                       GradientBoostingClassifier(loss=\"deviance\",\n                                                  random_state=123))\n\nhyperparam_grid = {\"gradientboostingclassifier__max_features\": [\"log2\", 0.5],\n                   \"gradientboostingclassifier__n_estimators\": [100, 300, 500],\n                   \"gradientboostingclassifier__learning_rate\": [0.001, 0.01, 0.1],\n                   \"gradientboostingclassifier__max_depth\": [1, 2, 3]}\n\ngs_gb = GridSearchCV(pip_gb,\n                      param_grid=hyperparam_grid,\n                      scoring=\"f1\",\n                      cv=10,\n                      n_jobs=-1)\n\ngs_gb.fit(X_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_gb.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_gb.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_gb.best_score_) * 100))\n\n\nThe best hyperparameters:\n-------------------------\nlearning_rate :  0.1\nmax_depth :  3\nmax_features :  0.5\nn_estimators :  500\nBest 10-folds CV f1-score: 97.88%.\n\n\n\n\nCode\n# Plot confusion matrix and ROC curve\nplot_conf_matrix_and_roc(gs_gb, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\nK-Nearest Neighbors\nKNN is called a lazy learning algorithm because it doesn’t learn or fit any parameter. It takes n_neighbors points from the training data closest to the point we’re interested to predict it’s class and take the mode (majority vote) of the classes for the neighboring point as its class. The two hyperparameters we’re going to tune are:\n\nn_neighbors: number of neighbors to use in prediction.\nweights: how much weight to assign neighbors based on:\n\n“uniform”: all neighboring points have the same weight.\n“distance”: use the inverse of euclidean distance of each neighboring point used in prediction.\n\n\nLet’s fit KNN classifier and plot confusion matrix and ROC curve.\n\n\nCode\n# Build KNN classifier\npip_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\nhyperparam_range = range(1, 20)\n\ngs_knn = GridSearchCV(pip_knn,\n                      param_grid={\"kneighborsclassifier__n_neighbors\": hyperparam_range,\n                                  \"kneighborsclassifier__weights\": [\"uniform\", \"distance\"]},\n                      scoring=\"f1\",\n                      cv=10,\n                      n_jobs=-1)\n\ngs_knn.fit(X_train, y_train)\n\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_knn.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_knn.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_knn.best_score_) * 100))\n\n\nThe best hyperparameters:\n-------------------------\nn_neighbors :  1\nweights :  uniform\nBest 10-folds CV f1-score: 98.24%.\n\n\n\n\nCode\nplot_conf_matrix_and_roc(gs_knn, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\nFor logistic regression, we’ll tune three hyperparameters:\n\npenalty: type of regularization, L2 or L1 regularization.\nC: the opposite of regularization of parameter \\(\\lambda\\). The higher C the less regularization. We’ll use values that cover the full range between unregularized to fully regularized where model is the mode of the examples’ label.\nfit_intercept: whether to include intercept or not.\n\nWe won’t use any non-linearities such as polynomial features.\n\n\nCode\n# Build logistic model classifier\npip_logmod = make_pipeline(StandardScaler(),\n                           LogisticRegression(class_weight=\"balanced\"))\n\nhyperparam_range = np.arange(0.5, 20.1, 0.5)\n\nhyperparam_grid = {\"logisticregression__penalty\": [\"l1\", \"l2\"],\n                   \"logisticregression__C\":  hyperparam_range,\n                   \"logisticregression__fit_intercept\": [True, False]\n                  }\n\ngs_logmodel = GridSearchCV(pip_logmod,\n                           hyperparam_grid,\n                           scoring=\"accuracy\",\n                           cv=2,\n                           n_jobs=-1)\n\ngs_logmodel.fit(X_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_logmodel.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_logmodel.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_logmodel.best_score_) * 100))\n\n\nThe best hyperparameters:\n-------------------------\nC :  1.0\nfit_intercept :  True\npenalty :  l1\nBest 10-folds CV f1-score: 77.20%.\n\n\n\n\nCode\nplot_conf_matrix_and_roc(gs_logmodel, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machine (SVM)\nSVM is comutationally very expensive to tune it’s hyperparameters for two reasons:\n\nWith big datasets, it becomes very slow.\nIt has good number of hyperparameters to tune that takes very long time to tune on a CPU.\n\nTherefore, we’ll use recommended hyperparameters’ values from the paper we mentioned before that showed to yield the best performane on Penn Machine Learning Benchmark 165 datasets. The hyperparameters that we usually look to tune are: - C, gamma, kernel, degree and coef0\n\n\nCode\n# Build SVM classifier\nclf_svc = make_pipeline(StandardScaler(),\n                        SVC(C=0.01,\n                            gamma=0.1,\n                            kernel=\"poly\",\n                            degree=5,\n                            coef0=10,\n                            probability=True))\n\nclf_svc.fit(X_train, y_train)\n\nsvc_cv_scores = cross_val_score(clf_svc,\n                                X=X_train,\n                                y=y_train,\n                                scoring=\"f1\",\n                                cv=10,\n                                n_jobs=-1)\n\n# Print CV\nprint(\"\\033[1m\" + \"\\033[94m\" + \"The 10-folds CV f1-score is: {:.2f}%\".format(\n       np.mean(svc_cv_scores) * 100))\n\n\nThe 10-folds CV f1-score is: 96.38%\n\n\n\n\nCode\nplot_conf_matrix_and_roc(clf_svc, X_test, y_test)"
  },
  {
    "objectID": "posts/employee-turnover/Employee-Turnover.html#conclusion",
    "href": "posts/employee-turnover/Employee-Turnover.html#conclusion",
    "title": "Predicting Employee Turnover",
    "section": "Conclusion",
    "text": "Conclusion\nLet’s conclude by printing out the test accuracy rates for all classifiers we’ve trained so far and plot ROC curves. Finally, we’ll pick the classifier that has the highest area under ROC curve.\n\n\nCode\n# Plot ROC curves for all classifiers\nestimators = {\"RF\": clf_rf,\n              \"LR\": gs_logmodel,\n              \"SVC\": clf_svc,\n              \"GBT\": gs_gb,\n              \"KNN\": gs_knn}\nplot_roc(estimators, X_test, y_test, (12, 8))\n\n# Print out accuracy score on test data\nprint(\"The accuracy rate and f1-score on test data are:\")\nfor estimator in estimators.keys():\n    print(\"{}: {:.2f}%, {:.2f}%.\".format(estimator,\n        accuracy_score(y_test, estimators[estimator].predict(X_test)) * 100,\n         f1_score(y_test, estimators[estimator].predict(X_test)) * 100))\n\n\nThe accuracy rate and f1-score on test data are:\nRF: 99.27%, 98.44%.\nLR: 76.33%, 61.29%.\nSVC: 95.90%, 91.69%.\nGBT: 97.97%, 95.74%.\nKNN: 97.23%, 94.33%.\n\n\n\n\n\n\n\n\n\nEven though Random Forest and Gradient Boosting Trees have almost equal auc, Random Forest has higher accuracy rate and an f1-score with 99.27% and 99.44% respectively. Therefore, we safely say Random Forest outperforms the rest of the classifiers. Let’s have a look of feature importances from Random Forest classifier.\n\n\nCode\n# Refit RF classifier\nclf_rf = RandomForestClassifier(n_estimators=50,\n                                criterion=\"entropy\",\n                                max_features=0.4,\n                                min_samples_leaf=1,\n                                class_weight=\"balanced\",\n                                n_jobs=-1,\n                                random_state=123)\n\n\nclf_rf.fit(StandardScaler().fit_transform(X_train), y_train)\n\n# Plot features importance\nimportances = clf_rf.feature_importances_\nindices = np.argsort(clf_rf.feature_importances_)[::-1]\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 18), importances[indices], align=\"center\")\nplt.xticks(range(1, 18), df.columns[df.columns != \"left\"][indices], rotation=90)\nplt.title(\"Feature Importance\", {\"fontsize\": 16});\n\n\n\n\n\n\n\n\n\nLooks like the five most important features are: - satisfaction_level - time_spend_company - average_montly_hours - number_project - lats_evaluation\nThe take home message is the following: - When dealing with imbalanced classes, accuracy is not a good method for model evaluation. AUC and f1-score are examples of metrics we can use. - Upsampling/downsampling, data synthetic and using balanced class weights are good strategies to try to improve the accuracy of a classifier for imbalanced classes datasets. - GridSearchCV helps tune hyperparameters for each learning algorithm. RandomizedSearchCV is faster and may outperform GridSearchCV especially when we have more than two hyperparameters to tune. - Principal Component Analysis (PCA) isn’t always recommended especially if the data is in a good feature space and their eigen values are very close to each other. - As expected, ensemble models outperforms other learning algorithms in most cases."
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#introduction",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#introduction",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Introduction",
    "text": "Introduction\nDropout is a regularization technique. On each iteration, we randomly shut down some neurons (units) on each layer and don’t use those neurons in both forward propagation and back-propagation. Since the units that will be dropped out on each iteration will be random, the learning algorithm will have no idea which neurons will be shut down on every iteration; therefore, force the learning algorithm to spread out the weights and not focus on some specific feattures (units). Moreover, dropout help improving generalization error by:\n\nSince we drop some units on each iteration, this will lead to smaller network which in turns means simpler network (regularization).\nCan be seen as an approximation to bagging techniques. Each iteration can be viewed as different model since we’re dropping randomly different units on each layer. This means that the error would be the average of errors from all different models (iterations). Therefore, averaging errors from different models especially if those errors are uncorrelated would reduce the overall errors. In the worst case where errors are perfectly correlated, averaging among all models won’t help at all; however, we know that in practice errors have some degree of uncorrelation. As result, it will always improve generalization error.\n\nWe can use different probabilities on each layer; however, the output layer would always have keep_prob = 1 and the input layer has high keep_prob such as 0.9 or 1. If a hidden layer has keep_prob = 0.8, this means that; on each iteration, each unit has 80% probablitity of being included and 20% probability of being dropped out.\nDropout is used a lot in computer vision problems because we have a lot of features and not a lot of data. Also, features (pixels) next to each other usually don’t add a lot of information. Therefore, models always suffer from overfitting.\nTo illustrate how dropout helps us reduce generalization error, we’ll use the same dataset we’ve used in the previous posts. The dataset has images for cats and non-cat. We’ll try to build a neural network to classify if the image has cat or not. Each image is 64 x 64 pixels on RGB scale. Let’s import the data and take a look at the shape as well as a sample of a cat image from the training set.\n\n\nCode\nimport os\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# local modules\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    initialize_parameters,\n    linear_activation_forward,\n    compute_cost,\n    linear_activation_backward,\n    update_parameters,\n    accuracy,\n)\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"figure.figsize\"] = (12, 6)\n\n\n\n\nCode\n# Import training data\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"])\nY_train = np.array(train_dataset[\"train_set_y\"])\n\n# Plot a sample image\nplt.imshow(X_train[50])\nplt.axis(\"off\")\n\n# Import test data\ntest_dataset = h5py.File(\"../../data/test_catvnoncat.h5\")\nX_test = np.array(test_dataset[\"test_set_x\"])\nY_test = np.array(test_dataset[\"test_set_y\"])\n\n# Transform data\nX_train = X_train.reshape(209, -1).T\nX_train = X_train / 255\nY_train = Y_train.reshape(-1, 209)\n\nX_test = X_test.reshape(50, -1).T\nX_test = X_test / 255\nY_test = Y_test.reshape(-1, 50)\n\n# print the new shape of both training and test datasets\nprint(\"Training data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_train.shape, Y_train.shape))\nprint(\"Test data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_test.shape, Y_test.shape))\n\n\nTraining data dimensions:\nX's dimension: (12288, 209), Y's dimension: (1, 209)\nTest data dimensions:\nX's dimension: (12288, 50), Y's dimension: (1, 50)"
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#implementation",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#implementation",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Implementation",
    "text": "Implementation\nNow, we’ll write the functions needed to apply dropout on both forward propagation and back-propagation. Note that we’ll utilize the functions we wrote in previous posts such as initialize_parameters.\n\n\nCode\ndef drop_out_matrices(layers_dims, m, keep_prob):\n    \"\"\"\n    Initializes the dropout matrices that will be used in both forward prop\n    and back-prop on each layer. We'll use random numbers from uniform\n    distribution.\n\n    Arguments\n    ---------\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    m : int\n        number of training examples.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n\n    Returns\n    -------\n    D : dict\n        dropout matrices for each layer l. Each dropout matrix on each layer\n        would have the same dimension as post activation output matrix \"A\".\n        For example: \"D1\" shape: number of units x number of examples.\n    \"\"\"\n    np.random.seed(1)\n    D = {}\n    L = len(layers_dims)\n\n    for l in range(L):\n        # initialize the random values for the dropout matrix\n        D[str(l)] = np.random.rand(layers_dims[l], m)\n        # Convert it to 0/1 to shut down neurons corresponding to each element\n        D[str(l)] = D[str(l)] &lt; keep_prob[l]\n        assert D[str(l)].shape == (layers_dims[l], m)\n    return D\n\n\ndef L_model_forward(X, parameters, D, keep_prob, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the output layer through looping over all units in topological\n    order.\n\n    X : 2d-array\n        input matrix of shape input_size x training_examples.\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    D : dict\n        dropout matrices for each layer l.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\",\"relu\".\n\n\n    Returns\n    -------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    caches : list\n        that contains L tuples where each layer has: A_prev, W, b, Z.\n    \"\"\"\n    A = X  # since input matrix A0\n    A = np.multiply(A, D[str(0)])\n    A /= keep_prob[0]\n    caches = []  # initialize the caches list\n    L = len(parameters) // 2  # number of layer in the network\n\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(\n            A_prev,\n            parameters[\"W\" + str(l)],\n            parameters[\"b\" + str(l)],\n            hidden_layers_activation_fn,\n        )\n        # shut down some units\n        A = np.multiply(A, D[str(l)])\n        # scale that value of units to keep expected value the same\n        A /= keep_prob[l]\n        caches.append(cache)\n\n    AL, cache = linear_activation_forward(\n        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\"\n    )\n    AL = np.multiply(AL, D[str(L)])\n    AL /= keep_prob[L]\n    caches.append(cache)\n    assert AL.shape == (1, X.shape[1])\n\n    return AL, caches\n\n\ndef L_model_backward(AL, Y, caches, D, keep_prob, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the gradient of output layer w.r.t weights, biases, etc. starting\n    on the output layer in reverse topological order.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector, output of the forward propagation\n        (L_model_forward()).\n    y : 2d-array\n        true \"label\" vector (containing 0 if non-cat, 1 if cat).\n    caches : list\n        list of caches for all layers.\n    D : dict\n        dropout matrices for each layer l.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n    hidden_layers_activation_fn :\n        activation function used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    grads : dict\n        gradients.\n    \"\"\"\n    Y = Y.reshape(AL.shape)\n    L = len(caches)\n    grads = {}\n\n    # dA for output layer\n    dAL = np.divide(AL - Y, np.multiply(AL, 1 - AL))\n    dAL = np.multiply(dAL, D[str(L)])\n    dAL /= keep_prob[L]\n\n    (\n        grads[\"dA\" + str(L - 1)],\n        grads[\"dW\" + str(L)],\n        grads[\"db\" + str(L)],\n    ) = linear_activation_backward(dAL, caches[L - 1], \"sigmoid\")\n    grads[\"dA\" + str(L - 1)] = np.multiply(grads[\"dA\" + str(L - 1)], D[str(L - 1)])\n    grads[\"dA\" + str(L - 1)] /= keep_prob[L - 1]\n\n    for l in range(L - 1, 0, -1):\n        current_cache = caches[l - 1]\n        (\n            grads[\"dA\" + str(l - 1)],\n            grads[\"dW\" + str(l)],\n            grads[\"db\" + str(l)],\n        ) = linear_activation_backward(\n            grads[\"dA\" + str(l)], current_cache, hidden_layers_activation_fn\n        )\n\n        grads[\"dA\" + str(l - 1)] = np.multiply(grads[\"dA\" + str(l - 1)], D[str(l - 1)])\n        grads[\"dA\" + str(l - 1)] /= keep_prob[l - 1]\n\n    return grads\n\n\ndef model_with_dropout(\n    X,\n    Y,\n    layers_dims,\n    keep_prob,\n    learning_rate=0.01,\n    num_iterations=3000,\n    print_cost=True,\n    hidden_layers_activation_fn=\"relu\",\n):\n    \"\"\"\n    Implements multilayer neural network with dropout using gradient descent as the\n    learning algorithm.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    keep_prob : list\n        probabilities of keeping a neuron (unit) active for each layer on each\n        iteration.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n    num_iterations : int\n        number of iterations of the optimization loop.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test\n        examples.\n    \"\"\"\n    # get number of examples\n    m = X.shape[1]\n\n    # to get consistents output\n    np.random.seed(1)\n\n    # initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # intialize cost list\n    cost_list = []\n\n    # implement gradient descent\n    for i in range(num_iterations):\n        # Initialize dropout matrices\n        D = drop_out_matrices(layers_dims, m, keep_prob)\n\n        # compute forward propagation\n        AL, caches = L_model_forward(\n            X, parameters, D, keep_prob, hidden_layers_activation_fn\n        )\n\n        # compute regularized cost\n        cost = compute_cost(AL, Y)\n\n        # compute gradients\n        grads = L_model_backward(\n            AL, Y, caches, D, keep_prob, hidden_layers_activation_fn\n        )\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # print cost\n        if (i + 1) % 100 == 0 and print_cost:\n            print(f\"The cost after {i + 1} iterations : {cost:.4f}.\")\n        # append cost\n        if i % 100 == 0:\n            cost_list.append(cost)\n\n    # plot the cost curve\n    plt.plot(cost_list)\n    plt.xlabel(\"Iteration (per hundreds)\")\n    plt.ylabel(\"Cost\")\n    plt.title(f\"Cost curve for the learning rate = {learning_rate}\")\n\n    return parameters"
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#application",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#application",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Application",
    "text": "Application\nFinally, we’re ready to build our neural network. First, we’ll build one fully connected network without dropout. That is to say, keep_prob = 1. Next, we’ll build another network where keep_prob &lt; 1. Lastly, we’ll compare the generalization error of both networks and see how dropout technique can help us in improving our generalization error.\n\n\nCode\n# setup layers dimensions, number of examples, and keep probabilities list\nm = X_train.shape[0]\nkeep_prob = [1, 1, 1, 1]\nlayers_dims = [m, 10, 10, 1]\n\n# train NN with no dropout\nparameters = model_with_dropout(\n    X_train,\n    Y_train,\n    layers_dims,\n    keep_prob=keep_prob,\n    learning_rate=0.03,\n    num_iterations=1000,\n    hidden_layers_activation_fn=\"relu\",\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"relu\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"relu\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations : 0.6555.\nThe cost after 200 iterations : 0.6468.\nThe cost after 300 iterations : 0.6447.\nThe cost after 400 iterations : 0.6442.\nThe cost after 500 iterations : 0.6440.\nThe cost after 600 iterations : 0.6440.\nThe cost after 700 iterations : 0.6440.\nThe cost after 800 iterations : 0.6440.\nThe cost after 900 iterations : 0.6440.\nThe cost after 1000 iterations : 0.6440.\nThe training accuracy rate: 65.55%.\nThe test accuracy rate: 34.00%.\n\n\n\n\n\n\n\n\n\n\n\nCode\n# setup keep probabilities list\nkeep_prob = [1, 0.5, 0.5, 1]\n\n# train NN with no dropout\nparameters = model_with_dropout(\n    X_train,\n    Y_train,\n    layers_dims,\n    keep_prob=keep_prob,\n    learning_rate=0.03,\n    num_iterations=1000,\n    hidden_layers_activation_fn=\"relu\",\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"relu\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"relu\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations : 0.6555.\nThe cost after 200 iterations : 0.6467.\nThe cost after 300 iterations : 0.6445.\nThe cost after 400 iterations : 0.6437.\nThe cost after 500 iterations : 0.6412.\nThe cost after 600 iterations : 0.6338.\nThe cost after 700 iterations : 0.6108.\nThe cost after 800 iterations : 0.5367.\nThe cost after 900 iterations : 0.4322.\nThe cost after 1000 iterations : 0.3114.\nThe training accuracy rate: 74.16%.\nThe test accuracy rate: 44.00%.\n\n\n\n\n\n\n\n\n\nAs the results above showed, the network with dropout improved on test accuracy rate by 30%. Note that this is just an illustrative example to show the effectiveness of the dropout technique. We chose an arbitrary probabilities in this example; however, we can tune the dropout probabilities on each layer to yield the best validation loss and accuracy."
  },
  {
    "objectID": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#conclusion",
    "href": "posts/coding-nn/dropout/Coding-Neural-Network-Dropout.html#conclusion",
    "title": "Coding Neural Network Part 5 - Dropout",
    "section": "Conclusion",
    "text": "Conclusion\nDropout is a very effective regularization technique that is used a lot in Convolutional Neural Networks. Below are some takeaways:\n\nSet keep_prob = 1 when using gradient checking; otherwise, it won’t work.\nDropout is used only during training. Don’t use it when testing/predicting new examples.\nThe lowest the keep_prob \\(\\rightarrow\\) the simpler the neural network. As keep_prob decreases, the bias increases and the variance decreases. Therefore, layers with more neurons are expected to have lower keep_prob to avoid overfitting.\nIt’s computationally a cheap way to improve generalization error and help resolve overfitting.\nOne can tune keep_prob to get the best results out of the task at hand."
  },
  {
    "objectID": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html",
    "href": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html",
    "title": "Coding Neural Network Part 2 - Gradient Checking",
    "section": "",
    "text": "In the previous post, Coding Neural Network - Forward and Backward Propagation, we implemented both forward propagation and backpropagation in numpy. However, implementing backpropagation from scratch is usually more prune to bugs/errors. Therefore, it’s necessary before running the neural network on training data to check if our implementation of backpropagation is correct. Before we start, let’s revisit what back-propagation is: We loop over the nodes in reverse topological order starting at the final node to compute the derivative of the cost with respect to each edge’s node tail. In other words, we compute the derivative of cost function with respect to all parameters, i.e \\(\\frac{\\partial J}{\\partial \\theta}\\) where \\(\\theta\\) represents the parameters of the model.\nThe way to test our implementation is by computing numerical gradients and compare it with gradients from backpropagation (analytical). There are two way of computing numerical gradients:\n\nRight-hand form: \\[\\frac{J(\\theta + \\epsilon) - J(\\theta)}{\\epsilon}\\tag{1}\\]\nTwo-sided form (see figure 1): \\[\\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2 \\epsilon}\\tag{2}\\]\n\n\n\n\nFigure 1: Two-sided numerical gradients\n\n\nTwo-sided form of approximating the derivative is closer than the right-hand form. Let’s illustrate that with the following example using the function \\(f(x) = x^2\\) by taking its derivative at \\(x = 3\\). - Analytical derivative: \\[\\nabla_x f(x) = 2x\\ \\Rightarrow\\nabla_x f(3) = 6\\] - Two-sided numerical derivative: \\[\\frac{(3 + 1e-2)^2 - (3 - 1e-2)^2}{2 * 1e-2} = 5.999999999999872\\] - Right-hand numerical derivative: \\[\\frac{(3 + 1e-2)^2 - 3^2}{1e-2} = 6.009999999999849\\] As we see above, the difference between analytical derivative and two-sided numerical gradient is almost zero; however, the difference between analytical derivative and right-sided derivative is 0.01. Therefore, we’ll use two-sided epsilon method to compute the numerical gradients.\nIn addition, we’ll normalize the difference between numerical. gradients and analytical gradients using the following formula: \\[\\frac{\\|grad - grad_{approx}\\|_2}{\\|grad\\|_2 + \\|grad_{approx}\\|_2}\\tag{3}\\] If the difference is \\(\\leq 10^{-7}\\), then our implementation is fine; otherwise, we have a mistake somewhere and have to go back and revisit backpropagation code.\nBelow are the steps needed to implement gradient checking: 1. Pick random number of examples from training data to use it when computing both numerical and analytical gradients. - Don’t use all examples in the training data because gradient checking is very slow. 2. Initialize parameters. 3. Compute forward propagation and the cross-entropy cost. 4. Compute the gradients using our back-propagation implementation. 5. Compute the numerical gradients using the two-sided epsilon method. 6. Compute the difference between numerical and analytical gradients.\nWe’ll be using functions we wrote in “Coding Neural Network - Forward Propagation and Backpropagation” post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\nLet’s first import the data.\n\n\nCode\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.linalg import norm\nimport seaborn as sns\n\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    initialize_parameters,\n    L_model_forward,\n    L_model_backward,\n    compute_cost,\n)\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\n\n\n\n\nCode\n# Import the data\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"]).T\ny_train = np.array(train_dataset[\"train_set_y\"]).T\nX_train = X_train.reshape(-1, 209)\ny_train = y_train.reshape(-1, 209)\n\nX_train.shape, y_train.shape\n\n\n((12288, 209), (1, 209))\n\n\nNext, we’ll write helper functions that faciltate converting parameters and gradients dictionaries into vectors and then re-convert them back to dictionaries.\n\n\nCode\ndef dictionary_to_vector(params_dict):\n    \"\"\"\n    Roll a dictionary into a single vector.\n\n    Arguments\n    ---------\n    params_dict : dict\n        learned parameters.\n\n    Returns\n    -------\n    params_vector : array\n        vector of all parameters concatenated.\n    \"\"\"\n    count = 0\n    for key in params_dict.keys():\n        new_vector = np.reshape(params_dict[key], (-1, 1))\n        if count == 0:\n            theta_vector = new_vector\n        else:\n            theta_vector = np.concatenate((theta_vector, new_vector))\n        count += 1\n\n    return theta_vector\n\n\ndef vector_to_dictionary(vector, layers_dims):\n    \"\"\"\n    Unroll parameters vector to dictionary using layers dimensions.\n\n    Arguments\n    ---------\n    vector : array\n        parameters vector.\n    layers_dims : list or array_like\n        dimensions of each layer in the network.\n\n    Returns\n    -------\n    parameters : dict\n        dictionary storing all parameters.\n    \"\"\"\n    L = len(layers_dims)\n    parameters = {}\n    k = 0\n\n    for l in range(1, L):\n        # Create temp variable to store dimension used on each layer\n        w_dim = layers_dims[l] * layers_dims[l - 1]\n        b_dim = layers_dims[l]\n\n        # Create temp var to be used in slicing parameters vector\n        temp_dim = k + w_dim\n\n        # add parameters to the dictionary\n        parameters[\"W\" + str(l)] = vector[\n            k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1])\n        parameters[\"b\" + str(l)] = vector[\n            temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n\n        k += w_dim + b_dim\n\n    return parameters\n\n\ndef gradients_to_vector(gradients):\n    \"\"\"\n    Roll all gradients into a single vector containing only dW and db.\n\n    Arguments\n    ---------\n    gradients : dict\n        storing gradients of weights and biases for all layers: dA, dW, db.\n\n    Returns\n    -------\n    new_grads : array\n        vector of only dW and db gradients.\n    \"\"\"\n    # Get the number of indices for the gradients to iterate over\n    valid_grads = [key for key in gradients.keys()\n                   if not key.startswith(\"dA\")]\n    L = len(valid_grads)// 2\n    count = 0\n    \n    # Iterate over all gradients and append them to new_grads list\n    for l in range(1, L + 1):\n        if count == 0:\n            new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n        else:\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)))\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n        count += 1\n        \n    return new_grads\n\n\nFinally, we’ll write the gradient checking function that will compute the difference between the analytical and numerical gradients and tell us if our implementation of back-propagation is correct. We’ll randomly choose 1 example to compute the difference.\n\n\nCode\ndef forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn=\"tanh\"):\n    \"\"\"\n    Implements the forward propagation and computes the cost.\n    \n    Arguments\n    ---------\n    X : 2d-array\n        input data, shape: number of features x number of examples.\n    parameters : dict\n        parameters to use in forward prop.\n    Y : array\n        true \"label\", shape: 1 x number of examples.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    cost : float\n        cross-entropy cost.\n    \"\"\"\n    # Compute forward prop\n    AL, _ = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n    # Compute cost\n    cost = compute_cost(AL, Y)\n\n    return cost\n\n\ndef gradient_check(\n        parameters, gradients, X, Y, layers_dims, epsilon=1e-7,\n        hidden_layers_activation_fn=\"tanh\"):\n    \"\"\"\n    Checks if back_prop computes correctly the gradient of the cost output by\n    forward_prop.\n    \n    Arguments\n    ---------\n    parameters : dict\n        storing all parameters to use in forward prop.\n    gradients : dict\n        gradients of weights and biases for all layers: dA, dW, db.\n    X : 2d-array\n        input data, shape: number of features x number of examples.\n    Y : array\n        true \"label\", shape: 1 x number of examples.\n    epsilon : \n        tiny shift to the input to compute approximate gradient.\n    layers_dims : list or array_like\n        dimensions of each layer in the network.\n    \n    Returns\n    -------\n    difference : float\n        difference between approx gradient and back_prop gradient\n    \"\"\"\n    \n    # Roll out parameters and gradients dictionaries\n    parameters_vector = dictionary_to_vector(parameters)\n    gradients_vector = gradients_to_vector(gradients)\n\n    # Create vector of zeros to be used with epsilon\n    grads_approx = np.zeros_like(parameters_vector)\n\n    for i in range(len(parameters_vector)):\n        # Compute cost of theta + epsilon\n        theta_plus = np.copy(parameters_vector)\n        theta_plus[i] = theta_plus[i] + epsilon\n        j_plus = forward_prop_cost(\n            X, vector_to_dictionary(theta_plus, layers_dims), Y,\n            hidden_layers_activation_fn)\n\n        # Compute cost of theta - epsilon\n        theta_minus = np.copy(parameters_vector)\n        theta_minus[i] = theta_minus[i] - epsilon\n        j_minus = forward_prop_cost(\n            X, vector_to_dictionary(theta_minus, layers_dims), Y,\n            hidden_layers_activation_fn)\n\n        # Compute numerical gradients\n        grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n\n    # Compute the difference of numerical and analytical gradients\n    numerator = norm(gradients_vector - grads_approx)\n    denominator = norm(grads_approx) + norm(gradients_vector)\n    difference = numerator / denominator\n\n    if difference &gt; 10e-7:\n        print (\"\\033[31mThere is a mistake in back-propagation \" +\\\n               \"implementation. The difference is: {}\".format(difference))\n    else:\n        print (\"\\033[32mThere implementation of back-propagation is fine! \"+\\\n               \"The difference is: {}\".format(difference))\n\n    return difference\n\n\n\n\nCode\n# Set up neural network architecture\nlayers_dims = [X_train.shape[0], 5, 5, 1]\n\n# Initialize parameters\nparameters = initialize_parameters(layers_dims)\n\n# Randomly selecting 1 example from training data\nperms = np.random.permutation(X_train.shape[1])\nindex = perms[:1]\n\n# Compute forward propagation\nAL, caches = L_model_forward(X_train[:, index], parameters, \"tanh\")\n\n# Compute analytical gradients\ngradients = L_model_backward(AL, y_train[:, index], caches, \"tanh\")\n\n# Compute difference of numerical and analytical gradients\ndifference = gradient_check(parameters, gradients, X_train[:, index], y_train[:, index], layers_dims)\n\n\nThere implementation of back-propagation is fine! The difference is: 3.02205552997035e-09\n\n\nCongratulations! Our implementation is correct :)"
  },
  {
    "objectID": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html#introduction",
    "href": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html#introduction",
    "title": "Coding Neural Network Part 2 - Gradient Checking",
    "section": "",
    "text": "In the previous post, Coding Neural Network - Forward and Backward Propagation, we implemented both forward propagation and backpropagation in numpy. However, implementing backpropagation from scratch is usually more prune to bugs/errors. Therefore, it’s necessary before running the neural network on training data to check if our implementation of backpropagation is correct. Before we start, let’s revisit what back-propagation is: We loop over the nodes in reverse topological order starting at the final node to compute the derivative of the cost with respect to each edge’s node tail. In other words, we compute the derivative of cost function with respect to all parameters, i.e \\(\\frac{\\partial J}{\\partial \\theta}\\) where \\(\\theta\\) represents the parameters of the model.\nThe way to test our implementation is by computing numerical gradients and compare it with gradients from backpropagation (analytical). There are two way of computing numerical gradients:\n\nRight-hand form: \\[\\frac{J(\\theta + \\epsilon) - J(\\theta)}{\\epsilon}\\tag{1}\\]\nTwo-sided form (see figure 1): \\[\\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2 \\epsilon}\\tag{2}\\]\n\n\n\n\nFigure 1: Two-sided numerical gradients\n\n\nTwo-sided form of approximating the derivative is closer than the right-hand form. Let’s illustrate that with the following example using the function \\(f(x) = x^2\\) by taking its derivative at \\(x = 3\\). - Analytical derivative: \\[\\nabla_x f(x) = 2x\\ \\Rightarrow\\nabla_x f(3) = 6\\] - Two-sided numerical derivative: \\[\\frac{(3 + 1e-2)^2 - (3 - 1e-2)^2}{2 * 1e-2} = 5.999999999999872\\] - Right-hand numerical derivative: \\[\\frac{(3 + 1e-2)^2 - 3^2}{1e-2} = 6.009999999999849\\] As we see above, the difference between analytical derivative and two-sided numerical gradient is almost zero; however, the difference between analytical derivative and right-sided derivative is 0.01. Therefore, we’ll use two-sided epsilon method to compute the numerical gradients.\nIn addition, we’ll normalize the difference between numerical. gradients and analytical gradients using the following formula: \\[\\frac{\\|grad - grad_{approx}\\|_2}{\\|grad\\|_2 + \\|grad_{approx}\\|_2}\\tag{3}\\] If the difference is \\(\\leq 10^{-7}\\), then our implementation is fine; otherwise, we have a mistake somewhere and have to go back and revisit backpropagation code.\nBelow are the steps needed to implement gradient checking: 1. Pick random number of examples from training data to use it when computing both numerical and analytical gradients. - Don’t use all examples in the training data because gradient checking is very slow. 2. Initialize parameters. 3. Compute forward propagation and the cross-entropy cost. 4. Compute the gradients using our back-propagation implementation. 5. Compute the numerical gradients using the two-sided epsilon method. 6. Compute the difference between numerical and analytical gradients.\nWe’ll be using functions we wrote in “Coding Neural Network - Forward Propagation and Backpropagation” post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\nLet’s first import the data.\n\n\nCode\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.linalg import norm\nimport seaborn as sns\n\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    initialize_parameters,\n    L_model_forward,\n    L_model_backward,\n    compute_cost,\n)\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\n\n\n\n\nCode\n# Import the data\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"]).T\ny_train = np.array(train_dataset[\"train_set_y\"]).T\nX_train = X_train.reshape(-1, 209)\ny_train = y_train.reshape(-1, 209)\n\nX_train.shape, y_train.shape\n\n\n((12288, 209), (1, 209))\n\n\nNext, we’ll write helper functions that faciltate converting parameters and gradients dictionaries into vectors and then re-convert them back to dictionaries.\n\n\nCode\ndef dictionary_to_vector(params_dict):\n    \"\"\"\n    Roll a dictionary into a single vector.\n\n    Arguments\n    ---------\n    params_dict : dict\n        learned parameters.\n\n    Returns\n    -------\n    params_vector : array\n        vector of all parameters concatenated.\n    \"\"\"\n    count = 0\n    for key in params_dict.keys():\n        new_vector = np.reshape(params_dict[key], (-1, 1))\n        if count == 0:\n            theta_vector = new_vector\n        else:\n            theta_vector = np.concatenate((theta_vector, new_vector))\n        count += 1\n\n    return theta_vector\n\n\ndef vector_to_dictionary(vector, layers_dims):\n    \"\"\"\n    Unroll parameters vector to dictionary using layers dimensions.\n\n    Arguments\n    ---------\n    vector : array\n        parameters vector.\n    layers_dims : list or array_like\n        dimensions of each layer in the network.\n\n    Returns\n    -------\n    parameters : dict\n        dictionary storing all parameters.\n    \"\"\"\n    L = len(layers_dims)\n    parameters = {}\n    k = 0\n\n    for l in range(1, L):\n        # Create temp variable to store dimension used on each layer\n        w_dim = layers_dims[l] * layers_dims[l - 1]\n        b_dim = layers_dims[l]\n\n        # Create temp var to be used in slicing parameters vector\n        temp_dim = k + w_dim\n\n        # add parameters to the dictionary\n        parameters[\"W\" + str(l)] = vector[\n            k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1])\n        parameters[\"b\" + str(l)] = vector[\n            temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n\n        k += w_dim + b_dim\n\n    return parameters\n\n\ndef gradients_to_vector(gradients):\n    \"\"\"\n    Roll all gradients into a single vector containing only dW and db.\n\n    Arguments\n    ---------\n    gradients : dict\n        storing gradients of weights and biases for all layers: dA, dW, db.\n\n    Returns\n    -------\n    new_grads : array\n        vector of only dW and db gradients.\n    \"\"\"\n    # Get the number of indices for the gradients to iterate over\n    valid_grads = [key for key in gradients.keys()\n                   if not key.startswith(\"dA\")]\n    L = len(valid_grads)// 2\n    count = 0\n    \n    # Iterate over all gradients and append them to new_grads list\n    for l in range(1, L + 1):\n        if count == 0:\n            new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n        else:\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)))\n            new_grads = np.concatenate(\n                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n        count += 1\n        \n    return new_grads\n\n\nFinally, we’ll write the gradient checking function that will compute the difference between the analytical and numerical gradients and tell us if our implementation of back-propagation is correct. We’ll randomly choose 1 example to compute the difference.\n\n\nCode\ndef forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn=\"tanh\"):\n    \"\"\"\n    Implements the forward propagation and computes the cost.\n    \n    Arguments\n    ---------\n    X : 2d-array\n        input data, shape: number of features x number of examples.\n    parameters : dict\n        parameters to use in forward prop.\n    Y : array\n        true \"label\", shape: 1 x number of examples.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    cost : float\n        cross-entropy cost.\n    \"\"\"\n    # Compute forward prop\n    AL, _ = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n    # Compute cost\n    cost = compute_cost(AL, Y)\n\n    return cost\n\n\ndef gradient_check(\n        parameters, gradients, X, Y, layers_dims, epsilon=1e-7,\n        hidden_layers_activation_fn=\"tanh\"):\n    \"\"\"\n    Checks if back_prop computes correctly the gradient of the cost output by\n    forward_prop.\n    \n    Arguments\n    ---------\n    parameters : dict\n        storing all parameters to use in forward prop.\n    gradients : dict\n        gradients of weights and biases for all layers: dA, dW, db.\n    X : 2d-array\n        input data, shape: number of features x number of examples.\n    Y : array\n        true \"label\", shape: 1 x number of examples.\n    epsilon : \n        tiny shift to the input to compute approximate gradient.\n    layers_dims : list or array_like\n        dimensions of each layer in the network.\n    \n    Returns\n    -------\n    difference : float\n        difference between approx gradient and back_prop gradient\n    \"\"\"\n    \n    # Roll out parameters and gradients dictionaries\n    parameters_vector = dictionary_to_vector(parameters)\n    gradients_vector = gradients_to_vector(gradients)\n\n    # Create vector of zeros to be used with epsilon\n    grads_approx = np.zeros_like(parameters_vector)\n\n    for i in range(len(parameters_vector)):\n        # Compute cost of theta + epsilon\n        theta_plus = np.copy(parameters_vector)\n        theta_plus[i] = theta_plus[i] + epsilon\n        j_plus = forward_prop_cost(\n            X, vector_to_dictionary(theta_plus, layers_dims), Y,\n            hidden_layers_activation_fn)\n\n        # Compute cost of theta - epsilon\n        theta_minus = np.copy(parameters_vector)\n        theta_minus[i] = theta_minus[i] - epsilon\n        j_minus = forward_prop_cost(\n            X, vector_to_dictionary(theta_minus, layers_dims), Y,\n            hidden_layers_activation_fn)\n\n        # Compute numerical gradients\n        grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n\n    # Compute the difference of numerical and analytical gradients\n    numerator = norm(gradients_vector - grads_approx)\n    denominator = norm(grads_approx) + norm(gradients_vector)\n    difference = numerator / denominator\n\n    if difference &gt; 10e-7:\n        print (\"\\033[31mThere is a mistake in back-propagation \" +\\\n               \"implementation. The difference is: {}\".format(difference))\n    else:\n        print (\"\\033[32mThere implementation of back-propagation is fine! \"+\\\n               \"The difference is: {}\".format(difference))\n\n    return difference\n\n\n\n\nCode\n# Set up neural network architecture\nlayers_dims = [X_train.shape[0], 5, 5, 1]\n\n# Initialize parameters\nparameters = initialize_parameters(layers_dims)\n\n# Randomly selecting 1 example from training data\nperms = np.random.permutation(X_train.shape[1])\nindex = perms[:1]\n\n# Compute forward propagation\nAL, caches = L_model_forward(X_train[:, index], parameters, \"tanh\")\n\n# Compute analytical gradients\ngradients = L_model_backward(AL, y_train[:, index], caches, \"tanh\")\n\n# Compute difference of numerical and analytical gradients\ndifference = gradient_check(parameters, gradients, X_train[:, index], y_train[:, index], layers_dims)\n\n\nThere implementation of back-propagation is fine! The difference is: 3.02205552997035e-09\n\n\nCongratulations! Our implementation is correct :)"
  },
  {
    "objectID": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html#conclusion",
    "href": "posts/coding-nn/gradient-checking/Coding-Neural-Network-Gradient-Checking.html#conclusion",
    "title": "Coding Neural Network Part 2 - Gradient Checking",
    "section": "Conclusion",
    "text": "Conclusion\nBelow are some key takeaways:\n\nTwo-sided numerical gradient approximates the analytical gradients more closely than right-side form.\nSince gradient checking is very slow:\n\nApply it on one or few training examples.\nTurn it off when training neural network after making sure that backpropagation’s implementation is correct.\n\nGradient checking doesn’t work when applying drop-out method. Use keep-prob = 1 to check gradient checking and then change it when training neural network.\nEpsilon = 10e-7 is a common value used for the difference between analytical gradient and numerical gradient. If the difference is less than 10e-7 then the implementation of backpropagation is correct.\nThanks to Deep Learning frameworks such as Tensorflow and Pytorch, we may find ourselves rarely implement backpropagation because such frameworks compute that for us; however, it’s a good practice to understand what happens under the hood to become a good Deep Learning practitioner."
  },
  {
    "objectID": "posts/python/Modules-And-Packages.html",
    "href": "posts/python/Modules-And-Packages.html",
    "title": "A Deep Dive into Python’s Modules and Packages",
    "section": "",
    "text": "Python’s simplicity and versatility are largely attributed to its extensive ecosystem of modules and packages. These essential components enable developers to write clean, reusable, and efficient code, whether for simple scripts or complex applications.\nThis article aims to deepen our understanding of Python’s modules and packages and the machinery involved, helping me become a more effective Python programmer. We will explore their structure and functionality, covering everything from the basics of importing to creating custom packages and managing dependencies. By unpacking the underlying machinery of how modules/packages get imported and what they really are, we’ll gain insights that will enhance our coding practices and project organization.\n\nIntroduction\n\nPython has only one type of module object regardless of the language the module was implemented it (C/Python/…)\nPackage provides a naming hierarchy to organize modules (same analogy to directory in Unix file system):\n\nAll packages are modules but not all modules are packages\nA module is a package that has __path__\nA package can include subpackages (sub-directories)\n\nThere are two types of packages:\n\nRegular packages are directories that have __init__.py. When importing a package/subpackage -&gt; implicitly executes all __init__.py files on the path and bound objects to names in the package’s namespace\n\nWhen import machinery is looking for the package, it stops once it finds it\n\nNamespace packages are directories that don’t have __init__.py\n\nWhen import machinery is looking for the package, it does not stop when it finds it and assuming there may be a regular package in some other paths in sys.path but keep a record of all namespace packages it found during the search. If it finds a regular package with that name -&gt; discard all namespace packages it found and import the regular package. If it doesn’t find any regular package with that name -&gt; Use all the namespace packages it found during the search and combine their paths in namespace_path so when we try to import subpackage or modules, it checks all the paths in the namespace_path (which is a list)\nThere can be multiple packages of the same name (under different directories) -&gt; They are all combined together and the namespace_path list would have the path for all of them. Therefore, the same package can be used to refer to completely different modules in different directories\nPython first scans the whole sys.path before deciding that the package is a namespace -&gt; If any name is found with __init__.py in it, it will give this priority and don’t continue.\n\n\nWhen importing subpackages such as foo.bar.baz, Python first imports foo, then foo.bar, then foo.bar.baz\n\nEach of these will be cached in sys.modules\n\n__init__.py makes a directory a python package\n\nWe can use it to import useful stuff from different modules/subpackages so it can be available to user\nWhen importing an object, it has __module__ attribute which determines the global environment for the object\nWe can define __all__ in __init__ as concatenation of all __all__ in modules\n\nExample: __all__ = foo.__all__ + bar.__all__ BUT we need to either first import foo and bar or import anything from them so they can be defined such as from .foo import * OR from .foo import y\n\n__init__ can be used to also initialize things and maybe monkeypatch some other modules\n\nEach package has __path__ attribute that helps when searching for subpackages. This will be given to path finder when loading subpackages\n\nIt is a list; similar to sys.path so we can change it. But it is not recommended\nExample: import math; math.__path__.append(~/Documents\")\n\nRelative import is preferred to absolute imports inside packages to avoid having issues if the package name is changed\nPackages get loaded once even if we import it multiple times\nWe can in theory upgrade a package in the cache like this:\n\nsys.modules[new_name] = package_name\n\nIf we use python -m package.module, it executes module as the main program and relative imports works. Otherwise, relative imports won’t work.\n\nm stands for module\n\nThe __main__ module is a special case relative to Python’s import system. The __main__ module is directly initialized at interpreter startup, much like sys and builtins. The manner in which __main__ is initialized depends on the flags and other options with which the interpreter is invoked\n__main__.py designates main for a package/subpackage and also allows package directory to be executable -&gt; explicitly marks the entry point. Examples:\n\npython package would look for __main__.py to execute it\npython -m package.subpackage would look for __main__.py inside package/subpackage to execute\n__package__ is set so the relative imports still work\nA lot of programming tools utilize this to their own benefit: python -m profile script.y OR python -m pdb script.py\nNOTE THAT __init__.py files on the path will still be executed\n\nDepending on how __main__ is initialized, __main__.__spec__ gets set appropriately or to None.\n\nWhen Python is started with the -m option, __spec__ is set to the module spec of the corresponding module or package. __spec__ is also populated when the __main__ module is loaded as part of executing a directory, zipfile or other sys.path entry.\nOtherwise, it will be set to None\n\n\n\n\n\nSys.path\n\nimportlib has a rich API to interact with import system. It is preferred over __import__()\n__import__ Only does module search and creation without the name binding\nimport Does everything. Module search, creation, and name binding. It calls __import__ under the hood\n.egg files are just directories or .zip files with extra metadata for package managers\nsys.path is where python looks to search for a module/package (last place) that we try to import. It traverses it from start-to-end\n\nIt has the name of directorires, .zipfiles, .egg files\nfirst match wins\nIf it can’t find it -&gt; can not be imported\n\nsys.prefix is where python is stored (os.py is the landmark) and sys.exec_prefix is where compiled binaries are stored (lib-dynload is the landmark)\n\nWith virtual environments -&gt; each one has its own sys.prefix\nIt is constructed from sys.prefix, PYTHONHOME, and site.py. Setting PYTHONHOME would override sys.prefix and sys.exec_prefic\nPython looks for its libraries starting from where it is and keep going up until the root of the file syetsm. It looks for os.py and use that location as a landmark\npython -S skips site.py\npython -vv to see what python tries to do with every statement\nSetting PYTHONPATH to some directories will insert them into the beginning of sys.path. Example:\n\nenv PYTHONPATH=\"/Users/imad/Documents python to run python with documents inserted at the beginning of the sys.apth\n\nsite.py appends the path to third-party libraries. This is where installed packages get stored. Example: /usr/local/lib/python3.4/site-packages\n\nPython now have builtin virtual environments that can create one using the venv module\n\npython -m venv env_name will create new environment called env_name\nThis environment will include few directories such as include, lib, site-packages, bin and pyvenv.cfg\nThis new environment has no third party libraries or any system wide libraries such as those in /usr/local\nAll third libraries will be installed in site-packages directory\nPython binary will refer to the original Python installation when the environment was created\nWe can use source path_to_env_name/bin/activate to activate the environment. deactivate to deactivate it. Finally, rm -r path_to_env_name or pyenv --rm if we create it using poetry\n\nFiles with .pth extension in site-packages directory get added to the sys.path. We can list directories in those files that will be added to sys.path for any new instance of Python\n\nPackage managers and other third-party packages use this kind of hack to add paths to the sys.path\n\nsitecustomize and usercustomize also can be used to add stuff to the sys.path\nFinally the current working directory will be added to the path (at the beginning)\n\n\n\nModules\n\nModules are just objects of type ModuleType. They act like a dictionary that holds references to objects it holds; module.__dict__\n\nWhen importing a module, it executes the module from top to bottom before returning to the caller\nModule can be namespace, py file, execution environment for statements or container of global variables\nWe can set/delete attributes. module.x = 10 is the same as module.__dict__['x'] = 10\nThe dictionary has preset attributes such as __path__, __loader__ …\nMain attributes:\n\n__name__ : # Module name\n__file__ : # Associated source file (if any)\n__doc__ : # Doc string\n__path__ : # Package path. It is used to look for package subcomponents\n__package__ : # The module’s __package__ attribute must be set. Its value must be a string, but it can be the same value as its __name__. When the module is a package, its __package__ value should be set to its __name__. When the module is not a package, __package__ should be set to the empty string for top-level modules, or for submodules, to the parent package’s name.\n__spec__ : # Module spec\n\n\nThe main difference between modules and packages is that packages have __path__ and __package__ defined (not None)\nsys.modules serves as a cache for all imported modules/packages\n\nIt is a dictionary so we can delete/set keys\nIf we delete a module, it will force Python to import it when we reimport it\nIf we set module key to None -&gt; result in ModuleNotFoundError\n\nEven if we import one object from a module/package, the module/package will be cached in the sys.modules but not available in the global name space\nThe module created during loading and passed to exec_module() may not be the one returned at the end of the import\n\nThis can happen if the imported module set the sys.modules[__name__] to some other module\n\nThe module’s attributes are set after creation and before execution\nExecution of the module is what populates the module’s __dict__ (namespace of the module). This is done by the loader\nWhen a submodule is loaded using any mechanism, a binding is placed in the parent module’s namespace to the submodule object. For example, if we have a package called spam that has a submodule foo and it imports any of its objects like from .foo import x, after importing spam, spam will have an attribute foo which is bound to the submodule -&gt; We can now use spam.foo\nRelative imports use leading dots. A single leading dot indicates a relative import, starting with the current package. Two or more leading dots indicate a relative import to the parent(s) of the current package, one level per dot after the first.\n\nRelative imports can only use this form of import: from &lt;&gt; import &lt;&gt;\nIt can’t use import .&lt;&gt; because this is not a valid expression\n\nAbsolute imports have to start from the top level package and go downward to refer to the module:\n\nfrom package.subpackage import module\nNot recommended because if we change the name of the package then we need to change all the import statements -&gt; relative imports are more robust and don’t care about namings\n\nProcess when importing a module/package (after locating it):\n\nFirst checks if it is cached. If not, continue\nIt creates a ModuleType object with that name\nCache the module in sys.modules\nExecutes the source code inside the module (first prefixing it with .py and then assign __file__)\n\nIn the case of the package/subpackage, it assign it the __init__.py file\nIt also executes all the __init__.py on the path\n\nAssign a variable to the module object\n\n\nimport sys, types\n\ndef import_module(modname):\n    # Check if it is in the cache first\n    if modname in sys.modules:\n        return sys.modules[modname]\n    \n    sourcepath = modname + '.py'\n    with open(sourcepath, 'r') as f:\n        sourcecode = f.read()\n    mod = types.ModuleType(modname)\n    mod.__file__ = sourcepath\n    \n    # Cache the module\n    sys.modules[modname] = mod\n    \n    # Convert it to Python ByteCode\n    code = compile(sourcecode, sourcepath, 'exec')\n    \n    # Execute the code in the module from top to bottom\n    # And update the state (globals) in the module's dictionary\n    exec(code, mod.__dict__)\n    \n    # We return the cached one in case there is some patching inside the module\n    return sys.modules[modname]\n\n\nModule Compilation\n\n\nPython put a lock when importing a module until it is done so that we don’t have multiple threads trying to import the same module at the same time\n__import__ is the machinery behind import statement\nWe can use importlib.import_module(module) which is the same thing as __import__\n\nimportlib.import_module(spam) is the same as import spam\nimportlib.import_module('.spam', __package__) is the same as from . import spam\nWe can track all imports as follows:\n\n\nimport builtins\n\ndef imp_mod(modname, *args, imp=__import__):\n    print(f\"Importing {modname}\")\n    return imp(modname, *args)\n\nbuiltins.__import__ = imp_mod\n\nModule Reloading:\n\nIt is not a good idea to reload a module because it creates zombies. Basically Python doesn’t try to clean up the dictionary from the old module, but instead exec() the new state of the module using the old module.__dict__. This means stuff from previous load may still exist and we end up having weird cases. This is how Python reloads a module:\n\ncode = open(module.__file__, 'rb').open()\nexec(code, module.__dict__)\n\nAlso, submodules that are loaded in the module/package don’t get reloaded. They still have their old version. Exampe: If module has import pandas as pd, when reloading the module it doesn’t reload pandas.\nAlso, if we have instances that use the old version of the module and then we reload -&gt; New instances of the same object (class) will refer to different code implementation than the instances created before the reload -&gt; Even though they refer to the same class, instances will have different types\n\nsys.path is only the small part of the import machinery\nImports is actually controlled by sys.meta_path\n\nIt is a list of importers\n\n[_frozen_importlib.BuiltinImporter,\n_frozen_importlib.FrozenImporter,\n_frozen_importlib_external.PathFinder,\n&lt;six._SixMetaPathImporter at 0x10c8769b0&gt;,\n&lt;pkg_resources.extern.VendorImporter at 0x10dbf9300&gt;]\n\nPython’s default sys.meta_path has three meta path finders, one that knows how to import built-in modules, one that knows how to import frozen modules, and one that knows how to import modules from an import path\nFor every import statement, it goes from start-to-end to know if sys.meta_path knows how to install it\n\n\nimportlib.util as imp\n\ndef find_spec(modname):\n    for imp in sys.meta_path:\n        spec = imp.find_spec(modname)\n        if spec:\n            return spec\n    return None\n\nModuleSpec of a module is its metadata that the loader uses to load it. We can also use importlib.util.find_spec() to get the module spec of any loaded package. If the package/module is not found -&gt; returns None. Example of pandas module spec:\n  ModuleSpec(name='pandas', loader=&lt;_frozen_importlib_external.SourceFileLoader object at 0x10e609f90&gt;, origin='/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/pandas/__init__.py', submodule_search_locations=['/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/pandas'])\n\nModule Spec main info:\n\nspec.name : # Full module name\nspec.parent : # Enclosing package\nspec.submodule_search_locations : # Package path\nspec.has_location : # Has external location\nspec.origin : # Source file location\nspec.cached : # Cached location\nspec.loader : # Loader object\n\nWe can use the loader from module spec to get the source code w/o importing it. They actually create the imported module:\n\nmodule = spec.loader.create_module(spec)\nif not module:\n    module = types.ModuleType(spec.name)\n    module.__file__ = spec.origin\n    module.__loader__ = spec.loader\n    module.__package__ = spec.parent\n    module.__path__ = spec.submodule_search_locations\n    module.__spec__ = spec\n\nWe can create module from spec with importlib.util.module_from_spec. This DOES NOT LOAD THE MODEL., it only creates it. To load the module, the module must be executed with spec.loader.exec_module(spec) and then cache it sys.modules[spec.name] module. exec_module will populate the __dict__ of the module.\n\nWe can execute modules lazily on first access. Implementation example:\n\nimport types\n\n\nclass _Module(types.ModuleType):\n    pass\n\n\nclass _LazyModule(_Module):\n\n    def __init__(self, spec):\n        super().__init__(spec.name) \n        self.__file__ = spec.origin\n        self.__package__ = spec.parent \n        self.__loader__ = spec.loader\n        self.__path__ = spec.submodule_search_locations \n        self.__spec__ = spec\n\n    def __getattr__(self, name):\n        self.__class__ = _Module\n        self.__spec__.loader.exec_module(self)\n        assert sys.modules[self.__name__] == self\n        return getattr(self, name)\nimport importlib.util, sys\n\ndef lazy_import(name):\n   # If already loaded, return the module\n    if name in sys.modules:\n        return sys.modules[name]\n    \n    # Not loaded. Find the spec\n    spec = importlib.util.find_spec(name)\n    if not spec:\n        raise ImportError(f'No module {name:r}')\n    \n    # Check for compatibility\n    if not hasattr(spec.loader, 'exec_module'):\n        raise ImportError('Not supported')\n\n    # Perform the lazy import\n    module = sys.modules[name] = _LazyModule(spec)\n    return module\n\nTherefore, the module create/loading has been decoupled in recent versions of Python\nWe can insert an importer to sys.meta_path that can change the behavior of imports\n\nIf it is in the beginning, it supercedes all other loaders and we can do crazy things\n\nimport sys\n\n\nclass Watcher(object):\n\n    @classmethod\n    def find_spec(cls, name, path, target=None):\n        print('Importing', name, path, target)\n        return None\n\n\nsys.meta_path.insert(0, Watcher)\nWe can also use this idea to add some logic such as autoinstall packages that are not found using pip. We insert the installer at the end of sys.meta_path\n\nimport sys\nimport subprocess\nimport importlib.util\n\n\nclass AutoInstall(object):\n    _loaded = set()\n\n    @classmethod\n    def find_spec(cls, name, path, target=None):\n        if path is None and name not in cls._loaded: \n            cls._loaded.add(name)\n            print(\"Installing\", name)\n            try:\n                out = subprocess.check_output(\n                          [sys.executable, '-m', 'pip', 'install', name])\n                return importlib.util.find_spec(name) \n            except Exception as! e:\n                print(\"Failed\")\n        return None\nsys.meta_path.append(AutoInstall)\n\nWe can also import packages not found on the system from some other systems such as Redis\nsys.path_hooks is responsible for the actual loading of the module/package depending on the path\n\nEach entry in the sys.path is tested against a list of path hooks to assosiate a module finder with each path entry\nPath finders are used to locate module and return module spec along with loader\nPath finders get cached in sys.path_importer_cache\n\nBoth loaders and finders have find_spec() that returns spec of module if they know how to find/load it. Otherwise, they return None\nWhat happens during import:\n\nmodname = 'somemodulename'\nfor entry in sys.path:\n    finder = sys.path_importer_cache[entry]\n    if finder:\n        spec = finder.find_spec(modname)\n        if spec:\n            break\nelse:\n    raise ImportError('No such module')\n...\n# Load module from the spec\n...\n\n\nExperiments\n\nsys.path.append(\"/Users/imad/Desktop/\")\n\n\nfrom pck.mod import X\n\npck.mod\n\n\n\nX\n\n100\n\n\n\nfrom pck.test import X\n\npck.test\n\n\n\nsys.modules[\"pck\"].__path__\n\n_NamespacePath(['/Users/imad/Documents/python-materials/modules-and-packages/pck', '/Users/imad/Documents/python-materials/modules-and-packages/pck', '/Users/imad/Desktop/pck'])\n\n\n\nfoo.__package__, foo.__path__\n\nAttributeError: module 'package.foo' has no attribute '__path__'\n\n\n\nglobals()[\"foo\"]\n\n&lt;module 'package.foo' from '/Users/imad/Documents/python-materials/modules-and-packages/package/foo.py'&gt;\n\n\n\ndef f():\n    pass\n\n\nfrom pandas import read_csv\n\n\nsys.path_hooks\n\n[zipimport.zipimporter,\n &lt;function _frozen_importlib_external.FileFinder.path_hook.&lt;locals&gt;.path_hook_for_FileFinder(path)&gt;]\n\n\n\nlist(sys.path_importer_cache.keys())[:10]\n\n['/Users/imad/anaconda3/envs/python-exp/lib/python310.zip',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/encodings',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/importlib',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/lib-dynload',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/PyYAML-6.0-py3.10-macosx-10.9-x86_64.egg',\n '/Users/imad/Documents/python-materials/modules-and-packages',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/ipykernel',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/json']\n\n\n\nfrom importlib.util import find_spec\n\n\nm = find_spec(\"mod\")\nm.loader.get_source(\"mod\")\n\n'y = 200\\nprint(y)\\n\\nclass A:\\n    print(\"A\")\\n'\n\n\n\nimport sys\nsys.meta_path\n\n[_frozen_importlib.BuiltinImporter,\n _frozen_importlib.FrozenImporter,\n _frozen_importlib_external.PathFinder,\n &lt;six._SixMetaPathImporter at 0x10c8769b0&gt;,\n &lt;pkg_resources.extern.VendorImporter at 0x10dbf9300&gt;]\n\n\n\nimport mod\n\n200\nA\n\n\n\na = mod.A()\n\n\nfrom importlib import reload\n\n\nreload(mod)\n\n200\nA\n\n\n&lt;module 'mod' from '/Users/imad/Documents/python-materials/modules-and-packages/mod.py'&gt;\n\n\n\nb = mod.A()\n\n\na.__class__, b.__class__, type(a) == type(b)\n\n(mod.A, mod.A, True)\n\n\n\nfrom importlib.util import find_spec\n\n\nfind_spec(\"sys\")\n\nModuleSpec(name='sys', loader=&lt;class '_frozen_importlib.BuiltinImporter'&gt;, origin='built-in')\n\n\n\nfind_spec(\"pandas\")\n\nModuleSpec(name='pandas', loader=&lt;_frozen_importlib_external.SourceFileLoader object at 0x10e609f90&gt;, origin='/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/pandas/__init__.py', submodule_search_locations=['/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/pandas'])\n\n\n\nimport importlib\n\n\nimportlib.import_module()\n\n\npd.__path__, pd.__name__, pd.__package__, pd.__file__, pd.__doc__\n\n(['/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/pandas'],\n 'pandas',\n 'pandas',\n '/Users/imad/anaconda3/envs/python-exp/lib/python3.10/site-packages/pandas/__init__.py',\n '\\npandas - a powerful data analysis and manipulation library for Python\\n=====================================================================\\n\\n**pandas** is a Python package providing fast, flexible, and expressive data\\nstructures designed to make working with \"relational\" or \"labeled\" data both\\neasy and intuitive. It aims to be the fundamental high-level building block for\\ndoing practical, **real world** data analysis in Python. Additionally, it has\\nthe broader goal of becoming **the most powerful and flexible open source data\\nanalysis / manipulation tool available in any language**. It is already well on\\nits way toward this goal.\\n\\nMain Features\\n-------------\\nHere are just a few of the things that pandas does well:\\n\\n  - Easy handling of missing data in floating point as well as non-floating\\n    point data.\\n  - Size mutability: columns can be inserted and deleted from DataFrame and\\n    higher dimensional objects\\n  - Automatic and explicit data alignment: objects can be explicitly aligned\\n    to a set of labels, or the user can simply ignore the labels and let\\n    `Series`, `DataFrame`, etc. automatically align the data for you in\\n    computations.\\n  - Powerful, flexible group by functionality to perform split-apply-combine\\n    operations on data sets, for both aggregating and transforming data.\\n  - Make it easy to convert ragged, differently-indexed data in other Python\\n    and NumPy data structures into DataFrame objects.\\n  - Intelligent label-based slicing, fancy indexing, and subsetting of large\\n    data sets.\\n  - Intuitive merging and joining data sets.\\n  - Flexible reshaping and pivoting of data sets.\\n  - Hierarchical labeling of axes (possible to have multiple labels per tick).\\n  - Robust IO tools for loading data from flat files (CSV and delimited),\\n    Excel files, databases, and saving/loading data from the ultrafast HDF5\\n    format.\\n  - Time series-specific functionality: date range generation and frequency\\n    conversion, moving window statistics, date shifting and lagging.\\n')"
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#introduction",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#introduction",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Introduction",
    "text": "Introduction\nA/B testing can be defined as a randomized controlled experiment that allows us to test if there is a causal relationship between a change to a website/app and the user behavior. The change can be visible such as location of a button on the homepage or invisible such as the ranking/recommendation algorithms and backend infrastructure.\nWeb/Mobile developers and business stakeholders always face the following dilemma: Should we try out all ideas and explore all options continuously? Or should we exploit the best available option and stick to it? The answer is, as in most cases, will be a trade-off between the two extremes. If we explore all the time, we’ll collect a lot of data and waste resources in testing inferior ideas and missing sales (e-commerce case). However, if we only exploit the available option and never try new ideas, we would be left behind and loose in the long-term with ever-changing markets.\nIn this series, we’ll explore solutions offered by Multi-armed Bandit Algorithms that have two main advantages over traditional A/B testing:\n\nSmoothly decrease exploration over time instead of sudden jumps.\nFocus resources on better options and not keep evaluating inferior options during the life of the experiment.\n\nWhat is Bandit Algorithms? Bandit Algorithms are algorithms that try to learn a rule of selecting a sequence of options that balance exploring available options and getting enough knowledge about each option and maximize profits by selecting the best option. Note that during the experiment, we only have knowledge about the options we tried. Therefore, every time we select an option that’s not the best one, we incur an opportunity cost of not selecting the best option; however, we also gain a new knowledge (feedback) about the selected option. In other words, we need to have enough feedback about each option to learn the best option. As a result, the best strategy would be to explore more at the beginning of the experiment until we know the best option and then start exploiting that option."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#epsilon-greedy-algorithm",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#epsilon-greedy-algorithm",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "epsilon-Greedy Algorithm",
    "text": "epsilon-Greedy Algorithm\nIn this notebook, we’ll cover epsilon-Greedy Algorithm. Greedy Algorithm can be defined as the algorithm that picks the best currently available option without taking into consideration the long-term effect of that decision, which may happen to be a suboptimal decision. Given that, we can define epsilon-Greedy Algorithm as a Greedy Algorithm that adds some randomness when deciding between options: Instead of picking always the best available option, randomly explore other options with a probability = \\(\\epsilon\\) or pick the best option with a probability = \\(1 - \\epsilon\\). Therefore, we can add randomness to the algorithm by increasing \\(\\epsilon\\), which will make the algorithm explores other options more frequently. Additionally, \\(\\epsilon\\) is a hyper-parameter that needs to be tuned based on the experiment, i.e. there is no value that works best on all experiments. Let’s explore how the algorithm works assuming we have two options: A and B (we can think of them as Control and Treatment groups). For each new user:\n\nAssume we have a coin that has a probability of coming heads = \\(\\epsilon\\) and a probability of coming tails = \\(1 - \\epsilon\\). Therefore,\n\nIf it comes heads, explore randomly the available options (exploration).\n\nThe probability of selecting any option is \\(\\frac{1}{2}\\).\n\nIf it comes tails, select the best option (exploitation).\n\n\nAs a result, the probability of selecting any option randomly if we have \\(N\\) options is \\(\\epsilon \\frac{1}{N}\\); however, the probability of selecting the best option is \\(1 - \\epsilon\\) (see figure 1).\n\n\n\nFigure 1: epsilon-Greedy Algorithm\n\n\nLet’s import the needed packages and implement the algorithm.\n\n\nCode\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Add module path to system path\nsys.path.append(os.path.abspath(\"../\"))\nfrom utils import plot_algorithm, compare_algorithms\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\n\n\n\nCode\nclass EpsilonGreedy:\n    def __init__(self, epsilon, counts=None, values=None):\n        self.epsilon = epsilon\n        self.counts = counts\n        self.values = values\n\n    def initialize(self, n_arms):\n        self.counts = np.zeros(n_arms, dtype=int)\n        self.values = np.zeros(n_arms, dtype=float)\n\n    def select_arm(self):\n        z = np.random.random()\n        if z &gt; self.epsilon:\n            # Pick the best arm\n            return np.argmax(self.values)\n        # Randomly pick any arm with prob 1 / len(self.counts)\n        return np.random.randint(0, len(self.values))\n\n    def update(self, chosen_arm, reward):\n        # Increment chosen arm's count by one\n        self.counts[chosen_arm] += 1\n        n = self.counts[chosen_arm]\n\n        # Recompute the estimated value of chosen arm using new reward\n        value = self.values[chosen_arm]\n        new_value = value * ((n - 1) / n) + reward / n\n        self.values[chosen_arm] = new_value\n\n\nFew things to note from the above implementation: - Initialization of values (rewards) affect the long term performance of the algorithm. - The larger the sample size (N), the less influential the rewards from the recent options since we are using the average of each option in the values array. - Values array will store the estimated values (average) of each option. - Counts is just an internal counter that keeps track of the number of times we selected each option."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#monte-carlo-simulations",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#monte-carlo-simulations",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nIn order to evaluate the algorithm, we will use Monte Carlo simulations. We’ll use 5000 simulations to overcome the randomness generated from the random number generator. Also, we’ll use Bernoulli distribution to get the reward from each option on each run. For each simulation: - Initialize the algorithm with no prior knowledge. - Loop over the time horizon: - Select the option. - Draw the reward for the selected option using Bernoulli distribution and the probability defined. - Update the counts and estimated values of selected arm.\nWe’ll define the % of reward (probability) of each option and test the performance of the algorithm using three different metrics:\n\nProbability of selecting the best option.\nAverage rewards. This metric is a better approximation if the options are similar.\nCumulative rewards. The previous two metrics are not fair metrics for algorithms with large epsilon where they sacrifice by exploring more options; however, cumulative rewards is what we should care about.\n\nMoreover, we’ll evaluate the algorithm using 5 different values of \\(\\epsilon\\): \\(0.1, 0.2, 0.3, 0.4, 0.5\\). Since in the literature they use arm instead of option for historical reasons, we’ll be using arm and option interchangeably.\n\n\nCode\nclass BernoulliArm:\n    def __init__(self, p):\n        self.p = p\n\n    def draw(self):\n        z = np.random.random()\n        if z &gt; self.p:\n            return 0.0\n        return 1.0\n\n\ndef test_algorithm(algo, arms, num_simulations, horizon):\n    # Initialize rewards and chosen_arms with zero 2d arrays\n    chosen_arms = np.zeros((num_simulations, horizon))\n    rewards = np.zeros((num_simulations, horizon))\n\n    # Loop over all simulations\n    for sim in range(num_simulations):\n        # Re-initialize algorithm's counts and values arrays\n        algo.initialize(len(arms))\n\n        # Loop over all time horizon\n        for t in range(horizon):\n            # Select arm\n            chosen_arm = algo.select_arm()\n            chosen_arms[sim, t] = chosen_arm\n\n            # Draw from Bernoulli distribution to get rewards\n            reward = arms[chosen_arm].draw()\n            rewards[sim, t] = reward\n\n            # Update the algorithms' count and estimated values\n            algo.update(chosen_arm, reward)\n\n    # Average rewards across all sims and compute cumulative rewards\n    average_rewards = np.mean(rewards, axis=0)\n    cumulative_rewards = np.cumsum(average_rewards)\n\n    return chosen_arms, average_rewards, cumulative_rewards\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.1, 0.1, 0.1, 0.1, 0.9]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=500, label=\"eps\")\n\n\n\n\n\n\n\n\n\nFew thing to note from the above graphs: - Regardless of the epsilon values, all algorithms learned the best option. - The algorithm picks options randomly; therefore, it’s not guaranteed to always pick the best option even if it found that option. That’s the main reason why none of the algorithms achieved a probability = 1 of selecting the best option or average rewards = % rewards of the best option even after they learned the best option. - As \\(\\epsilon\\) increases \\(\\rightarrow\\) increase the exploration \\(\\rightarrow\\) increases the chance of picking options randomly instead of the best option. - Algorithms with higher epsilon learn quicker but don’t use that knowledge in exploiting the best option. - Using accuracy in picking the best option and average rewards metrics, the algorithm \\(\\epsilon = 0.1\\) outperforms the rest; however, cumulative rewards metric shows that it takes that algorithm long time to outperform the algorithm with \\(\\epsilon = 0.2\\). - Depends on time planned to run the experiment, different values of epsilons may be more optimal. For example, \\(\\epsilon = 0.2\\) is the best value for almost anything at or below 400.\nLet’s run the experiment again to see how would the algorithm behave under the following settings: - Only two options. - 50 options. - 5 option that are very similar.\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.1, 0.9]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=500, label=\"eps\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [i for i in np.random.random(50)]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=250, label=\"eps\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.2, 0.18, 0.22, 0.19, 0.21]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n# Define epsilon value to check the performance of the algorithm using each one\nepsilon = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               hyper_params=epsilon, num_simulations=5000, horizon=500, label=\"eps\")\n\n\n\n\n\n\n\n\n\n\nWhen we had lower number of options, all algorithms were faster at learning the best option which can be seen by the steepness of all curves of the first two graphs when time &lt; 100. As a result, all algorithms had higher cumulative rewards than when we had 5 options.\nHaving large number of options made it hard on all algorithms to learn the best option and may need a lot more time to figure it out.\nLastly, when options are very similar (in terms of rewards), the probability of selecting the best option by all algorithms decreases over time. Let’s take the algorithm with \\(\\epsilon = 0.1\\) and see why is this the case. After some investigation, the algorithm was struggling in differentiating between the best option and the second best option since the difference between the % rewards is 1%. Therefore, the probability of selecting the best arm was around 50%."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#annealed-epsilon-greedy-algorithm",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#annealed-epsilon-greedy-algorithm",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Annealed epsilon-Greedy Algorithm",
    "text": "Annealed epsilon-Greedy Algorithm\nEpsilon value plays a major role in the performance of epsilon-Greedy algorithm and has to be tuned to the best of our knowledge in terms of the expectations of the estimated rewards of each option. Nonetheless, this estimation suffers from high uncertainty since most of the times either we have no clue what might work or the results would be against our intuition as user experience research has shown in multiple studies. Therefore, isn’t it nice if we can avoid setting up the epsilon values and make the algorithm parameter-free? That’s what Annealed epsilon-Greedy Algorithm does. We specify the rule of decaying epsilon with time and let the algorithm runs with no hyper-parameter configurations. The rule of we will use here is: \\(\\epsilon = \\frac{1}{log(time + 0.0000001)}\\). As we can see, at the beginning of the experiment, \\(\\epsilon\\) would be close to Inf and that means a lot of exploration; however, as time goes, \\(\\epsilon\\) start approaching zero and the algorithm would exploit more and more by selecting the best option.\nWe will evaluate the Annealed version using the same settings as before and compare it to standard version.\n\n\nCode\nclass AnnealingEpsilonGreedy(EpsilonGreedy):\n    def __init__(self, counts=None, values=None):\n        self.counts = counts\n        self.values = values\n\n    def select_arm(self):\n        # Epsilon decay schedule\n        t = np.sum(self.counts) + 1\n        epsilon = 1 / np.log(t + 0.0000001)\n\n        z = np.random.random()\n        if z &gt; epsilon:\n            # Pick the best arm\n            return np.argmax(self.values)\n        # Randomly pick any arm with prob 1 / len(self.counts)\n        return np.random.randint(0, len(self.values))\n\n\n\n\nCode\nnp.random.seed(1)\n# Average reward by arm\nmeans = [0.1, 0.1, 0.1, 0.1, 0.9]\nn_arms = len(means)\n# Shuffle the arms\nnp.random.shuffle(means)\n# Each arm will follow and Bernoulli distribution\narms = list(map(lambda mu: BernoulliArm(mu), means))\n# Get the index of the best arm to test if algorithm will be able to learn that\nbest_arm_index = np.argmax(means)\n\n# Plot the epsilon-Greedy algorithm\nplot_algorithm(alg_name=\"Annealing epsilon-Greedy\", arms=arms, best_arm_index=best_arm_index,\n               num_simulations=5000, horizon=500)\n\n\n\n\n\n\n\n\n\nEven though the accuracy of selecting the best option and the average rewards of the annealing epsilon-Greedy Algorithm is lower than the standard version, it has higher cumulative rewards. Also, since the real world is uncertain and we may not have any clue about the designed options, it may be preferred to use the annealing version under some scenarios."
  },
  {
    "objectID": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#conclusion",
    "href": "posts/bandit-algorithms/epsilon-Greedy-Algorithm.html#conclusion",
    "title": "Bandit Algorithms: epsilon-Greedy Algorithm",
    "section": "Conclusion",
    "text": "Conclusion\nepsilon-Greedy Algorithm works by going back and forth between exploration with probability = \\(\\epsilon\\) and exploitation with probability \\(1 - \\epsilon\\). Below are some takeaways:\n\nSetting the value of epsilon:\n\nIf we set \\(\\epsilon = 1\\), we would only explore the available options with a probability = \\(\\frac{1}{N}\\) of selecting any option. This will enable us to explore a lot of ideas at the expense of wasting resources by evaluating inferior options.\nIf we set \\(\\epsilon = 0\\), we would exploit the best option and never explore any new idea. This strategy would leave up behind our competitors given that the markets are so volatile.\n\nExploration should be high at the beginning of the experiment to gain the knowledge about all the available options. It should decrease as a function of time where at some point after having enough data about all options, the algorithm should focus on exploiting the best option.\nAll algorithms with different epsilon values learned the best option; however, they differ by the level of randomness of each algorithm in keep randomly exploring available options.\nTo get the best results of any Bandit algorithm, we should have a lot of data, which means to run the experiment longer in most cases.\nFor experiments that run for short period of time, traditional A/B testing may be better.\nInitialization of estimated rewards can affect the long-term performance of the algorithm. As a result, we may need to use previous experience and intuition to guide our initial values."
  },
  {
    "objectID": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#introduction",
    "href": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#introduction",
    "title": "Predicting Loan Repayment",
    "section": "Introduction",
    "text": "Introduction\nThe two most critical questions in the lending industry are: 1) How risky is the borrower? 2) Given the borrower’s risk, should we lend him/her? The answer to the first question determines the interest rate the borrower would have. Interest rate measures among other things (such as time value of money) the riskness of the borrower, i.e. the riskier the borrower, the higher the interest rate. With interest rate in mind, we can then determine if the borrower is eligible for the loan.\nInvestors (lenders) provide loans to borrowers in exchange for the promise of repayment with interest. That means the lender only makes profit (interest) if the borrower pays off the loan. However, if he/she doesn’t repay the loan, then the lender loses money.\nWe’ll be using publicly available data from LendingClub.com. The data covers the 9,578 loans funded by the platform between May 2007 and February 2010. The interest rate is provided to us for each borrower. Therefore, so we’ll address the second question indirectly by trying to predict if the borrower will repay the loan by its mature date or not. Through this excerise we’ll illustrate three modeling concepts:\n\nWhat to do with missing values.\nTechniques used with imbalanced classification problems.\nIllustrate how to build an ensemble model using two methods: blending and stacking, which most likely gives us a boost in performance.\n\nBelow is a short description of each feature in the data set:\n\ncredit_policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\npurpose: The purpose of the loan such as: credit_card, debt_consolidation, etc.\nint_rate: The interest rate of the loan (proportion).\ninstallment: The monthly installments ($) owed by the borrower if the loan is funded.\nlog_annual_inc: The natural log of the annual income of the borrower.\ndti: The debt-to-income ratio of the borrower.\nfico: The FICO credit score of the borrower.\ndays_with_cr_line: The number of days the borrower has had a credit line.\nrevol_bal: The borrower’s revolving balance.\nrevol_util: The borrower’s revolving line utilization rate.\ninq_last_6mths: The borrower’s number of inquiries by creditors in the last 6 months.\ndelinq_2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\npub_rec: The borrower’s number of derogatory public records.\nnot_fully_paid: indicates whether the loan was not paid back in full (the borrower either defaulted or the borrower was deemed unlikely to pay it back).\n\nLet’s load the data and check:\n\nData types of each feature\nIf we have missing values\nIf we have imbalanced data\n\n\n\nCode\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport fancyimpute\nfrom imblearn.pipeline import make_pipeline as imb_make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.ensemble import BalancedBaggingClassifier, EasyEnsemble\nfrom mlens.visualization import corrmat\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import Imputer, RobustScaler, FunctionTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (roc_auc_score, confusion_matrix,\n                             accuracy_score, roc_curve,\n                             precision_recall_curve, f1_score)\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\nfrom keras import models, layers, optimizers\n\nos.chdir(\"../\")\nfrom scripts.plot_roc import plot_roc_and_pr_curves\nos.chdir(\"notebooks/\")\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\n\n\n\nCode\n# Load the data\ndf = pd.read_csv(\"../data/loans.csv\")\n\n# Check both the datatypes and if there is missing values\nprint(f\"\\033[1m\\033[94mData types:\\n{11 * '-'}\")\nprint(f\"\\033[30m{df.dtypes}\\n\")\nprint(f\"\\033[1m\\033[94mSum of null values in each feature:\\n{35 * '-'}\")\nprint(f\"\\033[30m{df.isnull().sum()}\")\ndf.head()\n\n\nData types:\n-----------\ncredit_policy          int64\npurpose               object\nint_rate             float64\ninstallment          float64\nlog_annual_inc       float64\ndti                  float64\nfico                   int64\ndays_with_cr_line    float64\nrevol_bal              int64\nrevol_util           float64\ninq_last_6mths       float64\ndelinq_2yrs          float64\npub_rec              float64\nnot_fully_paid         int64\ndtype: object\n\nSum of null values in each feature:\n-----------------------------------\ncredit_policy         0\npurpose               0\nint_rate              0\ninstallment           0\nlog_annual_inc        4\ndti                   0\nfico                  0\ndays_with_cr_line    29\nrevol_bal             0\nrevol_util           62\ninq_last_6mths       29\ndelinq_2yrs          29\npub_rec              29\nnot_fully_paid        0\ndtype: int64\n\n\n\n\n\n\n\n\n\n\ncredit_policy\npurpose\nint_rate\ninstallment\nlog_annual_inc\ndti\nfico\ndays_with_cr_line\nrevol_bal\nrevol_util\ninq_last_6mths\ndelinq_2yrs\npub_rec\nnot_fully_paid\n\n\n\n\n0\n1\ndebt_consolidation\n0.1189\n829.10\n11.350407\n19.48\n737\n5639.958333\n28854\n52.1\n0.0\n0.0\n0.0\n0\n\n\n1\n1\ncredit_card\n0.1071\n228.22\n11.082143\n14.29\n707\n2760.000000\n33623\n76.7\n0.0\n0.0\n0.0\n0\n\n\n2\n1\ndebt_consolidation\n0.1357\n366.86\n10.373491\n11.63\n682\n4710.000000\n3511\n25.6\n1.0\n0.0\n0.0\n0\n\n\n3\n1\ndebt_consolidation\n0.1008\n162.34\n11.350407\n8.10\n712\n2699.958333\n33667\n73.2\n1.0\n0.0\n0.0\n0\n\n\n4\n1\ncredit_card\n0.1426\n102.92\n11.299732\n14.97\n667\n4066.000000\n4740\n39.5\n0.0\n1.0\n0.0\n0\n\n\n\n\n\n\n\n\n\n\nCode\n# Get number of positve and negative examples\npos = df[df[\"not_fully_paid\"] == 1].shape[0]\nneg = df[df[\"not_fully_paid\"] == 0].shape[0]\nprint(f\"Positive examples = {pos}\")\nprint(f\"Negative examples = {neg}\")\nprint(f\"Proportion of positive to negative examples = {(pos / neg) * 100:.2f}%\")\nplt.figure(figsize=(8, 6))\nsns.countplot(df[\"not_fully_paid\"])\nplt.xticks((0, 1), [\"Paid fully\", \"Not paid fully\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Count\")\nplt.title(\"Class counts\", y=1, fontdict={\"fontsize\": 20});\n\n\nPositive examples = 1533\nNegative examples = 8045\nProportion of positive to negative examples = 19.06%\n\n\n\n\n\n\n\n\n\nIt looks like we have only one categorical feature (“purpose”). Also, six features have missing values (no missing values in labels). Moreover, the data set is pretty imbalanced as expected where positive examples (“not paid fully”) are only 19%. We’ll explain in the next section how to handle all of them after giving an overview of ensemble methods."
  },
  {
    "objectID": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#modeling",
    "href": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#modeling",
    "title": "Predicting Loan Repayment",
    "section": "Modeling",
    "text": "Modeling\nEnsemble methods can be defined as combining several different models (base learners) into final model (meta learner) to reduce the generalization error. It relies on the assumption that each model would look at a different aspect of the data which yield to capturing part of the truth. Combining good performing models the were trained independently will capture more of the truth than a single model. Therefore, this would result in more accurate predictions and lower generalization errors.\n\nAlmost always ensemble model performance gets improved as we add more models.\nTry to combine models that are as much different as possible. This will reduce the correlation between the models that will improve the performance of the ensemble model that will lead to significantly outperform the best model. In the worst case where all models are perfectly correlated, the ensemble would have the same performance as the best model and sometimes even lower if some models are very bad. As a result, pick models that are as good as possible.\n\nDiﬀerent ensemble methods construct the ensemble of models in diﬀerent ways. Below are the most common methods:\n\nBlending: Averaging the predictions of all models.\nBagging: Build different models on different datasets and then take the majority vote from all the models. Given the original dataset, we sample with replacement to get the same size of the original dataset. Therefore, each dataset will include, on average, 2/3 of the original data and the rest 1/3 will be duplicates. Since each model will be built on a different dataset, it can be seen as a different model. Random Forest improves on default bagging trees by reducing the likelihood of strong features to picked on every split. In other words, it reduces the number of features available at each split from \\(n\\) features to, for example, \\(n/2\\) or \\(log(n)\\) features. This will reduce correlation –&gt; reduce variance.\nBoosting: Build models sequentially. That means each model learns from the residuals of the previous model. The output will be all output of each single model weighted by the learning rate (\\(\\lambda\\)). It reduces the bias resulted from bagging by learning sequentially from residuals of previous trees (models).\nStacking: Build k models called base learners. Then fit a model to the output of the base learners to predict the final output.\n\nSince we’ll be using Random Fores (bagging) and Gradient Boosting (boosting) classifiers as base learners in the ensemble model, we’ll illustrate only averaging and stacking ensemble methods. Therefore, modeling parts would be consisted of three parts:\n\nStrategies to deal with missing values.\nStrategies to deal with imbalanced datasets.\nBuild ensemble models.\n\nBefore going further, the following data preprocessing steps will be applicable to all models:\n\nCreate dummy variables from the feature “purpose” since its nominal (not ordinal) categorical variable. It’s also a good practice to drop the first one to avoid linear dependency between the resulted features since some algorithms may struggle with this issue.\nSplit the data into training set (70%), and test set (30%). Training set will be used to fit the model, and test set will be to evaluate the best model to get an estimation of generalization error. Instead of having validation set to tune hyperparameters and evaluate different models, we’ll use 10-folds cross validation because it’s more reliable estimate of generalization error.\nStandardize the data. We’ll be using RobustScaler so that the standarization will be less influenced by the outliers, i.e. more robust. It centers the data around the median and scale it using interquartile range (IQR). This step will be included in the pipelines for each model as a transformer so we will not do it separately.\n\n\n\nCode\n# Create dummy variables from the feature purpose\ndf = pd.get_dummies(df, columns=[\"purpose\"], drop_first=True)\ndf.head()\n\n\n\n\n\n\n\n\n\n\ncredit_policy\nint_rate\ninstallment\nlog_annual_inc\ndti\nfico\ndays_with_cr_line\nrevol_bal\nrevol_util\ninq_last_6mths\ndelinq_2yrs\npub_rec\nnot_fully_paid\npurpose_credit_card\npurpose_debt_consolidation\npurpose_educational\npurpose_home_improvement\npurpose_major_purchase\npurpose_small_business\n\n\n\n\n0\n1\n0.1189\n829.10\n11.350407\n19.48\n737\n5639.958333\n28854\n52.1\n0.0\n0.0\n0.0\n0\n0\n1\n0\n0\n0\n0\n\n\n1\n1\n0.1071\n228.22\n11.082143\n14.29\n707\n2760.000000\n33623\n76.7\n0.0\n0.0\n0.0\n0\n1\n0\n0\n0\n0\n0\n\n\n2\n1\n0.1357\n366.86\n10.373491\n11.63\n682\n4710.000000\n3511\n25.6\n1.0\n0.0\n0.0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0.1008\n162.34\n11.350407\n8.10\n712\n2699.958333\n33667\n73.2\n1.0\n0.0\n0.0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n1\n0.1426\n102.92\n11.299732\n14.97\n667\n4066.000000\n4740\n39.5\n0.0\n1.0\n0.0\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\nStrategies to deal with missing value\nAlmost always real world data sets have missing values. This can be due, for example, users didn’t fill some part of the forms or some transformations happened while collecting and cleaning the data before they send it to you. Sometimes missing values are informative and weren’t generated randomly. Therefore, it’s a good practice to add binary features to check if there is missing values in each row for each feature that has missing values. In our case, six features have missing values so we would add six binary features one for each feature. For example, “log_annual_inc” feature has missing values, so we would add a feature “is_log_annual_inc_missing” that takes the values \\(\\in \\{0, 1\\}\\). Good thing is that the missing values are in the predictors only and not the labels. Below are some of the most common strategies for dealing with missing values:\n\nSimply delete all examples that have any missing values. This is usually done if the missing values are very small compared to the size of the data set and the missing values were random. In other words, the added binary features did not improve the model. One disadvantage for this strategy is that the model will throw an error when test data has missing values at prediction.\nImpute the missing values using the mean of each feature separately.\nImpute the missing values using the median of each feature separately.\nUse Multivariate Imputation by Chained Equations (MICE). The main disadvantage of MICE is that we can’t use it as a transformer in sklearn pipelines and it requires to use the full data set when imputing the missing values. This means that there will be a risk of data leakage since we’re using both training and test sets to impute the missing values. The following steps explain how MICE works:\n\nFirst step: Impute the missing values using the mean of each feature separately.\nSecond step: For each feature that has missing values, we take all other features as predictors (including the ones that had missing values) and try to predict the values for this feature using linear regression for example. The predicted values will replace the old values for that feature. We do this for all features that have missing values, i.e. each feature will be used once as a target variable to predict its values and the rest of the time as a predictor to predict other features’ values. Therefore, one complete cycle (iteration) will be done once we run the model \\(k\\) times to predict the \\(k\\) features that have missing values. For our data set, each iteration will run the linear regression 6 times to predict the 6 features.\nThird step: Repeat step 2 until there is not much of change between predictions.\n\nImpute the missing values using K-Nearest Neighbors. We compute distance between all examples (excluding missing values) in the data set and take the average of k-nearest neighbors of each missing value. There’s no implementation for it yet in sklearn and it’s pretty inefficient to compute it since we’ll have to go through all examples to calculate distances. Therefore, we’ll skip this strategy in this notebook.\n\nTo evaluate each strategy, we’ll use Random Forest classifier with hyperparameters’ values guided by Data-driven Advice for Applying Machine Learning to Bioinformatics Problems as a starting point.\nLet’s first create binary features for missing values and then prepare the data for each strategy discussed above. Next, we’ll compute the 10-folds cross validation AUC score for all the models using training data.\n\n\nCode\n# Create binary features to check if the example is has missing values for all features that have missing values\nfor feature in df.columns:\n    if np.any(np.isnan(df[feature])):\n        df[\"is_\" + feature + \"_missing\"] = np.isnan(df[feature]) * 1\n\ndf.head()\n\n\n\n\n\n\n\n\n\n\ncredit_policy\nint_rate\ninstallment\nlog_annual_inc\ndti\nfico\ndays_with_cr_line\nrevol_bal\nrevol_util\ninq_last_6mths\n...\npurpose_educational\npurpose_home_improvement\npurpose_major_purchase\npurpose_small_business\nis_log_annual_inc_missing\nis_days_with_cr_line_missing\nis_revol_util_missing\nis_inq_last_6mths_missing\nis_delinq_2yrs_missing\nis_pub_rec_missing\n\n\n\n\n0\n1\n0.1189\n829.10\n11.350407\n19.48\n737\n5639.958333\n28854\n52.1\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1\n0.1071\n228.22\n11.082143\n14.29\n707\n2760.000000\n33623\n76.7\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0.1357\n366.86\n10.373491\n11.63\n682\n4710.000000\n3511\n25.6\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\n0.1008\n162.34\n11.350407\n8.10\n712\n2699.958333\n33667\n73.2\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\n0.1426\n102.92\n11.299732\n14.97\n667\n4066.000000\n4740\n39.5\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\n\nCode\n# Original Data\nX = df.loc[:, df.columns != \"not_fully_paid\"].values\ny = df.loc[:, df.columns == \"not_fully_paid\"].values.flatten()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=True, random_state=123, stratify=y)\nprint(f\"Original data shapes: {X_train.shape, X_test.shape}\")\n\n# Drop NA and remove binary columns\ntrain_indices_na = np.max(np.isnan(X_train), axis=1)\ntest_indices_na = np.max(np.isnan(X_test), axis=1)\nX_train_dropna, y_train_dropna = X_train[~train_indices_na, :][:, :-6], y_train[~train_indices_na]\nX_test_dropna, y_test_dropna = X_test[~test_indices_na, :][:, :-6], y_test[~test_indices_na]\nprint(f\"After dropping NAs: {X_train_dropna.shape, X_test_dropna.shape}\")\n\n# MICE data\nmice = fancyimpute.MICE(verbose=0)\nX_mice = mice.complete(X)\nX_train_mice, X_test_mice, y_train_mice, y_test_mice = train_test_split(\n    X_mice, y, test_size=0.2, shuffle=True, random_state=123, stratify=y)\nprint(f\"MICE data shapes: {X_train_mice.shape, X_test_mice.shape}\")\n\n\nOriginal data shapes: ((7662, 24), (1916, 24))\nAfter dropping NAs: ((7611, 18), (1905, 18))\nMICE data shapes: ((7662, 24), (1916, 24))\n\n\n\n\nCode\n# Build random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=500,\n                                max_features=0.25,\n                                criterion=\"entropy\",\n                                class_weight=\"balanced\")\n# Build base line model -- Drop NA's\npip_baseline = make_pipeline(RobustScaler(), rf_clf)\nscores = cross_val_score(pip_baseline,\n                         X_train_dropna, y_train_dropna,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mBaseline model's average AUC: {scores.mean():.3f}\")\n\n# Build model with mean imputation\npip_impute_mean = make_pipeline(Imputer(strategy=\"mean\"),\n                                RobustScaler(), rf_clf)\nscores = cross_val_score(pip_impute_mean,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mMean imputation model's average AUC: {scores.mean():.3f}\")\n\n# Build model with median imputation\npip_impute_median = make_pipeline(Imputer(strategy=\"median\"),\n                                  RobustScaler(), rf_clf)\nscores = cross_val_score(pip_impute_median,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mMedian imputation model's average AUC: {scores.mean():.3f}\")\n\n# Build model using MICE imputation\npip_impute_mice = make_pipeline(RobustScaler(), rf_clf)\nscores = cross_val_score(pip_impute_mice,\n                         X_train_mice, y_train_mice,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mMICE imputation model's average AUC: {scores.mean():.3f}\")\n\n\nBaseline model's average AUC: 0.651\nMean imputation model's average AUC: 0.651\nMedian imputation model's average AUC: 0.651\nMICE imputation model's average AUC: 0.656\n\n\nLet’s plot the feature importances to check if the added binary features added anything to the model.\n\n\nCode\n# fit RF to plot feature importances\nrf_clf.fit(RobustScaler().fit_transform(Imputer(strategy=\"median\").fit_transform(X_train)), y_train)\n\n# Plot features importance\nimportances = rf_clf.feature_importances_\nindices = np.argsort(rf_clf.feature_importances_)[::-1]\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 25), importances[indices], align=\"center\")\nplt.xticks(range(1, 25), df.columns[df.columns != \"not_fully_paid\"][indices], rotation=90)\nplt.title(\"Feature Importance\", {\"fontsize\": 16});\n\n\n\n\n\n\n\n\n\nGuided by the 10-fold cross validation AUC scores, it looks like all strategies have comparable results and missing values were generated randomly. Also, the added six binary features showed no importance when plotting feature importances from Random Forest classifier. Therefore, it’s safe to drop those features and use Median Imputation method as a transformer later on in the pipeline.\n\n\nCode\n# Drop generated binary features\nX_train = X_train[:, :-6]\nX_test = X_test[:, :-6]\n\n\n\n\nStrategies to deal with imbalanced data\nClassification problems in most real world applications have imbalanced data sets. In other words, the positive examples (minority class) are a lot less than negative examples (majority class). We can see that in spam detection, ads click, loan approvals, etc. In our example, the positive examples (people who haven’t fully paid) were only 19% from the total examples. Therefore, accuracy is no longer a good measure of performance for different models because if we simply predict all examples to belong to the negative class, we achieve 81% accuracy. Better metrics for imbalanced data sets are AUC (area under the ROC curve) and f1-score. However, that’s not enough because class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. As a result, we’ll explore different methods to overcome class imbalance problem.\n\nUnder-Sample: Under-sample the majority class with or w/o replacement by making the number of positive and negative examples equal. One of the drawbacks of under-sampling is that it ignores a good portion of training data that has valuable information. In our example, it would loose around 6500 examples. However, it’s very fast to train.\nOver-Sample: Over-sample the minority class with or w/o replacement by making the number of positive and negative examples equal. We’ll add around 6500 samples from the training data set with this strategy. It’s a lot more computationally expensive than under-sampling. Also, it’s more prune to overfitting due to repeated examples.\nEasyEnsemble: Sample several subsets from the majority class, build a classifier on top of each sampled data, and combine the output of all classifiers. More details can be found here.\nSynthetic Minority Oversampling Technique (SMOTE): It over-samples the minority class but using synthesized examples. It operates on feature space not the data space. Here how it works:\n\nCompute the k-nearest neighbors for all minority samples.\nRandomly choose number between 1-k.\nFor each feature:\n\nCompute the difference between minority sample and its randomly chosen neighbor (from previous step).\nMultiply the difference by random number between 0 and 1.\nAdd the obtained feature to the synthesized sample attributes.\n\nRepeat the above until we get the number of synthesized samples needed. More information can be found here.\n\n\nThere are other methods such as EditedNearestNeighbors and CondensedNearestNeighbors that we will not cover in this notebook and are rarely used in practice.\nIn most applications, misclassifying the minority class (false negative) is a lot more expensive than misclassifying the majority class (false positive). In the context of lending, loosing money by lending to a risky borrower who is more likely to not fully pay the loan back is a lot more costly than missing the opportunity of lending to trust-worthy borrower (less risky). As a result, we can use class_weight that changes the weight of misclassifying positive example in the loss function. Also, we can use different cut-offs assign examples to classes. By default, 0.5 is the cut-off; however, we see more often in applications such as lending that the cut-off is less than 0.5. Note that changing the cut-off from the default 0.5 reduce the overall accuracy but may improve the accuracy of predicting positive/negative examples.\nWe’ll evaluate all the above methods plus the original model without resampling as a baseline model using the same Random Forest classifier we used in the missing values section.\n\n\nCode\n# Build random forest classifier (same config)\nrf_clf = RandomForestClassifier(n_estimators=500,\n                                max_features=0.25,\n                                criterion=\"entropy\",\n                                class_weight=\"balanced\")\n\n# Build model with no sampling\npip_orig = make_pipeline(Imputer(strategy=\"mean\"),\n                         RobustScaler(),\n                         rf_clf)\nscores = cross_val_score(pip_orig,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mOriginal model's average AUC: {scores.mean():.3f}\")\n\n# Build model with undersampling\npip_undersample = imb_make_pipeline(Imputer(strategy=\"mean\"),\n                                    RobustScaler(),\n                                    RandomUnderSampler(), rf_clf)\nscores = cross_val_score(pip_undersample,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mUnder-sampled model's average AUC: {scores.mean():.3f}\")\n\n# Build model with oversampling\npip_oversample = imb_make_pipeline(Imputer(strategy=\"mean\"),\n                                    RobustScaler(),\n                                    RandomOverSampler(), rf_clf)\nscores = cross_val_score(pip_oversample,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mOver-sampled model's average AUC: {scores.mean():.3f}\")\n\n# Build model with EasyEnsemble\nresampled_rf = BalancedBaggingClassifier(base_estimator=rf_clf,\n                                         n_estimators=10, random_state=123)\npip_resampled = make_pipeline(Imputer(strategy=\"mean\"),\n                              RobustScaler(), resampled_rf)\n                             \nscores = cross_val_score(pip_resampled,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mEasyEnsemble model's average AUC: {scores.mean():.3f}\")\n\n# Build model with SMOTE\npip_smote = imb_make_pipeline(Imputer(strategy=\"mean\"),\n                              RobustScaler(),\n                              SMOTE(), rf_clf)\nscores = cross_val_score(pip_smote,\n                         X_train, y_train,\n                         scoring=\"roc_auc\", cv=10)\nprint(f\"\\033[1m\\033[94mSMOTE model's average AUC: {scores.mean():.3f}\")\n\n\nOriginal model's average AUC: 0.652\nUnder-sampled model's average AUC: 0.656\nOver-sampled model's average AUC: 0.651\nEasyEnsemble model's average AUC: 0.665\nSMOTE model's average AUC: 0.641\n\n\nEasyEnsemble method has the highest 10-folds CV with average AUC = 0.665.\n\n\nBuild Ensemble methods\nWe’ll build ensemble models using three different models as base learners: - Extra Gradient Boosting - Support Vector Classifier - Random Forest\nThe ensemble models will be built using two different methods: - Blending (average) ensemble model. Fits the base learners to the training data and then, at test time, average the predictions generated by all the base learners. - Use VotingClassifier from sklearn that: - fit all the base learners on the training data - at test time, use all base learners to predict test data and then take the average of all predictions. - Stacked ensemble model: Fits the base learners to the training data. Next, use those trained base learners to generate predictions (meta-features) used by the meta-learner (assuming we have only one layer of base learners). There are few different ways of training stacked ensemble model: - Fitting the base learners to all training data and then generate predictions using the same training data it was used to fit those learners. This method is more prune to overfitting because the meta learner will give more weights to the base learner who memorized the training data better, i.e. meta-learner won’t generate well and would overfit. - Split the training data into 2 to 3 different parts that will be used for training, validation, and generate predictions. It’s a suboptimal method because held out sets usually have higher variance and different splits give different results as well as learning algorithms would have fewer data to train. - Use k-folds cross validation where we split the data into k-folds. We fit the base learners to the (k - 1) folds and use the fitted models to generate predictions of the held out fold. We repeat the process until we generate the predictions for all the k-folds. When done, refit the base learners to the full training data. This method is more reliable and will give models that memorize the data less weight. Therefore, it generalizes better on future data.\nWe’ll use logistic regression as the meta-learner for the stacked model. Note that we can use k-folds cross validation to validate and tune the hyperparameters of the meta learner. We will not tune the hyperparameters of any of the base learners or the meta-learner; however, we will use some of the values recommended by the Pennsylvania Benchmarking Paper. Additionally, we won’t use EasyEnsemble in training because, after some experimentation, it didn’t improve the AUC of the ensemble model more than 2% on average and it was computationally very expensive. In practice, we sometimes are willing to give up small improvements if the model would become a lot more complex computationally. Therefore, we will use RandomUnderSampler. Also, we’ll impute the missing values and standardize the data beforehand so that it would shorten the code of the ensemble models and allows use to avoid using Pipeline. Additionally, we will plot ROC and PR curves using test data and evaluate the performance of all models.\n\n\nCode\n# Impute the missing data using features means\nimp = Imputer()\nimp.fit(X_train)\nX_train = imp.transform(X_train)\nX_test = imp.transform(X_test)\n\n# Standardize the data\nstd = RobustScaler()\nstd.fit(X_train)\nX_train = std.transform(X_train)\nX_test = std.transform(X_test)\n\n# Implement RandomUnderSampler\nrandom_undersampler = RandomUnderSampler()\nX_res, y_res = random_undersampler.fit_sample(X_train, y_train)\n# Shuffle the data\nperms = np.random.permutation(X_res.shape[0])\nX_res = X_res[perms]\ny_res = y_res[perms]\nX_res.shape, y_res.shape\n\n\n((2452, 18), (2452,))\n\n\n\n\nCode\n# Define base learners\nxgb_clf = xgb.XGBClassifier(objective=\"binary:logistic\",\n                            learning_rate=0.03,\n                            n_estimators=500,\n                            max_depth=1,\n                            subsample=0.4,\n                            random_state=123)\n\nsvm_clf = SVC(gamma=0.1,\n                C=0.01,\n                kernel=\"poly\",\n                degree=3,\n                coef0=10.0,\n                probability=True)\n\nrf_clf = RandomForestClassifier(n_estimators=300,\n                                max_features=\"sqrt\",\n                                criterion=\"gini\",\n                                min_samples_leaf=5,\n                                class_weight=\"balanced\")\n\n# Define meta-learner\nlogreg_clf = LogisticRegression(penalty=\"l2\",\n                                C=100,\n                                fit_intercept=True)\n\n# Fitting voting clf --&gt; average ensemble\nvoting_clf = VotingClassifier([(\"xgb\", xgb_clf),\n                               (\"svm\", svm_clf),\n                               (\"rf\", rf_clf)],\n                              voting=\"soft\",\n                              flatten_transform=True)\nvoting_clf.fit(X_res, y_res)\nxgb_model, svm_model, rf_model = voting_clf.estimators_\nmodels = {\"xgb\": xgb_model, \"svm\": svm_model,\n          \"rf\": rf_model, \"avg_ensemble\": voting_clf}\n\n# Build first stack of base learners\nfirst_stack = make_pipeline(voting_clf,\n                            FunctionTransformer(lambda X: X[:, 1::2]))\n# Use CV to generate meta-features\nmeta_features = cross_val_predict(first_stack,\n                                  X_res, y_res,\n                                  cv=10,\n                                  method=\"transform\")\n# Refit the first stack on the full training set\nfirst_stack.fit(X_res, y_res)\n# Fit the meta learner\nsecond_stack = logreg_clf.fit(meta_features, y_res)\n\n# Plot ROC and PR curves using all models and test data\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nfor name, model in models.items():\n            model_probs = model.predict_proba(X_test)[:, 1:]\n            model_auc_score = roc_auc_score(y_test, model_probs)\n            fpr, tpr, _ = roc_curve(y_test, model_probs)\n            precision, recall, _ = precision_recall_curve(y_test, model_probs)\n            axes[0].plot(fpr, tpr, label=f\"{name}, auc = {model_auc_score:.3f}\")\n            axes[1].plot(recall, precision, label=f\"{name}\")\nstacked_probs = second_stack.predict_proba(first_stack.transform(X_test))[:, 1:]\nstacked_auc_score = roc_auc_score(y_test, stacked_probs)\nfpr, tpr, _ = roc_curve(y_test, stacked_probs)\nprecision, recall, _ = precision_recall_curve(y_test, stacked_probs)\naxes[0].plot(fpr, tpr, label=f\"stacked_ensemble, auc = {stacked_auc_score:.3f}\")\naxes[1].plot(recall, precision, label=\"stacked_ensembe\")\naxes[0].legend(loc=\"lower right\")\naxes[0].set_xlabel(\"FPR\")\naxes[0].set_ylabel(\"TPR\")\naxes[0].set_title(\"ROC curve\")\naxes[1].legend()\naxes[1].set_xlabel(\"recall\")\naxes[1].set_ylabel(\"precision\")\naxes[1].set_title(\"PR curve\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nAs we can see from the chart above, stacked ensemble model didn’t improve the performance. One of the major reasons are that the base learners are considerably highly correlated especially Random Forest and Gradient Boosting (see the correlation matrix below).\n\n\nCode\n# Plot the correlation between base learners\nprobs_df = pd.DataFrame(meta_features, columns=[\"xgb\", \"svm\", \"rf\"])\ncorrmat(probs_df.corr(), inflate=True);\n\n\n\n\n\n\n\n\n\nIn addition, with classification problems where False Negatives are a lot more expensive than False Positives, we may want to have a model with a high precision rather than high recall, i.e. the probability of the model to identify positive examples from randomly selected examples. Below is the confusion matrix:\n\n\nCode\nsecond_stack_probs = second_stack.predict_proba(first_stack.transform(X_test))\nsecond_stack_preds = second_stack.predict(first_stack.transform(X_test))\nconf_mat = confusion_matrix(y_test, second_stack_preds)\n# Define figure size and figure ratios\nplt.figure(figsize=(16, 8))\nplt.matshow(conf_mat, cmap=plt.cm.Reds, alpha=0.2)\nfor i in range(2):\n    for j in range(2):\n        plt.text(x=j, y=i, s=conf_mat[i, j], ha=\"center\", va=\"center\")\nplt.title(\"Confusion matrix\", y=1.1, fontdict={\"fontsize\": 20})\nplt.xlabel(\"Predicted\", fontdict={\"fontsize\": 14})\nplt.ylabel(\"Actual\", fontdict={\"fontsize\": 14});\n\n\n&lt;matplotlib.figure.Figure at 0x12eb8e320&gt;\n\n\n\n\n\n\n\n\n\nLet’s finally check the partial dependence plots to see what are the most important features and their relationships with whether the borrower will most likely pay the loan in full before mature data. we will plot only the top 8 features to make it easier to read.\n\n\nCode\n# Plot partial dependence plots\ngbrt = GradientBoostingClassifier(loss=\"deviance\",\n                                  learning_rate=0.1,\n                                  n_estimators=100,\n                                  max_depth=3,\n                                  random_state=123)\ngbrt.fit(X_res, y_res)\nfig, axes = plot_partial_dependence(gbrt, X_res,\n                                    np.argsort(gbrt.feature_importances_)[::-1][:8],\n                                    n_cols=4,\n                                    feature_names=df.columns[:-6],\n                                    figsize=(14, 8))\nplt.subplots_adjust(top=0.9)\nplt.suptitle(\"Partial dependence plots of borrower not fully paid\\n\"\n             \"the loan based on top most influential features\")\nfor ax in axes: ax.set_xticks(())\nfor ax in [axes[0], axes[4]]: ax.set_ylabel(\"Partial dependence\")\n\n\n\n\n\n\n\n\n\nAs we might expected, borrowers with lower annual income and less FICO scores are less likely to pay the loan fully; however, borrowers with lower interest rates (riskier) and smaller installments are more likely to pay the loan fully."
  },
  {
    "objectID": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#conclusion",
    "href": "posts/predict-loan-repayment/Predicting-Loan-Repayment.html#conclusion",
    "title": "Predicting Loan Repayment",
    "section": "Conclusion",
    "text": "Conclusion\nMost classification problems in the real world are imbalanced. Also, almost always data sets have missing values. In this notebook, we covered strategies to deal with both missing values and imbalanced data sets. We also explored different ways of building ensembles in sklearn. Below are some takeaway points:\n\nThere is no definitive guide of which algorithms to use given any situation. What may work on some data sets may not necessarily work on others. Therefore, always evaluate methods using cross validation to get a reliable estimates.\nSometimes we may be willing to give up some improvement to the model if that would increase the complexity much more than the percentage change in the improvement to the evaluation metrics.\nIn some classification problems, False Negatives are a lot more expensive than False Positives. Therefore, we can reduce cut-off points to reduce the False Negatives.\nWhen building ensemble models, try to use good models that are as different as possible to reduce correlation between the base learners. We could’ve enhanced our stacked ensemble model by adding Dense Neural Network and some other kind of base learners as well as adding more layers to the stacked model.\nEasyEnsemble usually performs better than any other resampling methods.\nMissing values sometimes add more information to the model than we might expect. One way of capturing it is to add binary features for each feature that has missing values to check if each example is missing or not."
  },
  {
    "objectID": "projects-index.html",
    "href": "projects-index.html",
    "title": "Projects",
    "section": "",
    "text": "Transient Ischemic Attack\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\n\nBuild a classifier to predict whether a given patient will have TIA and deploy the best model as a RESTful API.\n\n\n\nImad Dabbura\n\n\nApr 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Modeling with Postgres\n\n\n\nData Engineering\n\n\n\nBuild a Relational Database using Star Schema of song’s users activities data.\n\n\n\nImad Dabbura\n\n\nApr 8, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "misc-notes.html",
    "href": "misc-notes.html",
    "title": "Miscellaneous Notes",
    "section": "",
    "text": "Collection of notes about personal growth, programming, and other stuff that I find useful.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 1, 2024\n\n\nNotes on Shipping Projects\n\n\nImad Dabbura\n\n\n\n\nJun 10, 2024\n\n\nFavorite Essays\n\n\nImad Dabbura\n\n\n\n\nDec 25, 2022\n\n\nNotes From Random Readings in Personal Growth\n\n\nImad Dabbura\n\n\n\n\nSep 15, 2022\n\n\nHow to be a Better Programmer\n\n\nImad Dabbura\n\n\n\n\nJan 26, 2022\n\n\nThe Science of Setting and Achieving Goals\n\n\nImad Dabbura\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Latest version of my resume is available upon request."
  },
  {
    "objectID": "posts/anomaly-detection/Anomaly-Detection.html#introduction",
    "href": "posts/anomaly-detection/Anomaly-Detection.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly Detection is the identification of examples or events that don’t confront to an expected pattern or the majority of examples. Roughly speaking, it’s the process of identifying an example that is not normal (outlier) given the distribution of the data. Outlier is an example that deviates so much from the other examples that arouse suspicions that it was generated by different data generating process. Mainly, such outliers would have a very low probability (on the very end of both left and right tails of the probability density function) that they belong to the same data generating process.\nThe algorithm works as follows: 1. Fit a Gaussian Probability Density Function (PDF) for each feature in the training dataset. 1. Calculate the mean and the variance of each feature: \\[\\mu_j = \\frac{1}{m}\\sum_{i = 1}^mx_j^i\\\\{}\\] \\[\\sigma^2_j = \\frac{1}{m}\\sum_{i = 1}^m(x_j^i - \\mu_j)^2\\\\{}\\] Where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance that controls the shape of the density function. 2. Compute the density function for each feature using the following formula:\n\\[p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\\\\{}\\] Since the mean and the variance are sensitive to outliers, we use training dataset that has only normal examples to fit the model and calculate both the mean vector and the covariance matrix. 2. Compute the gaussian density by taking the product of all features’ density functions. 3. If \\(p(x) &lt; \\epsilon\\) then anomaly; otherwise, normal. Epsilon controls how sensitive the detection algorithm is. If \\(\\epsilon\\) is large \\(\\rightarrow\\) flag a lot of the examples as anomalous and that would increase the False Positives. However, If \\(\\epsilon\\) is small \\(\\rightarrow\\) very small portion of the examples will be flagged as anomalous and that would increase the False Negatives. 4. Use Cross Validation for tuning the hyper-parameter \\(\\epsilon\\) that yields the best performance metrics value. F1 score is commonly used: \\[F_1 = 2 \\frac{precision * recall}{precision + recall}\\\\{}\\] Where:\\[precision = \\frac{tp}{tp + fp}\\\\{}\\] \\[recall = \\frac{tp}{tp + fn}\\\\{}\\] tp: True Positive, fp: False Positive, fn: False Negative.\nWe have two kinds of anomaly detection algorithms: 1. Univariate Gaussian Density Function \\[p(x) = \\prod_{j = 1}^{n}p(x_j; \\mu_j, \\sigma_j^2)\\\\{}\\] \\[ = p(x_1; \\mu_1, \\sigma_1^2)*p(x_2; \\mu_2, \\sigma_2^2)* ... * p(x_n; \\mu_n, \\sigma_j^n)\\\\{}\\] * It assumes that all features are independent. Therefore, the covariance between all pairs of features is zero. * It’s computationally faster and more efficient. * Use it if we have very large number of features. * Make sure to add features manually that captures unusual values for combination of features; such as \\(x_3 = \\frac {x_2}{x_1}\\). Otherwise, the algorithm may fail to detect anomalies that takes values that are considered normal when looked at each feature separately but are unusual when looking at values of all features together such as having high value for feature 2 compared to low value for feature 1.\n\nMultivariate Gaussian Density Function \\[p(x) = \\prod_{j = 1}^{n}p(x_j; \\mu_j, \\sigma_j^2)\\\\{}\\] \\[p(x; \\mu, \\sigma^2) = \\frac{1}{(2\\pi)^{(n / 2)}(\\det\\sum)^{1 / 2}}e^{\\frac{-1}{2}(x - \\mu)^T\\sum^{-1}(x - \\mu)}\\\\{}\\] Where \\(\\sum\\) is n x n covariance matrix: \\[\\sum = \\begin{bmatrix}\n\\sigma_1^2&\\sigma_{12}&\\cdots&\\sigma_{1n}\\\\\n\\sigma_{21}&\\sigma_2^2&\\cdots&0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{n1} & 0 & 0 & \\sigma_n^2\n\\end{bmatrix}\\] Where \\(\\sigma_{12} = \\sigma_{21}\\) is the covariance between features 1&2. Therefore, the covariance matrix is symmetric positive (semi) definite.\n\nComputationally expensive\nUse it when number of examples \\(\\geq\\) 10 times number of features, i.e. \\(m \\geq 10n\\)\nIf some features are linearly dependent or number of examples is less than number of features \\(\\rightarrow\\) covariance matrix won’t be invertible\nNo need to add more features to capture unusual values of combination of features because it captures that through covariances of all pairs of features\nUnivariate density function can be derived from Multivariate density function where covariance matrix would be a diagonal matrix. Therefore, \\(\\sigma_{ij} = 0\\) for all \\(i \\neq j\\)\n\n\nThere are some assumptions made implicitly here: - For each feature, \\(X_i\\)’s are IID (independently and identically distributed). - Using Central Theorem (CLT): the distribution of sum of iid random variable are approximately normal. Therefore, this would allow us to fit normal distribution that’s parameterized by \\(\\mu\\) and \\(\\sigma^2\\). - \\(\\mu\\) and \\(\\sum\\) will be estimated using maximum-likelihood estimation method.\nWhen fitting multivariate probability distribution using the above assumptions, we’ll use that pdf to estimate the probability that each example from the validation/test set was generated by this pdf. If the probability is smaller that \\(\\epsilon\\), then we believe that such example was generated by different mutlivariate PDF and, therefor, classified as anomaly (outlier).\nIn this exercise, we’ll implement an anomaly detection algorithm to detect anomalous behavior in server computers. The features measure the throughput (mb/s) and latency (ms) of response of each server. While servers were operating, \\(m = 307\\) examples of how they were behaving were captured. We suspect that the vast majority of them are normal (non-anomalous) examples of the servers operating normally.\nLet’s first load and plot the data:\n\n\nCode\nimport numpy as np\nfrom numpy.linalg import pinv, det\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.io import loadmat, whosmat\nimport scipy.optimize as opt\nimport seaborn as sns\nfrom warnings import filterwarnings\n\n%matplotlib inline\nsns.set_context('notebook')\nplt.style.use('fivethirtyeight')\nfilterwarnings('ignore')"
  },
  {
    "objectID": "posts/anomaly-detection/Anomaly-Detection.html#functions",
    "href": "posts/anomaly-detection/Anomaly-Detection.html#functions",
    "title": "Anomaly Detection",
    "section": "Functions",
    "text": "Functions\n\n\nCode\n# Compute guassian distribution fn\ndef gaussian_estimate(X_train, X_val, gaussian_type='univariate'):\n    '''\n    parameters\n    ----------\n    X_train: array-like\n        training features matrix m x n that has only normal examples.\n    X_val: array-like\n        cross validation features matrix that has anomalous and normal\n        examples.\n    gussian_type: str\n        univariate or multivariate.\n    \n    Returns\n    -------\n    pdf: array-like\n        multivariate pdf vector of n x 1\n    '''\n    # number of training examples and features\n    m, n = X_train.shape\n    # number of cv examples\n    mval = X_val.shape[0]\n\n    # compute mean and covariance matrix\n    mu = X_train.mean(axis=0)\n    cov = (1 / (m)) * (X_train - mu).T.dot(X_train - mu)\n\n    # convert the covariance matrix to diagonal if it's a univariate\n    if gaussian_type == 'univariate':\n        z = np.zeros_like(cov)\n        np.fill_diagonal(z, np.diagonal(cov))\n        cov = z\n\n    # compute determinant and inverse of covariance matrix\n    cov_det = det(cov)\n    cov_inv = pinv(cov)\n\n    # compute pdf vector\n    pdf = ((2 * np.pi) ** (-n / 2)) * (cov_det ** (-0.5)) *\\\n        np.exp(-0.5 * np.sum(np.multiply((X_val - mu).dot(cov_inv),\n                                         (X_val - mu)), axis=1))\n\n    return pdf\n\n\n# Hyperparameter tuning of epsilon using cv dataset\ndef select_threshold(y_val, p_val):\n    '''\n    parameters\n    ----------\n    y_val: array-like\n        label whether a validation example is normal (0) or anomaly (1).\n    p_val: array-like\n        pdf for validated examples.\n    \n    Returns\n    -------\n    eplsion : float\n        best epsilon value tuned on validation data.\n    F1_score : float\n        F1 score using epsilon tuned on validation data.\n    '''\n    # initialize epsilon and F1 score values\n    best_epsilon = 0\n    best_F1 = 0\n\n    # compute stepsize for each iteration\n    epsilon_stepsize = (p_val.max() - p_val.min()) / 1000\n\n    for epsilon in np.arange(p_val.min(), p_val.max(), epsilon_stepsize):\n        # get predictions vector\n        pred = ((p_val &lt; epsilon) * 1).reshape(-1, 1)\n\n        # compute true positives, false positives, false negatives\n        tp = np.sum((pred == 1) & (y_val == 1))\n        fp = np.sum((pred == 1) & (y_val == 0))\n        fn = np.sum((pred == 0) & (y_val == 1))\n\n        # compute precision and recall\n        precision_ = tp / (tp + fp)\n        recall_ = tp / (tp + fn)\n\n        # compute F1 score\n        F1 = 2 * ((precision_ * recall_) / (precision_ + recall_))\n        # if F1 score &gt; best_F1, set best_F1 = F1\n        if F1 &gt; best_F1:\n            best_F1 = F1\n            best_epsilon = epsilon\n\n    return best_epsilon, best_F1\n\n\n\n\nCode\n# Load data\ndata = loadmat('../data/servers_anomaly_detection.mat')\n\n# Training data\nX = data['X']\n\n# Cross validation data\nX_val = data['Xval']\ny_val = data['yval']\n\n# Plot data\nfig, ax = plt.subplots(figsize = (8, 8))\nplt.scatter(X[:, 0], X[:, 1], s = 50, c = 'blue')\nplt.axis([0, 30, 0, 30])\nplt.xlabel('Latency (ms)')\nplt.ylabel('Throughput (mb/s)')\nplt.gca().set_aspect('equal')\n# plt.title('Scatter plot of the first dataset');\n\n\n\n\n\n\n\n\n\n\n\nCode\n# plt.subplots(1, 2, 1)\nsns.kdeplot(X[:, 0])\nsns.kdeplot(X[:, 1])\n\n\n\n\n\n\n\n\n\nNow, we’ll first estimate the Gaussian distribution for both the training and cross validation sets. Note that we use training dataset that has ONLY normal examples when computing mean and covariance and then use cross validation that has both normal and anomalous examples to know the best epsilon.\n\n\nCode\n# Fit guassian distribution on both training and CV examples\nptrain = gaussian_estimate(X, X)\npval = gaussian_estimate(X, X_val, gaussian_type='multivariate')\n\n# Tune epsilon\nepsilon, F1 = select_threshold(y_val, pval)\nprint(f'The best epsilon tuned using CV that yielded the best' +\n      f'F1-score {F1:.3f} is: {epsilon}.')\n\n\nThe best epsilon tuned using CV that yielded the bestF1-score 0.875 is: 9.065769728392737e-05.\n\n\nWe’ll use the value of epsilon that we tuned using CV to see what examples were anomalous based on our algorithm. Below is the scatter plot of the training data where red points are anomalous examples.\n\n\nCode\n# Get the index of the outlier\noutliers = np.where(ptrain &lt; epsilon)\n\n# Plot data\nfig, ax = plt.subplots(figsize=(10, 6))\nplt.scatter(X[:, 0], X[:, 1], s=50, c='blue', label='Normal Examples')\nplt.scatter(X[outliers[0], 0], X[outliers[0], 1], s=60, c='red', label='Anomalous Examples')\nplt.axis([0, 30, 0, 30])\nplt.xlabel('Latency (ms)')\nplt.ylabel('Throughput (mb/s)')\nplt.legend(loc='upper right')\nplt.title('Scatter plot of the training dataset');\n\n\n\n\n\n\n\n\n\nFinally, we’ll try to fit Gaussian distribution on training dataset that has 1000 examples and 11 features. Note that in both examples we used Multivariate not Univariate Gaussian distribution.\n\n\nCode\n# Load data\ndata = loadmat('../data/ex8data2.mat')\n\n# Training data\nX = data['X']\n\n# Cross validation data\nXval = data['Xval']\nyval = data['yval']\n\n# Fit guassian distribution on both training and CV examples\nptrain = gaussian_estimate(X, X, gaussian_type='multivariate')\npval = gaussian_estimate(X, Xval, gaussian_type='multivariate')\n\n# Tune epsilon\nepsilon, F1 = select_threshold(yval, pval)\nprint(f'The best epsilon tuned using CV that yielded the best' + \\\n      'F1-score {F1:.3f} is: {epsilon}.')\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/ex8data2.mat'\n\n\nUsing the best-epsilon value we got above, we can then classify any example as anomaly if \\(p(x) &lt; \\epsilon\\); otherwise, it’s normal."
  },
  {
    "objectID": "posts/anomaly-detection/Anomaly-Detection.html#conclusion",
    "href": "posts/anomaly-detection/Anomaly-Detection.html#conclusion",
    "title": "Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe implementation of the variance/covariance in the detection algorithms has \\(m\\) in the denominator not \\((m - 1)\\) because with large datasets this doesn’t make a difference. However, the unbiased estimator of the variance should have \\((m - 1)\\) in the denominator not \\(m\\).\nAnomaly detection vs Supervised learning:\n\nUse Anomaly Detection when you have large number of negative examples and very small number of positive examples. The reason is because the supervised learning algorithm wouldn’t be able to have enough examples to learn about the scene especially if the future anomalies are nothing like training anomalies\nUse Supervised Learning algorithms such as logistic regression if you have enough positive examples that make the learning easy on the algorithm and probably it would outperform Anomaly Detection algorithms.\n\n\nUnivariate PDF performs well most of the times compared to Multivariate PDF and scale really well."
  },
  {
    "objectID": "posts/dl-systems/dl-systems.html",
    "href": "posts/dl-systems/dl-systems.html",
    "title": "Notes for Deep Learning Systems",
    "section": "",
    "text": "Below are some notes I wrote down while developing tiny_pytorch library.\n\nDL frameworks:\n\nCaffe (Only C++):\n\nDefine a computation in terms of layers\nEach layer implements forward and backward methods\nThe change would be inplace\nThe backward pass implementation is natural and intuitive from Hinton’s paper\n\nTensorflow:\n\nConstruct static graph before we can execute it\nMake it easy for optimizations such as fusing operations together, reusing memory, execute only what is needed at run-time\nHard to debug and experiment with the output of each step\nIt is not intuitive and has its own programming language\n\nPytorch:\n\nConstruct dynamic computation graph called define by run\nVery easy to debug and experiment with computation graph\nCan mix python control flow and computations\nTypically slower. New advancement allowed for JIT compilation to speed up the execution.\n\n\nInitialization:\n\nThe effect of weights initialization persists throughout training\n\nIt affects the relative norms of the activations and the gradients over time\n\nWeights don’t change much from their initial values after training\n\nIt may change is some direction/dimension more but overall the weights don’t change much relative to the initial values especially for deep NN\n\nProper weight initializations speed up training and converge to much lower error rates\nWe can judge the effect of initialization by computing the norm of both the weights and the gradients at every layer over all iterations\n\nLayer Normalization:\n\nIt fixes the problem of varying norms of layer activations as well as solve the problem where we have only one sample in the batch that batch normalization have\nThe main drawback is that is it is hard to drive the network to low loss especially for fully connected networks because after applying the layer norm the difference between activations for different examples will be lost because it forces the mean to be zero and the standard deviation to be 1, but relative norms are very important feature to discriminate between classes\n\nBatch Normalization:\n\nHelps trains much faster by normalizing across all examples and maintain different relative norms between activations within layers\n\nRegularization:\n\nThe premise is that the magnitude of the parameters is a good proxy for complexity. Smoother functions don’t change dramatically for small changes and thus they have smaller weights. As a result, we can limit the magnitude of the parameters to be small by adding a penalty to the loss function such as L2 regularization so the weights are smaller.\n\nHardware acceleration:\n\nWe need to pay attention to memory alignment because hardware can only load data into caches in one-go as a cache line of 64 bytes if they are aligned. Otherwise, we need more than one load.\nMost BLAS libraries are implemented using Fortran that uses column-major format to store ND-arrays.\nStride format is more general and we can use it to get any format including row-major or column-major format.\n\nExample: With 2d array A:\n\nrow-major: stride[0] = A.shape[1], stride[1] = 1\ncolumn-major: stride[0] = 1, stride[1] = A.shape[1]\n\nAdvantages:\n\nWe can have slices that use the same underlying memory by with different offsets to beging the slice as well as shape and stride. The same thing can be used with transpose by swapping the strides and change the shape. Finally, broadcast can be done by inserting a stride equal to 0.\n\nDisadvantages:\n\nMemory access becomes not contiguous which makes vectorization hard. Some linear algebra libraries require arrays to be compact.\n\n\nParallelization: We can use OpenMP to parallelize the operation by executing it using multiple threads on different cores in parallel.\n\nExample: #pragma use omp parallel for before the loop.\n\n\nHardware acceleration implementation:\n\nCPU/GPU or any other device let us create flat array which is a consecutive slots of memory together. Therefore, we need shape, stride, and offset to create ND arrays view of flat array\nAll operations leads to creating new output array to store the results regardless of the backend device\nChanging the device of an array involves copying the data from one device to the other. In needle, we are using numpy as a bridge. So we first create a numpy array of the input array and then create the final array on the intended device\nWith shape, stride, and offset: we can create different views of the same array that all share the same underlying memory. The output array is just a different NDArray data structure with the same underlying array.\n\nSlice\nTranspose\nBroadcast\n\nWhen we do slicing, the resulting view may not be contiguous (compact). This means we can’t pass the array handler because the operation will be applied to all the elements of the original array. Therefore, we need to first check if the input array is compact before doing the operation.\n\nThe array is compact if offset = 0 & strides correspond to the row-major order. Otherwise, we need to make it compact by creating new array with the sliced data (creating a copy with new memory slots).\nSome frameworks such as Numpy and Pytorch allows users to do operations on non-contiguous arrays and they handle that in their implementations w/o the need to create new contiguous array. However, in some cases they may still force us to have contiguous array such as in matmul operations to have faster access to the elements and speed up operations.\n\n\nTraining larger models: large datasets require large models which have large capacity. This will put pressure on compute devices and put them to the limit.\n\nShared memory on each core is typically 64KB.\nGlobal memory of GPU becomes the bottleneck for large models because it typically has 10GB for most devices and we can’t fit most larger models in 10GB.\nTechniques to save memory:\n\nInference: Use few buffers that will hold the activations of the layers. Every few layers, we reuse the same buffers we used in earlier computations. So instead of allocating N buffers, we can use 2 or 3 buffers that will be reused in the forward computations.\nTraining: Because each activation is needed for the backward pass to compute the gradients, we can’t just deallocate the buffers. As a workaround we can use checkpointing. This Technique involves checkpointing K layers/activations which is typically sqrt(N) and not store the whole N activations. During the backward pass, we recompute the values needed by running forward pass again for small segments at a time. This would make sublinear memory allocation and allows us to have bigger models at the expense of more computations due to segments forward pass done during gradient computation. We can pick activations that don’t require heavy computations such as ReLU as non-stored activations as opposed to something like convolution which is computation-heavy.\n\nParallel and Distributed Training: We can leverage multiple GPU devices that are possibly distributed across several worker nodes to train the model. Parallelism can either be by model partitioning or data partitioning:\n\nModel partitioning: break up the computation graph into multiple parts and pipeline the computations such that different worker nodes will be performing different parts of the computation at the same time and do the communication through send/recv at the boundaries of the graph.\nData partitioning: every worker runs the same replica of the model but using different micro batches and then the gradients can be summed from all workers since they are independent.\n\nWe can either use parameter server that receives all the gradients from all workers and then sum them up and perform the updates on the weights which then is sent to all workers so that they can be used in next micro batches. Each worker can start sending the gradients as soon as they are ready before waiting for all the gradients to be finished. This will increase the parallelism and allows workers to send the gradients while other gradient computation is being performed.\nOr we can use allreduce() which sums up all gradients from all workers and send them back so each worker then can perform the updates\n\n\n\nGANs: The main goal is to generate images by the generator model that looks real and hard to tell they are fake, which means making the distribution of the generated images as close as possible to the distribution of the real images. This is done by training a discriminator and a generator which are trained through iterative process:\n\nGenerator’s input is random vector and tries to generate image that looks real by maximizing the negative log-likelihood of the discriminator loss. This means make it harder for the discriminator to predict that it is fake and tries to bring the probability to 1.\nDiscriminator’s input is both fake images generated by the generator and real images where fake images would have label = 0 and real images would have label = 1 and tries to minimize the negative log-likelihood. So the role of the discriminator is to guide the generator to generate better images and make it focus on what it takes to generate fake images that can’t be differentiated from real images.\nIt is called adversarial because the generator is able to learn subtle corner cases that looks the same for the human eye but are actually different for each distribution\nDeconvolution or Conv2dTranspose is the opposite of convolution when we take a small vector and convert it to a much bigger space such as image\n\nSequence Modeling:\n\nRNNs: the whole past compacted in the last hidden state that makes learning harder due to the long history which may lead to exploding/vanishing gradients. Also, the far past such as x1 will have less weight if at all compared to xt even with LSTM.\nDirect prediction: We use all the past inputs to predict the next output; no hidden state. This is a drawback because we don’t have compact representation (latent state) of the past.\nTemporal CNN: It works well for tasks such as speech generation but suffers from the relatively small receptive field. This means hidden state at any time step t can only capture few of the past history and this is not good. We can increase filter size or use dilation but still is not optimal.\nWe can use attention that take all the history and weight them (attention matrix) so we are sure we’re incorporating all the past\n\nModel Deployment:\n\nConsiderations:\n\nApplication env restrictions: model-size, no-python\nLeverage existing HW accelerators such as mobile GPUs, NPUs, accelerated CPU instructions\nIntegrate with applications"
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#bias-variance-trade-off",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#bias-variance-trade-off",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\nGeneralization (test) error is the most important metric in Machine/Deep Learning. It gives us an estimate on the performance of the model on unseen data. Test error is decomposed into 3 parts (see above figure): Variance, Squared-Bias, and Irreducible Error. Models with high bias are not complex enough (too simple) for the data and tend to underfit. The simplest model is taking the average (mode) of target variable and assign it to all predictions. On the contrary, models with high variance overfit the training data by closely follow (mimick) the training data where the learning algorithm will follow the signal and the noise. Note that as the complexity (flexibility) of the model increases → the model will become less interpretable such as Neural Networks. Below is the bias-variance decomposition: \\[MSE = E(y - \\widehat{y})^2\\\\{}\\] \\[= E(y - f + f - \\widehat{y})^2\\\\{}\\] \\[= E\\big\\{(y - f)^2 + 2(y - f)(f - \\widehat{y}) + (f - \\widehat{y})^2\\big\\};\\quad substitute\\ y = f + \\epsilon\\\\{}\\] \\[= E\\big\\{(\\epsilon + f - f)^2 + 2(\\epsilon + f - f)(f - \\widehat{y}) + (f - \\widehat{y})^2\\big\\}\\\\{}\\] \\[= E(\\epsilon)^2 + E(\\epsilon)E(f - \\widehat{y}) + E(f - \\widehat{y})^2; \\quad where\\ E(\\epsilon) = 0\\\\{}\\] \\[= E(\\epsilon)^2 + E(f - \\widehat{y})^2;\\quad add\\ and\\ subtract\\ E(\\widehat{y})\\\\{}\\] \\[= E(\\epsilon)^2 + E(f - E(\\widehat{y}) + E(\\widehat{y}) - \\widehat{y})^2\\\\{}\\] \\[= E(\\epsilon)^2 + E(f - E(\\widehat{y}))^2 + E(\\widehat{y} - E(\\widehat{y}))^2\\\\{}\\] \\[\\Rightarrow MSE = var(\\widehat{y}) + (Bias(\\widehat{y}))^2 + var(\\epsilon)\\\\{}\\] Where:\n\n\\(var(\\epsilon)\\): Irreducible error that resulted from omitted features and unmeasured variation with each example.\n\\(Bias(\\widehat{y})\\): Error that is introduced by approximating a real-life problem with a simple model.\n\\(var(\\widehat{y})\\): amount by which \\(\\widehat{y}\\) would change if we estimated it using different data set.\n\nTherefore, we can control only the variance and the bias of the \\(\\widehat{y}\\) BUT NOT irreducible error. As a result, our job is to try to estimate the right level of complexity to achieve the lowest test error."
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#regularization",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#regularization",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "Regularization",
    "text": "Regularization\nRegularization adds stability to the learning algorithm by making it less sensitive to the training data and processes. Since we don’t know and have no access to the true function that we can use to compare our estimated function with it, the best strategy would be to build a very complex model that fits the training data really well (overfitting) and regularize it so that it would have a good generalization (test) error. When using regularization, we try to reduce the generalization error and that may lead to increase the training error in the process which is okay because what we care about is how well the model generalizes. With regularization, we try to bring back the very complex model that suffers from overfitting to a good model by increasing bias and reducing variance. This builds on the assumption that complex model has large parameters and simple model has small parameters.\nBelow are some methods used for regularization:\n\nL2 Parameter Regularization: It’s also known as weight decay. This method adds L2 norm penalty to the objective function to drive the weights towards the origin. Even though this method shrinks all weights by the same proportion towards zero; however, it will never make any weight to be exactly zero.\nL1 Parameter Regularization (Lasso): It can be seen as a feature selection method because; in contrast to L2 regularization, some weights will be actually zero. It shrinks all weights by the same amount by adding L1 norm penalty to the objective function.\nDropout: Dropout can be seen as an approximation to bagging techniques. On each iteration, we randomly shut down some neurons on each layer and don’t use those neurons in both forward propagation and back-propagation. This will force the neural network to spread out weights and not focus on specific neurons because it will never know which neurons will show up on each iteration. Therefore, it can be seen as training different model on each iteration. Also, since we drop some neurons on each iteration, this will lead to smaller network which in turns means simpler network.\nAugmentation: Add fake data by using the training examples and adding distortions to them such as rescaling and rotating the images in the case of image recognition. The idea here is that it’s always better to train the model on more data to achieve better performance. Note that augmented examples don’t add much information to the model as much as independent examples do but still it’s a valid alternative when collecting more data is not feasible.\nEarly Stopping: This method tries to optimize the cost function and regularize it so that it would have lower generalization error. The way it works is that on each iteration we record the validation error. If the validation error improves, we store a copy of the parameters and will continue until the optimization algorithm terminates. It’s a good method if computational time and resources is an issue for us.\n\nIn this post, we’ll cover L2 parameter regularization."
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#l2-parameter-regularization",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#l2-parameter-regularization",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "L2 Parameter Regularization",
    "text": "L2 Parameter Regularization\nWe normally don’t regularize bias and regularize weights only. We can use hessian matrix and it’s eigenvalues and eigenvectors to see the sensitivity of the weights to the weight decay. The weight \\(w_i\\) will be rescaled using \\(\\frac{\\lambda_i}{\\lambda_i + \\alpha}\\) where \\(\\lambda_i\\) (eigenvalue) measures the sensitivity of hessian matrix in that direction (eigenvector) and \\(\\alpha\\) is the regularized hyperparameter. Therefore,\n\nIf \\(\\lambda_i &gt;&gt; \\alpha\\), the cost function is very sensitive in that direction and the corresponding weight reduces the cost significantly \\(\\Rightarrow\\) don’t decay (shrink) much.\nIf \\(\\lambda_i &lt;&lt; \\alpha\\), the cost function is not sensitive in that direction and the corresponding weight doesn’t reduce the cost significantly \\(\\Rightarrow\\) decay (shrink) away towards zero.\n\nThe objective function (binary cross-entropy) would then change from: \\[J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}\\] To: \\[J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{binary cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_{l=1}^L\\sum\\limits_{i=1}^{n^l}\\sum\\limits_{j=1}^{n^{l-1}} W_{j,i}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}\\] Also, the new gradients and the update equation would be: \\[\\nabla_w J_{regularized} = \\nabla_w J + \\frac{\\lambda}{m}w\\\\{}\\] \\[w = w - \\alpha\\nabla_w J - \\alpha\\frac{\\lambda}{m}w\\\\{}\\] \\[\\Rightarrow w = w\\underbrace{(1 - \\alpha\\frac{\\lambda}{m})}_\\text{weight decay} - \\nabla J\\] Note that here \\(\\alpha\\) is the learning rate and \\(\\lambda\\) is the regularized hyperparameter. As \\(\\lambda\\) increases, the bias increases (and the model becomes less flexible) with the following extreme cases (see figure 1):\n\n\\(\\lambda = 0\\), no regularization.\n\\(\\lambda \\rightarrow \\infty\\), model becomes very simple where all weights are essentially zero. In the case of regression, we would end-up with the intercept only which is equal to the average of the target variable.\n\n\n\n\nFigure 1: Model complexity (underfitting/overfitting) as a function of regularization parameter\n\n\nIt sometimes maybe helpful to see how L2 parameter regularization works using normal equation. The normal quation is: \\[W = (X^TX + \\lambda I)^{-1}X^TY\\tag{3}\\] This means that:\n\nAdding \\(\\lambda\\) to the variance would decrease the weight since \\(w_i = \\frac{cov_{x, y}}{\\sigma^2_x}\\).\nEven if \\(X^TX\\) is not invertible, adding \\(\\lambda\\) to each feature will make it full rank matrix \\(\\Rightarrow\\) invertible.\n\nTo illustrate how regularization helps us reduce generalization error, we’ll use the cats_vs_dogs dataset. The dataset has images for cats and dogs. We’ll try to build a neural network to classify if the image has a cat or a dog. Each image is 64 x 64 pixels on RGB scale.\nWe’ll be using functions we wrote in “Coding Neural Network - Forward and Backward Propagation” post to initialize parameters, compute forward propagation, cross-entropy cost, gradients, etc.\nLet’s import the data and take a look at the shape as well as a sample of a cat image from the training set.\n\n\nCode\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    initialize_parameters,\n    L_model_forward,\n    compute_cost,\n    relu_gradient,\n    sigmoid_gradient,\n    tanh_gradient,\n    update_parameters,\n    accuracy,\n)\nfrom gradient_checking import dictionary_to_vector\nfrom load_dataset import load_dataset_catvsdog\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"figure.figsize\"] = (12, 6)\n\n\n\n\nCode\n# Import training data\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"])\nY_train = np.array(train_dataset[\"train_set_y\"])\n\n# Plot a sample image\nplt.imshow(X_train[50])\nplt.axis(\"off\")\n\n# Import test data\ntest_dataset = h5py.File(\"../../data/test_catvnoncat.h5\")\nX_test = np.array(test_dataset[\"test_set_x\"])\nY_test = np.array(test_dataset[\"test_set_y\"])\n\n# Transform data\nX_train = X_train.reshape(209, -1).T\nX_train = X_train / 255\nY_train = Y_train.reshape(-1, 209)\n\nX_test = X_test.reshape(50, -1).T\nX_test = X_test / 255\nY_test = Y_test.reshape(-1, 50)\n\n# print the new shape of both training and test datasets\nprint(\"Training data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_train.shape, Y_train.shape))\nprint(\"Test data dimensions:\")\nprint(\"X's dimension: {}, Y's dimension: {}\".format(X_test.shape, Y_test.shape))\n\n\nTraining data dimensions:\nX's dimension: (12288, 209), Y's dimension: (1, 209)\nTest data dimensions:\nX's dimension: (12288, 50), Y's dimension: (1, 50)\n\n\n\n\n\n\n\n\n\nThe training set has 209 examples and the test set has 50 examples. Let’s first write all the helper functions that would help us write the multi-layer neural network.\n\n\nCode\ndef compute_cost_reg(AL, y, parameters, lambd=0):\n    \"\"\"\n    Computes the Cross-Entropy cost function with L2 regularization.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    y : 2d-array\n        true \"label\" vector.\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    cost : float\n        binary cross-entropy cost.\n    \"\"\"\n    # number of examples\n    m = y.shape[1]\n\n    # compute traditional cross entropy cost\n    cross_entropy_cost = compute_cost(AL, y)\n\n    # convert parameters dictionary to vector\n    parameters_vector = dictionary_to_vector(parameters)\n\n    # compute the regularization penalty\n    L2_regularization_penalty = (lambd / (2 * m)) * np.sum(np.square(parameters_vector))\n\n    # compute the total cost\n    cost = cross_entropy_cost + L2_regularization_penalty\n\n    return cost\n\n\ndef linear_backword_reg(dZ, cache, lambd=0):\n    \"\"\"\n    Computes the gradient of the output w.r.t weight, bias, & post-activation\n    output of (l - 1) layers at layer l.\n\n    Arguments\n    ---------\n    dZ : 2d-array\n        gradient of the cost w.r.t. the linear output (of current layer l).\n    cache : tuple\n        values of (A_prev, W, b) coming from the forward propagation in the\n        current layer.\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of the previous layer l-1).\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l).\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l).\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = (1 / m) * np.dot(dZ, A_prev.T) + (lambd / m) * W\n    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n\n    assert dA_prev.shape == A_prev.shape\n    assert dW.shape == W.shape\n    assert db.shape == b.shape\n\n    return dA_prev, dW, db\n\n\ndef linear_activation_backward_reg(dA, cache, activation_fn=\"relu\", lambd=0):\n    \"\"\"\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient for current layer l.\n    cache : tuple\n        values of (linear_cache, activation_cache).\n    activation : str\n        activation used in this layer: \"sigmoid\", \"tanh\", or \"relu\".\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of previous layer l-1),\n        same shape as A_prev.\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l), same shape as W.\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l), same shape as b.\n    \"\"\"\n    linear_cache, activation_cache = cache\n\n    if activation_fn == \"sigmoid\":\n        dZ = sigmoid_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)\n\n    elif activation_fn == \"tanh\":\n        dZ = tanh_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)\n\n    elif activation_fn == \"relu\":\n        dZ = relu_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)\n\n    return dA_prev, dW, db\n\n\ndef L_model_backward_reg(AL, y, caches, hidden_layers_activation_fn=\"relu\", lambd=0):\n    \"\"\"\n    Computes the gradient of output layer w.r.t weights, biases, etc. starting\n    on the output layer in reverse topological order.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector, output of the forward propagation\n        (L_model_forward()).\n    y : 2d-array\n        true \"label\" vector (containing 0 if non-cat, 1 if cat).\n    caches : list\n        list of caches for all layers.\n    hidden_layers_activation_fn :\n        activation function used on hidden layers: \"tanh\", \"relu\".\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    grads : dict\n        gradients.\n    \"\"\"\n    y = y.reshape(AL.shape)\n    L = len(caches)\n    grads = {}\n\n    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n\n    (\n        grads[\"dA\" + str(L - 1)],\n        grads[\"dW\" + str(L)],\n        grads[\"db\" + str(L)],\n    ) = linear_activation_backward_reg(dAL, caches[L - 1], \"sigmoid\", lambd)\n\n    for l in range(L - 1, 0, -1):\n        current_cache = caches[l - 1]\n        (\n            grads[\"dA\" + str(l - 1)],\n            grads[\"dW\" + str(l)],\n            grads[\"db\" + str(l)],\n        ) = linear_activation_backward_reg(\n            grads[\"dA\" + str(l)], current_cache, hidden_layers_activation_fn, lambd\n        )\n\n    return grads\n\n\ndef model_with_regularization(\n    X,\n    y,\n    layers_dims,\n    learning_rate=0.01,\n    num_epochs=3000,\n    print_cost=False,\n    hidden_layers_activation_fn=\"relu\",\n    lambd=0,\n):\n    \"\"\"\n    Implements L-Layer neural network.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n     num_epochs : int\n        number of times to over the training data.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n    lambd : float\n        regularization hyperparameter.\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test\n        examples.\n    \"\"\"\n    # get number of examples\n    m = X.shape[1]\n\n    # to get consistents output\n    np.random.seed(1)\n\n    # initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # intialize cost list\n    cost_list = []\n\n    # implement gradient descent\n    for i in range(num_epochs):\n        # compute forward propagation\n        AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n        # compute regularized cost\n        reg_cost = compute_cost_reg(AL, y, parameters, lambd)\n\n        # compute gradients\n        grads = L_model_backward_reg(AL, y, caches, hidden_layers_activation_fn, lambd)\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # print cost\n        if (i + 1) % 100 == 0 and print_cost:\n            print(\"The cost after {} iterations: {}\".format((i + 1), reg_cost))\n\n        # append cost\n        if i % 100 == 0:\n            cost_list.append(reg_cost)\n\n    # plot the cost curve\n    plt.plot(cost_list)\n    plt.xlabel(\"Iterations (per hundreds)\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Cost curve for the learning rate = {}\".format(learning_rate))\n\n    return parameters\n\n\nNow we’re ready to train the neural network. We’ll first build a neural network with no regularization and then one with regularization to see which one has lower generalization error. Note that \\(\\lambda\\) should be tuned to get the best results but we’ll here choose an arbitrary value to illustrate the concept. Both neural netwotks would have 2 hidden layers where each hidden layer has 5 units.\n\n\nCode\n# set up layers dimensions\nlayers_dims = [X_train.shape[0], 5, 5, 1]\n\n# train NN\nparameters = model_with_regularization(\n    X_train,\n    Y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_epochs=2500,\n    print_cost=True,\n    hidden_layers_activation_fn=\"tanh\",\n    lambd=0,\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"tanh\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"tanh\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations: 0.6555634398145331\nThe cost after 200 iterations: 0.6467746423961933\nThe cost after 300 iterations: 0.6446638811282552\nThe cost after 400 iterations: 0.6441400737542232\nThe cost after 500 iterations: 0.6440063101787575\nThe cost after 600 iterations: 0.6439697872317176\nThe cost after 700 iterations: 0.6439570623358253\nThe cost after 800 iterations: 0.6439491872993496\nThe cost after 900 iterations: 0.6439407592837082\nThe cost after 1000 iterations: 0.6439294591543208\nThe cost after 1100 iterations: 0.6439131091764411\nThe cost after 1200 iterations: 0.6438883396380859\nThe cost after 1300 iterations: 0.6438489715870495\nThe cost after 1400 iterations: 0.6437825798034876\nThe cost after 1500 iterations: 0.6436617691190204\nThe cost after 1600 iterations: 0.6434191397054715\nThe cost after 1700 iterations: 0.642864008138056\nThe cost after 1800 iterations: 0.6413476000796884\nThe cost after 1900 iterations: 0.6360827945885947\nThe cost after 2000 iterations: 0.6124050450908987\nThe cost after 2100 iterations: 0.511236042160854\nThe cost after 2200 iterations: 0.5001328693867211\nThe cost after 2300 iterations: 0.36442904496020256\nThe cost after 2400 iterations: 0.3392740578590327\nThe cost after 2500 iterations: 0.4183570598370425\nThe training accuracy rate: 83.73%.\nThe test accuracy rate: 72.00%.\n\n\n\n\n\n\n\n\n\nThe training accuracy is 82.30% but the test accuracy is 78%. The difference between training and test accuracy is not that much, i.e. we don’t have a lot of overfitting. Therefore, a little bit of regularization may help such as \\(\\lambda = 0.02\\). Values of \\(\\lambda\\)s that practitioners recommend are: 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24.\n\n\nCode\n# train NN with regularization\nparameters = model_with_regularization(\n    X_train,\n    Y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_epochs=2500,\n    print_cost=True,\n    hidden_layers_activation_fn=\"tanh\",\n    lambd=0.02,\n)\n\n# print the test accuracy\nprint(\n    \"The training accuracy rate: {}\".format(\n        accuracy(X_train, parameters, Y_train, \"tanh\")[-7:]\n    )\n)\nprint(\n    \"The test accuracy rate: {}\".format(\n        accuracy(X_test, parameters, Y_test, \"tanh\")[-7:]\n    )\n)\n\n\nThe cost after 100 iterations: 0.6558634554205135\nThe cost after 200 iterations: 0.6470807090618383\nThe cost after 300 iterations: 0.6449737235917311\nThe cost after 400 iterations: 0.6444519406797673\nThe cost after 500 iterations: 0.6443191828114609\nThe cost after 600 iterations: 0.6442831256251426\nThe cost after 700 iterations: 0.6442705985766486\nThe cost after 800 iterations: 0.6442628048800636\nThe cost after 900 iterations: 0.6442544325786784\nThe cost after 1000 iterations: 0.6442432311807257\nThe cost after 1100 iterations: 0.6442270988055475\nThe cost after 1200 iterations: 0.6442027847231018\nThe cost after 1300 iterations: 0.6441643410411311\nThe cost after 1400 iterations: 0.6440998547029029\nThe cost after 1500 iterations: 0.6439832000181198\nThe cost after 1600 iterations: 0.6437505375793907\nThe cost after 1700 iterations: 0.6432228625403317\nThe cost after 1800 iterations: 0.6417982979158361\nThe cost after 1900 iterations: 0.6369273437378263\nThe cost after 2000 iterations: 0.6152774362019153\nThe cost after 2100 iterations: 0.5207828651496841\nThe cost after 2200 iterations: 0.5209315970380933\nThe cost after 2300 iterations: 0.5769347472395975\nThe cost after 2400 iterations: 0.39380136480047884\nThe cost after 2500 iterations: 0.33629411438613926\nThe training accuracy rate: 88.52%.\nThe test accuracy rate: 58.00%.\n\n\n\n\n\n\n\n\n\nAs the results above show, we improved the generalization error by increasing the test accuracy from 78% to 80%. On the other hand, training accuracy decreased from 82.30% to 65.55%."
  },
  {
    "objectID": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#conclusion",
    "href": "posts/coding-nn/regularization/Coding-Neural-Network-Regularization.html#conclusion",
    "title": "Coding Neural Network Part 4 - Regularization",
    "section": "Conclusion",
    "text": "Conclusion\nRegularization is an effective technique to resolve overfitting. Since we don’t know true distribution of the data, empirical risk, which is based of empirical distribution, is prone to overfitting. Therefore, the best strategy is to fit training data really well and then use a regularization technique so that the model generalizes well. L2 parameter regularization along with Dropout are two of the most widely used regularization technique in machine learning.\n\nOne of the implicit assumptions of regularization techniques such as L2 and L1 parameter regularization is that the value of the parameters should be zero and try to shrink all parameters towards zero. It’s meant to avoid following the training data very well which makes the learning algorithm picks some noise that is not helpful when applied on unseen data.\nThe value of \\(\\lambda\\) should be tuned to get the best generalization error. We typically use validation set when comparing models with values for \\(\\lambda\\)s and pick the one with the lowest validation error.\nOnly use regularization if the model suffers from overfitting, i.e training error &lt;&lt; validation error.\nIf after using regularization the validation error is still high, then we’re most likely in the underfitting region. In other words, our model is still too simple and already has high bias. Therefore, add complexity to the model and then use regularization.\nSince the majority of tasks we try to solve don’t have enough data (or expensive to collect more data), overfitting will be more prevalent in Deep Learning than underfitting given the complexity of neural networks."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#introduction",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#introduction",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Introduction",
    "text": "Introduction\nOptimization, in Machine Learning/Deep Learning contexts, is the process of changing the model’s weights to improve its performance. In other words, it’s the process of finding the best weights in the predefined hypothesis space to get the best possible performance. There are three kinds of optimization algorithms:\n\nOptimization algorithm that is not iterative and simply solves for one point.\nOptimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.\nOptimization algorithm that is iterative in nature and applied to a set of problems that have non-convex loss functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates.\n\nIn this post, we’ll look at three different cases of parameters’ initialization and see how this affects the error rate:\n\nInitialize all weights to zero.\nInitialize weights to random values from standard normal distribution or uniform distribution and multiply it by a scalar such as 10.\nInitialize weights based on:\n\nXavier recommendation.\nKaiming He recommendation.\n\n\nWe’ll be using functions we wrote in “Coding Neural Network - Forward Propagation and Backpropagation” post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\nTo illustrate the above cases, we’ll use the cats vs dogs dataset which consists of 50 images for cats and 50 images for dogs. Each image is 150 pixels x 150 pixels on RGB color scale. Therefore, we would have 67,500 features where each column in the input matrix would be one image which means our input data would have 67,500 x 100 dimension.\nLet’s first load the data and show a sample of two images before we start the helper functions.\n\n\nCode\nimport sys\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsys.path.append(\"../../scripts/\")\nfrom coding_neural_network_from_scratch import (\n    L_model_forward,\n    compute_cost,\n    L_model_backward,\n    update_parameters,\n    accuracy,\n)\nfrom load_dataset import load_dataset_catvsdog\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")\n\n\n\n\nCode\nX, Y = load_dataset_catvsdog(\"../../data\")\n\n# show a sample of of a cat and a dog image\nindex_cat = np.argmax(Y)\nindex_dog = np.argmin(Y)\nplt.subplot(1, 2, 1)\nplt.imshow(X[:, index_cat].reshape(150, 150, 3).astype(int))\nplt.axis(\"off\")\nplt.subplot(1, 2, 2)\nplt.imshow(X[:, index_dog].reshape(150, 150, 3).astype(int))\nplt.axis(\"off\")\n\n# standarize the data\nX = X / 255\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef model(\n    X,\n    Y,\n    layers_dims,\n    learning_rate=0.01,\n    num_iterations=1000,\n    print_cost=True,\n    hidden_layers_activation_fn=\"relu\",\n    initialization_method=\"he\",\n):\n    \"\"\"\n    Implements multilayer neural network using gradient descent as the\n    learning algorithm.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n    num_iterations : int\n        number of iterations of the optimization loop.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n    initialization_method : str\n        specify the initialization method to be used: \"he\", \"xavier\".\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test\n        examples.\n    \"\"\"\n    np.random.seed(1)\n\n    # initialize cost list\n    cost_list = []\n\n    # initialize parameters\n    if initialization_method == \"zeros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization_method == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    else:\n        parameters = initialize_parameters_he_xavier(layers_dims, initialization_method)\n\n    # iterate over num_iterations\n    for i in range(num_iterations):\n        # iterate over L-layers to get the final output and the cache\n        AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n        # compute cost to plot it\n        cost = compute_cost(AL, Y)\n\n        # iterate over L-layers backward to get gradients\n        grads = L_model_backward(AL, Y, caches, hidden_layers_activation_fn)\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # append each 100th cost to the cost list\n        if (i + 1) % 100 == 0 and print_cost:\n            print(\"The cost after {} iterations is: {}\".format(i + 1, cost))\n\n        if i % 100 == 0:\n            cost_list.append(cost)\n\n    # plot the cost curve\n    plt.figure(figsize=(12, 8))\n    plt.plot(cost_list)\n    plt.xlabel(\"Iterations (per hundreds)\", fontsize=14)\n    plt.ylabel(\"Cost\", fontsize=14)\n    plt.title(\n        \"Cost curve: learning rate = {} and {} initialization method\".format(\n            learning_rate, initialization_method\n        ),\n        y=1.05,\n        fontsize=16,\n    )\n\n    return parameters"
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-all-weights-to-zero",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-all-weights-to-zero",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Initializing all weights to zero",
    "text": "Initializing all weights to zero\nHere, we’ll initialize all weight matrices and biases to zeros and see how this would affect the error rate as well as the learning parameters.\n\n\nCode\ndef initialize_parameters_zeros(layers_dims):\n    \"\"\"\n    Initializes the parameters dictionary to all zeros for both weights and\n    bias.\n\n    Arguments\n    ---------\n    layer_dims : list\n        input size and size of each layer, length: number of layers + 1.\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    for l in range(1, L):\n        parameters[\"W\" + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n    return parameters\n\n\n\n\nCode\n# train NN with zeros initialization weights\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X, Y, layers_dims, hidden_layers_activation_fn=\"tanh\", initialization_method=\"zeros\"\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 0.6931471805599453\nThe cost after 200 iterations is: 0.6931471805599453\nThe cost after 300 iterations is: 0.6931471805599453\nThe cost after 400 iterations is: 0.6931471805599453\nThe cost after 500 iterations is: 0.6931471805599453\nThe cost after 600 iterations is: 0.6931471805599453\nThe cost after 700 iterations is: 0.6931471805599453\nThe cost after 800 iterations is: 0.6931471805599453\nThe cost after 900 iterations is: 0.6931471805599453\nThe cost after 1000 iterations is: 0.6931471805599453\n\n\n'The accuracy rate is: 50.00%.'\n\n\n\n\n\n\n\n\n\nAs the cost curve shows, the neural network didn’t learn anything! That is because of symmetry between all neurons which leads to all neurons have the same update on every iteration. Therefore, regardless of how many iterations we run the optimization algorithms, all the neurons would still get the same update and no learning would happen. As a result, we must break symmetry when initializing weights so that the model would start learning on each update of the gradient descent."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-with-big-random-values",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-with-big-random-values",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Initializing weights with big random values",
    "text": "Initializing weights with big random values\nThere is no big difference if the random values are initialized from standard normal distribution or uniform distribution so we’ll use standard normal distribution in our examples. Also, we’ll multiply the random values by a big number such as 10 to show that initializing weights to big values may cause our optimization to have higher error rates (and even diverge in some cases). Let’s now train our neural network where all weight matrices have been intitialized using the following formula: np.random.randn() * 10\n\n\nCode\ndef initialize_parameters_random(layers_dims):\n    \"\"\"\n    Initializes the parameters dictionary rabdomly from standard normal\n    distribution multiplied by 10 for weight matrices and zeros for bias\n    vectors.\n\n    Arguments\n    ---------\n    layer_dims : list\n        input size and size of each layer, length: number of layers + 1.\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    for l in range(1, L):\n        parameters[\"W\" + str(l)] = (\n            np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n        )\n        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n    return parameters\n\n\n\n\nCode\n# train NN with random initialization weights\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X,\n    Y,\n    layers_dims,\n    hidden_layers_activation_fn=\"tanh\",\n    initialization_method=\"random\",\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 1.2413142077549013\nThe cost after 200 iterations is: 1.1258751902393416\nThe cost after 300 iterations is: 1.0989052435267657\nThe cost after 400 iterations is: 1.084096647128233\nThe cost after 500 iterations is: 1.070695329210598\nThe cost after 600 iterations is: 1.0574847320236294\nThe cost after 700 iterations is: 1.0443168708889223\nThe cost after 800 iterations is: 1.031157857251139\nThe cost after 900 iterations is: 1.0179838815204905\nThe cost after 1000 iterations is: 1.0047670885153432\n\n\n'The accuracy rate is: 55.00%.'\n\n\n\n\n\n\n\n\n\nRandom initialization here is helping but still the loss function has high value and may take long time to converge and achieve a significantly low value."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-based-on-he-and-xavier-recommendations",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#initializing-weights-based-on-he-and-xavier-recommendations",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Initializing weights based on He and Xavier recommendations",
    "text": "Initializing weights based on He and Xavier recommendations\nWe’ll explore two initialization methods:\n\nKaiming He method is best applied when activation function applied on hidden layers is Rectified Linear Unit (ReLU). so that the weight on each hidden layer would have the following variance: \\[var(W^l) = \\frac{2}{n^{l - 1}}\\] We can achieve this by multiplying the random values from standard normal distribution by \\(\\sqrt{\\frac{2}{number\\ of\\ units\\ in \\ previous\\ layer}}\\)\nXavier method is best applied when activation function applied on hidden layers is Hyperbolic Tangent so that the weight on each hidden layer would have the following variance: \\[var(W^l) = \\frac{1}{n^{l - 1}}\\] We can achieve this by multiplying the random values from standard normal distribution by \\(\\sqrt{\\frac{1}{number\\ of\\ units\\ in \\ previous\\ layer}}\\)\n\nWe’ll train the network using both methods and look at the results.\n\n\nCode\ndef initialize_parameters_he_xavier(layers_dims, initialization_method=\"he\"):\n    \"\"\"\n    Initializes the parameters dictionary for weights based on \"He\" and\n    \"Xavier\" methods and zeros for bias vectors.\n\n    Arguments\n    ---------\n    layer_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    initialization_method : str\n        specify the initialization method to be used: \"he\", \"xavier\".\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    if initialization_method == \"he\":\n        for l in range(1, L):\n            parameters[\"W\" + str(l)] = np.random.randn(\n                layers_dims[l], layers_dims[l - 1]\n            ) * np.sqrt(2 / layers_dims[l - 1])\n            parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n    elif initialization_method == \"xavier\":\n        for l in range(1, L):\n            parameters[\"W\" + str(l)] = np.random.randn(\n                layers_dims[l], layers_dims[l - 1]\n            ) * np.sqrt(1 / layers_dims[l - 1])\n            parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n    return parameters\n\n\n\n\nCode\n# train NN where all weights were initialized based on He recommendation\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X, Y, layers_dims, hidden_layers_activation_fn=\"tanh\", initialization_method=\"he\"\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 0.6203611406013776\nThe cost after 200 iterations is: 0.5123234659821594\nThe cost after 300 iterations is: 0.46677237621933015\nThe cost after 400 iterations is: 0.3187255789472014\nThe cost after 500 iterations is: 0.42826666616322456\nThe cost after 600 iterations is: 0.36422647591323054\nThe cost after 700 iterations is: 0.5706298604437564\nThe cost after 800 iterations is: 0.7309174559673725\nThe cost after 900 iterations is: 0.26898635456817815\nThe cost after 1000 iterations is: 0.46540333116680943\n\n\n'The accuracy rate is: 98.00%.'\n\n\n\n\n\n\n\n\n\n\n\nCode\n# train NN where all weights were initialized based on Xavier recommendation\nlayers_dims = [X.shape[0], 5, 5, 1]\nparameters = model(\n    X,\n    Y,\n    layers_dims,\n    hidden_layers_activation_fn=\"tanh\",\n    initialization_method=\"xavier\",\n)\n\naccuracy(X, parameters, Y, \"tanh\")\n\n\nThe cost after 100 iterations is: 0.6336209080471575\nThe cost after 200 iterations is: 0.6427985039433041\nThe cost after 300 iterations is: 0.36738403251144874\nThe cost after 400 iterations is: 0.47375556172838623\nThe cost after 500 iterations is: 0.2851099368160619\nThe cost after 600 iterations is: 0.3806391238429354\nThe cost after 700 iterations is: 0.29261677834759703\nThe cost after 800 iterations is: 0.23118565026967375\nThe cost after 900 iterations is: 0.6721421723051113\nThe cost after 1000 iterations is: 0.11528517185494602\n\n\n'The accuracy rate is: 100.00%.'\n\n\n\n\n\n\n\n\n\nAs shown from applying the four methods, parameters’ initial values play a huge role in achieving low cost values as well as converging and achieve lower training error rates. The same would apply to test error rate if we had test data."
  },
  {
    "objectID": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#conclusion",
    "href": "posts/coding-nn/param-initialization/Coding-Neural-Network-Parameters-Initialization.html#conclusion",
    "title": "Coding Neural Network Part 3 - Parameters’ Initialization",
    "section": "Conclusion",
    "text": "Conclusion\nDeep Learning frameworks make it easier to choose between different initialization methods without worrying about implementing it ourselves. Nonetheless, it’s important to understand the critical role initial values of the parameters in the overall performance of the network. Below are some key takeaways:\n\nWell chosen initialization values of weights leads to:\n\nSpeed up convergence of gradient descent.\nIncrease the likelihood of gradient descent to find lower training and generalization error rates.\n\nBecause we’re dealing with iterative optimization algorithms with non-convex loss function, different initializations lead to different results.\nRandom initialization is used to break symmetry and make sure different hidden units can learn different things.\nDon’t initialize to values that are too large.\nKaiming He (He) initialization works well for neural networks with ReLU activation function.\nXavier initialization works well for neural networks with Hyperbolic Tangent activation function."
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#why-neural-networks",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#why-neural-networks",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "Why Neural Networks?",
    "text": "Why Neural Networks?\nAccording to Universal Approximate Theorem, Neural Networks can approximate as well as learn and represent any function given a large enough layer and desired error margin. The way neural network learns the true function is by building complex representations on top of simple ones. On each hidden layer, the neural network learns new feature space by first compute the affine (linear) transformations of the given inputs and then apply non-linear function which in turn will be the input of the next layer. This process will continue until we reach the output layer. Therefore, we can define neural network as information flows from inputs through hidden layers towards the output. For a 3-layers neural network, the learned function would be: \\(f(x) = f_3(f_2(f_1(x)))\\) where:\n\n\\(f_1(x)\\): Function learned on first hidden layer\n\\(f_2(x)\\): Function learned on second hidden layer\n\\(f_3(x)\\): Function learned on output layer\n\nTherefore, on each layer we learn different representation that gets more complicated with later hidden layers.Below is an example of a 3-layers neural network (we don’t count input layer):\n\n\n\nFigure 1: Neural Network with two hidden layers\n\n\nFor example, computers can’t understand images directly and don’t know what to do with pixels data. However, a neural network can build a simple representation of the image in the early hidden layers that identifies edges. Given the first hidden layer output, it can learn corners and contours. Given the second hidden layer, it can learn parts such as nose. Finally, it can learn the object identity.\nSince truth is never linear and representation is very critical to the performance of a machine learning algorithm, neural network can help us build very complex models and leave it to the algorithm to learn such representations without worrying about feature engineering that takes practitioners very long time and effort to curate a good representation.\nThe post has two parts:\n\nCoding the neural network: This entails writing all the helper functions that would allow us to implement a multi-layer neural network. While doing so, I’ll explain the theoretical parts whenever possible and give some advices on implementations.\nApplication: We’ll use the neural network we coded in the first part on image recognition problem to see if the network we built will be able to detect if the image has a cat or a dog and see it working :)\n\n\n\nCode\nimport os as os\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_context(\"notebook\")\nplt.style.use(\"fivethirtyeight\")"
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#i.-coding-the-neural-network",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#i.-coding-the-neural-network",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "I. Coding The Neural Network",
    "text": "I. Coding The Neural Network\n\nForward Propagation\nThe input \\(X\\) provides the initial information that then propagates to the hidden units at each layer and finally produce the output \\(\\widehat{Y}\\). The architecture of the network entails determining its depth, width, and activation functions used on each layer. Depth is the number of hidden layers. Width is the number of units (nodes) on each hidden layer since we don’t control neither input layer nor output layer dimensions. There are quite a few set of activation functions such Rectified Linear Unit, Sigmoid, Hyperbolic tangent, etc. Research has proven that deeper networks outperform networks with more hidden units. Therefore, it’s always better and won’t hurt to train a deeper network (with diminishing returns).\nLets first introduce some notations that will be used throughout the post:\n\n\\(W^l\\): Weights matrix for the \\(l^{th}\\) layer\n\\(b^l\\): Bias vector for the \\(l^{th}\\) layer\n\\(Z^l\\): Linear (affine) transformations of given inputs for the \\(l^{th}\\) layer\n\\(g^l\\): Activation function applied on the \\(l^{th}\\) layer\n\\(A^l\\): Post-activation output for the \\(l^{th}\\) layer\n\\(dW^l\\): Derivative of the cost function w.r.t \\(W^l\\) (\\(\\frac{\\partial J}{\\partial W^l}\\))\n\\(db^l\\): Derivative of the cost function w.r.t \\(b^l\\) (\\(\\frac{\\partial J}{\\partial b^l})\\))\n\\(dZ^l\\): Derivative of the cost function w.r.t \\(Z^l\\) (\\(\\frac{\\partial J}{\\partial Z^l}\\))\n\\(dA^l\\): Derivative of the cost function w.r.t \\(A^l\\) (\\(\\frac{\\partial J}{\\partial A^l}\\))\n\\(n^l\\): Number of units (nodes) of the \\(l^{th}\\) layer\n\\(m\\): Number of examples\n\\(L\\): Number of layers in the network (not including the input layer)\n\nNext, we’ll write down the dimensions of a multi-layer neural network in the general form to help us in matrix multiplication because one of the major challenges in implementing a neural network is getting the dimensions right.\n\n\\(W^l,\\ dW^l\\): Number of units (nodes) in \\(l^{th}\\) layer x Number of units (nodes) in \\(l - 1\\) layer\n\\(b^l,\\ db^l\\): Number of units (nodes) in \\(l^{th}\\) layer x 1\n\\(Z^l,\\ dZ^l\\): Number of units (nodes) in \\(l^{th}\\) layer x number of examples\n\\(A^l,\\ dA^l\\): Number of units (nodes) in \\(l^{th}\\) layer x number of examples\n\nThe two equations we need to implement forward propagations are: \\[Z^l = W^lA^{l - 1} + b ^l\\tag1\\\\{}\\] \\[A^l = g^l(Z^l) = g^l(W^lA^{l - 1} + b ^l)\\tag2\\] These computations will take place on each layer.\n\n\nParameters Initialization\nWe’ll first initialize the weight matrices and the bias vectors. It’s important to note that we shouldn’t initialize all the parameters to zero because doing so will lead the gradients to be equal and on each iteration the output would be the same and the learning algorithm won’t learn anything. Therefore, it’s important to randomly initialize the parameters to values between 0 and 1. It’s also recommended to multiply the random values by small scalar such as 0.01 to make the activation units active and be on the regions where activation functions’ derivatives are not close to zero.\n\n\nCode\n# Initialize parameters\ndef initialize_parameters(layers_dims):\n    \"\"\"\n    Initialize parameters dictionary.\n\n    Weight matrices will be initialized to random values from uniform normal\n    distribution.\n    bias vectors will be initialized to zeros.\n\n    Arguments\n    ---------\n    layers_dims : list or array-like\n        dimensions of each layer in the network.\n\n    Returns\n    -------\n    parameters : dict\n        weight matrix and the bias vector for each layer.\n    \"\"\"\n    np.random.seed(1)\n    parameters = {}\n    L = len(layers_dims)\n\n    for l in range(1, L):\n        parameters[\"W\" + str(l)] = (\n            np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01\n        )\n        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n\n        assert parameters[\"W\" + str(l)].shape == (layers_dims[l], layers_dims[l - 1])\n        assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n\n    return parameters\n\n\n\n\nActivation Functions\nThere is no definitive guide for which activation function works best on specific problems. It’s a trial and error process where one should try different set of functions and see which one works best on the problem at hand. We’ll cover 4 of the most commonly used activation functions:\n\nSigmoid function (\\(\\sigma\\)): \\(g(z) = \\frac{1}{1 + e^{-z}}\\). It’s recommended to be used only on the output layer so that we can easily interpret the output as probabilities since it has restricted output between 0 and 1. One of the main disadvantages for using sigmoid function on hidden layers is that the gradient is very close to zero over a large portion of its domain which makes it slow and harder for the learning algorithm to learn.\nHyperbolic Tangent function: \\(g(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\). It’s superior to sigmoid function in which the mean of its output is very close to zero, which in other words center the output of the activation units around zero and make the range of values very small which means faster to learn. The disadvantage that it shares with sigmoid function is that the gradient is very small on good portion of the domain.\nRectified Linear Unit (ReLU): \\(g(z) = max\\{0, z\\}\\). The models that are close to linear are easy to optimize. Since ReLU shares a lot of the properties of linear functions, it tends to work well on most of the problems. The only issue is that the derivative is not defined at \\(z = 0\\), which we can overcome by assigning the derivative to 0 at \\(z = 0\\). However, this means that for \\(z\\leq 0\\) the gradient is zero and again can’t learn.\nLeaky Rectified Linear Unit: \\(g(z) = max\\{\\alpha*z, z\\}\\). It overcomes the zero gradient issue from ReLU and assigns \\(\\alpha\\) which is a small value for \\(z\\leq 0\\).\n\nIf you’re not sure which activation function to choose, start with ReLU.\nNext, we’ll implement the above activation functions and draw a graph for each one to make it easier to see the domain and range of each function.\n\n\nCode\n# Define activation functions that will be used in forward propagation\ndef sigmoid(Z):\n    \"\"\"\n    Computes the sigmoid of Z element-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = 1 / (1 + np.exp(-Z))\n\n    return A, Z\n\n\ndef tanh(Z):\n    \"\"\"\n    Computes the Hyperbolic Tagent of Z elemnet-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = np.tanh(Z)\n\n    return A, Z\n\n\ndef relu(Z):\n    \"\"\"\n    Computes the Rectified Linear Unit (ReLU) element-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = np.maximum(0, Z)\n\n    return A, Z\n\n\ndef leaky_relu(Z):\n    \"\"\"\n    Computes Leaky Rectified Linear Unit element-wise.\n\n    Arguments\n    ---------\n    Z : array\n        output of affine transformation.\n\n    Returns\n    -------\n    A : array\n        post activation output.\n    Z : array\n        output of affine transformation.\n    \"\"\"\n    A = np.maximum(0.1 * Z, Z)\n\n    return A, Z\n\n\n\n\nCode\n# Plot the 4 activation functions\nz = np.linspace(-10, 10, 100)\n\n# Computes post-activation outputs\nA_sigmoid, z = sigmoid(z)\nA_tanh, z = tanh(z)\nA_relu, z = relu(z)\nA_leaky_relu, z = leaky_relu(z)\n\n# Plot sigmoid\nplt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(z, A_sigmoid, label=\"Function\")\nplt.plot(z, A_sigmoid * (1 - A_sigmoid), label=\"Derivative\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\frac{1}{1 + e^{-z}}$\")\nplt.title(\"Sigmoid Function\", fontsize=16)\n# Plot tanh\nplt.subplot(2, 2, 2)\nplt.plot(z, A_tanh, \"b\", label=\"Function\")\nplt.plot(z, 1 - np.square(A_tanh), \"r\", label=\"Derivative\")\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\frac{e^z - e^{-z}}{e^z + e^{-z}}$\")\nplt.title(\"Hyperbolic Tangent Function\", fontsize=16)\n# plot relu\nplt.subplot(2, 2, 3)\nplt.plot(z, A_relu, \"g\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$max\\{0, z\\}$\")\nplt.title(\"ReLU Function\", fontsize=16)\n# plot leaky relu\nplt.subplot(2, 2, 4)\nplt.plot(z, A_leaky_relu, \"y\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$max\\{0.1z, z\\}$\")\nplt.title(\"Leaky ReLU Function\", fontsize=16)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nFeed Forward\nGiven its inputs from previous layer, each unit computes affine transformation \\(z = W^Tx + b\\) and then apply an activation function \\(g(z)\\) such as ReLU element-wise. During the process, we’ll store (cache) all variables computed and used on each layer to be used in back-propagation. We’ll write first two helper functions that will be used in the L-model forward propagation to make it easier to debug. Keep in mind that on each layer, we may have different activation function.\n\n\nCode\n# Define helper functions that will be used in L-model forward prop\ndef linear_forward(A_prev, W, b):\n    \"\"\"\n    Computes affine transformation of the input.\n\n    Arguments\n    ---------\n    A_prev : 2d-array\n        activations output from previous layer.\n    W : 2d-array\n        weight matrix, shape: size of current layer x size of previuos layer.\n    b : 2d-array\n        bias vector, shape: size of current layer x 1.\n\n    Returns\n    -------\n    Z : 2d-array\n        affine transformation output.\n    cache : tuple\n        stores A_prev, W, b to be used in backpropagation.\n    \"\"\"\n    Z = np.dot(W, A_prev) + b\n    cache = (A_prev, W, b)\n\n    return Z, cache\n\n\ndef linear_activation_forward(A_prev, W, b, activation_fn):\n    \"\"\"\n    Computes post-activation output using non-linear activation function.\n\n    Arguments\n    ---------\n    A_prev : 2d-array\n        activations output from previous layer.\n    W : 2d-array\n        weight matrix, shape: size of current layer x size of previuos layer.\n    b : 2d-array\n        bias vector, shape: size of current layer x 1.\n    activation_fn : str\n        non-linear activation function to be used: \"sigmoid\", \"tanh\", \"relu\".\n\n    Returns\n    -------\n    A : 2d-array\n        output of the activation function.\n    cache : tuple\n        stores linear_cache and activation_cache. ((A_prev, W, b), Z) to be used in backpropagation.\n    \"\"\"\n    assert (\n        activation_fn == \"sigmoid\" or activation_fn == \"tanh\" or activation_fn == \"relu\"\n    )\n\n    if activation_fn == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n\n    elif activation_fn == \"tanh\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = tanh(Z)\n\n    elif activation_fn == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n\n    assert A.shape == (W.shape[0], A_prev.shape[1])\n\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\n\n\ndef L_model_forward(X, parameters, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the output layer through looping over all units in topological\n    order.\n\n    Arguments\n    ---------\n    X : 2d-array\n        input matrix of shape input_size x training_examples.\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    caches : list\n        that contains L tuples where each layer has: A_prev, W, b, Z.\n    \"\"\"\n    A = X\n    caches = []\n    L = len(parameters) // 2\n\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(\n            A_prev,\n            parameters[\"W\" + str(l)],\n            parameters[\"b\" + str(l)],\n            activation_fn=hidden_layers_activation_fn,\n        )\n        caches.append(cache)\n\n    AL, cache = linear_activation_forward(\n        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation_fn=\"sigmoid\"\n    )\n    caches.append(cache)\n\n    assert AL.shape == (1, X.shape[1])\n\n    return AL, caches\n\n\n\n\nCost\nWe’ll use the binary Cross-Entropy cost. It uses the log-likelihood method to estimate its error. The cost is: \\[J(W, b) = -\\frac{1}{m}\\sum_{i = 1}^m\\big(y^ilog(\\widehat{y^i}) + (1 - y^i)log(1 - \\widehat{y^i})\\big)\\tag3\\] The above cost function is convex; however, neural network usually stuck on a local minimum and is not guaranteed to find the optimal parameters. We’ll use here gradient-based learning.\n\n\nCode\n# Compute cross-entropy cost\ndef compute_cost(AL, y):\n    \"\"\"\n    Computes the binary Cross-Entropy cost.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector of shape 1 x training_examples.\n    y : 2d-array\n        true \"label\" vector.\n\n    Returns\n    -------\n    cost : float\n        binary cross-entropy cost.\n    \"\"\"\n    m = y.shape[1]\n    cost = -(1 / m) * np.sum(\n        np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL))\n    )\n\n    return cost\n\n\n\n\nBack-Propagation\nBackpropagation allows the information to go back from the cost backward through the network in order to compute the gradient. Therefore, loop over the nodes starting at the final node in reverse topological order to compute the derivative of the final node output with respect to each edge’s node tail. Doing so will help us know who is responsible for the most error and change the parameters in that direction. The following derivatives’ formulas will help us write the back-propagate functions: \\[dA^L = \\frac{A^L - Y}{A^L(1 - A^L)}\\tag4\\\\{}\\] \\[dZ^L = A^L - Y\\tag5\\\\{}\\] \\[dW^l = \\frac{1}{m}dZ^l{A^{l - 1}}^T\\tag6\\\\{}\\] \\[db^l = \\frac{1}{m}\\sum_i(dZ^l)\\tag7\\\\{}\\] \\[dA^{l - 1} = {W^l}^TdZ^l\\tag8\\\\{}\\] \\[dZ^{l} = dA^l*g^{'l}(Z^l)\\tag9\\\\{}\\] Since \\(b^l\\) is always a vector, the sum would be across rows (since each column is an example).\n\n\nCode\n# Define derivative of activation functions w.r.t z that will be used in back-propagation\ndef sigmoid_gradient(dA, Z):\n    \"\"\"\n    Computes the gradient of sigmoid output w.r.t input Z.\n\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient, of any shape.\n    Z : 2d-array\n        input used for the activation fn on this layer.\n\n    Returns\n    -------\n    dZ : 2d-array\n        gradient of the cost with respect to Z.\n    \"\"\"\n    A, Z = sigmoid(Z)\n    dZ = dA * A * (1 - A)\n\n    return dZ\n\n\ndef tanh_gradient(dA, Z):\n    \"\"\"\n    Computes the gradient of hyperbolic tangent output w.r.t input Z.\n\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient, of any shape.\n    Z : 2d-array\n        input used for the activation fn on this layer.\n\n    Returns\n    -------\n    dZ : 2d-array\n        gradient of the cost with respect to Z.\n    \"\"\"\n    A, Z = tanh(Z)\n    dZ = dA * (1 - np.square(A))\n\n    return dZ\n\n\ndef relu_gradient(dA, Z):\n    \"\"\"\n    Computes the gradient of ReLU output w.r.t input Z.\n\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient, of any shape.\n    Z : 2d-array\n        input used for the activation fn on this layer.\n\n    Returns\n    -------\n    dZ : 2d-array\n        gradient of the cost with respect to Z.\n    \"\"\"\n    A, Z = relu(Z)\n    dZ = np.multiply(dA, np.int64(A &gt; 0))\n\n    return dZ\n\n\n# define helper functions that will be used in L-model back-prop\ndef linear_backword(dZ, cache):\n    \"\"\"\n    Computes the gradient of the output w.r.t weight, bias, and post-activation\n    output of (l - 1) layers at layer l.\n\n    Arguments\n    ---------\n    dZ : 2d-array\n        gradient of the cost w.r.t. the linear output (of current layer l).\n    cache : tuple\n        values of (A_prev, W, b) coming from the forward propagation in the current layer.\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of the previous layer l-1).\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l).\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l).\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = (1 / m) * np.dot(dZ, A_prev.T)\n    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n\n    assert dA_prev.shape == A_prev.shape\n    assert dW.shape == W.shape\n    assert db.shape == b.shape\n\n    return dA_prev, dW, db\n\n\ndef linear_activation_backward(dA, cache, activation_fn):\n    \"\"\"\n    Arguments\n    ---------\n    dA : 2d-array\n        post-activation gradient for current layer l.\n    cache : tuple\n        values of (linear_cache, activation_cache).\n    activation : str\n        activation used in this layer: \"sigmoid\", \"tanh\", or \"relu\".\n\n    Returns\n    -------\n    dA_prev : 2d-array\n        gradient of the cost w.r.t. the activation (of the previous layer l-1), same shape as A_prev.\n    dW : 2d-array\n        gradient of the cost w.r.t. W (current layer l), same shape as W.\n    db : 2d-array\n        gradient of the cost w.r.t. b (current layer l), same shape as b.\n    \"\"\"\n    linear_cache, activation_cache = cache\n\n    if activation_fn == \"sigmoid\":\n        dZ = sigmoid_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n\n    elif activation_fn == \"tanh\":\n        dZ = tanh_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n\n    elif activation_fn == \"relu\":\n        dZ = relu_gradient(dA, activation_cache)\n        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n\n    return dA_prev, dW, db\n\n\ndef L_model_backward(AL, y, caches, hidden_layers_activation_fn=\"relu\"):\n    \"\"\"\n    Computes the gradient of output layer w.r.t weights, biases, etc. starting\n    on the output layer in reverse topological order.\n\n    Arguments\n    ---------\n    AL : 2d-array\n        probability vector, output of the forward propagation (L_model_forward()).\n    y : 2d-array\n        true \"label\" vector (containing 0 if non-cat, 1 if cat).\n    caches : list\n        list of caches for all layers.\n    hidden_layers_activation_fn :\n        activation function used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    grads : dict\n        with the gradients.\n    \"\"\"\n    y = y.reshape(AL.shape)\n    L = len(caches)\n    grads = {}\n\n    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n\n    (\n        grads[\"dA\" + str(L - 1)],\n        grads[\"dW\" + str(L)],\n        grads[\"db\" + str(L)],\n    ) = linear_activation_backward(dAL, caches[L - 1], \"sigmoid\")\n\n    for l in range(L - 1, 0, -1):\n        current_cache = caches[l - 1]\n        (\n            grads[\"dA\" + str(l - 1)],\n            grads[\"dW\" + str(l)],\n            grads[\"db\" + str(l)],\n        ) = linear_activation_backward(\n            grads[\"dA\" + str(l)], current_cache, hidden_layers_activation_fn\n        )\n\n    return grads\n\n\n# define the function to update both weight matrices and bias vectors\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update the parameters' values using gradient descent rule.\n\n    Arguments\n    ---------\n    parameters : dict\n        contains all the weight matrices and bias vectors for all layers.\n    grads : dict\n        stores all gradients (output of L_model_backward).\n\n    Returns\n    -------\n    parameters : dict\n        updated parameters.\n    \"\"\"\n    L = len(parameters) // 2\n\n    for l in range(1, L + 1):\n        parameters[\"W\" + str(l)] = (\n            parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n        )\n        parameters[\"b\" + str(l)] = (\n            parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n        )\n\n    return parameters"
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#application",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#application",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "Application",
    "text": "Application\nThe dataset that we’ll be working on has 209 images. Each image is 64 x 64 pixels on RGB scale. We’ll build a neural network to classify if the image has a cat or not. Therefore, \\(y^i \\in \\{0, 1\\}.\\)\n\nWe’ll first load the images.\nShow sample image for a cat.\nReshape input matrix so that each column would be one example. Also, since each image is 64 x 64 x 3, we’ll end up having 12,288 features for each image. Therefore, the input matrix would be 12,288 x 209.\nStandardize the data so that the gradients don’t go out of control. Also, it will help hidden units have similar range of values. For now, we’ll divide every pixel by 255 which shouldn’t be an issue. However, it’s better to standardize the data to have a mean of 0 and a standard deviation of 1.\n\n\n\nCode\n# Import training dataset\ntrain_dataset = h5py.File(\"../../data/train_catvnoncat.h5\")\nX_train = np.array(train_dataset[\"train_set_x\"])\ny_train = np.array(train_dataset[\"train_set_y\"])\n\ntest_dataset = h5py.File(\"../../data/test_catvnoncat.h5\")\nX_test = np.array(test_dataset[\"test_set_x\"])\ny_test = np.array(test_dataset[\"test_set_y\"])\n\n# print the shape of input data and label vector\nprint(\n    f\"\"\"Original dimensions:\\n{20 * '-'}\\nTraining: {X_train.shape}, {y_train.shape}\nTest: {X_test.shape}, {y_test.shape}\"\"\"\n)\n\n# plot cat image\nplt.figure(figsize=(6, 6))\nplt.imshow(X_train[50])\nplt.axis(\"off\")\n\n# Transform input data and label vector\nX_train = X_train.reshape(209, -1).T\ny_train = y_train.reshape(-1, 209)\n\nX_test = X_test.reshape(50, -1).T\ny_test = y_test.reshape(-1, 50)\n\n# standarize the data\nX_train = X_train / 255\nX_test = X_test / 255\n\nprint(\n    f\"\"\"\\nNew dimensions:\\n{15 * '-'}\\nTraining: {X_train.shape}, {y_train.shape}\nTest: {X_test.shape}, {y_test.shape}\"\"\"\n)\n\n\nOriginal dimensions:\n--------------------\nTraining: (209, 64, 64, 3), (209,)\nTest: (50, 64, 64, 3), (50,)\n\nNew dimensions:\n---------------\nTraining: (12288, 209), (1, 209)\nTest: (12288, 50), (1, 50)\n\n\n\n\n\n\n\n\n\nNow, our dataset is ready to be used and test our neural network implementation. Let’s first write multi-layer model function to implement gradient-based learning using predefined number of iterations and learning rate.\n\n\nCode\n# Define the multi-layer model using all the helper functions we wrote before\n\n\ndef L_layer_model(\n    X,\n    y,\n    layers_dims,\n    learning_rate=0.01,\n    num_iterations=3000,\n    print_cost=True,\n    hidden_layers_activation_fn=\"relu\",\n):\n    \"\"\"\n    Implements multilayer neural network using gradient descent as the\n    learning algorithm.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    layers_dims : list\n        input size and size of each layer, length: number of layers + 1.\n    learning_rate : float\n        learning rate of the gradient descent update rule.\n    num_iterations : int\n        number of iterations of the optimization loop.\n    print_cost : bool\n        if True, it prints the cost every 100 steps.\n    hidden_layers_activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    parameters : dict\n        parameters learnt by the model. They can then be used to predict test examples.\n    \"\"\"\n    np.random.seed(1)\n\n    # initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # intialize cost list\n    cost_list = []\n\n    # iterate over num_iterations\n    for i in range(num_iterations):\n        # iterate over L-layers to get the final output and the cache\n        AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n\n        # compute cost to plot it\n        cost = compute_cost(AL, y)\n\n        # iterate over L-layers backward to get gradients\n        grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn)\n\n        # update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        # append each 100th cost to the cost list\n        if (i + 1) % 100 == 0 and print_cost:\n            print(f\"The cost after {i + 1} iterations is: {cost:.4f}\")\n\n        if i % 100 == 0:\n            cost_list.append(cost)\n\n    # plot the cost curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(cost_list)\n    plt.xlabel(\"Iterations (per hundreds)\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Loss curve for the learning rate = {learning_rate}\")\n\n    return parameters\n\n\ndef accuracy(X, parameters, y, activation_fn=\"relu\"):\n    \"\"\"\n    Computes the average accuracy rate.\n\n    Arguments\n    ---------\n    X : 2d-array\n        data, shape: number of examples x num_px * num_px * 3.\n    parameters : dict\n        learnt parameters.\n    y : 2d-array\n        true \"label\" vector, shape: 1 x number of examples.\n    activation_fn : str\n        activation function to be used on hidden layers: \"tanh\", \"relu\".\n\n    Returns\n    -------\n    accuracy : float\n        accuracy rate after applying parameters on the input data\n    \"\"\"\n    probs, caches = L_model_forward(X, parameters, activation_fn)\n    labels = (probs &gt;= 0.5) * 1\n    accuracy = np.mean(labels == y) * 100\n\n    return f\"The accuracy rate is: {accuracy:.2f}%.\"\n\n\nNext, we’ll train two versions of the neural network where each one will use different activation function on hidden layers: One will use rectified linear unit (ReLU) and the second one will use hyperbolic tangent function (tanh). Finally we’ll use the parameters we get from both neural networks to classify test examples and compute the test accuracy rates for each version to see which activation function works best on this problem.\n\n\nCode\n# Setting layers dims\nlayers_dims = [X_train.shape[0], 5, 5, 1]\n\n# NN with tanh activation fn\nparameters_tanh = L_layer_model(\n    X_train,\n    y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_iterations=3000,\n    hidden_layers_activation_fn=\"tanh\",\n)\n\n# Print the accuracy\naccuracy(X_test, parameters_tanh, y_test, activation_fn=\"tanh\")\n\n\nThe cost after 100 iterations is: 0.6556\nThe cost after 200 iterations is: 0.6468\nThe cost after 300 iterations is: 0.6447\nThe cost after 400 iterations is: 0.6441\nThe cost after 500 iterations is: 0.6440\nThe cost after 600 iterations is: 0.6440\nThe cost after 700 iterations is: 0.6440\nThe cost after 800 iterations is: 0.6439\nThe cost after 900 iterations is: 0.6439\nThe cost after 1000 iterations is: 0.6439\nThe cost after 1100 iterations is: 0.6439\nThe cost after 1200 iterations is: 0.6439\nThe cost after 1300 iterations is: 0.6438\nThe cost after 1400 iterations is: 0.6438\nThe cost after 1500 iterations is: 0.6437\nThe cost after 1600 iterations is: 0.6434\nThe cost after 1700 iterations is: 0.6429\nThe cost after 1800 iterations is: 0.6413\nThe cost after 1900 iterations is: 0.6361\nThe cost after 2000 iterations is: 0.6124\nThe cost after 2100 iterations is: 0.5112\nThe cost after 2200 iterations is: 0.5001\nThe cost after 2300 iterations is: 0.3644\nThe cost after 2400 iterations is: 0.3393\nThe cost after 2500 iterations is: 0.4184\nThe cost after 2600 iterations is: 0.2372\nThe cost after 2700 iterations is: 0.4299\nThe cost after 2800 iterations is: 0.3064\nThe cost after 2900 iterations is: 0.2842\nThe cost after 3000 iterations is: 0.1902\n\n\n'The accuracy rate is: 70.00%.'\n\n\n\n\n\n\n\n\n\n\n\nCode\n# NN with relu activation fn\nparameters_relu = L_layer_model(\n    X_train,\n    y_train,\n    layers_dims,\n    learning_rate=0.03,\n    num_iterations=3000,\n    hidden_layers_activation_fn=\"relu\",\n)\n\n# Print the accuracy\naccuracy(X_test, parameters_relu, y_test, activation_fn=\"relu\")\n\n\nThe cost after 100 iterations is: 0.6556\nThe cost after 200 iterations is: 0.6468\nThe cost after 300 iterations is: 0.6447\nThe cost after 400 iterations is: 0.6441\nThe cost after 500 iterations is: 0.6440\nThe cost after 600 iterations is: 0.6440\nThe cost after 700 iterations is: 0.6440\nThe cost after 800 iterations is: 0.6440\nThe cost after 900 iterations is: 0.6440\nThe cost after 1000 iterations is: 0.6440\nThe cost after 1100 iterations is: 0.6439\nThe cost after 1200 iterations is: 0.6439\nThe cost after 1300 iterations is: 0.6439\nThe cost after 1400 iterations is: 0.6439\nThe cost after 1500 iterations is: 0.6439\nThe cost after 1600 iterations is: 0.6439\nThe cost after 1700 iterations is: 0.6438\nThe cost after 1800 iterations is: 0.6437\nThe cost after 1900 iterations is: 0.6435\nThe cost after 2000 iterations is: 0.6432\nThe cost after 2100 iterations is: 0.6423\nThe cost after 2200 iterations is: 0.6395\nThe cost after 2300 iterations is: 0.6259\nThe cost after 2400 iterations is: 0.5408\nThe cost after 2500 iterations is: 0.5262\nThe cost after 2600 iterations is: 0.4727\nThe cost after 2700 iterations is: 0.4386\nThe cost after 2800 iterations is: 0.3493\nThe cost after 2900 iterations is: 0.1877\nThe cost after 3000 iterations is: 0.3641\n\n\n'The accuracy rate is: 42.00%.'"
  },
  {
    "objectID": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#conclusion",
    "href": "posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html#conclusion",
    "title": "Coding Neural Network Part 1 - Forward & Backward Propagation",
    "section": "Conclusion",
    "text": "Conclusion\nThe purpose of this artcile is to code Deep Neural Network step-by-step and explain the important concepts while doing that. We don’t really care about the accuracy rate at this moment since there are tons of things we could’ve done to increase the accuracy which would be the subject of following artciles. Below are some takeaways:\n\nEven if neural network can represent any function, it may fail to learn for two reasons:\n\nThe optimization algorithm may fail to find the best value for the parameters of the desired (true) function. It can stuck in a local optimum.\nThe learning algorithm may find different functional form that is different than the intended function due to overfitting.\n\nEven if neural network rarely converges and always stuck in a local minimum, it is still able to reduce the cost significantly and come up with very complex models with high test accuracy.\nThe neural network we used in this artcile is standard fully connected network. However, there are two other kinds of networks:\n\nConvolutional NN: Where not all nodes are connected. It’s best in class for image recognition.\nRecurrent NN: There is a feedback connections where output of the model is fed back into itself. It’s used mainly in sequence modeling.\n\nThe fully connected neural network also forgets what happened in previous steps and also doesn’t know anything about the output.\nThere are number of hyperparameters that we can tune using cross validation to get the best performance of our network:\n\nLearning rate (\\(\\alpha\\)): Determines how big the step for each update of parameters.\n\nSmall \\(\\alpha\\) leads to slow convergence and may become computationally very expensive.\nLarge \\(\\alpha\\) may lead to overshooting where our learning algorithm may never converge.\n\nNumber of hidden layers (depth): The more hidden layers the better, but comes at a cost computationally.\nNumber of units per hidden layer (width): Research proven that huge number of hidden units per layer doesn’t add to the improvement of the network.\nActivation function: Which function to use on hidden layers differs among applications and domains. It’s a trial and error process to try different functions and see which one works best.\nNumber of iterations.\n\nStandardize data would help activation units have similar range of values and avoid gradients to go out of control."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#introduction",
    "href": "posts/clustering/Kmeans-Clustering.html#introduction",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Introduction",
    "text": "Introduction\nClustering is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. In other words, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance. The decision of which similarity measure to use is application-specific.\nClustering analysis can be done on the basis of features where we try to find subgroups of samples based on features or on the basis of samples where we try to find subgroups of features based on samples. We’ll cover here clustering based on features. Clustering is used in market segmentation; where we try to fined customers that are similar to each other whether in terms of behaviors or attributes, image segmentation/compression; where we try to group similar regions together, document clustering based on topics, etc.\nUnlike supervised learning, clustering is considered an unsupervised learning method since we don’t have the ground truth to compare the output of the clustering algorithm to the true labels to evaluate its performance. We only want to try to investigate the structure of the data by grouping the data points into distinct subgroups.\nIn this post, we will cover only Kmeans which is considered as one of the most used clustering algorithms due to its simplicity."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#kmeans-algorithm",
    "href": "posts/clustering/Kmeans-Clustering.html#kmeans-algorithm",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Kmeans Algorithm",
    "text": "Kmeans Algorithm\nKmeans algorithm is an iterative algorithm that tries to partition the dataset into \\(K\\) pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the inter-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.\nThe way kmeans algorithm works is as follows:\n\nSpecify number of clusters \\(K\\).\nInitialize centroids by first shuffling the dataset and then randomly selecting \\(K\\) data points for the centroids without replacement.\nKeep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn’t changing.\n\nCompute the sum of the squared distance between data points and all centroids.\nAssign each data point to the closest cluster (centroid).\nCompute the centroids for the clusters by taking the average of the all data points that belong to each cluster.\n\n\nThe approach kmeans follows to solve the problem is called Expectation-Maximization. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it).\nThe objective function is: \\[J = \\sum_{i = 1}^{m}\\sum_{k = 1}^{K}w_{ik}\\|x^i - \\mu_k\\|^2\\\\{}\\] where \\(w_{ik} = 1\\) for data point \\(x^i\\) if it belongs to cluster \\(k\\); otherwise, \\(w_{ik} = 0\\). Also, \\(\\mu_k\\) is the centroid of \\(x^i\\)’s cluster.\nIt’s a minimization problem of two parts. We first minimize J w.r.t. \\(w_{ik}\\) and treat \\(\\mu_k\\) fixed. Then we minimize J w.r.t. \\(\\mu_k\\) and treat \\(w_{ik}\\) fixed. Technically speaking, we differentiate J w.r.t. \\(w_{ik}\\) first and update cluster assignments (E-step). Then we differentiate J w.r.t. \\(\\mu_{k}\\) and recompute the centroids after the cluster assignments from previous step (M-step). Therefore, E-step is: \\[\\frac{\\partial J}{\\partial w_{ik}} = \\sum_{i = 1}^{m}\\sum_{k = 1}^{K}\\|x^i - \\mu_k\\|^2\\\\{}\\] \\[\n\\Rightarrow\n\\begin{equation}\n  w_{ik} = \\begin{cases}\n    1 & \\text{if $k = arg min_j\\ \\|x^i - \\mu_j\\|^2$}\\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\\tag{1}\\\\{}\n\\] In other words, assign the data point \\(x^i\\) to the closest cluster judged by its sum of squared distance from cluster’s centroid.\nAnd M-step is: \\[\\ \\frac{\\partial J}{\\partial \\mu_k} = 2\\sum_{i = 1}^{m}w_{ik}(x^i - \\mu_k) = 0\\\\{}\\] \\[\\Rightarrow \\mu_k = \\frac{\\sum_{i = 1}^{m}w_{ik}x^i}{\\sum_{i = 1}^{m}w_{ik}}\\tag{2}\\\\{}\\] Which translates to recomputing the centroid of each cluster to reflect the new assignments.\nFew things to note here:\n\nSince clustering algorithms including kmeans use distance-based measurements to determine the similarity between data points, it’s recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income.\nGiven kmeans iterative nature and the random initialization of centroids at the start of the algorithm, different initializations may lead to different clusters since kmeans algorithm may stuck in a local optimum and may not converge to global optimum. Therefore, it’s recommended to run the algorithm using different initializations of centroids and pick the results of the run that that yielded the lower sum of squared distance.\nAssignment of examples isn’t changing is the same thing as no change in within-cluster variation: \\[\\frac{1}{m_k}\\sum_{i = 1}^{m_k}\\|x^i - \\mu_{c^k}\\|^2\\]"
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#implementation",
    "href": "posts/clustering/Kmeans-Clustering.html#implementation",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Implementation",
    "text": "Implementation\nWe’ll use simple implementation of kmeans here to just illustrate some concepts. Then we will use sklearn implementation that is more efficient take care of many things for us.\n\n\nCode\nimport numpy as np\nfrom numpy.linalg import norm\n\n\nclass Kmeans:\n    '''Implementing Kmeans algorithm.'''\n\n    def __init__(self, n_clusters, max_iter=100, random_state=123):\n        self.n_clusters = n_clusters\n        self.max_iter = max_iter\n        self.random_state = random_state\n\n    def initializ_centroids(self, X):\n        np.random.RandomState(self.random_state)\n        random_idx = np.random.permutation(X.shape[0])\n        centroids = X[random_idx[:self.n_clusters]]\n        return centroids\n\n    def compute_centroids(self, X, labels):\n        centroids = np.zeros((self.n_clusters, X.shape[1]))\n        for k in range(self.n_clusters):\n            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n        return centroids\n\n    def compute_distance(self, X, centroids):\n        distance = np.zeros((X.shape[0], self.n_clusters))\n        for k in range(self.n_clusters):\n            row_norm = norm(X - centroids[k, :], axis=1)\n            distance[:, k] = np.square(row_norm)\n        return distance\n\n    def find_closest_cluster(self, distance):\n        return np.argmin(distance, axis=1)\n\n    def compute_sse(self, X, labels, centroids):\n        distance = np.zeros(X.shape[0])\n        for k in range(self.n_clusters):\n            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n        return np.sum(np.square(distance))\n    \n    def fit(self, X):\n        self.centroids = self.initializ_centroids(X)\n        for i in range(self.max_iter):\n            old_centroids = self.centroids\n            distance = self.compute_distance(X, old_centroids)\n            self.labels = self.find_closest_cluster(distance)\n            self.centroids = self.compute_centroids(X, self.labels)\n            if np.all(old_centroids == self.centroids):\n                break\n        self.error = self.compute_sse(X, self.labels, self.centroids)\n    \n    def predict(self, X):\n        distance = self.compute_distance(X, old_centroids)\n        return self.find_closest_cluster(distance)"
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#applications",
    "href": "posts/clustering/Kmeans-Clustering.html#applications",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Applications",
    "text": "Applications\nkmeans algorithm is very popular and used in a variety of applications such as market segmentation, document clustering, image segmentation and image compression, etc. The goal usually when we undergo a cluster analysis is either:\n\nGet a meaningful intuition of the structure of the data we’re dealing with.\nCluster-then-predict where different models will be built for different subgroups if we believe there is a wide variation in the behaviors of different subgroups. An example of that is clustering patients into different subgroups and build a model for each subgroup to predict the probability of the risk of having heart attack.\n\nIn this post, we’ll apply clustering on two cases: - Geyser eruptions segmentation (2-D dataset). - Image compression.\n\nKmeans on Geyser’s Eruptions Segmentation\nWe’ll first implement the kmeans algorithm on 2D dataset and see how it works. The dataset has 272 observations and 2 features. The data covers the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. We will try to find \\(K\\) subgroups within the data points and group them accordingly. Below is the description of the features:\n\neruptions (float): Eruption time in minutes.\nwaiting (int): Waiting time to next eruption.\n\nLet’s plot the data first:\n\n\nCode\n# Modules\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets.samples_generator import (make_blobs,\n                                                make_circles,\n                                                make_moons)\nfrom sklearn.cluster import KMeans, SpectralClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n%matplotlib inline\nsns.set_context('notebook')\nplt.style.use('fivethirtyeight')\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\n\n\nCode\n# Import the data\ndf = pd.read_csv('../data/old_faithful.csv')\n\n# Plot the data\nplt.figure(figsize=(6, 6))\nplt.scatter(df.iloc[:, 0], df.iloc[:, 1])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of raw data');\n\n\n\n\n\n\n\n\n\nWe’ll use this data because it’s easy to plot and visually spot the clusters since its a 2-dimension dataset. It’s obvious that we have 2 clusters. Let’s standardize the data first and run the kmeans algorithm on the standardized data with \\(K = 2\\).\n\n\nCode\n# Standardize the data\nX_std = StandardScaler().fit_transform(df)\n\n# Run local implementation of kmeans\nkm = Kmeans(n_clusters=2, max_iter=100)\nkm.fit(X_std)\ncentroids = km.centroids\n\n# Plot the clustered data\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],\n            c='green', label='cluster 1')\nplt.scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],\n            c='blue', label='cluster 2')\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,\n            c='r', label='centroid')\nplt.legend()\nplt.xlim([-2, 2])\nplt.ylim([-2, 2])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of clustered data', fontweight='bold')\nax.set_aspect('equal');\n\n\n\n\n\n\n\n\n\nThe above graph shows the scatter plot of the data colored by the cluster they belong to. In this example, we chose \\(K = 2\\). The symbol **’*’** is the centroid of each cluster. We can think of those 2 clusters as geyser had different kinds of behaviors under different scenarios.\nNext, we’ll show that different initializations of centroids may yield to different results. I’ll use 9 different random_state to change the initialization of the centroids and plot the results. The title of each plot will be the sum of squared distance of each initialization.\nAs a side note, this dataset is considered very easy and converges in less than 10 iterations. Therefore, to see the effect of random initialization on convergence, I am going to go with 3 iterations to illustrate the concept. However, in real world applications, datasets are not at all that clean and nice!\n\n\nCode\nn_iter = 9\nfig, ax = plt.subplots(3, 3, figsize=(16, 16))\nax = np.ravel(ax)\ncenters = []\nfor i in range(n_iter):\n    # Run local implementation of kmeans\n    km = Kmeans(n_clusters=2,\n                max_iter=3,\n                random_state=np.random.randint(0, 1000, size=1))\n    km.fit(X_std)\n    centroids = km.centroids\n    centers.append(centroids)\n    ax[i].scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],\n                  c='green', label='cluster 1')\n    ax[i].scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],\n                  c='blue', label='cluster 2')\n    ax[i].scatter(centroids[:, 0], centroids[:, 1],\n                  c='r', marker='*', s=300, label='centroid')\n    ax[i].set_xlim([-2, 2])\n    ax[i].set_ylim([-2, 2])\n    ax[i].legend(loc='lower right')\n    ax[i].set_title(f'{km.error:.4f}')\n    ax[i].set_aspect('equal')\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\nAs the graph above shows that we only ended up with two different ways of clusterings based on different initializations. We would pick the one with the lowest sum of squared distance.\n\n\nImage Compression\nIn this part, we’ll implement kmeans to compress an image. The image that we’ll be working on is 396 x 396 x 3. Therefore, for each pixel location we would have 3 8-bit integers that specify the red, green, and blue intensity values. Our goal is to reduce the number of colors to 30 and represent (compress) the photo using those 30 colors only. To pick which colors to use, we’ll use kmeans algorithm on the image and treat every pixel as a data point. That means reshape the image from height x width x channels to (height * width) x channel, i,e we would have 396 x 396 = 156,816 data points in 3-dimensional space which are the intensity of RGB. Doing so will allow us to represent the image using the 30 centroids for each pixel and would significantly reduce the size of the image by a factor of 6. The original image size was 396 x 396 x 24 = 3,763,584 bits; however, the new compressed image would be 30 x 24 + 396 x 396 x 4 = 627,984 bits. The huge difference comes from the fact that we’ll be using centroids as a lookup for pixels’ colors and that would reduce the size of each pixel location to 4-bit instead of 8-bit.\nFrom now on we will be using sklearn implementation of kmeans. Few thing to note here:\n\nn_init is the number of times of running the kmeans with different centroid’s initialization. The result of the best one will be reported.\ntol is the within-cluster variation metric used to declare convergence.\nThe default of init is k-means++ which is supposed to yield a better results than just random initialization of centroids.\n\n\n\nCode\n# Read the image\nimg = imread('images/my_image.jpg')\nimg_size = img.shape\n\n# Reshape it to be 2-dimension\nX = img.reshape(img_size[0] * img_size[1], img_size[2])\n\n# Run the Kmeans algorithm\nkm = KMeans(n_clusters=30)\nkm.fit(X)\n\n# Use the centroids to compress the image\nX_compressed = km.cluster_centers_[km.labels_]\nX_compressed = np.clip(X_compressed.astype('uint8'), 0, 255)\n\n# Reshape X_recovered to have the same dimension as the original image 128 * 128 * 3\nX_compressed = X_compressed.reshape(img_size[0], img_size[1], img_size[2])\n\n# Plot the original and the compressed image next to each other\nfig, ax = plt.subplots(1, 2, figsize = (12, 8))\nax[0].imshow(img)\nax[0].set_title('Original Image')\nax[1].imshow(X_compressed)\nax[1].set_title('Compressed Image with 30 colors')\nfor ax in fig.axes:\n    ax.axis('off')\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\nWe can see the comparison between the original image and the compressed one. The compressed image looks close to the original one which means we’re able to retain the majority of the characteristics of the original image. With smaller number of clusters we would have higher compression rate at the expense of image quality. As a side note, this image compression method is called lossy data compression because we can’t reconstruct the original image from the compressed image."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#evaluation-methods",
    "href": "posts/clustering/Kmeans-Clustering.html#evaluation-methods",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Evaluation Methods",
    "text": "Evaluation Methods\nContrary to supervised learning where we have the ground truth to evaluate the model’s performance, clustering analysis doesn’t have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. Moreover, since kmeans requires \\(k\\) as an input and doesn’t learn it from data, there is no right answer in terms of the number of clusters that we should have in any problem. Sometimes domain knowledge and intuition may help but usually that is not the case. In the cluster-predict methodology, we can evaluate how well the models are performing based on different \\(K\\) clusters since clusters are used in the downstream modeling.\nIn this post we’ll cover two metrics that may give us some intuition about \\(k\\):\n\nElbow method\nSilhouette analysis\n\n\nElbow Method\nElbow method gives us an idea on what a good \\(k\\) number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters’ centroids. We pick \\(k\\) at the spot where SSE starts to flatten out and forming an elbow. We’ll use the geyser dataset and evaluate SSE for different values of \\(k\\) and see where the curve might form an elbow and flatten out.\n\n\nCode\n# Run the Kmeans algorithm and get the index of data points clusters\nsse = []\nlist_k = list(range(1, 10))\n\nfor k in list_k:\n    km = KMeans(n_clusters=k)\n    km.fit(X_std)\n    sse.append(km.inertia_)\n\n# Plot sse against k\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, sse, '-o')\nplt.xlabel(r'Number of clusters $k$')\nplt.ylabel('Sum of squared distance');\n\n\n\n\n\n\n\n\n\nThe graph above shows that \\(k = 2\\) is not a good choice. Sometimes it’s still hard to figure out a good number of clusters to use because the curve is monotonically decreasing and may not show any elbow or has an obvious point where the curve starts flattening out.\n\n\nSilhouette Analysis\nSilhouette analysis can be used to determine the degree of separation between clusters. For each sample:\n\nCompute the average distance from all data points in the same cluster (\\(a^i\\)).\nCompute the average distance from all data points in the closest cluster (\\(b^i\\)).\nCompute the coefficient: \\[\\frac{b^i - a^i}{max(a^i, b^i)}\\] The coefficient can take values in the interval [-1, 1].\n\nIf it is 0 –&gt; the sample is very close to the neighboring clusters.\nIt it is 1 –&gt; the sample is far away from the neighboring clusters.\nIt it is -1 –&gt; the sample is assigned to the wrong clusters.\n\n\nTherefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters. We’ll use here geyser dataset again because its cheaper to run the silhouette analysis and it is actually obvious that there is most likely only two groups of data points.\n\n\nCode\nfor i, k in enumerate([2, 3, 4]):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n    \n    # Run the Kmeans algorithm\n    km = KMeans(n_clusters=k)\n    labels = km.fit_predict(X_std)\n    centroids = km.cluster_centers_\n\n    # Get silhouette samples\n    silhouette_vals = silhouette_samples(X_std, labels)\n\n    # Silhouette plot\n    y_ticks = []\n    y_lower, y_upper = 0, 0\n    for i, cluster in enumerate(np.unique(labels)):\n        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n        cluster_silhouette_vals.sort()\n        y_upper += len(cluster_silhouette_vals)\n        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n        y_lower += len(cluster_silhouette_vals)\n\n    # Get the average silhouette score and plot it\n    avg_score = np.mean(silhouette_vals)\n    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n    ax1.set_yticks([])\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_xlabel('Silhouette coefficient values')\n    ax1.set_ylabel('Cluster labels')\n    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n    \n    # Scatter plot of data colored with labels\n    ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels)\n    ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n    ax2.set_xlim([-2, 2])\n    ax2.set_xlim([-2, 2])\n    ax2.set_xlabel('Eruption time in mins')\n    ax2.set_ylabel('Waiting time to next eruption')\n    ax2.set_title('Visualization of clustered data', y=1.02)\n    ax2.set_aspect('equal')\n    plt.tight_layout()\n    plt.suptitle(f'Silhouette analysis using k = {k}',\n                 fontsize=16, fontweight='semibold', y=1.05);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the above plots show, n_clusters=2 has the best average silhouette score of around 0.75 and all clusters being above the average shows that it is actually a good choice. Also, the thickness of the silhouette plot gives an indication of how big each cluster is. The plot shows that cluster 1 has almost double the samples than cluster 2. However, as we increased n_clusters to 3 and 4, the average silhouette score decreased dramatically to around 0.48 and 0.39 respectively. Moreover, the thickness of silhouette plot started showing wide fluctuations. The bottom line is: Good n_clusters will have a well above 0.5 silhouette average score as well as all of the clusters have higher than the average score."
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#drawbacks",
    "href": "posts/clustering/Kmeans-Clustering.html#drawbacks",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Drawbacks",
    "text": "Drawbacks\nKmeans algorithm is good in capturing structure of the data if clusters have a spherical-like shape. It always try to construct a nice spherical shape around the centroid. That means, the minute the clusters have a complicated geometric shapes, kmeans does a poor job in clustering the data. We’ll illustrate three cases where kmeans will not perform well.\nFirst, kmeans algorithm doesn’t let data points that are far-away from each other share the same cluster even though they obviously belong to the same cluster. Below is an example of data points on two different horizontal lines that illustrates how kmeans tries to group half of the data points of each horizontal lines together.\n\n\nCode\n# Create horizantal data\nX = np.tile(np.linspace(1, 5, 20), 2)\ny = np.repeat(np.array([2, 4]), 20)\ndf = np.c_[X, y]\n\nkm = KMeans(n_clusters=2)\nkm.fit(df)\nlabels = km.predict(df)\ncentroids = km.cluster_centers_\n\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.scatter(X, y, c=labels)\nplt.xlim([0, 6])\nplt.ylim([0, 6])\nplt.text(5.1, 4, 'A', color='red')\nplt.text(5.1, 2, 'B', color='red')\nplt.text(2.8, 4.1, 'C', color='red')\nax.set_aspect('equal')\n\n\n\n\n\n\n\n\n\nKmeans considers the point ‘B’ closer to point ‘A’ than point ‘C’ since they have non-spherical shape. Therefore, points ‘A’ and ‘B’ will be in the same cluster but point ‘C’ will be in a different cluster. Note the Single Linkage hierarchical clustering method gets this right because it doesn’t separate similar points).\nSecond, we’ll generate data from multivariate normal distributions with different means and standard deviations. So we would have 3 groups of data where each group was generated from different multivariate normal distribution (different mean/standard deviation). One group will have a lot more data points than the other two combined. Next, we’ll run kmeans on the data with \\(K = 3\\) and see if it will be able to cluster the data correctly. To make the comparison easier, I am going to plot first the data colored based on the distribution it came from. Then I will plot the same data but now colored based on the clusters they have been assigned to.\n\n\nCode\n# Create data from three different multivariate distributions\nX_1 = np.random.multivariate_normal(mean=[4, 0], cov=[[1, 0], [0, 1]], size=75)\nX_2 = np.random.multivariate_normal(mean=[6, 6], cov=[[2, 0], [0, 2]], size=250)\nX_3 = np.random.multivariate_normal(mean=[1, 5], cov=[[1, 0], [0, 2]], size=20)\ndf = np.concatenate([X_1, X_2, X_3])\n\n# Run kmeans\nkm = KMeans(n_clusters=3)\nkm.fit(df)\nlabels = km.predict(df)\ncentroids = km.cluster_centers_\n\n# Plot the data\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\nax[0].scatter(X_1[:, 0], X_1[:, 1])\nax[0].scatter(X_2[:, 0], X_2[:, 1])\nax[0].scatter(X_3[:, 0], X_3[:, 1])\nax[0].set_aspect('equal')\nax[1].scatter(df[:, 0], df[:, 1], c=labels)\nax[1].scatter(centroids[:, 0], centroids[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\nfor i, c in enumerate(centroids):\n    ax[1].scatter(c[0], c[1], marker='$%d$' % i, s=50, alpha=1, edgecolor='r')\nax[1].set_aspect('equal')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nLooks like kmeans couldn’t figure out the clusters correctly. Since it tries to minimize the within-cluster variation, it gives more weight to bigger clusters than smaller ones. In other words, data points in smaller clusters may be left away from the centroid in order to focus more on the larger cluster.\nLast, we’ll generate data that have complicated geometric shapes such as moons and circles within each other and test kmeans on both of the datasets.\n\n\nCode\n# Cricles\nX1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n\n# Moons\nX2 = make_moons(n_samples=1500, noise=0.05)\n\nfig, ax = plt.subplots(1, 2)\nfor i, X in enumerate([X1, X2]):\n    fig.set_size_inches(18, 7)\n    km = KMeans(n_clusters=2)\n    km.fit(X[0])\n    labels = km.predict(X[0])\n    centroids = km.cluster_centers_\n\n    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=labels)\n    ax[i].scatter(centroids[0, 0], centroids[0, 1], marker='*', s=400, c='r')\n    ax[i].scatter(centroids[1, 0], centroids[1, 1], marker='+', s=300, c='green')\nplt.suptitle('Simulated data', y=1.05, fontsize=22, fontweight='semibold')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nAs expected, kmeans couldn’t figure out the correct clusters for both datasets. However, we can help kmeans perfectly cluster these kind of datasets if we use kernel methods. The idea is we transform to higher dimensional representation that make the data linearly separable (the same idea that we use in SVMs). Different kinds of algorithms work very well in such scenarios such as SpectralClustering, see below:\n\n\nCode\n# Cricles\nX1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n\n# Moons\nX2 = make_moons(n_samples=1500, noise=0.05)\n\nfig, ax = plt.subplots(1, 2)\nfor i, X in enumerate([X1, X2]):\n    fig.set_size_inches(18, 7)\n    sp = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')\n    sp.fit(X[0])\n    labels = sp.labels_\n    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=labels)\nplt.suptitle('Simulated data', y=1.05, fontsize=22, fontweight='semibold')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/clustering/Kmeans-Clustering.html#conclusion",
    "href": "posts/clustering/Kmeans-Clustering.html#conclusion",
    "title": "K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks",
    "section": "Conclusion",
    "text": "Conclusion\nKmeans clustering is one of the most popular clustering algorithms and usually the first thing practitioners apply when solving clustering tasks to get an idea of the structure of the dataset. The goal of kmeans is to group data points into distinct non-overlapping subgroups. It does a very good job when the clusters have a kind of spherical shapes. However, it suffers as the geometric shapes of clusters deviates from spherical shapes. Moreover, it also doesn’t learn the number of clusters from the data and requires it to be pre-defined. To be a good practitioner, it’s good to know the assumptions behind algorithms/methods so that you would have a pretty good idea about the strength and weakness of each method. This will help you decide when to use each method and under what circumstances. In this post, we covered both strength, weaknesses, and some evaluation methods related to kmeans.\nBelow are the main takeaways:\n\nScale/standardize the data when applying kmeans algorithm.\nElbow method in selecting number of clusters doesn’t usually work because the error function is monotonically decreasing for all \\(k\\)s.\nKmeans gives more weight to the bigger clusters.\nKmeans assumes spherical shapes of clusters (with radius equal to the distance between the centroid and the furthest data point) and doesn’t work well when clusters are in different shapes such as elliptical clusters.\nIf there is overlapping between clusters, kmeans doesn’t have an intrinsic measure for uncertainty for the examples belong to the overlapping region in order to determine for which cluster to assign each data point.\nKmeans may still cluster the data even if it can’t be clustered such as data that comes from uniform distributions."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html",
    "href": "posts/mlsys/designing-ml-systems.html",
    "title": "Designing ML Systems",
    "section": "",
    "text": "In today’s rapidly evolving technological landscape, designing robust and scalable machine learning systems has become a critical skill for organizations seeking to leverage artificial intelligence effectively. While many focus on model accuracy and algorithmic improvements, the success of a machine learning system ultimately depends on its thoughtful architecture and implementation. This article explores the essential best practices that separate production-ready ML systems from experimental prototypes."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#project-scoping",
    "href": "posts/mlsys/designing-ml-systems.html#project-scoping",
    "title": "Designing ML Systems",
    "section": "Project Scoping",
    "text": "Project Scoping\n\nDuring training, we care more about throughput. However, once we deploy the model, we care more about latency.\n\nIncreased latency in production leads to reduction in customer satisfaction and conversion rates, which are more important than a relatively more accurate predictions.\nIt is common to focus on the high percentiles of the latency such as 90th/95th/99th or even 99.9th percentiles.\n\nMap ML metrics to business metrics and see how improvement in ML model would lead to an improvement in business metrics.\n\nIn some cases, ML is just a small part of the overall process that makes it hard to attribute loss/revenue to a particular component in the process.\n\nRequirements of ML systems:\n\nReliability: ML systems performs correct function in the case of failures. How do we check if predictions are not wrong?\nScalability: ML system can grow in:\n\nModel complexity\nNumber of models\nNumber of requests served by each ML model\n\nMaintainability: Code should be documented. Code, data, and artifacts should be versioned. Experiments and models should be able to be reproduced\nAdaptability: ML system should adapt quickly to change in data distribution and business requirements\n\nDeveloping an ML system is an iterative process that we typically go back and forth between different steps such as: scoping project, data engineering, model development, model deployment, monitoring and retraining, business analysis, etc.\nFor multiclass classification problems with a lot of classes, it may be helpful to frame it as a hierarchical classification where each example is first classified into few major classes then another classifier is used to classify subclasses and so on.\n\nFor each class, ML model needs at least 100 samples to learn to classify that class.\n\nFor mutlilabel classification problems, we either build a binary classifier for each label or use one model for all labels\n\nIt is more challenging to build one model with multilabel because now we need to figure out how to get predictions out of raw probabilities\n\nIf we have multiple objectives, it is better to decouple the objectives and train different model for each objective. Finally, get the final score of each prediction by combining the output from each model using different weight for each objective (which can be tuned based on business priorities). This way we can change one model without the need to retrain all the models.\nData-centric approach to ML model development tends to lead to the best results as compared to algorithm-centric approach. Data is the critical piece in obtaining good performance if we have decent architecture."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#types-of-data",
    "href": "posts/mlsys/designing-ml-systems.html#types-of-data",
    "title": "Designing ML Systems",
    "section": "Types of Data",
    "text": "Types of Data\n\nFirst-party data: Data collected by companies about their customers\nSecond-party data: Data collected by another companies about their customers\nThird-party data: Data collected on the public on who aren’t their customers\nUser data are the most messy and require heavy cleanups"
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#data-passing-modes",
    "href": "posts/mlsys/designing-ml-systems.html#data-passing-modes",
    "title": "Designing ML Systems",
    "section": "Data Passing Modes",
    "text": "Data Passing Modes\n\nDatabases. Not recommended for applications with strong latency requirements\nServices using requests such as REST or RPC. Recommended for applications that rely more on logic than data. It is called request-driven/microservice architecture, which is the most common\nReal-time transport such as Kafka. Recommended for applications that are data-heavy. It is called event-driven architecture."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#feature-types",
    "href": "posts/mlsys/designing-ml-systems.html#feature-types",
    "title": "Designing ML Systems",
    "section": "Feature Types",
    "text": "Feature Types\n\nBatch features (also called static features) are features that aren’t expected to change frequently, which are computed using batch processing.\nStreaming features (also called dynamic features) are features that changes quickly, which are computed using stream processing."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#sampling-methods",
    "href": "posts/mlsys/designing-ml-systems.html#sampling-methods",
    "title": "Designing ML Systems",
    "section": "Sampling Methods",
    "text": "Sampling Methods\n\nNonprobability sampling such as selecting the most recent data\nRandom sampling\nStratified random sampling\nWeighted sampling\nReservoir sampling\nImportance sampling"
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#models-feedback",
    "href": "posts/mlsys/designing-ml-systems.html#models-feedback",
    "title": "Designing ML Systems",
    "section": "Model’s Feedback",
    "text": "Model’s Feedback\n\nExtracting labels from feedback: User feedback goes through different stages where each stage has different volume, strength of signal, and feedback loop length\nThere is a trade-off between accuracy and speed of the feedback loop window. The shorter the feedback loop window the less accurate the labels. But also the longer the feedback loop window the longer to notice and fix model problems."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#data-labeling",
    "href": "posts/mlsys/designing-ml-systems.html#data-labeling",
    "title": "Designing ML Systems",
    "section": "Data Labeling",
    "text": "Data Labeling\n\nWeak supervision: Have no labeled data and use labeling functions to label data (Snorkel)\nSemi supervision: Have limited labeled data that are used to generate more labels through different methods such as perturbation\nTransfer learning\nActive learning: Selectively pick the samples to train the model on such as the most confident wrong samples"
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#class-imbalance",
    "href": "posts/mlsys/designing-ml-systems.html#class-imbalance",
    "title": "Designing ML Systems",
    "section": "Class imbalance",
    "text": "Class imbalance\nThere are different degrees of class imbalance.\n\nIt affects learning algorithms in many ways:\n\nAlgorithm didn’t have sufficient signals from rare classes\nAlgorithm may use simple heuristics to always output majority class to get best metric\nRare classes have typically asymmetric costs of errors such as if the X-ray is cancerous. Therefore, we might need to build a model that is more accurate on the rare classes and less accurate on the majority class(es)\n\nThe more complex the problem -&gt; the more sensitive the algorithm(s) to class imbalance. If classes are linearly separable, class imbalance has no effect.\nSolutions:\n\nData resampling methods:\n\nUndersampling\nOversampling\nSMOTE: oversampling technique\nTomek links: undersampling technique where two similar samples from opposite classes are chosen and the one from majority class is dropped\nDynamic sampling: Undersample majority class to all classes have the same number of samples and train the model on the resampled data. Then fine tune the model on the original imbalanced data\n\nAlgorithm methods:\n\nDefine cost sensitive matrix that will be used in the loss function\nClass-balanced loss where each sample has weight of misclassification that is inversely proportional to the number of samples it belongs to\nFocal loss: Focus on learning the samples that model has difficulty in classifying; i.e. has the low probability of being right\nEnsembles sometimes help with class imbalance"
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#data-augmentation",
    "href": "posts/mlsys/designing-ml-systems.html#data-augmentation",
    "title": "Designing ML Systems",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nLabel-preserving transformations such as randomly flipping images or replace word with its synonym in NLP tasks\nPerturbation that adds noise to the data but still preserve the label. BERT uses such technique where 15% of the tokens are chosen randomly and 10% of such chosen tokens will be replaced by random tokens.\nData synthesis: create data from existing data such as mixup or using templates in NLP"
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#feature-engineering",
    "href": "posts/mlsys/designing-ml-systems.html#feature-engineering",
    "title": "Designing ML Systems",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nHaving the right features give us the biggest performance boost compared to algorithmic techniques and hyperparameter-tuning. It requires domain-specific knowledge, SME, and algorithm-specific knowledge.\n\nMissing values. Be careful that not all missing values are NaN. Some systems use predefined values such as empty string or 1999 for missing values.\n\nReasons for missing values:\n\nMissing not at random: When the value is missing due to related observed value such as some type of customers don’t disclose their age.\nMissing at random: When the value is missing due to the value itself such as when people with high income mostly decide to not disclose their income.\nMissing completely at random: When there is no pattern for missing values such as customer forgot to insert the value. This is very rare in practice.\n\nHandling missing values:\n\nDeletion:\n\nColumns deletion. Not recommended for almost all cases.\nRow deletion.\n\nIt might be okay if the % of rows with missing values are very small especially for large datasets and it is missing completely at random\nIt is very risking to delete such rows if the missing are either not at random or at random\n\n\nImputation. The most common is median/mode imputation. Make sure that if the filled value is constant to not be a possible value for the feature such as -999\nIt is almost always a good idea to add an indicator for each feature that indicates whether a value is missing\n\n\nScaling. Most classical ML algorithms such as logistic regression and gradient-boosted trees as well as NN require that features are normalized.\n\nWe can normalize to have range [0, 1] or [-1, 1] if we don’t want to make any assumption on the distribution of the data. This would change the distribution of the data.\nStandardization. This would preserve the distribution. Be careful that the statistics used from training data may be out of date when used during inference if the distribution of the feature changes dramatically. Therefore, we need to train the model frequently to update such statistics.\nOther transformation: log(1 + x) or sqrt(x)\n\nDiscretization/Binning. Convert the feature into small buckets (bins) which will make the feature categorical. It is useful if values within a bucket is not that different in terms of the feature itself or the behavior of the customer. It is rarely proven to be useful. It is a challenge to pick the number of bins or the bin boundaries. Quantiles are commonly used for bins boundaries.\nEncoding categorical features. Some features have fixed categories such as marital status while others have categories that change all the time such as product names.\n\nIt is risky to encode infrequent/unknown categories to one category.\nHashing is pretty good especially if the hashing space is big enough where collision rate is low. We don’t have to worry about unknown categories in production.\n\nFeature crossing. It helps learn nonlinear relationships between features especially for models that aren’t good at learning nonlinear relationships such as logistic regression and tree-based models. The caveat is that it makes the feature space of the new feature blow up and requires more data to learn all these possible values. For example, if two features have 10 unique categories each, then the new feature would possible have 10 x 10 = 100 possible values."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#data-leakage",
    "href": "posts/mlsys/designing-ml-systems.html#data-leakage",
    "title": "Designing ML Systems",
    "section": "Data leakage",
    "text": "Data leakage\nIt refers to the case where some form of the label leaks into the feature(s) that aren’t available during inference. Common causes for data leakage:\n\nSplitting time-correlated data randomly. This is due to the fact that trends are time-correlated and the model would have access to future things it wouldn’t have otherwise.\nScaling before splitting\nFill missing values before splitting\nPoor handling data duplication before splitting. Check for duplicates or near-duplicates before/after splitting\nLeakage from data generation process. Need to check with SME because this is the hardest cause of data leakage to uncover because it requires detailed knowledge about the process.\nGroup leakage: group of data have correlated labels but are spread into different splits such as images of the same person are spread into training and test\nFeature importance is very important to avoid useless features as well as checking for data leakage. It is also helpful to interpret the model.\n\nBe careful of features that have very high importance\nIt is better to remove features if they are useless even if the model can ignore them\n\nFeature generalization:\n\nFeatures coverage (% of missing values). Check whether the coverage is similar between splits. Also, if the coverage is low, check if it adds value to the model; otherwise, delete it.\nDistribution of values between data splits. Aim for 100% overlaps of values for all features between the splits.\n\nBuild a model to predict whether a row is in train or valid using the features used to build the main model. If the new model is good enough, check the most important features because those will be the features that have different distributions between train and valid splits.\nThere is a trade-off between generalization and specifity. IS_RUSH_HOUR is more generalizable than HOUR_OF_DAY."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#model-development",
    "href": "posts/mlsys/designing-ml-systems.html#model-development",
    "title": "Designing ML Systems",
    "section": "Model development",
    "text": "Model development\n\nBaselines:\n\nRandom\nMost common\nSimple heuristics\nRandom Forest is also a good baseline\n\nEvaluation methods:\n\nPerturbation tests. Ideally, we don’t want the output of the model to change much if inputs are close to each other. We can add random noise to the test data to see how the model behaves and check its sensitivity.\nDirectional expectation tests. Check if changes in some features would lead to changes in the output in the expected direction; otherwise, the model might be learning the wrong thing.\nModel calibration. Unless the model is explicitly trained using LogLoss, the probabilities from the model won’t be calibrated. We need to calibrate the model so that the probability produced by the model should match the underlying probability distribution of the data.\nConfidence level measurement. We can avoid showing uncertain predictions and maybe involve human in the loop or ask the user for more info. For example, predict positive class if probability &gt;= 80% and negative class if probability &lt;= 20%.\nSlice-based evaluation. Evaluate the model on different slices of data to either make sure the model’s performance across different slices are acceptable OR if we care about critical slices we can confirm our requirements. This also gives us indication of how to improve the model. We can discover slices by:\n\nSimple heuristics that come from domain knowledge and heavy EDA of the data\nError analysis\nSlice finder algorithms"
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#model-deployment",
    "href": "posts/mlsys/designing-ml-systems.html#model-deployment",
    "title": "Designing ML Systems",
    "section": "Model Deployment",
    "text": "Model Deployment\n\nOnline prediction: Predictions generated on demand using either streaming features OR both streaming and batch features.\n\nPros: Always adapt to new behaviors/trends. Also don’t have to compute predictions not needed and waste compute/storage\nCons: Constrained by latency\n\nBatch prediction: Predictions generated periodically using batch features.\n\nPros: Not constrained by latency and utilize vectorization/parallel computations and allows to use very complex models that takes time to generate predictions\nCons: Model is not responsive to changes in customers behaviors/trends\n\nModel compression:\n\nKnowledge distillation\nPruning\nQuantization\n\nCloud vs Edge:\n\nCloud is much easier to setup and very flexible at the cost of privacy and network latency.\nEdge avoid network latency and privacy concerns but device must have enough memory, compute, and battery. Also, It takes more engineering work and model compression to make the model run on devices with a reasonable latency."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#causes-of-ml-system-failures",
    "href": "posts/mlsys/designing-ml-systems.html#causes-of-ml-system-failures",
    "title": "Designing ML Systems",
    "section": "Causes of ML System Failures",
    "text": "Causes of ML System Failures\n\nSoftware system failures such 1) server is down, 2) hardware failures, 3) dependency failure, 4) deployment failure. Addressing such failures require more of SWE skills than ML skills.\nML-specific failures. They are hard to detect and fix. Examples of such failures such as 1) data processing/feature engineering errors, 2) wrong hyperparameter values, 3) data distribution shifts."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#data-distribution-shifts",
    "href": "posts/mlsys/designing-ml-systems.html#data-distribution-shifts",
    "title": "Designing ML Systems",
    "section": "Data Distribution Shifts",
    "text": "Data Distribution Shifts\n\nProduction data is usually different than training data:\n\nProduction data is not the same as the training data\nProduction data is not stationary. It can change suddenly/gradually/seasonally.\n\nSometimes data shifts are due to internal errors that have nothing the data such as bugs in data pipeline, missing values are incorrectly inputted, inconsistency between training and inference features, wrong model version, etc.\nEdge case is related to model performance in which the model performs badly on such cases. ML systems need to take into account such edge/corner cases and decide what to do with them when the model face them in production.\nDegenerate feedback loop is when the model’s output affect user behavior which affects the input to the ML model which again affects its output. This is why popular recommendations get more popular because the model first recommends them, then the users are more likely to click on them than the rest which will feed back to the model which will make it recommend them more with higher probabilities.\n\nWe can use different metrics to detect degenerate feedback loop such as hit rate against popularity and the accuracy of the model for item groups with different popularities feedback loop such as hit rate against popularity and the accuracy of the model for item groups with different popularities.\nCorrecting degenerate feedback loop can be through either randomization of recommendation until we gather enough feedback of an item that reflects its true quality OR change the positions of the recommendation\n\nTypes of data shifts:\n\nCovariate shift: When \\(P(X)\\) changes but \\(P(Y/X)\\) remains the same. For example, if age distribution changes but probability of success given a certain age remains the same. It can happen for many reasons during training such as over/under sampling and selection bias. It also can happen in production such as doing campaigns to attract certain group of users.\nLabel shift: When \\(P(Y)\\) changes but \\(P(X/Y)\\) remains the same.\nConcept shift: When \\(P(Y/X)\\) changes but \\(P(X)\\) remains the same. Happens typically for seasonal/cyclic trends.\nFeature change such as possible values for a feature changes or the unit of measurement changes such as age is now in months instead of years.\nLabel schema changes such as we have more classes or possible values changes in the case of regression.\n\nWe can monitor \\(P(X)\\), \\(P(Y)\\), \\(P(Y/X)\\), and \\(P(X/Y)\\). We can monitor spatially and temporally.\n\nTemporal monitoring is more complicated and requires monitoring at different granularity using different time windows as well as using sliding statistics and cumulative statistics.\n\nRetraining: Either retrain from scratch or continue training from last checkpoint. Also, we need to decide which data to use for retraining: Last month, old data plus new data, only data that started to drift. We need to do some experiments to figure out which strategy works best for a given use case.\nMonitoring:\n\nMonitor operations-related metrics such as uptime\nMonitor ML-specific metrics such as accuracy-related metrics, predictions feedback, prediction and feature distribution, feature validation (using libraries such as Great Expectations)."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#continual-learning",
    "href": "posts/mlsys/designing-ml-systems.html#continual-learning",
    "title": "Designing ML Systems",
    "section": "Continual Learning",
    "text": "Continual Learning\nHow our deployed model gets (re)trained and the frequency.\n\nStateless retraining means training the model from scratch on every run using some predefined historical data window.\nStateful training means continue training using new data to update the model with new data. Much less compute needed and less complicated. It only works if the architecture is still the same and the features are still the same. If anyone changes, we may need to retrain from scratch.\n\nThe best use cases for such training strategy are the ones that collect natural labels from users after making predictions; Otherwise, it will be bottlenecked by the availability of labeled data.\nThe evaluation period also affects how fast we can deploy new models especially if we are dealing with imbalanced datasets.\nThe main drawback of this approach is that the new model is susceptible to attacks that would result in the model outputting catastrophic predictions.\nA model update may be triggered based on time, performance, volume, or data drifts.\nExperiments with the importance of data freshness by training the model on different datasets from the past to predict current month and see how the performance deteriorates as we go back."
  },
  {
    "objectID": "posts/mlsys/designing-ml-systems.html#testing-in-production",
    "href": "posts/mlsys/designing-ml-systems.html#testing-in-production",
    "title": "Designing ML Systems",
    "section": "Testing in Production",
    "text": "Testing in Production\n\nTest on holdout data split\nTest on the most recent data (backtest) such as last day\nShadow deployment. The main disadvantage is the compute cost because now we’re doing two predictions for each data point.\n\nDeploy the candidate model in parallel with the existing model.\nFor each incoming request, route it to both models to make predictions, but only serve the existing model’s prediction to the user.\nLog the predictions from the new model for analysis purposes.\n\nA/B Testing. We just need to figure out the sample size needed to achieve the needed statistical significance. It is the most common in the industry.\n\nDeploy the candidate model alongside the existing model.\nA percentage of traffic is routed to the new model for predictions; the rest is routed to the existing model for predictions. It’s common for both variants to serve prediction traffic at the same time. However, there are cases where one model’s predictions might affect another model’s predictions —e.g., in ridesharing’s dynamic pricing, a model’s predicted prices might influence the number of available drivers and riders, which, in turn, influence the other model’s predictions. In those cases, you might have to run your variants alternatively, e.g., serve model A one day and then serve model B the next day.\nMonitor and analyze the predictions and user feedback, if any, from both models to determine whether the difference in the two models’ performance is statistically significant\n\nCanary release. Reduces the risk of introducing the model by gradually rolling it out to the users and monitor the performance of the new (candidate) model.\n\nDeploy the candidate model alongside the existing model. The candidate model is called the canary.\nA portion of the traffic is routed to the candidate model.\nIf its performance is satisfactory, increase the traffic to the candidate model. If not, abort the canary and route all the traffic back to the existing model.\nStop when either the canary serves all the traffic (the candidate model has replaced the existing model) or when the canary is aborted.\n\nInterleaving experiment. We can randomly provide predictions/recommendations from different models to the same user and see how the user interact with these recommendations. It is different than A/B testing in the sense that we don’t split user base to receive recommendations from specific models but the same user receives recommendations from different models.\nBandits algorithm (least adopted). It is much more efficient than A/B testing in that it requires much less data to determine which model is better. It is stateful (as opposed to A/B testing which is stateless). It routes traffic based on exploitation/exploration criteria."
  },
  {
    "objectID": "posts/airflow/Dependencies-Between-Tasks.html",
    "href": "posts/airflow/Dependencies-Between-Tasks.html",
    "title": "Airflow Part 5 - Dependencies Between Tasks",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nDependencies in airflow is specified using the right shift symbol &gt;&gt;. It tells Airflow which tasks should be run first before running other tasks.\nBasic Dependenices:\n\nLinear dependenies: a &gt;&gt; b &gt;&gt; c. This means that a has to run before b which should run before c. If any task fails, the downstream task won’t run and the errors are propagated to them from preceding tasks. They can only run after the errors are fixed for that interval.\nFan-in/Fan-out dependencies: \n\nFan-in: When 1 task is dependent on &gt;= 2 tasks to run. join_datasets is fan-in task. Fan-in tasks can be specified as: [clean_sales, clean_weather] &gt;&gt; join_datasets\nFan-out: When &gt;= 2 tasks are dependent on 1 task to run. start is a fan-out task. Fan-out tasks can be specified as: start &gt;&gt; [fetch_sales, fetch_weather]\nThis is how we can specify dependencies for the DAG in the above picture:\n\nstart &gt;&gt; [fetch_sales, fetch_weather]\nfetch_sales &gt;&gt; clean_sales\nfetch_weather &gt;&gt; clean_weather\n[clean_sales, clean_weather] &gt;&gt; join_datasets\njoin_datasets &gt;&gt; train_model\ntrain_model &gt;&gt; deploy_model\n\nBranching:\n\nWe can take care of conditional execution of code paths inside the task, i.e. inside Python script in the case of PythonOperator. Depending on some condition during execution, different code paths and logic will be followed. The main disadvantages of this approach is that 1) it is hard to figure out with code path is being executed on each run from tree/graph view unless we have logging enabled, 2) Adds more complexity to the code structure, 3) May not let us use specialized operators that abstract aways a lot of the boilerplate code such as PostgresOperator. For example, if we have fetch data from either CSV or SQL database depending on condition at execution.\nWe can add BrachPythonOperatortask that takes a Python callable to determine which tasks to execute next. The Python callable has to return the task_id of the task (or list of task_id) that Airflow should execute next. Example: \n\ndef _pick_erp_system(**context):\n    if context[\"execution_date\"] &lt; ERP_SWITCH_DATE:\n       return \"fetch_sales_old\"\n    else:\n       return \"fetch_sales_new\"\n\npick_erp_system = BranchPythonOperator(\n    task_id=\"pick_erp_system\",\n    python_callable=_pick_erp_system,\n    )\n\nstart &gt;&gt; [pick_erp_system, fetch_weather]\npick_erp_system &gt;&gt; [fetch_sales_old, fetch_sales_new]\nfetch_sales_old &gt;&gt; clean_sales_old\nfetch_sales_new &gt;&gt; clean_sales_new\nfetch_weather &gt;&gt; clean_weather\n[clean_sales_old, clean_sales_new, clean_weather] &gt;&gt; join_datasets\njoin_datasets &gt;&gt; train_model\ntrain_model &gt;&gt; deploy_model\n\nSince downstream tasks only get scheduled & executed if all thier downstream tasks finished successfully, jon_datasets task will never success because with the above dependency either clean_sales_old or clean_sales_new would execute BUT NOT BOTH. We can adjust this using trigger_rule argument (default is \"all_success\" in the operatror by specifying \"non_failed\". This will run downstream task if all downstream tasks haven’t failed even if they never executed. Therefore, we can change trigger_rule for join_datasest task.\nA better approach is to create DummyOperator that does nothing but join both branches and become the upstream task before join_datasets such as below: \n\njoin_branch = DummyOperator(\n   task_id=\"join_erp_branch\",\n   trigger_rule=\"none_failed\"\n    )\n\nstart &gt;&gt; [pick_erp_system, fetch_weather]\npick_erp_system &gt;&gt; [fetch_sales_old, fetch_sales_new]\nfetch_sales_old &gt;&gt; clean_sales_old\nfetch_sales_new &gt;&gt; clean_sales_new\n[clean_sales_old, clean_sales_new] &gt;&gt; join_branch\nfetch_weather &gt;&gt; clean_weather\n[joen_erp_branch, clean_weather] &gt;&gt; join_datasets\njoin_datasets &gt;&gt; train_model\ntrain_model &gt;&gt; deploy_model\nConditional tasks. Sometimes we only want to execute a task if a condition is true, otherwise, the task should be skipped. For example, if we want to only deploy the model on the most recent data and we don’t want deploy_model to always execute if we are doing backfilling -&gt; Create a conditional upstream task that checks the condition and raise Exception if the condition is False so deploy_model will be skipped. \n\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.operators.python import PythonOperator\ndef _latest_only(**context):\n    # execution_time is the first time in the schedule interval\n    # So following_schedule is the next execution_date\n    left_window = context[\"dag\"].following_schedule(context[\"execution_date\"])\n    right_window = context[\"dag\"].following_schedule(left_window)\n    now = pendulum.now(\"utc\")\n    # Since execution of DAG starts after last time point passed of the \n    # schedule interval -&gt; \n    if not left_window &lt; now &lt;= right_window:\n        raise AirflowSkipException(\"Not the most recent run!\")\n\nlatest_only = PythonOperator(task_id=\"latest_only\", python_callable=_latest_only, dag=dag)\nlatest_only &gt;&gt; deplpy_model\n\nTrigger rules: The triggering of Airflow tasks is controlled by the trigger rules which define the behavior of tasks and allow us to configure each task to respond to different situations.\n\nBe default, scheduler picks tasks ready to be executed when all its upstreams tasks were executed successfully and put it in the execute queue. The scheduler always checks downstream tasks if they are ready by checking all their downstream task completion state. Once there is a slot/worker, it will be executed. If any of the upstream tasks failed, it would have failed state and the upstream task won’t be scheduled and have state=upstream_failed. This is called progagation because the error is propagated from upstream to downstream tasks. This is the default trigger_rule which is all_success. If any of the down\nIf any of the upstream task is skipped -&gt; downstream task will be skipped as well (propagation).\nTrigger rules:\n\nall_success: Triggers when all parent tasks have executed successfully\nall_failed: Triggers when all parent tasks have failed or due to failure in their parents\nall_done: Triggers when all parent tasks finished executing regardless of their state. Good to cleanup and shutdown resources regardless of the execution state of the workflow\none_failed: Triggers when at least 1 parent task failed and doesn’t wait for other parent tasks to finish\none_success: Triggers when at least 1 parent task succeeded and doesn’t wait for other parent tasks to finish\nnone_failed: Triggers if no parent task has failed but either completed successfully or skipped\nnone_skipped: Triggers if no parent task has skipped but either completed successfully or failed\ndummy: Triggers regardless of the parent tasks state. Useful for testing"
  },
  {
    "objectID": "posts/airflow/Best-Practices.html",
    "href": "posts/airflow/Best-Practices.html",
    "title": "Airflow Part 8 - Best Practices",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nStick to coding style conventions by using tools like flake8, pylint, black\nThere are two ways to define DAGs. Stick to one of them:\n\nWith context manager:\n\nwith DAG(...) as dag:\n  task1 = PythonOperator(...)\n  task2 = PythonOperator(...)\n\nTraditional:\n\ndag = DAG(...)\ntask1 = PythonOperator(..., dag=dag)\ntask2 = PythonOperator(..., dag=dag)\nThere are also multiple ways to define dependencies. Stick to one of them:\n\ntask1 &gt;&gt; task2\ntask1 &lt;&lt; task2\n[task1] &gt;&gt; task2\ntask1.set_downstream(task2)\ntask2.set_upstream(task1)\n\nWhen loading config files, make sure to understand where the loading happens:\n\nAt the top level on the scheduler\nAt the DAG level when it is parsed\nOr when the DAG is executing -&gt; in the worker\n\nAvoid doing any computation in DAG definition:\n\nAt the top level, it will be computed every time the DAG is loaded\nIn the DAG definition, it will be executed every time the DAG is parsed by the scheduler\nIn the task, it will be computed when the task is executed on the worker machine\n\nFetch credentials within the task, so they are only fetched once the task is executed\nUse factory methods to generate DAGs or set of tasks that are almost typical with few minor changes. Example:\n\ndef generate_tasks(dataset_name, raw_dir, processed_dir, preprocess_script, output_dir, dag):\n    raw_path = os.path.join(raw_dir, dataset_name, \"{ds_nodash}.json\") \n    processed_path = os.path.join(\n    processed_dir, dataset_name, \"{ds_nodash}.json\" )\n    output_path = os.path.join(output_dir, dataset_name, \"{ds_nodash}.json\")\n    fetch_task = BashOperator(\n        task_id=f\"fetch_{dataset_name}\",\n        bash_command=f\"echo 'curl http://example.com/{dataset_name}.json{raw_path}.json'\", dag=dag,\n        )\n    preprocess_task = BashOperator(\n        task_id=f\"preprocess_{dataset_name}\",\n        bash_command=f\"echo '{preprocess_script} {raw_path} {processed_path}'\", dag=dag,\n    )\n    export_task = BashOperator(\n        task_id=f\"export_{dataset_name}\",\n        bash_command=f\"echo 'cp {processed_path} {output_path}'\", dag=dag,\n       )\n        fetch_task &gt;&gt; preprocess_task &gt;&gt; export_task\n    return fetch_task, export_task\n\nwith DAG(\n    dag_id=\"01_task_factory\",\n    start_date=airflow.utils.dates.days_ago(5),\n    schedule_interval=\"@daily\",\n) as dag:\n    for dataset in [\"sales\", \"customers\"]:\n        generate_tasks(\n            dataset_name=dataset,\n            raw_dir=\"/data/raw\", \n            processed_dir=\"/data/processed\", \n            output_dir=\"/data/output\",\n            preprocess_script=f\"preprocess_{dataset}.py\", dag=dag\n        )\n\nWe can use TaskGroup to group related tasks into groups that will help us navigating the DAG in the UI. This is very helpful when DAGs become very complicated\nCreate new DAGs for big changes such as renaming/removing tasks or changing the schedule_date/interval so we can keep the historical info about old DAGs and not confuse the scheduler. Scheduler database has instances of the runs of each DAG\nMake sure that tasks are idempotenet -&gt; Regardless when they run, If given the same input the should produce the same output. Therefore, be careful when writing data. We may want to overwrite or upsert to avoid appending the same data\n\nAlso, tasks should not have side effects\n\nAvoid writing intermediate results on local filesystem because each task runs independently (and mostly on different machines) -&gt; Use cloud shared storage such as Amazon’s S3 bucket where all workers can access it\nWe can use SLAs on each DAG/task where Airflow will notify if they don’t finish within SLA. DAG takes sla argument"
  },
  {
    "objectID": "posts/airflow/What-Is-Airflow.html",
    "href": "posts/airflow/What-Is-Airflow.html",
    "title": "Airflow Part 1 - What is Airflow?",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\nAirflow is workflow orchestration tool that is written in Python at Airbnb. The workflow is also written in Python. It defines the workflow as a DAG so it is easy to determine the dependencies between tasks. If any task failed, we don’t need to rerun the workflow again, we can just run the failed task and all the tasks that depend on it. We can also do backfilling by running the pipeline/tasks for time intervals in the past.\nAirflow consists of mainly three components:\n\nThe Airflow scheduler: Parses DAGs, checks their schedule interval, and (if the DAGs’ schedule has passed) starts scheduling the DAGs’ tasks for execution by passing them to the Airflow workers.\nThe Airflow workers: Pick up tasks that are scheduled for execution and execute them. As such, the workers are responsible for actually “doing the work.”\nThe Airflow webserver: Visualizes the DAGs parsed by the scheduler and provides the main interface for users to monitor DAG runs and their results. It uses the metadata database which has all the logs and other metadata about tasks and workflows.\n\nConceptually, the scheduling algorithm follows the following steps:\n\nFor each open (= uncompleted) task in the graph, do the following: – For each edge pointing toward the task, check if the “upstream” task on the other end of the edge has been completed. – If all upstream tasks have been completed, add the task under consideration to a queue of tasks to be executed.\nExecute the tasks in the execution queue, marking them completed once they finish performing their work.\nJump back to step 1 and repeat until all tasks in the graph have been completed.\n\nThe scheduler in Airflow runs roughly through the following steps:\n\nOnce users have written their workflows as DAGs, the files containing these DAGs are read by the scheduler to extract the corresponding tasks, dependen- cies, and schedule interval of each DAG.\nFor each DAG, the scheduler then checks whether the schedule interval for the DAG has passed since the last time it was read. If so, the tasks in the DAG are scheduled for execution.\nFor each scheduled task, the scheduler then checks whether the dependencies (= upstream tasks) of the task have been completed. If so, the task is added to the execution queue.\nThe scheduler waits for several moments before starting a new loop by jumping back to step 1.\n\n\nAirflow can be run:\n\nIn python virtual environment\nInside Docker containers. In this case, Airflow scheduler, webserver, and metastore would run each in separate containers\n\nThe main disadvantages of Airflow are:\n\nIt can get very messy and hard to understand for complex workflows\nIt is best used for batch/recurring jobs NOT streaming jobs\nMainly support static DAGs and hard to implement dynamic DAGs. Imagine you’re reading from a database and you want to create a step to process each record in the database (e.g. to make a prediction), but you don’t know in advance how many records there are in the database, Airflow won’t be able to handle that.\nIt is monolithic, which means it packages the entire workflow into one container. If two different steps in your workflow have different requirements, you can, in theory, create different containers for them using Airflow’s DockerOperator, but it’s not that easy to do so.\nAirflow’s DAGs are not parameterized, which means you can’t pass parameters into your workflows. So if you want to run the same model with different learning rates, you’ll have to create different workflows.\n\nTo setup Airflow locally inside Python virtual env:\n\npip install apache-airflow\nairflow init db # Initialize metastore locally using SQLite; not recommended for production\nairflow users create –username admin –password admin –firstname Anonymous –lastname Admin –role Admin –email admin@example.org # Create user\nairflow webserver # Start web server to use web UI\nairflow scheduler # Start scheduler, don’t use sequential in production"
  },
  {
    "objectID": "posts/airflow/Sharing-Data-Between-Tasks.html",
    "href": "posts/airflow/Sharing-Data-Between-Tasks.html",
    "title": "Airflow Part 6 - Sharing Data Between Tasks",
    "section": "",
    "text": "Note\n\n\n\nOther than my experience and the documentation, the main resource behind this post and figures is the fantastic book: Data Pipelines with Apache. Airflow.\n\n\n\nIt is meant to exchange messages between tasks, which is some form of shared state\nWe can use dag instance to push/pull data between tasks:\n\nconext[\"dag_instance\"].xcom_push(key=\"data_name\", value=\"value\") to push data to metastore. It also store the dag_id, task_id, & execution_date.\nconext[\"dag_instance\"].xcom_pull(key=\"data_name\") which pull the shared data. We can also specify dag_id and execution_date.\nWe can also access push/pull methods in templates using task_instance.xcom_push() or task_instance.xcom_pull()\nWe can view the shared data on the UI by going to Admin -&gt; XComs\n\nLimitations:\n\nXComs data will be pickled and stored in the database -&gt; The objects have to be serializable\nSize limitations:\n\nSQLite—Stored as BLOB type, 2GB limit\nPostgreSQL—Stored as BYTEA type, 1 GB limit\nMySQL—Stored as BLOB type, 64 KB limit\n\nIt create hidden dependency between tasks because now the task the pushes the shared state has to push the data before the task that pulls the data. Airflow won’t manage/respect this dependency the developer has to document this and make sure this is not an issue based on the tasks’ order\n\nDue to its limitations in terms of size, we can create custom backends for XComs by defining a class that inherits from BaseXCom and implements two static methods. Airflow will use this class. It can be added to xcom_backend parameter in the Airflow configWe can use cheap/large storage services on the cloud such as Amazon S3, Azure Blob Storage, or Google GCS.\n\nfrom typing import Any\nfrom airflow.models.xcom import BaseXCom\n\nclass CustomXComBackend(BaseXCom):\n    \n    @staticmethod\n    def serialize(value: Any):\n        ...\n    \n    @staticmethod\n    def deserialize(result):\n        ...\n\nIf most of tasks are PythonOperators, we can use Taskflow API that takes care of passing state between tasks and avoid the boilerplate code that we have to write with regular API. We need to just decorate the function that we use in the PythonOperator with @task and Airflow will take care of the rest by passed XCom data between tasks. Example:\n\n\nfrom airflow.decorators import task\n\n\nwith DAG(...) as dag:\n    start = DummyOperator(task_id=\"start\")\n    start &gt;&gt; fetch_sales\n    start &gt;&gt; fetch_weather\n    fetch_sales &gt;&gt; clean_sales\n    fetch_weather &gt;&gt; clean_weather\n    [clean_sales, clean_weather] &gt;&gt; join_datasets\n    \n    @task\n    def train_model():\n        model_id = str(uuid.uuid4())\n        # Airflow will figure out that the return value is XCom\n        # and would take care of pushing it\n        return model_id\n\n    @task\n    def deploy_model(model_id: str):\n        # Airflow would realize that this task uses XCom so it passes\n        # it from XCom\n        print(f\"Deploying model {model_id}\")\n\nmodel_id = train_model()\ndeploy_model(model_id)\n\n# Now train_model and deploy_model will be new tasks\n# with explicit dependeny. \n# The task type is PythonDecoratedOperator\njoin_datasets &gt;&gt; model_id\n\nAny data passed between Taskflow-style tasks will be stored as XComs and subject to the same limitations of XCom\nThe main limitation of Taskflow API is that it is still only for PythonOperators"
  },
  {
    "objectID": "posts/nlp/GPT2-From-Scratch.html",
    "href": "posts/nlp/GPT2-From-Scratch.html",
    "title": "Coding GPT2/3 (124M) From Scratch",
    "section": "",
    "text": "Introduction\nTODO\n\nIn the first iteration of the training, we want all tokens to have almost same probability and thus the loss on each one would be the same. The probability of each token would be \\(1/vocab\\_sz\\) -&gt; \\(loss \\approx -log(1/vocab\\_sz)\\) because the probability distribution would be diffused.\n3e-4 learning rate is good for AdamW optimizer for debugging\nWe want the model to overfit 1 batch to make sure it is running correctly\nWeight sharing between token embedding and the final linear layer (also called classifier or LM head) because we want the tokens that are semantically similar to have similar probability when predicting next token.\n\nThis also has huge advantage on computational efficiency as those matrices have a lot parameters. For GPT2, each one has \\(50257 * 768 \\approx 38.5M\\) which is \\(1/3\\) of the GPT2 model.\nAs a result of Weight sharing, gradient update will be addition from the two branches: classifier and token embedding\n\nFor tokens that don’t appear in the training data, we want their probabilities to be very close to zero\nCPU can continue running even if Cuda kernels are not done. This is because CPU is kinda scheduling the kernels on the GPU and doesn’t wait for them to finish -&gt; Use torch.cuda.synchronize() so CPU only presumes when scheduled kernels finish execution to get better timings\n\n\n\nImplementation\n\n\nCode\nimport inspect\nimport math\nimport os\nimport sys\nimport time\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom typing import Callable\n\nimport tiktoken\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as opt\nfrom torch.distributed import destroy_process_group, init_process_group\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\n\n\nCode\ndef annealer(func: Callable):\n    wraps(func)\n\n    def annealer_wrapper(*args, **kwargs):\n        return partial(func, *args, **kwargs)\n\n    return annealer_wrapper\n\n\n@annealer\ndef lin_sched(start, end, pos):\n    \"\"\"Linear scheduler.\"\"\"\n    return start + (end - start) * pos\n\n\n@annealer\ndef cos_sched(start, end, pos):\n    \"\"\"Cosine scheduler.\"\"\"\n    return start + (1 + math.cos(math.pi * (1 - pos))) * (end - start) / 2\n\n\ndef combine_scheds(pcts, scheds):\n    \"\"\"\n    Combine multiple schedulers, each run for a given percentage of the\n    training process.\n    \"\"\"\n    assert len(pcts) == len(scheds), \"Each scheduler should have its `pct`.\"\n    assert sum(pcts) == 1.0, \"Sum of the `pcts` should be equal to 1.\"\n    pcts = torch.tensor([0] + listify(pcts))\n    assert (pcts &gt;= 0).all(), \"All percentages should be non-negative.\"\n    pcts = torch.cumsum(pcts, 0)\n\n    def _inner(pos):\n        idx = (pos &gt;= pcts).nonzero().max()\n        actual_pos = (pos - pcts[idx]) / (pcts[idx + 1] - pcts[idx])\n        return scheds[idx](actual_pos)\n\n    return _inner\n\n\n\n@dataclass\nclass GPTConfig:\n    block_sz: int = 1024\n    vocab_sz: int = (\n        50257  # 50000 BPE merges + 256 byte tokens + 1 for &lt;|endoftext|&gt; token\n        # which will delimits different documents. This token's index is 50256\n    )\n    n_layer: int = 12\n    n_embd: int = 768\n    n_head: int = 12\n    lr: int = 3e-4\n    batch_sz: int = 4\n\n\nclass MLP(nn.Module):\n    def __init__(self, config: GPTConfig):\n        # Point-wise feed-forward network that applies non-linearity\n        # on every token sepearately. THERE IS NO INTERACTION BETWEEN TOKENS\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n        self.gelu = nn.GELU(approximate=\"tanh\")\n        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n\n    def forward(self, x):\n        return self.c_proj(self.gelu(self.c_fc(x)))\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        # NOTE: Bias is not needed when we use Pytorch's Flash attention\n        # self.register_buffer(\n        #     \"bias\",\n        #     torch.tril(torch.ones(config.block_sz, config.block_sz)).view(\n        #         1, 1, config.block_sz, config.block_sz\n        #     ),\n        # )\n\n    def forward(self, x):\n        B, T, C = x.shape\n        qkv = self.c_attn(x)\n        # q/k/v is B x T x n_embd each\n        q, k, v = torch.split(qkv, self.n_embd, dim=-1)\n        # Reshape q/k/v to B x n_head x T x (n_embd / n_head)\n        # So each head would be learning different kind of\n        # relationships\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        # attn is B x T x T\n        # attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.shape[-1]))\n        # # Mask out future tokens\n        # attn = attn.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n        # attn = F.softmax(attn, dim=-1)\n        # # y is B x T x n_embd\n        # y = attn @ v\n        # Uses Flash attention that never materialize attention matrices for\n        # each head and is aware of the memory hierarchy and tries to reduce\n        # read/writes with more FLOPs -&gt; Speed up since we're memory bound\n        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.c_proj(y)\n\n\nclass Block(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n        self.attn = CausalSelfAttention(config)\n\n    def forward(self, x):\n        # Use Pre-layer normalization which deviates from the\n        # transformer original paper that uses post-layer normalization.\n        # This should help stabilize training\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass GPT2(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(config.vocab_sz, config.n_embd),\n                wpe=nn.Embedding(config.block_sz, config.n_embd),\n                h=nn.ModuleList(\n                    [Block(config) for _ in range(config.n_layer)]\n                ),\n                # Final layer norm after all transformer layers\n                ln_f=nn.LayerNorm(config.n_embd),\n            )\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_sz, bias=False)\n\n        # Weigth sharing between the token embedding layer and\n        # last linear layer (LM head classifier). The rationale is\n        # that tokens that are semantically similar to each other in\n        # the embedding space should have similar probabilities in the\n        # softmax of the LM head layer\n        # Also, these matrices are one of the biggest matrices in the the model\n        # This means, for model like GPT2, we save almost 30 % of the parameters\n        # by sharing the weight matrices (50257 * 768) / 124M = ~31%\n        self.transformer.wte.weight = self.lm_head.weight\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        # The following initialization comes from gpt2 src code\n        # NOTE: Becuase token embedding and classifier weights are shared,\n        # out initialization logic will initialize the weight matrix twice\n        # but shouldn't be an issue since they're being initialized with the\n        # same std and mean\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            # We're changing std because residual path affect std\n            # by increasing it on every layer so we need to adjust\n            # it so we still have the same std = 0.02\n            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n                # `2` here becauase every layer has two blocks:\n                # Attention block and MLP block\n                std *= (2 * self.config.n_layer) ** -0.5\n            nn.init.normal_(module.weight, std=std)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            # We're initializing the token and positional embeddings\n            # with the same std but the paper initialized the positional\n            # embedding with std = 0.01\n            nn.init.normal_(module.weight, std=0.02)\n\n    def forward(self, x, targets=None):\n        T = x.shape[-1]\n        assert (\n            T &lt;= self.config.block_sz\n        ), f\"Sequence length must be &lt;= {self.config.block_sz}, got {T}\"\n        pos_emb = self.transformer.wpe(\n            torch.arange(0, T, dtype=torch.long, device=x.device)\n        )\n        tok_emb = self.transformer.wte(x)\n        x = pos_emb + tok_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        # logits is B x T x vocab_sz\n        logits = self.lm_head(x)\n        loss = None\n        if targets is not None:\n            # F.cross_entropy expects the 2nd dimension to be probabilities\n            loss = F.cross_entropy(\n                logits.view(-1, self.config.vocab_sz), targets.view(-1)\n            )\n        return logits, loss\n\n    def configure_optimizer(self, weight_decay, lr, device):\n        params_dict = {\n            pn: p for pn, p in self.named_parameters() if p.requires_grad\n        }\n        decay_params = [p for p in params_dict.values() if p.ndim &gt;= 2]\n        nondecay_params = [p for p in params_dict.values() if p.ndim &lt; 2]\n        params_groups = [\n            {\"params\": decay_params, \"weight_decay\": weight_decay},\n            {\"params\": nondecay_params, \"weight_decay\": 0.0},\n        ]\n        fused_available = \"fused\" in inspect.signature(opt.AdamW).parameters\n        use_fused = fused_available and \"cuda\" in device\n        return opt.AdamW(\n            params_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n        )\n\n    @torch.no_grad\n    def generate(self, idxs: torch.tensor, max_tokens: int = 5):\n        for i in range(max_tokens):\n            # x would be B x T x vocab_sz\n            idxs = idxs[:, -self.config.block_sz :]\n            logits, _ = self(idxs)\n            # Get probs for last token to predict next token\n            # This would be B x vocab_sz\n            logits = logits[:, -1, :]\n            # Apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)\n            # Each would be B x 50\n            topk_probs, topk_idxs = torch.topk(probs, 50, dim=-1)\n            # idx is B x 1\n            idx = torch.multinomial(topk_probs, 1)\n            idx = torch.gather(topk_idxs, -1, idx)\n            idxs = torch.cat([idxs, idx], dim=1)\n        return idxs\n\n\nclass DataLoaderLight:\n    def __init__(\n        self,\n        batch_sz: int,\n        block_sz: int,\n        process_rank: int = 0,\n        number_processes: int = 1,\n    ) -&gt; None:\n        self.batch_sz = batch_sz\n        self.block_sz = block_sz\n        self.process_rank = process_rank\n        self.number_processes = number_processes\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        encoder = tiktoken.get_encoding(\"gpt2\")\n        self.tokens = torch.tensor(encoder.encode(text), dtype=torch.long)\n        self.current_pos = batch_sz * block_sz * process_rank\n\n    def __len__(self):\n        return len(self.tokens) // (self.batch_sz * self.block_sz)\n\n    def next_batch(self):\n        buf = self.tokens[\n            self.current_pos : self.current_pos\n            + self.batch_sz * self.block_sz\n            + 1\n        ]\n        x = buf[:-1].view(self.batch_sz, self.block_sz)\n        y = buf[1:].view(self.batch_sz, self.block_sz)\n        # Each process will process batch_sz x block_sz tokens in each\n        # iteration -&gt; with number_processes processes, total tokens processed\n        # in each iteration is batch_sz x block_sz x number_processes. In the\n        # case of one process, total tokens would be batch_sz x block_sz\n        self.current_pos += (\n            self.batch_sz * self.block_sz * self.number_processes\n        )\n        if self.current_pos + (\n            self.batch_sz * self.block_sz * self.number_processes\n        ) + self.number_processes &gt; len(self):\n            self.current_pos = 0\n        return x, y\n\n\nif __name__ == \"__main__\":\n    ###########\n    # Distributed Data Parallel\n    ###########\n    # torchrun command sets the following environment variables:\n    # RANK: Id of the process in the process group. It is an int 0-WORLD_SIZE\n    # LOCAL_RANK: In the case of multi-nodes, LOCAL_RANK is the id of\n    #             the process in the same node\n    # WORLD_SIZE: Total number of processes\n    ddp = int(os.getenv(\"RANK\", -1)) != -1  # Check if it is a ddp run\n    if ddp:\n        # DDP requires CUDA so we need to set the device for each process\n        # so only one process can run per device\n        assert torch.cuda.is_available(), \"DDP requires CUDA\"\n        init_process_group(backend=\"nccl\")\n        ddp_rank = int(os.getenv(\"RANK\"))\n        ddp_local_rank = int(os.getenv(\"LOCAL_RANK\"))\n        ddp_world_size = int(os.getenv(\"WORLD_SIZE\"))\n        device = f\"cuda:{ddp_local_rank}\"\n        torch.cuda.set_device(device)\n        # master process will do more things such as checkpointing and logging\n        # while other processes would assist in the computations\n        master_process = ddp_rank == 0\n    else:\n        ddp_rank = 0\n        ddp_local_rank = 0\n        ddp_world_size = 1\n        master_process = True\n        if torch.cuda.is_available():\n            device = \"cuda\"\n        elif torch.backends.mps.is_built():\n            device = \"mps\"\n            torch.mps.manual_seed(1337)\n        else:\n            device = \"cpu\"\n    print(device)\n    torch.manual_seed(1337)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(1337)\n    ##########\n    # Initialize model and optimizer\n    ##########\n    # Everything in GPUs is a power of 2 such as tiling ops\n    # So try to always have matrices be power of 2. Here we\n    # change the vocab_sz by rounding it up to the closest\n    # number that is power of. This will increase space overhead\n    # but would speed up computations\n    model = GPT2(GPTConfig(vocab_sz=50304)).to(device)\n    # Speed up model by building statis graph that analyzes all ops\n    # and optimizes them such as fusing some of them to avoid unnecessary\n    # trips to memory\n    # model = torch.compile(model)\n    if ddp:\n        model = DDP(model, device_ids=[ddp_local_rank])\n    raw_model = model.module if ddp else model\n    max_lr = 3e-4\n    min_lr = max_lr * 0.1\n    warmup_steps = 10\n    max_steps = 50\n    sched = combine_scheds(\n        [warmup_steps / max_steps, 1 - (warmup_steps / max_steps)],\n        [lin_sched(min_lr, max_lr), cos_sched(max_lr, min_lr)],\n    )\n    # TODO: Move building of optimizer inside GPT2\n    # Don't decay biases and 1D tensors such as layer norm tensors (scales and\n    # biases) linear layer's tensors\n    # optimizer = opt.AdamW(\n    #     model.parameters(), lr=GPTConfig.lr, betas=(0.9, 0.95), eps=1e-8\n    # )\n    optimizer = raw_model.configure_optimizer(\n        weight_decay=0.1, lr=max_lr, device=device\n    )\n\n    ##########\n    # Run training loop\n    #########\n    # NOTE: In order to run 0.5M tokens per fwd/bwd iteration, we need to\n    # use gradient accumulation because we can't fit it in almost any commodity\n    # GPY -&gt; We only do backward after we loop through ~0.5M tokens.\n    total_batch_sz = 2**19  # closest number to 0.5M\n    assert (\n        total_batch_sz\n        % (GPTConfig.batch_sz * GPTConfig.block_sz * ddp_world_size)\n        == 0,\n        \"total batch size must be divisible by micro batch_sz x block_sz x ddp_world_size\",\n    )\n    grad_accum_steps = total_batch_sz // (\n        GPTConfig.batch_sz * GPTConfig.block_sz * ddp_world_size\n    )\n    if master_process:\n        print(f\"Total desired batch size: {total_batch_sz}\")\n        print(f\"Calculated gradient accumulation steps: {grad_accum_steps}\")\n\n    train_dl = DataLoaderLight(\n        batch_sz=GPTConfig.batch_sz, block_sz=GPTConfig.block_sz\n    )\n    # Pytorch will use TensorFloat32 if available, else use FP32\n    # But the weights will still be stored as FP32. It is just the\n    # operations would be executed as TF32 if available\n    torch.set_float32_matmul_precision(\"high\")\n\n    for step in range(max_steps):\n        start = time.time()\n        x, y = train_dl.next_batch()\n        x = x.to(device)\n        y = y.to(device)\n        # code.interact(local=locals())\n        optimizer.zero_grad()\n        loss_accum = 0.0\n        for macro_step in range(grad_accum_steps):\n            if device == \"cuda\":\n                # Tensors that will be greatly affected by less precission such\n                # loss, layernorm would still be in FP32\n                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n                    logits, loss = model(x, y)\n            else:\n                logits, loss = model(x, y)\n            # Just accumulation gradients yield to summation of objective but\n            # we want mean so we weight each loss by 1/grad_accum_steps\n            loss /= grad_accum_steps\n            loss_accum += loss.detach()\n            # To avoid syncing the gradients between the processes after every\n            # macro step, we disable it and only allows the sync up of\n            # gradients after we finish all gradient accumulation in each\n            # process\n            if ddp:\n                model.require_backward_grad_sync = (\n                    macro_step == grad_accum_steps - 1\n                )\n            loss.backward()\n        # Each process would have its own loss_accum tensor, so to get the\n        # average loss_accum across all processes, we to compute the average of\n        # all loss_accum in all processes\n        if ddp:\n            dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n        # Clips gradient to global norm. It is very useful to avoid having a\n        # very high loss for some batch(es) that would have very high loss\n        # which would learn to high gradients and huge updates\n        # In the beginning of training it is normal to have high norms as the\n        # model initialized randomly\n        norm = nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # TODO: Use ParamScheduler from `cmn_ai`\n        lr = sched(step / max_steps)\n        for pg in optimizer.param_groups:\n            pg[\"lr\"] = lr\n        optimizer.step()\n        end = time.time()\n        elapsed_time = end - start\n        token_per_sec = (\n            GPTConfig.batch_sz\n            * GPTConfig.block_sz\n            * grad_accum_steps\n            * ddp_world_size\n        ) / (elapsed_time)\n        print(\n            f\"step {step}, loss: {loss.item()}, lr {lr:.4e}, norm: {norm:.2f}, time: {elapsed_time:.2f}s, tok/sec: {token_per_sec:.2f}\"\n        )\n\n    if ddp:\n        # Kills all processes\n        destroy_process_group()\n\n\n\nConclusion\nTODO\n\n\nResources\n\nGPT2: Language Models are Unsupervised Multitask Learners\nGPT3: Language Models are Few-Shot Learners"
  },
  {
    "objectID": "posts/nlp/BPE-Tokenizer.html#detailed-walk-through",
    "href": "posts/nlp/BPE-Tokenizer.html#detailed-walk-through",
    "title": "Byte-level Byte-Pair Encoding (BPE)",
    "section": "Detailed Walk-through",
    "text": "Detailed Walk-through\n\ntext = \"A Programmer’s Introduction to Unicode March 3, 2017\"\ntokens = text.encode(\"utf-8\")  # raw bytes\ntokens = list(\n    tokens\n)  # convert to a list of integers in range 0..255 for convenience\n\n\ndef get_stats(ids):\n    counts = {}\n    for pair in zip(ids, ids[1:]):\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\n\n[(k,v) for k,v in get_stats(tokens).items()][:10]\n\n[((65, 32), 1),\n ((32, 80), 1),\n ((80, 114), 1),\n ((114, 111), 2),\n ((111, 103), 1),\n ((103, 114), 1),\n ((114, 97), 1),\n ((97, 109), 1),\n ((109, 109), 1),\n ((109, 101), 1)]\n\n\n\ndef merge(ids, pair, idx):\n    newids = []\n    i = 0\n    while i &lt; len(ids):\n        if i &lt; len(ids) - 1 and tuple(ids[i : i + 2]) == pair:\n            newids.append(idx)\n            i += 2\n        else:\n            newids.append(ids[i])\n            i += 1\n    return newids\n\n\nvocab_sz = 276  # i.e. we want to have only 20 merges\nn_merges = vocab_sz - 256\nids = list(tokens)\nmerges = {}\nfor i in range(n_merges):\n    stats = get_stats(ids)\n    top_pair = max(stats, key=stats.get)\n    idx = 256 + i\n    ids = merge(ids, top_pair, idx)\n    merges[top_pair] = idx\n    break\n\n\ndef encode(text, merges):\n    tokens = list(text.encode(\"utf-8\"))\n    while len(tokens) &gt;= 2:\n        stats = get_stats(tokens)\n        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n        if pair not in merges:\n            break\n        tokens = merge(tokens, pair, merges[pair])\n    return tokens\n\n\ndef decode(ids, vocab):\n    tokens = b\"\".join(vocab[idx] for idx in ids)\n    text = tokens.decode(\"utf-8\", errors=\"replace\")\n    return text\n\n\nvocab = {idx: bytes([idx]) for idx in range(256)}\nfor (p0, p1), idx in merges.items():\n    vocab[idx] = vocab[p0] + vocab[p1]\n\n\ntext == decode(encode(text, merges), vocab)\n\nTrue"
  },
  {
    "objectID": "posts/nlp/BPE-Tokenizer.html#clean-implementation",
    "href": "posts/nlp/BPE-Tokenizer.html#clean-implementation",
    "title": "Byte-level Byte-Pair Encoding (BPE)",
    "section": "Clean Implementation",
    "text": "Clean Implementation\n\nclass BPETokenizer:\n    \"\"\"Byte-pair encoder.\"\"\"\n\n    def __init__(self, vocab_sz: int):\n        \"\"\"\n        Args:\n            vocab_sz (int): Vocabulary size.\n        \"\"\"\n        self.vocab_sz = vocab_sz\n        self.vocab = {}\n        self.merges = {}\n\n    def train(self, text: Iterable[str]):\n        \"\"\"Train Byte-pair encoder.\"\"\"\n        ids = list(text.encode(\"utf-8\"))\n        for i in range(256, self.vocab_sz):\n            stats = self._get_stats(ids)\n            pair = max(stats, key=stats.get)\n            idx = i\n            self.merges[pair] = idx\n            ids = self._merge(ids, pair, idx)\n        self.vocab = self._build_vocab(ids)\n\n    def encode(self, text):\n        \"\"\"Encode string to bytes using vocabulary built during training.\"\"\"\n        ids = list(text.encode(\"utf-8\"))\n\n        # If text is empty or has one character -&gt; it is already encoded from previous step\n        while len(ids) &gt;= 2:\n            # stats is used only for getting pairs next to each other\n            stats = self._get_stats(ids)\n            # Because we built vocab (and merges) bottom-up, we need to encode\n            # idx from smallest index because some later pairs depend on pairs\n            # occured before\n            # If a pair doesn't exist, it wouldn't participate in the list\n            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n            if pair not in self.merges:\n                break  # No more pairs to merge\n            idx = self.merges[pair]\n            ids = self._merge(ids, pair, idx)\n        return ids\n\n    def decode(self, tokens: Iterable[int]):\n        \"\"\"Decode tokens into string using the vocabulary built during training.\"\"\"\n        tokens = b\"\".join(self.vocab[idx] for idx in tokens)\n        # It is important to replace tokens that were not seen during training\n        # with `?`; otherwise, it would fail\n        return tokens.decode(\"utf-8\", errors=\"replace\")\n\n    def _get_stats(self, ids: Iterable[int]):\n        \"\"\"Get pair counts.\"\"\"\n        counts = {}\n        for pair in zip(ids, ids[1:]):\n            counts[pair] = counts.get(pair, 0) + 1\n        return counts\n\n    def _merge(self, ids: Iterable[int], pair: Iterable[int], idx: int):\n        \"\"\"Merge pairs that match `pair` with new index `idx`.\"\"\"\n        newids = []\n        i = 0\n        while i &lt; len(ids):\n            if i &lt; len(ids) - 1 and tuple(pair) == tuple(ids[i : i + 2]):\n                newids.append(idx)\n                i += 2\n            else:\n                newids.append(ids[i])\n                i += 1\n        return newids\n\n    def _build_vocab(self, ids: Iterable[int]):\n        \"\"\"Build vocabulary from 0-255 bytes and merges.\"\"\"\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        # Here we assume the items returned would be in the same order they were inserted. This is Okay starting in Python 3.10\n        for (p0, p1), idx in self.merges.items():\n            # This would be a concatenation of the bytes\n            vocab[idx] = vocab[p0] + vocab[p1]\n        return vocab\n\n\ntext = requests.get(\"https://docs.python.org/3/library/stdtypes.html#bytes.decode\").text\n\n\ntokenizer = BPETokenizer(300)\n\n\ntokenizer.train(text)\n\n\ntokenizer.decode(tokenizer.encode(text)) == text\n\nTrue"
  },
  {
    "objectID": "posts/nlp/Tokenization-Strategies.html#character-tokenization",
    "href": "posts/nlp/Tokenization-Strategies.html#character-tokenization",
    "title": "Tokenization Strategies",
    "section": "Character Tokenization",
    "text": "Character Tokenization\nThis is the simplest tokenization strategy where we simply break down the text at the character level. Then the characters will be fed to the model. Consider the following example:\n\ntext = \"I love NLP!\"\nlist(text)\n\n['I', ' ', 'l', 'o', 'v', 'e', ' ', 'N', 'L', 'P', '!']\n\n\nFrom here, it is easy to convert each character into integers that would be fed to the model. This step is called numericalization. We can numericalize the above text by first building the vocabulary, and then convert each character to its corresponding index as follows:\n\nvocab = {char: idx for idx, char in enumerate(sorted(list(text)))}\nprint(vocab)\n\n{' ': 1, '!': 2, 'I': 3, 'L': 4, 'N': 5, 'P': 6, 'e': 7, 'l': 8, 'o': 9, 'v': 10}\n\n\nNow we can simply map each token (character in this case) to its own corresponding index:\n\n[vocab[char] for char in text]\n\n[3, 1, 8, 9, 10, 7, 1, 5, 4, 6, 2]\n\n\n\nAdvantages:\n\nHelps us avoid misspellings and rare words\nVery small vocabulary. Therefore, embedding and output layer would be small which means less computation\n\nDrawbacks:\n\nSequences length will be very long\nLinguistic structures such as words now need to be learned from data. This requires much more data, memory, and computation\nBecause we have fixed context length for most LLM and the sequence would be very long, we would lose the ability to attend the important tokens from before"
  },
  {
    "objectID": "posts/nlp/Tokenization-Strategies.html#word-tokenization",
    "href": "posts/nlp/Tokenization-Strategies.html#word-tokenization",
    "title": "Tokenization Strategies",
    "section": "Word Tokenization",
    "text": "Word Tokenization\nThe other extreme of word tokenization is to split text into words and then map each word to its corresponding index in the vocabulary. The simplest form would be to split on whitespace (which work well for English but not other languages such as Japanese that don’t have a well-defined idea of a word):\n\ntext.split()\n\n['I', 'love', 'NLP!']\n\n\n\nvocab = {char: idx for idx, char in enumerate(sorted(text.split()))}\nprint(vocab)\n\n{'I': 0, 'NLP!': 1, 'love': 2}\n\n\n\n[vocab[word] for word in text.split()]\n\n[0, 2, 1]\n\n\nMost tokenizers would include rules and heuristics that try to separate parts of meaning even when there are no spaces such as “doesn’t” into “does n’t”.\n\nAdvantages:\n\nSequences length will be short\n\nDrawbacks:\n\nSize of the vocabulary will explode for large corpus due to the fact that words can include declinations, misspellings, or punctuations. If the vocabulary size has 1m words and the embedding dimension is 512 -&gt; the first embedding layer would be ~ 0.5 billion parameters!\n\nWe can work around this issue by including top n most frequent words. For example, if we include top 100,000 words -&gt; the first embedding layer would be ~ 0.5 million parameters. However, because all other words will be mapped to the UNK token, the model has no idea about the words associated with the UNK token and we may lose some important information\n\nSome languages don’t have well-defined idea of what constitute a word\nBecause we have so many tokens that are either rare or never happened in the training data, these tokens would either never been activated or maybe fe passes activated them which is not good to get good embedding vector for them. Therefore, they are occupying memory w/o being that useful"
  },
  {
    "objectID": "posts/nlp/Tokenization-Strategies.html#subword-tokenization",
    "href": "posts/nlp/Tokenization-Strategies.html#subword-tokenization",
    "title": "Tokenization Strategies",
    "section": "Subword Tokenization",
    "text": "Subword Tokenization\nSplit words into smaller parts based on the most frequent sub-strings. Therefore, we want to keep the most frequent words as unique entities but split the rare words into smaller units to allow us to deal with misspellings and complex words. This will help us achieve the best of both wolds: 1) manageable vocabulary size, 2) keep frequent words as their own entities, and 3) deal with complex and misspelling words.\nThe subword tokenizers are typically learned from pretraining corpus using statistical rules and algorithms. We will cover the most common ones: WordPiece and SentencePiece:\n\nWordPiece\nWordPiece tokenizer is used by the DistilBERT model. The vocabulary is first initialized with individual characters in the language, then the most frequent combinations of symbols in the vocabulary are iteratively added to the vocabulary. The process is:\n\nInitialize the vocabulary with all the characters in the text.\nBuild a language model on the training corpus using vocabulary build previously.\nGenerate a new word by combining two units out of the current vocabulary to increment the word vocabulary by one. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.\nGoto 2 until a predefined limit of vocabulary size is reached or the likelihood increase falls below a certain threshold.\n\nLet’s illustrate by example using 🤗 transformers library.\n\nfrom transformers import DistilBertTokenizer\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nencoded_text = tokenizer(text)\nencoded_text\n\n{'input_ids': [101, 1045, 2293, 17953, 2361, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"])\n\n['[CLS]', 'i', 'love', 'nl', '##p', '!', '[SEP]']\n\n\nLet’s explain the output of the DistilBERT tokenizer:\n\n[CLS] is a special token that is used to indicate the start of a sequence\n[SEP] is also a special token to separate multiple sequences\n## prefix indicates that the previous string isn’t whitespace\n\nThis shows that nlp is not common token, so it was split into two tokens\n\nWe can also see that ! has its own token\n\nWe can reconstruct that encoded text as follows:\n\ntokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"])\n)\n\n'[CLS] i love nlp ! [SEP]'\n\n\n\n\nSentencePiece\nSentencePiece implements byte-pair-encoding (BPE) and unigram language modeling. It encodes the raw text as a sequence of Unicode characters. This is very useful in multilingual corpora because many languages, such as Japanese, don’t have whitespace characters. Also, it is agnostic about accents and punctuations. That is why it is commonly used in multilingual model training.\nByte-pair-encoding works as follows:\n\nInitialize the vocabulary with all the characters in the text plus end-of-word symbol\nFind the most common adjacent characters\nReplace instances of the character pair with the new subword\nGoto step2 until desired vocab size\n\nLet’s again use 🤗 transformers library to tokenize the same text.\n\nfrom transformers import XLMRobertaTokenizer\n\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\nencoded_text = tokenizer(text)\nencoded_text\n\n{'input_ids': [0, 87, 5161, 541, 37352, 38, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"])\n\n['&lt;s&gt;', '▁I', '▁love', '▁N', 'LP', '!', '&lt;/s&gt;']\n\n\nLet’s explain the output of the XLM-ROBERTA tokenizer:\n\n&lt;s&gt; is a special token that is used to indicate the start of a sequence\n&lt;/s&gt; is also a special token to indicate the end of the sequence\n_ prefix indicates that the previous string is whitespace\n\nThis shows that nlp is not common token, so it was split into two tokens\n\nWe can also see that ! has its own token\n\nWe can reconstruct that encoded text as follows:\n\ntokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"])\n)\n\n'&lt;s&gt; I love NLP!&lt;/s&gt;'"
  },
  {
    "objectID": "posts/dl/why-not-softmax.html",
    "href": "posts/dl/why-not-softmax.html",
    "title": "Why Having the Last Layer in a Neural Network as Raw Logits is Better Than Softmax",
    "section": "",
    "text": "When designing a neural network, particularly for classification tasks, the choice of the final layer is a crucial design decision. Many practitioners instinctively apply a softmax activation to the logits (the raw, unnormalized outputs of the final layer) to produce a probability distribution over the classes. While softmax has its uses, there are several drawbacks to applying it directly in the final layer, making it preferable to leave the final layer as raw logits in many cases. This article explores why using raw logits is often better, focusing on the cons of softmax activation and providing resources for further understanding."
  },
  {
    "objectID": "posts/dl/why-not-softmax.html#what-is-softmax",
    "href": "posts/dl/why-not-softmax.html#what-is-softmax",
    "title": "Why Having the Last Layer in a Neural Network as Raw Logits is Better Than Softmax",
    "section": "What Is Softmax?",
    "text": "What Is Softmax?\nThe softmax function transforms a vector of raw logits into a probability distribution. For a vector of logits ( z ), the softmax function is defined as:\n\\[\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)}\\]\nThis ensures the output values are in the range ([0, 1]) and sum to 1, representing probabilities for each class.\nWhile this sounds appealing, there are several drawbacks to applying softmax directly in the output layer of a neural network."
  },
  {
    "objectID": "posts/dl/why-not-softmax.html#cons-of-using-softmax-in-the-final-layer",
    "href": "posts/dl/why-not-softmax.html#cons-of-using-softmax-in-the-final-layer",
    "title": "Why Having the Last Layer in a Neural Network as Raw Logits is Better Than Softmax",
    "section": "Cons of Using Softmax in the Final Layer",
    "text": "Cons of Using Softmax in the Final Layer\n\nRedundancy with Loss Functions\nMany popular loss functions, such as the categorical cross-entropy loss, already include the softmax operation internally. For instance:\n\nIn frameworks like TensorFlow and PyTorch, the CrossEntropyLoss function expects raw logits as input and applies the softmax internally during the calculation.\nIf you pre-apply softmax in the final layer and then pass the results to the cross-entropy loss, you introduce numerical instability and redundancy.\n\nThis redundancy can lead to issues such as:\n\nReduced numerical precision due to the compounding of exponents in the softmax calculation.\nHigher computational cost, as softmax is unnecessarily computed twice.\n\n\n\nNumerical Instability\nSoftmax involves exponentiating the logits, which can lead to overflow or underflow issues, particularly when working with large or small logits. For example:\n\nIf logits are extremely large (e.g., ( z_i = 1000 )), the exponential term ( (z_i) ) can result in infinity, causing numerical instability.\nSimilarly, if logits are very small (e.g., ( z_i = -1000 )), the exponential term becomes close to zero, leading to loss of precision.\n\nBy keeping the logits raw, you avoid these issues because modern loss functions (like cross-entropy) are designed to handle raw logits more effectively.\n\n\nTraining Instability\nUsing softmax in the final layer can lead to training instability, especially when the model produces logits with large magnitudes or when the dataset has imbalanced classes. Here’s why:\n\nExploding Gradients: The softmax function uses exponential terms, which can grow very large when the logits are large. This can cause gradients to explode, leading to unstable updates during backpropagation.\nVanishing Gradients: If one logit is much larger than the others, the softmax output will assign nearly all the probability to that class, causing the gradients for other classes to approach zero. This can slow down training or make it harder for the model to learn meaningful updates for underrepresented classes.\nImbalance Sensitivity: In the case of class imbalance, softmax can exacerbate the problem by driving the model to overfit to the majority class, as the softmax probabilities heavily favor the dominant logits. Raw logits give the loss function more room to handle such imbalances effectively.\n\nBy leaving the final layer as raw logits, you mitigate these risks, as the loss function can better manage the relationship between logits and gradients without the constraints imposed by softmax.\n\n\nLoss of Interpretability for Certain Tasks\nSoftmax enforces a probability distribution on the output, which may not always be desirable. For example:\n\nIn multi-label classification tasks, where multiple classes can be “true” simultaneously, softmax forces the outputs to compete against each other, which can lead to incorrect predictions. In such cases, sigmoid activation is often used instead, or raw logits are left for downstream processing.\nIn some advanced machine learning tasks, raw logits provide more flexibility for interpretation or post-processing, which is lost when softmax is applied.\n\n\n\nEncourages Overconfidence in Predictions\nSoftmax tends to amplify the confidence of predictions by exaggerating the differences between logits. For example:\n\nIf two logits are close in value, the softmax output will still assign a disproportionately high probability to the larger logit.\nThis can lead to overconfident predictions, even when the model is unsure (i.e., the logits are close in value). Overconfidence can make it harder to assess the true uncertainty of the model.\n\nBy keeping raw logits, you retain the original scale of the outputs, which can be useful for understanding the model’s confidence and uncertainty."
  },
  {
    "objectID": "posts/dl/why-not-softmax.html#conclusion",
    "href": "posts/dl/why-not-softmax.html#conclusion",
    "title": "Why Having the Last Layer in a Neural Network as Raw Logits is Better Than Softmax",
    "section": "Conclusion",
    "text": "Conclusion\nWhile softmax is a widely used activation function, its application as the final layer in a neural network is not always the best choice. Its redundancy with loss functions, numerical instability, training instability, loss of interpretability, lack of flexibility for post-processing, and tendency to encourage overconfidence are significant drawbacks. By keeping the final layer as raw logits, you maintain numerical stability, computational efficiency, and flexibility for downstream tasks.\nIn modern machine learning workflows, it is often better to leave the last layer as raw logits and let the loss function or downstream processing handle the conversion to probabilities when needed. This approach not only avoids the pitfalls of softmax but also aligns better with best practices in many frameworks.\nFor a deeper dive into this topic, check out the resources provided above to enhance your understanding of raw logits and softmax!"
  },
  {
    "objectID": "posts/dl/why-not-softmax.html#resources",
    "href": "posts/dl/why-not-softmax.html#resources",
    "title": "Why Having the Last Layer in a Neural Network as Raw Logits is Better Than Softmax",
    "section": "Resources",
    "text": "Resources\nIf you want to dive deeper into this topic, here are some resources to help you learn more about raw logits, softmax, and their implications in neural networks:\n\nDeep Learning Book by Ian Goodfellow et al.\n\nChapter 6 on “Feedforward Networks” provides a detailed explanation of softmax and alternatives for output layers.\n\nPyTorch Documentation on CrossEntropyLoss\n\nExplains why you should use raw logits with CrossEntropyLoss and how the loss function integrates softmax internally.\n\nOn Calibration of Modern Neural Networks\n\nDiscusses softmax’s tendency to produce overconfident predictions and explores techniques for improving calibration."
  },
  {
    "objectID": "posts/c/program-startup-notes.html",
    "href": "posts/c/program-startup-notes.html",
    "title": "C Program Startup",
    "section": "",
    "text": "Figure 1: Linux x86 program startup(Source)"
  },
  {
    "objectID": "posts/c/program-startup-notes.html#introduction",
    "href": "posts/c/program-startup-notes.html#introduction",
    "title": "C Program Startup",
    "section": "Introduction",
    "text": "Introduction\nIn this post, I will try to write down the steps of C program execution on x86. I used to believe that all C programs start execution at main, or at least this was my understanding from different books/courses until my best friend gdb debugger showed the symbol for _start. This is how I got curious until I got to the bottom of it. Below are my notes that I took during my learning."
  },
  {
    "objectID": "posts/c/program-startup-notes.html#conclusion",
    "href": "posts/c/program-startup-notes.html#conclusion",
    "title": "C Program Startup",
    "section": "Conclusion",
    "text": "Conclusion\nSo starting program will call execve that starts the loader that at some point pass control to _start, which calls __libc_start_main which calls __libc_csu_init which calls _init."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#introduction",
    "href": "posts/optimization-algorithms/gradient-descent.html#introduction",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Introduction",
    "text": "Introduction\nOptimization refers to the task of minimizing/maximizing an objective function \\(f(x)\\) parameterized by \\(x\\). In machine/deep learning terminology, it’s the task of minimizing the cost/loss function \\(J(w)\\) parameterized by the model’s parameters \\(w \\in \\mathbb{R}^d\\). Optimization algorithms (in case of minimization) have one of the following goals: - Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum. - Find the lowest possible value of the objective function within its neighbor. That’s usually the case if the objective function is not convex as the case in most deep learning problems.\nThere are three kinds of optimization algorithms:\n\nOptimization algorithm that is not iterative and simply solves for one point.\nOptimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.\nOptimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters’ initialization plays a critical role in speeding up convergence and achieving lower error rates.\n\nGradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function \\(J(w)\\) w.r.t to the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate \\(\\alpha\\). Therefore, we follow the direction of the slope downhill until we reach a local minimum.\nIn this notebook, we’ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent.\nLet’s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let’s assume that the logistic regression model has only two parameters: weight \\(w\\) and bias \\(b\\).\n\nInitialize weight \\(w\\) and bias \\(b\\) to any random numbers.\nPick a value for the learning rate \\(\\alpha\\). The learning rate determines how big the step would be on each iteration.\n\n\nIf \\(\\alpha\\) is very small, it would take long time to converge and become computationally expensive.\nIF \\(\\alpha\\) is large, it may fail to converge and overshoot the minimum.\n\nTherefore, plot the cost function against different values of \\(\\alpha\\) and pick the value of \\(\\alpha\\) that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges (see figure 1).\n\n\n\nFigure 1: Gradient descent with different learning rates Source\n\n\n\nThe most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1, 0.3.\n\n\nMake sure to scale the data if it’s on very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (see figure 2).\n\n\n\n\nFigure 2: Gradient descent: normalized versus unnormalized level curves\n\n\nScale the data to have \\(\\mu = 0\\) and \\(\\sigma = 1\\). Below is the formula for scaling each example: \\[\\\\{}\\frac{x_i - \\mu}{\\sigma}\\tag{1}\\\\{} \\] 4. On each iteration, take the partial derivative of the cost function \\(J(w)\\) w.r.t each parameter (gradient): \\[\\frac{\\partial}{\\partial w}J(w) = \\nabla_w J\\tag{2}\\\\{}\\] \\[\\frac{\\partial}{\\partial b}J(w) = \\nabla_b J\\tag{3}\\\\{}\\] The update equations are: \\[w = w - \\alpha \\nabla_w J\\tag{4}\\\\{}\\] \\[b = b - \\alpha \\nabla_b J\\tag{5}\\\\{}\\] * For the sake of illustration, assume we don’t have bias. If the slope of the current values of \\(w &gt; 0\\), this means that we are to the right of optimal \\(w^*\\). Therefore, the update will be negative, and will start getting close to the optimal values of \\(w^*\\). However, if it’s negative, the update will be positive and will increase the current values of \\(w\\) to converge to the optimal values of \\(w^*\\) (see figure 3):\n\n\n\nFigure 3: Gradient descent. An illustration of how gradient descent algorithm uses the first derivative of the loss function to follow downhill it’s minimum.\n\n\n\nContinue the process until the cost function converges. That is, until the error curve becomes flat and doesn’t change.\nIn addition, on each iteration, the step would be in the direction that gives the maximum change since it’s perpendicular to level curves at each step.\n\nNow let’s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter’s update (learning step)."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#batch-gradient-descent",
    "href": "posts/optimization-algorithms/gradient-descent.html#batch-gradient-descent",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\nBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples: \\[w = w - \\alpha \\nabla_w J(w)\\tag{6}\\]\nfor i in range(num_epochs):\ngrad = compute_gradient(data, params)\nparams = params - learning_rate * grad\nThe main advantages:\n\nWe can use fixed learning rate during training without worrying about learning rate decay.\nIt has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex.\nIt has unbiased estimate of gradients. The more the examples, the lower the standard error.\n\nThe main disadvantages:\n\nEven though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets.\nEach step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#mini-batch-gradient-descent",
    "href": "posts/optimization-algorithms/gradient-descent.html#mini-batch-gradient-descent",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Mini-Batch Gradient Descent",
    "text": "Mini-Batch Gradient Descent\nInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of \\(b\\) examples:\n\\[w = w - \\alpha \\nabla_w J(x^{\\{i:i + b\\}}, y^{\\{i: i + b\\}}; w)\\tag{7}\\\\{}\\]\n\nShuffle the training dataset to avoid pre-existing order of examples.\nPartition the training dataset into \\(b\\) mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch.\n\nfor i in range(num_epochs):\nnp.random.shuffle(data)\nfor batch in radom_minibatches(data, batch_size=32):\n    grad = compute_gradient(batch, params)\n    params = params - learning_rate * grad\nThe batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2.\nThe main advantages:\n\nFaster than Batch version because it goes through a lot less examples than Batch (all examples).\nRandomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning.\nWith batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error.\nEven though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur.\n\nThe main disadvantages:\n\nIt won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges.\nDue to the noise, the learning steps have more oscillations (see figure 4) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum.\n\n\n\n\nFigure 4: Gradient descent: batch versus mini-batch loss function\n\n\nWith large training datasets, we don’t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size \\(b = m\\), we get the Batch Gradient Descent."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#stochastic-gradient-descent",
    "href": "posts/optimization-algorithms/gradient-descent.html#stochastic-gradient-descent",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example \\((x^i, y^i)\\). Therefore, learning happens on every example:\n\\[w = w - \\alpha \\nabla_w J(x^i, y^i; w)\\tag{7}\\]\n\nShuffle the training dataset to avoid pre-existing order of examples.\nPartition the training dataset into \\(m\\) examples.\n\nfor i in range(num_epochs):\n    np.random.shuffle(data)\n    for example in data:\n        grad = compute_gradient(example, params)\n        params = params - learning_rate * grad\nIt shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD:\n\nIt adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time.\nWe can’t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step.\n\nBelow is a graph that shows the gradient descent’s variants and their direction towards the minimum:\n\n\n\nFigure 5: Gradient descent variants’ trajectory towards minimum\n\n\nAs the figure above shows, SGD direction is very noisy compared to mini-batch."
  },
  {
    "objectID": "posts/optimization-algorithms/gradient-descent.html#challenges",
    "href": "posts/optimization-algorithms/gradient-descent.html#challenges",
    "title": "Gradient Descent Algorithm and Its Variants",
    "section": "Challenges",
    "text": "Challenges\nBelow are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch:\n\nGradient descent is a first-order optimization algorithm, which means it doesn’t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if:\nSecond derivative = 0 \\(\\rightarrow\\) the curvature is linear. Therefore, the step size = the learning rate \\(\\alpha\\).\nSecond derivative &gt; 0 \\(\\rightarrow\\) the curvature is going upward. Therefore, the step size &lt; the learning rate \\(\\alpha\\) and may lead to divergence.\nSecond derivative &lt; 0 \\(\\rightarrow\\) the curvature is going downward. Therefore, the step size &gt; the learning rate \\(\\alpha\\).\n\nAs a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge. - If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function (see figure 7).\n\n\n\nFigure 6: Gradient descent fails to exploit the curvature information contained in the Hessian matrix. Source\n\n\n\nThe norm of the gradient \\(g^Tg\\) is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients’ norm is increasing, we’re able to achieve a very low error rates (see figure 8).\n\n\n\n\nFigure 7: Gradient norm. Source\n\n\n\nIn small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 9). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive.\n\n\n\n\nFigure 8: Saddle point\n\n\n\nAs discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets.\nAll parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#introduction",
    "href": "projects/data-modeling-with-postgres/index.html#introduction",
    "title": "Data Modeling with Postgres",
    "section": "Introduction",
    "text": "Introduction\nThe goal of this project is to build a PostgreSQL database utilizing the data on users activity and songs metadata. Building the database helps us do complex analytics regarding users activity as well as song play analysis."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#data",
    "href": "projects/data-modeling-with-postgres/index.html#data",
    "title": "Data Modeling with Postgres",
    "section": "Data",
    "text": "Data\nThe songs’ metadata sourse is a subset of the Million Song Dataset. Also, the users’ activities is a simulated data using eventsim. The data resides in two main directories:\n\nSongs metadata: collection of JSON files that describes the songs such as title, artist name, year, etc.\nLogs data: collection of JSON files where each file covers the users activities over a given day."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#methodology",
    "href": "projects/data-modeling-with-postgres/index.html#methodology",
    "title": "Data Modeling with Postgres",
    "section": "Methodology",
    "text": "Methodology\nWe’ll build the database by optimizing the tables around efficient reads for complex queries. To do that, Star schema will be used utilizing dimensional modeling as follows:\n\nFact table: songplays.\nDimensions tables: songs, artist, users, time.\n\nThe three most important advantages of using Star schema are:\n\nDenormalized tables.\nSimplified queries.\nFast aggregation."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#how-to",
    "href": "projects/data-modeling-with-postgres/index.html#how-to",
    "title": "Data Modeling with Postgres",
    "section": "HOW TO",
    "text": "HOW TO\nThe source code is available in three separate Python scripts. Below is a brief description of the main files:\n\nsql_queries.py has all the queries needed to both create/drop tables for the database as well as a SQL query to get song_id and artist_id from other tables since they are not provided in logs dataset.\ncreate_tables.py creates the database, establish the connection and creates/drops all the tables required using sql_queries module.\netl.py build the pipeline that extracts the data from JSON files, does some transformation (such as adding different time attributes from timestamp) and then insert all the data into the corresponding tables.\n\nTherefore, we first run create_tables.py then etl.py to create the database, create tables, and then insert the data using the ETL pipeline."
  },
  {
    "objectID": "projects/data-modeling-with-postgres/index.html#examples",
    "href": "projects/data-modeling-with-postgres/index.html#examples",
    "title": "Data Modeling with Postgres",
    "section": "Examples",
    "text": "Examples\n%load_ext sql\n%sql postgresql://student:student@127.0.0.1/sparkifydb\n%%sql\nSELECT COUNT(*) from songplays;\n\n&gt;&gt;&gt; postgresql://student:***@127.0.0.1/sparkifydb\n\n&gt;&gt;&gt; 1 rows affected.\n\n&gt;&gt;&gt; Out[1]: count 6820\nCheck out the project on github."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today Imad Learned",
    "section": "",
    "text": "In an effort to learn in public, below is a collection of things that I learn day-to-day mainly in technology and languages.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 26, 2025\n\n\nAssigning Array to Numpy Slice\n\n\nImad Dabbura\n\n\n\n\nNov 27, 2024\n\n\nClass Evaluation Steps\n\n\nImad Dabbura\n\n\n\n\nOct 25, 2024\n\n\nSimilarities and Differences between Threads and Coroutine Tasks\n\n\nImad Dabbura\n\n\n\n\nOct 2, 2024\n\n\nCoroutines Don’t Need Synchronization\n\n\nImad Dabbura\n\n\n\n\nJun 12, 2024\n\n\nInteractive Code Interpreter\n\n\nImad Dabbura\n\n\n\n\nMay 27, 2024\n\n\nString Comparison\n\n\nImad Dabbura\n\n\n\n\nJan 18, 2024\n\n\nComparing Repo’s History on Github\n\n\nImad Dabbura\n\n\n\n\nDec 19, 2023\n\n\nssh Configuration\n\n\nImad Dabbura\n\n\n\n\nDec 18, 2023\n\n\nExclude Shell Command From History\n\n\nImad Dabbura\n\n\n\n\nDec 7, 2023\n\n\nStatus Code Returned by Shell’s Pipeline\n\n\nImad Dabbura\n\n\n\n\nOct 26, 2023\n\n\nDifference Between Effective & Real UID/GID\n\n\nImad Dabbura\n\n\n\n\nOct 16, 2023\n\n\nDirectory Access Permissions/Representation\n\n\nImad Dabbura\n\n\n\n\nOct 4, 2023\n\n\nLexical Scope in C\n\n\nImad Dabbura\n\n\n\n\nOct 3, 2023\n\n\nLinkage in C\n\n\nImad Dabbura\n\n\n\n\nAug 23, 2023\n\n\nUseful Tmux Notes\n\n\nImad Dabbura\n\n\n\n\nAug 22, 2023\n\n\nParametrization\n\n\nImad Dabbura\n\n\n\n\nAug 21, 2023\n\n\nTest Outcomes\n\n\nImad Dabbura\n\n\n\n\nAug 18, 2023\n\n\nNotes on Vim\n\n\nImad Dabbura\n\n\n\n\nAug 15, 2023\n\n\nShift by K &gt;= w\n\n\nImad Dabbura\n\n\n\n\nAug 14, 2023\n\n\nNotes on Data Type Conversion\n\n\nImad Dabbura\n\n\n\n\nAug 1, 2023\n\n\nUnintuitive Behavior on Operations with Signed & Unsigned Operands\n\n\nImad Dabbura\n\n\n\n\nApr 24, 2023\n\n\nPseudorandom Number Generator\n\n\nImad Dabbura\n\n\n\n\nMar 27, 2023\n\n\nHiding Test Traceback\n\n\nImad Dabbura\n\n\n\n\nMar 25, 2023\n\n\nSelecting Tests with -m\n\n\nImad Dabbura\n\n\n\n\nMar 22, 2023\n\n\nDebugging with pytest Flags\n\n\nImad Dabbura\n\n\n\n\nMar 21, 2023\n\n\nTesting Non-installable Scripts\n\n\nImad Dabbura\n\n\n\n\nMar 15, 2023\n\n\nThe Role of __init__.py in pytest\n\n\nImad Dabbura\n\n\n\n\nMar 13, 2023\n\n\nDifference Between pytest’s skip, skipif, & xfail Markers\n\n\nImad Dabbura\n\n\n\n\nFeb 27, 2023\n\n\nWhich Containers Should Be In The Same Pod\n\n\nImad Dabbura\n\n\n\n\nFeb 2, 2023\n\n\nDeclarative vs Imperative Configuration\n\n\nImad Dabbura\n\n\n\n\nFeb 2, 2023\n\n\nTips to Reduce Docker Images\n\n\nImad Dabbura\n\n\n\n\nJan 31, 2023\n\n\nGitOps\n\n\nImad Dabbura\n\n\n\n\nJan 29, 2023\n\n\nOverriding vs Non-overriding Descriptors\n\n\nImad Dabbura\n\n\n\n\nJan 26, 2023\n\n\nAttribute Access\n\n\nImad Dabbura\n\n\n\n\nJan 21, 2023\n\n\nRANK vs DENSE_RANK vs ROW\n\n\nImad Dabbura\n\n\n\n\nJan 20, 2023\n\n\nCommand Substitution in the Shell\n\n\nImad Dabbura\n\n\n\n\nJan 19, 2023\n\n\nProcess Substitution in the Shell\n\n\nImad Dabbura\n\n\n\n\nJan 16, 2023\n\n\n@property\n\n\nImad Dabbura\n\n\n\n\nJan 1, 2023\n\n\nContext Manager Protocol\n\n\nImad Dabbura\n\n\n\n\nDec 29, 2022\n\n\nContext Manager Decorator\n\n\nImad Dabbura\n\n\n\n\nDec 28, 2022\n\n\nReusing Fixtues By All Tests\n\n\nImad Dabbura\n\n\n\n\nDec 27, 2022\n\n\nTesting Expected Exceptions\n\n\nImad Dabbura\n\n\n\n\nDec 26, 2022\n\n\nFixtures\n\n\nImad Dabbura\n\n\n\n\nDec 26, 2022\n\n\nFixtures’ Scopes\n\n\nImad Dabbura\n\n\n\n\nDec 26, 2022\n\n\nLogical Execution of try/except/else/finally Blocks\n\n\nImad Dabbura\n\n\n\n\nDec 24, 2022\n\n\nWhen Does iter() Called Internally\n\n\nImad Dabbura\n\n\n\n\nDec 22, 2022\n\n\nRunning Tests\n\n\nImad Dabbura\n\n\n\n\nDec 19, 2022\n\n\nTest Discovery\n\n\nImad Dabbura\n\n\n\n\nDec 14, 2022\n\n\nRetrieving Object’s Attributes\n\n\nImad Dabbura\n\n\n\n\nDec 13, 2022\n\n\nOperator Overloading\n\n\nImad Dabbura\n\n\n\n\nDec 11, 2022\n\n\nSequence Slicing\n\n\nImad Dabbura\n\n\n\n\nDec 8, 2022\n\n\nUsing __slots__ to Store Instance’s Attributes\n\n\nImad Dabbura\n\n\n\n\nDec 4, 2022\n\n\nInstance vs Class vs Static Method\n\n\nImad Dabbura\n\n\n\n\nDec 1, 2022\n\n\nObject Destruction\n\n\nImad Dabbura\n\n\n\n\nNov 29, 2022\n\n\n== vs is\n\n\nImad Dabbura\n\n\n\n\nNov 27, 2022\n\n\nClosures\n\n\nImad Dabbura\n\n\n\n\nNov 25, 2022\n\n\nFirst Class Objects\n\n\nImad Dabbura\n\n\n\n\nNov 21, 2022\n\n\nClearing Registers\n\n\nImad Dabbura\n\n\n\n\nNov 20, 2022\n\n\nAborting Git’s Commit & Rebase\n\n\nImad Dabbura\n\n\n\n\nNov 19, 2022\n\n\nParallel vs Series Execution of Macros\n\n\nImad Dabbura\n\n\n\n\nNov 16, 2022\n\n\nTemporary Files & Directories\n\n\nImad Dabbura\n\n\n\n\nNov 14, 2022\n\n\nManaged vs External Tables\n\n\nImad Dabbura\n\n\n\n\nNov 6, 2022\n\n\nLists vs Tuples\n\n\nImad Dabbura\n\n\n\n\nNov 3, 2022\n\n\nPython Object Lookup Process\n\n\nImad Dabbura\n\n\n\n\nNov 1, 2022\n\n\nPython Logging\n\n\nImad Dabbura\n\n\n\n\nOct 27, 2022\n\n\nPython Wheel Files\n\n\nImad Dabbura\n\n\n\n\nOct 19, 2022\n\n\nC Program Memory Layout\n\n\nImad Dabbura\n\n\n\n\nOct 17, 2022\n\n\nListing tmux Sessions\n\n\nImad Dabbura\n\n\n\n\nOct 15, 2022\n\n\nOuter Joins & Where Clause\n\n\nImad Dabbura\n\n\n\n\nOct 13, 2022\n\n\nWhat Is Null Actually?\n\n\nImad Dabbura\n\n\n\n\nOct 12, 2022\n\n\nLogical Execution Order of SQL Query\n\n\nImad Dabbura\n\n\n\n\nOct 4, 2022\n\n\nDetach Tensor From Computation Graph\n\n\nImad Dabbura\n\n\n\n\nSep 30, 2022\n\n\nCharacter Array vs String Constant Pointers\n\n\nImad Dabbura\n\n\n\n\nSep 28, 2022\n\n\nImmutable Objects and Augmented Assignment Operators\n\n\nImad Dabbura\n\n\n\n\nSep 25, 2022\n\n\nModules and Packages\n\n\nImad Dabbura\n\n\n\n\nSep 24, 2022\n\n\nI/O Redirection\n\n\nImad Dabbura\n\n\n\n\nSep 23, 2022\n\n\nLoading Modules and Packages\n\n\nImad Dabbura\n\n\n\n\nSep 22, 2022\n\n\nNamespace Packages\n\n\nImad Dabbura\n\n\n\n\nSep 21, 2022\n\n\nUseful Tricks with Null Device\n\n\nImad Dabbura\n\n\n\n\nSep 20, 2022\n\n\nDecorators\n\n\nImad Dabbura\n\n\n\n\nSep 19, 2022\n\n\nSave Memory When Operating on Tensors\n\n\nImad Dabbura\n\n\n\n\nSep 18, 2022\n\n\nLazy Layers\n\n\nImad Dabbura\n\n\n\n\nSep 16, 2022\n\n\nList Available GPUs\n\n\nImad Dabbura\n\n\n\n\nSep 15, 2022\n\n\nConditional Iterators\n\n\nImad Dabbura\n\n\n\n\nSep 14, 2022\n\n\nMutable Default Function Arguments\n\n\nImad Dabbura\n\n\n\n\nSep 12, 2022\n\n\nMutability and Inplace Operations\n\n\nImad Dabbura\n\n\n\n\nSep 10, 2022\n\n\nUser Defined Classes are Hashable\n\n\nImad Dabbura\n\n\n\n\nSep 6, 2022\n\n\n__getitem__ Makes Object an Iterator\n\n\nImad Dabbura\n\n\n\n\nSep 5, 2022\n\n\nScope of Variables in Comprehensions\n\n\nImad Dabbura\n\n\n\n\nSep 3, 2022\n\n\nItemview and Keyview Attributes of Dictionary are Subclasses of Set\n\n\nImad Dabbura\n\n\n\n\nSep 1, 2022\n\n\nTruthiness of Python Objects\n\n\nImad Dabbura\n\n\n\n\nAug 31, 2022\n\n\nLine Continuation\n\n\nImad Dabbura\n\n\n\n\nAug 30, 2022\n\n\nExecuting vs Sourcing Script\n\n\nImad Dabbura\n\n\n\n\nAug 29, 2022\n\n\nVimium: Vim Key Bindings for Chrome\n\n\nImad Dabbura\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers-summaries/llama2.html",
    "href": "papers-summaries/llama2.html",
    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "section": "",
    "text": "Llama 2: Open Foundation and Fine-Tuned Chat Models\n\nThesis: With clever algorithmic tweaking and focus on helpfulness/safety the new chat models are great for dialogue use cases.\nMethods:\n\nModels:\n\nBase LLMs: 7B, 13B, 34B, 70B\nChat LLMs: 7B, 13B, 34B, 70B\nUse KV cache during inference. The task of LLM is next token prediction. At each step we’re only interested in the prediction of the last token but we still need to give the previous tokens (context) that were already computed from before. So at step 10, we would’ve already computed the attention scores for token1 9 times and token2 8 times. Also, we’re not interested in the attention scores for future tokens for causal inference since the attention score matrix is lower triangular. To summarize:\n\nCan we cache the dot-products of tokens from previous steps?\nCan we not compute attention scores for successor tokens?\nCan we not compute prediction for previous tokens since we already have them?\n\nKV achieves this by appending the key and query vectors at each time step and compute hidden vector for the current word. So at time step 10, we would have 10 tokens’ vectors in K and V caches and we would compute dot-product of only token 10 and all previous tokens to get attention score vector which is then applied to vector cache that will yield hidden vector that will be fed to classifier head to predict next token\nWithout KV cache, memory wasn’t the bottleneck; however, KV memory access becomes the bottleneck.\n\nTo remedy this issue, multi-query attention is used where only query has multi-heads but keys and values don’t have multiple heads -&gt; Different query heads would share the same keys and values. The drawback of this approach is slight degradation in performance.\nAs a middle ground solution between multi-head query (as in vanilla transformer) and multi-query attention, Group Multi-Query attention is introduced where we would have multiple query heads in which query heads in the same group share the same keys and values\n\n\nData:\n\nOnly publicly available data that is compatible with open source\nIncreased size from [[llama]] by \\(40\\%\\)\n\nTrained on 2 trillion tokens for all models\n\n\nChat models:\n\n~28k high quality instructions dataset for SFT + public available datasets -&gt; ~100k dataset size\nReward model:\n\nHuman annotators are presented with an instruction and two sample responses from different variants of the model (with maybe different temperature) to pick which one they prefer and why according to safety and helpfulness guidelines.\n\nAlso, they would assign label for the difference between the chosen response and the other response as follows:\n\nsignificantly better, better, slightly better, or negligibly better/unsure\nEach category would have fixed value so that reward model would learn to assign large gaps between responses of different preferences\n\n\n\n\nTokenizer: BPE algorithm using implementation from Sentence-Piece\nModel:\n\nSimilar architecture to [[llama]]\nDoubled the context length to 4k\nUsed group-query attention for 34B and 70B models\nReward model:\n\nTrained progressively as new batch of data is available.\nThe model is the same as pre-trained chat model\nClassification head was replaced with linear layer to generate a score for the response. Given a prompt and model response, model scores the response\nBinary ranking loss is used to \\[\\mathcal{L}_{ranking} = -log(\\sigma(r_\\theta(x,y_{c}) - r_\\theta(x,y_{r}) - m(r)))\\]where \\(r_\\theta(x, y)\\) is the scalar score output for prompt \\(x\\) and completion \\(y\\) with model weights \\(\\theta\\). \\(y_{c}\\) is the preferred response that annotators choose and \\(y_{r}\\) is the rejected counterpart, and the margin \\(m(r)\\) is a discrete function of the preference rating\n\nTwo reward models were trained: Helpfulness reward model and safety reward model. The reason for two models instead of 1 is that some response can be helpful and not safe and vice versa.\n\nChat model is further fine-tuned using RLHF\n\nThrough PPO\nRejection sampling: The largest model (70B) is used to score k samples for each prompt. Create dataset that has the prompt and the response with the highest score to further fine-tune the smaller models\n\nGhostAttention method is proposed so chat model(s) adhere to the role it is given in the beginning in all responses in multi-turn conversations (dialogue). For example, if the model is asked to always respond with emojis, then responses to any question should include only emojis. The rationale behind it is that Chat models tend to forget their role after few responses. Synthetic dataset created:\n\nSamples an instruction that should be followed in a conversation\nConcatenates this instruction to every user message within the conversation\nSamples responses to each message using the model from the latest round of RLHF\nFinally, such dataset can be used for further fine-tuning models for follow instructions over long conversations\n\n\nContribution: The first highly competitive open-source LLMs. More specifically, the chat models and the efforts spent in fine-tuning them through data collection and different techniques such as rejection sampling and GhostAttn.\nTakeaways:\n\nQuality instructions dataset is all you need to get a good chat model that follows user’s instructions and engage in helpful dialogue\nTime spent on aligning LLM to be helpful and safety yielded a high quality chat models\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/instructgpt.html",
    "href": "papers-summaries/instructgpt.html",
    "title": "InstructGPT: Training language models to follow instructions with human feedback",
    "section": "",
    "text": "InstructGPT: Training language models to follow instructions with human feedback\n\nThesis: Build a model that follows user’s instructions by being helpful, not harmful, and don’t make up facts (hallucinates) using reinforcement learning from human feedback.\nMethods:\n\nTrain LLM in a supervised manner on dataset created by 40 labellers that write demonstrations of prompts submitted to OpenAI API as well as hand-written prompts. Therefore, dataset would have prompts and desired output for the prompts. This will give us supervised LLM baseline\nTrain Reward Model (RM) on dataset that labeller creates by ranking model’s outputs on prompts passed to supervised fine-tuned model from above. For each prompt, there would be multiple outputs that are ranked from best to worst\nUse reinforcement learning (RL), proximate policy optimization (PPO), to fine-tune train the supervised LLM baseline to maximize the reward model (RM) using PPO algorithm\nModels are evaluated on prompts from customers that were not present in the training data\nModel sizes: 1.3B, 6B, and 175B\nData:\n\nSplit on user ID so that same user can’t be in both training and valid/test datasets.\nEach user can have at most 200 prompts\nPPO dataset doesn’t have labels\n\nModels:\n\nSupervised fine-tuning (SFT): Trained on prompt-response pairs.\n\nModel is the pre-trained model\nWe don’t compute loss on prompt tokens, only response tokens\nIt has similar language modeling objective as pre-training, but here we’re trying to align response to high quality responses and not just next word prediction as is the case in pre-training\n\nReward Model:\n\nIt is typically the SFT model but we change the classifier layer into linear layer that 1 feature_out that would be the score\n\n\n\nContribution:\n\nProvide procedure to align LLM with user instructions using SFT and RLHF\n\nImportance:\n\n1.3B InstructGPT is 85% preferred over 175B GPT model and 71% preferred over prompted 175B GPT model by labellers.\nInstructGPT generalize pretty well to labellers that weren’t part of the labellers that generated the training data. It also generalizes to follow instructions for tasks/languages that were rare or not present in the training data\n\nTakeaways: 3-step fine-tuning using human feedback is all you need to align LLMs with user’s instructions and make such models usable and helpful as well as reduce harm and toxicity\nImprovements:\n\n40 labellers are not representative for the user population that will be affected by the model(s)\nPrompt comparisons are mostly generated by 1 labeller. Maybe we need to have multiple labellers and weight the labeller that belongs to the group affected by the response more than other labellers\nPrompt responses that look toxic/harmful may not be so given the context\nSometimes it is desirable to generate harmful responses as per user’s instruction. Should this be allowed?\nModel still hallucinates and make up facts as well as generate toxic/biased responses\n\n\n#nlp #llm #fine-tuning"
  },
  {
    "objectID": "papers-summaries/lora.html",
    "href": "papers-summaries/lora.html",
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "section": "",
    "text": "LoRA: Low-Rank Adaptation of Large Language Models\n\nThesis: To reduce the hardware requirement for a full-fine-tuning (storage and memory), low rank decomposition matrices will be trained for every transformer layer (only attention weights). It is based on the fact that \\(\\Delta{W}\\) matrices have low intrinsic dimension, which means they can be represented using much lower-ranked matrices. In other words, they have redundant information and a lot columns/rows can be obtained by linear combination of other columns/rows in the matrix (weight matrices are rank-deficient i.e. they have much lower rank than their actual dimensions). The proposed solution doesn’t add inference latency overhead as other adaptation methods do, can be combined with other adaptation methods because LoRA is orthogonal to other adaptation methods, and allows to have one shared instance of LLM between different downstream tasks. Switching between different tasks is very efficient as we just need to swap out the low rank matrices for attention weights which are much smaller than LLM pre-trained weights.\nNotes:\n\nLow Rank Adaptation (LoRA) is a fine-tuning method that uses rank decomposition matrices to fine-tune LLMs by training rank decomposition matrices for every Transformer layer (inject two matrices for each layer) and freeze all pre-trained parameters. It reduces trainable parameters by 10,000x and reduces GPU memory requirement by 3x. The performance is on-par or better than full fine-tuning to adapt to downstream tasks PLUS it doesn’t introduce latency penalty as other adaptation methods do (add more modules per layer or reduce usable sequence length).\nLoRA optimizes rank decomposition matrices of dense layers change during adaptation. The rank can be as low as \\(r = 1\\) or \\(r = 2\\) for matrices that have high dimension as high as \\(r = 12,288\\).\nAdvantages of LoRA:\n\nLets us have one pre-trained frozen LLM that is shared between different downstream tasks. Each task has its own A & B rank decomposition matrices for each dense layer. Therefore, switching tasks is simply switching those low rank decomposition matrices. Therefore, reduces storage/memory requirement as well as switching costs.\nSince pre-trained model weights are trained, we don’t need to keep state for such parameters and only update the newly introduced low rank matrices\nIncorporating the newly trained low rank matrices with pre-trained parameters adds no latency overhead because they can easily be merged due to linear design (as simple as addition for some cases)\nCan be combined with other adaptation methods such as prefix tuning\n\n\\(W_0 + \\Delta{W} = W_0 + BA\\) where \\(W_0 \\in R^{dxk}\\), \\(A \\in R^{rxk}\\), \\(B \\in R^{dxr}\\)\n\n\\(A\\) is initialized with random Gaussian\n\\(B\\) is initialized with 0\nTherefore, \\(BA\\) is zero at the beginning and the first batch would only use original weights\n\n\\(h = W_0x + \\Delta{Wx} = W_0x + BAx\\)\n\n\\(\\Delta{W}\\) is scaled with \\(\\alpha/r\\)\n\nWe can store \\(W_0 + BA\\) and load them for different tasks. Or we can just store \\(BA\\) and when switching between tasks, we perform \\(W_0 - BA + (BA)'\\)\nwhere \\((BA)'\\) are the low rank matrices for the new task\n\n\nAdapter layers run sequentially and can’t be parallelized. This adds latency overhead even though adapter layers have &lt; 1% of original model parameters\nThe adaptation happens for attention weights only in this paper: \\(W_q, W_v\\)\n\nGiven a parameter budget, adapting \\(W_q, W_v\\) Or \\(W_q, W_k, W_v, W_o\\) yield the best results. The more weight matrices we adapt -&gt; lower rank to stay within parameter budget\n\n\n\n#nlp #llm #fine-tuning"
  },
  {
    "objectID": "papers-summaries/mixtral.html",
    "href": "papers-summaries/mixtral.html",
    "title": "Mixtral of Experts",
    "section": "",
    "text": "Mixtral of Experts\n\nThesis: Mixtral-8x7B through its use of Mixture of Experts (MoE) helps train and generate text very efficiently and achieve much better performance than comparable dense models with the same computational cost. MoE allows to train much bigger models than its counter dense models due to the fact that each token in each layer will processed by 2 experts (not 8) which reduces computational cost.\nMethods:\n\nModel: Follows [[mistral-7b]] with the following changes:\n\ncontext_len = 32768\nnum_experts = 8\ntop_k_experts = 2\n\n\nContribution: Open sourced Mistral-8x7B base and chat models. Also provides efficient implementations for sparse MoE on single and multiple GPUs such as Efficient Parallelism\nImprovements:\n\nTakes more VRAM as the full model needs to be loaded into memory to be able to use it even though each token is used by 2 experts only\nMore prone to overfitting\nHard to fine-tune\n\nNotes:\n\nMixtral 8x7B is sparse Mixture of Experts (SMoE) that is based largely on Mistral-7B with the difference that each layer has 8 experts. Each expert is a Feed-Forward-Network (FFN). There is a gate (router) layer between self-attention layer and FFN layer that selects top 2 experts for each token. More specifically, the router network is a linear layer that takes \\(d_{model}\\) for each token and produce logits for each expert. Top 2 experts based on the logits will be chosen for each token to process it and there would weighted sum based on the softmax of the top 2 logits to combine the output of the 2 experts. Tokens maybe process by different experts in different layers.\nExperts can be thought of as each trained on subset of data that they become specialized on it\nSince each token is processed by top 2 experts on every layer, we can have very large models w/o affecting latency and throughput of the model because each token has access to only fraction of the model’s parameters (13B) and not the full 47B\nTrained on multilingual data with context size of 32k\nOutperforms or matches the performance of Llama2-70B and GPT3.5\n\nHugely outperforms Llama2-70B on tasks that require common-sense reasoning and code generation\n\nMixtral 8x7B - Instruct outperforms Gemini 1.5 Pro, Claude 2.1, and GPT3.5 turbo on human evaluation benchmarks\nMoE allows us to train larger models w/o for the same fixed computational cost. This means we can increase the capacity/complexity of the model while keeping the computational budget fixed\nExperts: each layer has several “experts” that are standalone neural network modules or layers with independent sets of parameters.\nRouter: a parametric (and learnable) gating mechanism that selects a (sparse) group of experts used to process each input.\nAchieves similar performance of dense models with same computation much faster -&gt; learns much faster\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/dense-passage-retrieval.html",
    "href": "papers-summaries/dense-passage-retrieval.html",
    "title": "Dense Passage Retrieval for Open-Domain Question Answering",
    "section": "",
    "text": "Dense Passage Retrieval for Open-Domain Question Answering\n\nThesis: use dense retrieval as opposed to sparse retrieval used in methods such as BM25/TF-IDF to retrieve relevant passages from provided context. The dense representation is learned using dual encoders (BERT) with questions and passages dataset\nMethodology:\n\nTrain using in-batch question/answers where negative passages are simply all the answers for different questions in the batch\nSimilarity function is just inner product of the dense vectors from question and passage encoders\nBERT-base-uncased is used as the encoder for both question and passage\n\nContributions: Finetuning the retrieval on question/answer pairs outperform the best BM25 on ODQA datasets. Also, having high precision retrieval trained using DPR results in higher end-to-end QA systems training (only training the reader).\nImprovements:\n\nThere is no interaction between question and passage(s)\nTraining retrieval and reader is independent; can be seen as both positive and negative\n\nNotes:\n\nLarge sparse representations obtained from models such as BM25 suffers from having difficulties of mapping question to passages if the words have similar meanings but are completely different. The learned representation is obtained from fine-tuning dual encoders (BERT) using small dataset (1000 of pairs of questions/answers)\n\n\n#nlp #rag"
  },
  {
    "objectID": "papers-summaries/toolformer.html",
    "href": "papers-summaries/toolformer.html",
    "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
    "section": "",
    "text": "Toolformer: Language Models Can Teach Themselves to Use Tools\n\nThesis: Train a model that learns in a self-supervised setting which API to call, when to call the API, how to pass the parameters, and how to incorporate the API results within the body of the text.\nMethods:\n\nDataset(s):\n\nTo avoid changing the vocabulary of the LM:\n\n[ token refers &lt;API&gt;\n] token refers &lt;/API&gt;\n-&gt; token refers to all tokens after it as result of API call\n\nUse few-shot examples for each tool and ask the LM to generate examples\nSample positions \\(i\\bel{1,...,n}\\) by selecting the top-k positions that have probabilities \\(p &gt; threshold\\)\nFor each position, generate m API calls\nExecute each API call and embed the results in the text\nFilter API calls using weighted cross entropy. The text with the API call with its results should have (intuitively) lower cross entropy than the text with no API call or an API call with its input but without its results. Therefore, we calculate the weighted cross entropy for 1) empty string, 2) API call with its input but without its results, and 3) API call with both input and output\n\nIf the API call with its results is useful for the generation of the future tokens, then the difference between the weighted cross entropy of min(weighted cross entropy of both no API call and API call with input only) and API call with results \\(&gt;=\\) filtering threshold (1.0 in the paper)\n\nFor all positions that remained after sampling and filtering, we augment every text sequence with API calls at all positions remained. For example, if a sequence has 5 positions, we embed the API details and the response at each position for all API calls\n\nModel(s):\nGPT-J is the LM used for fine-tuning\nTraining:\n\nFine-tuning using the augmented dataset and language modeling objective\nGreedy decoding is used. If &lt;API&gt; token is one of the top 10 tokens at any position, API call will be made at that position\n\nOnly one API call is allowed for any input sequence to avoid getting stuck in a loop of API calls\n\n\nInference:\n\nTypical decoding is used until the generated token is -&gt; where the API call is executed and the response is embedded and the generation continues\n\n\nContribution: Toolformer model that still generalizes to natural language tasks and knows when/how to call API calls.\nLimitation:\n\nFine-tuned LM doesn’t know how to perform chain of API calls\nLM is sensitive to the wording of inputs\nSample inefficient\nModel is not capable of interactively working with tool\n\nNotes:\n\nUse LM to generate a synthetic annotated data for API calls using a few examples for each API\nToolformer must not loose its generality in performing other language-related tasks &lt;/API&gt; token inserted and the generation continues\nTools expored:\n\nQA system: Use LM that was trained on natural language dataset\nWikipedia: Use BM-25 on top of Wikipedia search dump\nCalendar: Only return the current date\nCalculator: Supports only 4 arithmetic ops (+, -, *, /)\nMachine translation: Translate from any language to English. The source language is detected using fasttext classifier\n\nBigger models make better use of the APIs and benefit from them\nSmaller models, below 775M, don’t see a benefit from using APIs\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/gpt.html",
    "href": "papers-summaries/gpt.html",
    "title": "GPT: Improving Language Understanding by Generative Pre-Training",
    "section": "",
    "text": "GPT: Improving Language Understanding by Generative Pre-Training\n\nThesis: With abundance of unlabeled text and the huge cost of acquiring/labeling text data, we can use semi-supervised learning that utilizes the abundance of unlabeled text along with language modeling objective to train a model that learns general linguistic knowledge. This would lead to learning general representations that can be used with minimal changes to perform downstream tasks discriminatively.\nMethods:\n\nModel:\n\nDecoder transformer with 12 transformer blocks\nToken and positional embeddings\n768 embedding dimension\n512 context window size\nBPE tokenizer with 40,000 merges\nDropout = 0.1 for all residual, embedding, and attention\nBatch size = 64\n\nTwo stage training.\n\nGenerative Pre-training that uses language modeling objective with unlabeled text. This means we try to predict the next word given the previous k-tokens where k is the context window\nDiscriminative fine-tuning pre-trained model on downstream tasks using labeled data so the model can adapt to new task. Depending on the downstream tasks, we may need to transform the input so we can utilize the pre-trained model w/o any changes. All tasks would have pre-trained model followed by linear layer and softmax\n\nOnly classification doesn’t require any changes to input.\nWe use the representation of the predicted word to be the input of the linear layer\nInputs for all tasks have &lt;s&gt; and &lt;e&gt; tokens that are initialized randomly\n\n\n\n\nContribution: Introduce two training phases (generative pre-training and discriminative fine-tuning) using decoder-only transformer architecture that would learn general linguistic knowledge with no changes to the pre-trained model when tuning on downstream tasks\nTakeaways: Training large language models on large unlabeled datasets is a major step forward to acquire deeper and general linguistic knowledge that can be utilized by downstream tasks with shorter fine-tuning. Also, transferring more transformer layers improve performance for all tasks which suggest that each layer helps learn more language skills\nImportance: We can use the same model for any NLP tasks with no changes to the architecture and utilize general language knowledge model acquired during pre-training. The changes may only be needed for input/output layer\nImprovements: The current solution doesn’t work well on short sequences.\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/code-llama2.html",
    "href": "papers-summaries/code-llama2.html",
    "title": "Code Llama: Open Foundation Models for Code",
    "section": "",
    "text": "Code Llama: Open Foundation Models for Code\n\nThesis: SOTA code-generation/completion models among open LLMs with infilling capabilities and large context windows. The code models are all based on Llama2 pre-trained models.\nMethods:\n\nModels:\n\nAll models were initialized from pre-trained Llama2 models [[llama2]]\nAll models were fine-tuned with infilling code objective on 500B tokens from code-heavy dataset except the 34B model, which was trained on 1T tokens\nCode-Llama is a foundational model for general code generation tasks\n\nFine-tuned on 16k context sequences with infilling objective\n\nCode-Llama Python for python language\n\nFine-tuned on Python code with 100B tokens w/o infilling objective\nThen fine-tuned on 16k context sequence with 20B tokens\n\nCode-Llama Instruct for zero-shot instruction and safer responses\n\nFine-tuned on 16k context sequence with 20B tokens\nThen instruction fine-tuning using human-generated data and self-instruct dataset generated by Llama2 model prompted to generate code problems and Code Llama2 to generate unit tests and solutions (5B tokens)\nThe model proved to be more helpful at a moderate cost in generation performance\n\nSizes: 7B, 13B, 34B, 70B\nCode completion/generation models should have much larger contexts at the scope of full repo which is much larger than the 4096 tokens for Llama base model. Therefore, add fine-tuning step with much larger contexts up to 100k tokens. This is done by change RoPE parameters for positional encodings\nBPE tokenizer\n\nDatasets:\n\nPublicly available dataset\n8% of the training dataset is sourced from natural language dataset that discusses questions around code\nSmall proportion of batches are from natural language datasets to retain natural language understanding\nFor Code-Llama Instruct, instruction datasets from [[llama2]] is used. Additionally, synthetic dataset was used to create programming questions, solution, and unit tests. For questions, Llama2 70B was used but Llama2 7B was used to generate solutions and unit tests.\n\nInfilling objective: Move parts of the training sequence to the end and the reordered sequence is predicted autoregressively. In other words, we move span of the tokens from the sequence to the end and the model’s context would be the token before the span and after and would predict the span itself [[fim]]\n\nFIM rate is 90%\n50% of transformations are PSM while the other half are SPM\nSplitting the documents happens at the character level where spans are selected randomly from uniform distribution\n\n\nContribution:\n\n4 Different sizes of open models that range from 7B to 70B parameters\nPython-specific code models\nModels that are more aligned with user instructions through Code-Llama2 Intsruct\n\nTakeaways:\n\nModel specialization improves performance. Code-Llama Python &gt; Code-Llama &gt; Llama2 for python code generation\nPerplexity decreases as context length increases up to 100K\nModel size adds performance gains\nInfilling models suffer slight drop in performance on downstream tasks\nStarting from base model that was trained on both code and language and then fine-tuned on code datasets outperforms a model that is trained from scratch on code only but is much more useful in real-world applications\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/roberta.html",
    "href": "papers-summaries/roberta.html",
    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "section": "",
    "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach\n\nThesis: Better hyperparameter/design decisions is all you need to improve BERT’s performance to be on-par or better than all other pretraining models/objectives. BERT was significantly undertrained and not much time was spent to optimize hyperparameters. RoBERTa basically is just a reimplementation of BERT with some minor changes to tokenizer, hyperparameters, datasets, etc.\nMethods:\n\nModel: similar to [[bert]]\nTraining methodology:\n\nTrain on 512 sequence length all the time as opposed to BERT that trains for only \\(10\\%\\) of the steps on 512 sequences and the rest \\(90\\%\\) of the steps on 256 sequences\nMixed-precision training\nStatic vs dynamic masking:\n\nBERT used static masking where the training data was duplicated 10 times and the masking pattern for each training copy for each sequence was generated once during preprocessing. Therefore, for 40 epochs, each sequence with the same mask would be seen 4 times\nDynamic masking generates mask pattern every time sequence is fed to the model.\n\nSentence-pair vs sequence-pair (w/ nsp loss):\n\nBERT used sequence pair where each sequence can have multiple linguistic sentences\nSentence-pair: Each input has linguistic sentence pair that are either sampled from the same document or different document. Performed worse, may be due to difficulty of BERT learning long-range dependencies\n\nfull-sentences vs doc-sentences (w/o nsp loss):\n\nfull-sentences mean sentences are sampled from contiguous text that can cross document boundaries (insert document seperator special token).\ndoc-sentences is the same as full-sentences but can’t cross document boundaries. This means we may end up with less tokens than 512 if we sample close to the document edge. Performs slightly better than full-sentences but requires changing batch size. To avoid complexity, RoBERTa uses full-sentences\n\nBatch size: It is easier to train large batch sizes with less steps because we can use distributed data parallel training. Other works have shown that increasing batch sizes improves downstream tasks performance\nBPE tokenizer with 50K vocabulary size\n\n\nContribution: Improve hyperparameter decision choices of BERT that proved that BERT was massively undertrained\n\nTrain longer, with larger batch, over larger dataset and longer sequences\nRemove next sentence prediction\nUse dynamic pattern for masked language (BERT uses static)\nCC-News dataset that is similar in size to private datasets that are used to fine-tune. This dataset will be used to control for the effectiveness of increasing pre-trained dataset size\n\nImportance: deep bidirectional transformer with MLM and smart design choices is all you need :)\nTakeaways: RoBERTa proved that MLM is still a good training objective that can lead to a model that matches or exceeds other models on many benchmarks.\nImprovements:\n\nIt isn’t clear what is the effect of different % for MLM\nIn this setup, if we have multiple masked tokens in one sentence, the prediction of the each masked token doesn’t take into account the prediction of other masked tokens.\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/llm-as-judge.html",
    "href": "papers-summaries/llm-as-judge.html",
    "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
    "section": "",
    "text": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n\nThesis: Provide scalable, interpretable, automated method to evaluate LLM alignment with human preference. There is discrepancy between LLM benchmarks and human preferences, especially when it comes to chatbot assistants and we need to have hybrid benchmarks that cover both core capabilities and instruction following in a multi-turn conversations.\nMethods:\n\nTypes of LLM-as-a-Judge:\n\nPairwise comparison: Presented with a question and two answers, the LLM judge is asked to determine which answer is better or if they tie\nSingle answer grading: LLM judge is asked to grade an answer given its question\nReference-guided answer grading: LLM judge is asked to score the answer according to grading criteria based on the referenced answer\n\nMulti-turn questions are presented in one prompt and the LLM judge is asked to focus on the answers of the second question from each model\nAgreement is measured with 58 human experts on the MT-bench and thousands of crowd-users on the Chatbot Arena\n\nContribution:\n\nTwo benchmarks:\n\nMulti-turn questions set (MT-bench): Evaluate chatbot’s ability to follow instructions in multi-turn conversations. It includes 80 high quality multi-turn open-ended questions, 10 questions per use case categories\nChatbot Arena: Crowdsourced platform that anonymous users use to chat between two chatbots at the same time and rank the responses according to their personal preferences. It has 30K votes for users who interacted with the models. It covers wider use cases than MT-bench as there is no restrictions of what the user should ask\n\nSystematic study of LLM-as-a-judge to approximate human preferences\n\nImportance: Provides a scalable and alternative to human annotators to align LLMs to human preferences\nTakeaways: Strong LLMs can be used as a judge to approximate human preferences to score answer(s) of questions with more than 80% agreement rate. This maybe due to the fact that strong LLMs have been trained using RLHF. Single answer grading agreement is on par with human agreement (&gt; 80%)\nImprovements:\n\nPosition bias: Most LLMs suffer from position bias where they give higher score for the first answer even when we flip the answers or the answers are very similar and are indistinguishable. This can be addressed by calling LLM judge twice and swap the positions of the answers and only declare it a win if the LLM is consistent\nVerbosity bias: LLMs tend to prefer longer answers even if they are less accurate and low quality\nSelf-enhancement bias: LLMs tend to favor their answers\nLimited reasoning: LLMs may not know the correct answer. This can be addressed by chain-of-thought (CoT) OR reference-guided answers which tend to work much better. Paper basically provides reference answer in the prompt.\n\n\n#nlp #fine-tuning #eval"
  },
  {
    "objectID": "papers-summaries/chain-of-thought-prompting.html",
    "href": "papers-summaries/chain-of-thought-prompting.html",
    "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
    "section": "",
    "text": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n\nThesis: Chain-of-thought prompting facilitates Chain-of-thought reasoning, which is an emergent ability for large scale LLMs. Through Chain-of-thought prompting, we can improve reasoning capabilities significantly, which in turn improves the accuracy on arithmetic, commonsense, and symbolic reasoning tasks.\nMethods:\n\nChain-of-thought prompting template\n\nContribution:\n\nChain-of-thought prompting technique\nCoT allows for easier debugging since the model provide the intermediate steps that it used to arrive at the final answer. This is also helpful for interpretability\nCoT uses more computation to solve intermediate/smaller tasks before getting the final answer\nCoT prompt works across different LLMs\nThere is no meaningful difference in the performance of CoT prompting based on number of examples, their order, and the type of examples\n\nTakeaways: We can improve LLMs performance on reasoning tasks using few-shot CoT prompting w/o the need for training from scratch or fine-tuning. This is true especially for large scale LLMs as because such models acquire better semantic understanding and reasoning capabilities with scale (emergent ability of LLMs)\nImprovements:\n\nCoT prompting works for sufficiently large LLMs, so it may not work well with smaller LLMs or have smaller gains\nWe have to have task-specific prompt template where examples and intermediate steps (chain of thoughts) must be carefully chosen to realize the performance boost\nThere is a decent variance in performance for different CoT prompting examples, which require more human effort to make sure we’re realizing the full potential of this approach. Even though we have variance in performance, CoT prompting performed better than few-shot prompting. So prompt engineering STILL MATTERS\nThere is no guarantee that the intermediate steps generated by the model are correct and may also lead to the wrong answer\nWe don’t know if the chain of thought the model is following is reasoning\nInference cost for large scale LLMs with few-shot CoT prompting\n\nNotes:\n\nChain-of-thought is a series of intermediate steps\nFew-shot prompt doesn’t help on reasoning tasks\nCoT reasoning abilities improve with scale\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/gpt3.html",
    "href": "papers-summaries/gpt3.html",
    "title": "GPT3: Language Models are Few-Shot Learners",
    "section": "",
    "text": "GPT3: Language Models are Few-Shot Learners\n\nThesis: Move from architecture agnostic to task agnostic paradigm where we pre-train a very large language model on sufficiently large and diverse data that would result in a model that learned a wide variety of skills and absorbed patterns, which would allow the model to recognize or learn tasks at inference time on the fly w/o any training. Few-shot is what you need to perform non-trivial tasks, but zero-shot and one-shot are a good-start.\nMethods:\n\nModel:\n\nContext window size = 2048\nModel architecture follows [[gpt2]]\n\\(175B\\) parameters\n96 layers\nembedding dimension is \\(12,288\\)\n96 heads\n\n\nContribution:\n\n175B LLM that learned variety of tasks\nIllustrations of in-context learning potential and the differences between zero-, one-, and few-shot.\n\nTakeaways:\n\nModel size follows power-law with validation logloss which correlates with performance of downstream tasks\nFew-shot leads to almost comparable results as fine-tuned models for some NLP tasks\n\nImprovements/Limitations:\n\nScale of the model is prohibitively expensive to run inference -&gt; we may need to use some form of model distillation because GPT3 have wide variety of skills that may not be needed in specific domains\nIt’s autoregressive nature doesn’t allow to look at surrounding context in both directions, which is needed to perform some tasks such as fill in the blanks\nThe model doesn’t seem to have basic understanding of the physical world\nThe model lacks the context from video and other real-world physical interactions\nModel has poor sample efficiency as it sees a lot of text during pre-training that human doesn’t see during its lifetime\nSelf-supervised training objective is approaching its limits and we may need to change the objective by using something like reinforcement learning or add different modalities such as videos to provide better grounding\nThe model sometimes generate repetitive text, suffers from coherence in long-range dependencies of sequence, generate contradictory sequences, etc.\nWe don’t know if the model actually learns new tasks from scratch at inference time or simply recognizing patterns\nTokens are all weighted equally during prediction, which is not right because there are more important tokens that others\nThe model is not interpretable\n\nImportance:\n\nThe largest LLM so far that helps us to use it is a wide variety of tasks w/o fine-tuning. For complex tasks, few-shot is very useful and leads to great results in some settings.\n\nNotes:\n\nLarger models show steeper improvement for in-context learning through better performance with few-shot demonstrations\nCreate language systems that are able to perform any language tasks on the fly w/o the need for fine-tuning to adapt to domain-specific tasks. Therefore, move from architecture agnostic paradigm where we first pre-train models in semi-supervised fashion then fine-tune it on task-specific data to task agnostic paradigm where we only pre-train very large LMs on very large and diverse datasets and use only text to specify task with few examples and the model would learn (with no gradients -&gt; in-context learning) what it is supposed to do\nThe reason to have task-agnostic models are:\n\nNot all language tasks have large enough supervised training data to fine-tune the model on\nLarge models are meant to have large capacity to absorb general language understanding. When fine-tuned on narrow domain where training distribution is narrow, such models tend to pick up spurious correlations -&gt; model’s generalization suffers outside training distribution\nHumans don’t need thousand of examples to perform any new task. They need just task specification or at most few examples\nFinally, we want the model to have the same flexibility as humans to be able to switch between different tasks smoothly such as doing arithmetic in a dialogue\n\nWe don’t know whether LM recognizes task from skills and patterns learned during training or learns the task at inference time from the examples provided\n\nLarger models demonstrate stronger ability to recognize tasks\n\nLog loss is correlated with downstream tasks performance\nMeta-learning in LM means the model learns a set of broad skills and pattern recognition abilities at training time that allows it to recognize or adapt to the new task at inference time\nPerformance of GPT3 improves with:\n\nIncreasing model size\nIncreasing dataset size and diversity\nDescription of the task in natural language\nMore demonstrative examples\n\nThe larger the model size -&gt; the bigger the gap between zero-shot, one-shot, and few-shot examples. Therefore, LLMs suggest they are meta-learners\nDisadvantages of fine-tuning:\n\nRequires large labeled dataset for every task\nDoesn’t generalize well for out-of-distribution\nExploit spurious correlations/features in the training data\n\nZero-shot: Gives the model natural description of the task we want to solve. It is the most convenient and mimic how humans solve most of the tasks. Also, it is more robust and avoid spurious correlations. The main disadvantage is that the task may be not clear and LLM may not be able to figure out what is actually needed.\nOne-shot: Gives the model natural description of the task we want to solve plus and 1 demonstration of task and answer. Humans also use something like this in settings where it is not clear how the output should be presented.\nFew-shot: Gives the model natural description of the task we want to solve plus few demonstrations. This approach still requires some kind of labeled data as we need more examples for tasks and their outputs. The number of examples are determined based on the difficulty of the task and the context window size.\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/reflexion.html",
    "href": "papers-summaries/reflexion.html",
    "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
    "section": "",
    "text": "Reflexion: Language Agents with Verbal Reinforcement Learning\n\nThesis: Improve reasoning skills through dynamic memory and self-reflection. At each step, the actor model generates a trajectory that is passed to the evaluator to score. Both trajectory, environment observation, and score is fed to the self-reflection model to write verbal feedback, which will be stored in the dynamic memory and used as a context for the Actor so it learns from previous errors and correct trajectory\nMethods:\n\nActor: LLM that generates actions and text. It uses frameworks such as CoT or ReAct and the model is prompted to generate actions/text based on the state observation. There is also memory that adds context to the Actor model\nEvaluator: LLM that scores the outputs of the Actor\nSelf-Reflection: LLM that writes verbal reinforcement to help the Actor self-improve\n\nContribution:\n\nReflexion paradigm that uses verbal reinforcement from previous trial/error and generates summary (self-reflection) that would be part of the LLM context in episodic memory\n\nTakeaways: Self-reflection is key to improve reasoning skills of agents and is much cheaper computationally than traditional RL\nImprovements:\n\nRelies the on the capabilities of the LLMs and heuristics to evaluation actions, which is not guaranteed to be successful\n\nNotes:\n\nVerbally learn from past mistakes and maintain an episodic memory that has the previous feedback signals. The feedback from previous action is converted to textual summary, which is then stored in the episodic memory\nIt can incorporate any data type (source or natural language) and any source (internal or external)\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/gpt2.html",
    "href": "papers-summaries/gpt2.html",
    "title": "GPT2: Language Models are Unsupervised Multitask Learners",
    "section": "",
    "text": "GPT2: Language Models are Unsupervised Multitask Learners\n\nThesis: Train large models that generalize well to any downstream tasks w/o any fine-tuning or training needed. The main motivation is to have competent generalist model instead of narrow experts that are trained on specific dataset for specific domains. Zero-shot framework allows us to just use pre-trained large language models to perform any task. This is possible because the LM would need to predict the task before being able to predict the next tokens or string.\nMethods:\n\nUse BPE build on utf-8 bytes to get middle ground between word-level and character-level LM\nModel:\n\nVery similar of GPT except:\n\nContext window size increased from 512 to 1024\nBatch size increased from 64 to 512\nLayer normalization is added to input -&gt; pre-activation\n\n\n\nContribution: WebText dataset with sufficiently large and diverse data as well as zero-shot demonstrations and larger version of GPT with largest model having \\(1.5B\\) parameters\nTakeaways: Training high capacity LLMs on very large and diverse datasets such as WebText will force the model to learn how to perform different tasks indirectly in order to predict next token from different domains and tasks. This allows us to use such models in zero-shot setup to perform different downstream tasks\nImportance: Major step to move towards training-free LLMs adaptation to downstream tasks with zero-shot.\nNotes:\n\nLLMs store information in their parameters as is shown from answering questions which require factoid answers (closed-book)\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/fim.html",
    "href": "papers-summaries/fim.html",
    "title": "Efficient Training of Language Models to Fill in the Middle",
    "section": "",
    "text": "Efficient Training of Language Models to Fill in the Middle\n\nThesis: The paper demonstrates that training Autoregressive (AR) LLMs with infilling capabilities using Fill-in-the-middle methods improve code infilling w/o compromising on the traditional left-to-right AR loss for natural language generation.\nMethods:\n\nModels:\n\n8 model sizes from 50M to ~7B trained\n\nDataset(s):\n\nEach model is trained on 100B token of code with/without FIM -&gt; two versions\nEach model is trained on 100B token of language with/without FIM -&gt; two versions\nSplitting documents happen at the character-level before tokenization\nFor the prefix-suffix-middle (PSM):\n\nTraining:\n\n&lt;PRE&gt; ◦ Enc(prefix) ◦ &lt;SUF&gt; ◦ Enc(suffix) ◦ &lt;MID&gt; ◦ Enc(middle)\nInference\n\n&lt;PRE&gt; ◦ Enc(prefix) ◦ &lt;SUF&gt; ◦ Enc(suffix) ◦ &lt;MID&gt;\n\n\n\nFor the suffix-prefix-middle (SPM):\n\nTraining:\n\n&lt;PRE&gt; ◦ Enc(suffix) ◦ &lt;SUF&gt; ◦ Enc(prefix) ◦ &lt;MID&gt; ◦ Enc(middle)\n\nInference\n\n&lt;PRE&gt; ◦ Enc(suffix) ◦ &lt;SUF&gt; ◦ Enc(prefix) ◦ &lt;MID&gt;\n\n\n&lt;EOT&gt; is appended to the end of documents and included in training so the model can learn to communicate when the infilling/connection of prefix and suffix is done\nGeneration continues until the &lt;EOT&gt;. If the model didn’t generate &lt;EOT&gt; after a predefined number of tokens, we return best of n-samples\n\nSPM proved to be slightly better than PSM probably because prefix and middle are naturally contiguous chunk of tokens and model doesn’t need to pay attention to sentinel tokens to figure out of changes in continuation\nFIM rate used during training was 50%\nExperiments showed that higher FIM rates don’t cause a degradation in performance except for 100% FIM rate\n\nContribution:\n\nShow how different variants of large scale LLMs trained with FIM learned Infilling capability w/o compromising their left-to-right generation abilities\nBest hyperparameters for FIM transformation such as FIM rate and different FIM transformations such as prefix-suffix-middle or suffix-prefix-middle\nIllustrate how FIM fine-tuning is computationally much less efficient than FIM pretraining\nTwo new benchmark datasets:\n\nRandom span infilling\nRandom span infilling light\n\n\nTakeaways:\n\nFill-in-the-middle (FIM): Model trained with 50% FIM were shown that they don’t incur any performance penalty yet they learn new capability even though they only see the the original sequences 50% of the time\nFIM Fine-tuning pretrained models that were trained originally left-to-right isn’t sample efficient and won’t help models acquire infilling capabilities relative to its size -&gt; Train with FIM from scratch to achieve good performance on infilling tasks\nApply FIM at the character-level before tokenization\nUse PSM and SPM jointly during training\n\nNotes:\n\nTraditional causal decoder LLMs are not capable of conditioning on the prefix context\nEncoder-only and Encoder-Decoder architectures can condition on both prefixes and suffixes; however, context window is typically much shorter at training time than it is useful at inference in real-world applications such as coding assistant that may require the current file and other files in the current directory to generate an accurate code\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/colbert.html",
    "href": "papers-summaries/colbert.html",
    "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
    "section": "",
    "text": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\n\nThesis: ColBERT is a middle ground approach that tries to combine the expressiveness of query-document interaction at less computational cost. It does this through its delayed interactions between query and document. Instead of doing interaction across all documents, we retrieve top-k documents and then do interaction using maximum cosine similarity.\nMethod(s):\n\nColBERT inner workings:\n\nEmbed query using query encoder (BERT). This will result in contextualized embedding for each token\nEmbed documents using document encoder (BERT) offline and index them\nGet top-k documents using query serving subsystem\nUse Maximum Similarity between query and document embeddings:\n\nFor each token in query:\n\nCompare its embeddings with all the tokens’ embeddings from the document using cosine similarity\nPick the maximum cosine similarity\n\nAt the end we would have n (number of query tokens) maximum cosine similarity scores. Sum them up to get the relevance of this document to the query\n\n\nQuery and Document encoders share the same BERT model\nQuery is prepended with [CLS] and [Q] tokens.\n\nIf query is longer that \\(N_q\\), it is truncated\nIf shorter, append [mask] tokens up to \\(N_q\\). The padded masked tokens are called query augmentation, which allows BERT to expand queries with new terms or re-weigh existing terms.\n\nDocument is prepended with [CLS] and [D] tokens. No truncation or appending [mask] token to document. The embeddings for punctuation symbols are filtered out\nThere is linear layer w/o activations on top of BERT embedding to reduce hidden dimension of embeddings. This reduces the cost of transferring the embedding from CPU to GPU during reranking.\nEmbeddings are normalized using \\(L_2\\) norm so inner products between query and documents would produce score [-1, 1] which is equivalent to cosine similarity.\nIndexing of documents happen offline. To maximize the throughput, indexer split documents into groups and within each group, documents are sorted based on their length. For each batch (such as b=128), document length is capped to the longest document and pad the shorter documents\nColBERT as reranker:\n\nRetrieve top-k documents using query serving subsystem retriever. The output would be tensor \\(D\\) of dimension k x \\(N_d\\) x h where \\(N_d\\) is number of tokens which is the number of tokens for the longest document in the top-k documents\nMultiply \\(D\\) by \\(Q\\) which has dimension \\(N_q\\) x h. Therefore, \\(D\\ @\\ Q.T\\) would have k x \\(N_d\\) x \\(N_q\\) dimension.\nWe then take maximum for each token for all tokens in each document then sum the maximums to get the relevancy score for each document\n\nColBERT end-to-end:\n\nFAISS is used to index embeddings of documents\nFor each token in the query, run vector search to get \\(k'\\) &lt; \\(k\\) documents. Therefore, we would end up with \\(N_q\\) x \\(k'\\) documents. Finally, filter out duplicate documents to get \\(k &lt;= N_q\\) x \\(k'\\) documents\nRerank the k-documents as we did in ColBERT reranker\n\n\nTakeaway: ColBERT kind of gives us the best of two worlds: the expressiveness from the interaction of query and documents and the efficiency due to late interactions because reranking happens on top-k documents that were already precomputed offline. ColBERT showed its effectiveness as both reranker and end-to-end.\nContributions: Late interactions of query and documents to improve relevancy w/o increasing computational cost\n\n#nlp #rag"
  },
  {
    "objectID": "papers-summaries/bert.html",
    "href": "papers-summaries/bert.html",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "section": "",
    "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\nThesis: Build architecture agnostic model that learns rich contextualized representations of tokens from attending to both right and left contexts. Having access to surrounding context from both directions is critical to build such representations which are critical for token and sentence level tasks such as classification and QA.\nMethods:\n\nModel:\n\nEncoder-only transformer\nThere are two models:\n\nBase: L = 12, hidden_dim = 768, n_heads = 12 -&gt; total_params = 110M\nLarge: L = 24, hidden_dim = 1024, n_heads = 16 -&gt; total_params = 340M\n\n512 context window size\n\nDue to computational cost, train \\(90\\%\\) of the steps with sequence length of 128 and last \\(10\\%\\) steps with sequence length of 512 tokens\n\nTraining loss is the sum of the mean MLM likelihoods and the mean of the NSP likelihood\n\n“Sentence” in BERT refers to contiguous span of text and not necessarily a linguistic sentence\nTokenizer: WordPiece with 30,000 vocabulary\n[CLS] is prepended to every sequence as the first token. The final hidden vector of this token is used the consolidated representation for the full sequence in classification tasks where an output layer would typically be added to project it to output needed to classification tasks such as vocabulary or number of classes\n“Sequence” input consists of 2 sentences separated by [SEP] token during pre-training. Therefore, we have two sentences: A & B.\n\nWe would have token embedding for each sentence\nWe would add token embedding E[A] for each token in sentence A and token embedding E[B] for each token in sentence B\nTherefore, input token embedding would be the sum of token embedding, position embedding, and segment embedding\n\nThe final hidden vector of each token is the contextualized token embedding \\(T_i\\in{R^H}\\)\nMasked language model (MLM): predict the masked word\n\nFor every sequence, \\(15\\%\\) of the token positions will be masked at random. Out of the \\(15\\%\\) token positions:\n\n\\(80\\%\\) will be replaced with the [MASK] token\n\\(10\\%\\) will be replaced with random token. This doesn’t harm the language understanding because it happens only \\(1.5\\%\\) (\\(10\\%\\) x \\(15\\%\\))\n\\(10\\%\\) will keep the same token to bias the representation towards the actual observed token\n\nThis would force the model to keep contextualized token representation as the model can’t predict which token will be masked\nFor masked tokens, use their final hidden representations as input to softmax layer over the vocabulary to get the prediction (probability distribution). Cross entropy loss is used over these masked tokens\n\nNext sentence prediction (NSP): Predict whether sentence B is the next sentence to A. This objective helps understand the relationships between sentences, which is needed for tasks such as QA.\n\nEvery sentence A would be followed by:\n\n\\(50\\%\\) of the time with actual next sentence from corpus\n\\(50\\%\\) of the time with random sentence from corpus\n\nThe [CLS] final hidden vector would be an input to softmax layer as binarized classifier to predict whether sentence B is next sentence.\n\nFine-tuning:\n\nChange input/output based on the downstream task.\n\nFor classification, input has [CLS] followed by the sentence (full text). The [CLS] final hidden vector would be fed to softmax layer to get the prediction -&gt; newly added parameters are \\(W \\in R^{K x H}\\) where \\(K\\) is number of classes\nFor QA, input has [CLS] followed by question tokens (sentence A) followed by separator [SEP] followed by answer tokens (sentence B). Two new tokens are introduced: S \\(\\in{R^H}\\) for start span token and E \\(\\in{R^H}\\) for end span token. Each token of the paragraph (sentence B) final hidden vector is dot-product with S followed by softmax over all tokens in sentence B to get the start span. The token with highest probability would be the start token. The same is done for end token E. The objective is the sum of the likelihoods of both the start and the end tokens.\n\nEach task would almost have the same model (except the output layer) and initialized with the same pre-trained model -&gt; Each task would have its own fine-tuned model\n\n\nContribution:\n\nImportance of contextualized token representations using deep bidirectional transformer\n\nImportance: This the first transformer-based architecture that uses deep bidirectional transformer (encoder-only) to create rich representations for tokens. Different downstream tasks can use the same model but may add output layer specific to the task and obtains better performance than auto-regressive or LMs or left-to-right models in general\nTakeaways:\n\nFine-tuning outperforms feature-based approaches even if we use representations from BERT from different layers.\nModel size leads to improve performance on downstream tasks\n\nImprovements:\n\nIt isn’t clear what is the effect of different % for MLM\nI am not convinced we need NSP to achieve better performance\nIn this setup, if we have multiple masked tokens in one sentence, the prediction of the each masked token doesn’t take into account the prediction of other masked tokens.\n\nNotes:\n\nPre-trained language representations usage approaches:\n\nFeature-based approach: Use pre-trained token representations in task-specific architectures as embeddings and train the architecture. ELMO is an example of this approach\nFine-tuning approach: Fine-tune all parameters of the model to the downstream task such as GPT/BERT\n\nELMO word embeddings, which are concatenated word embeddings from bidirectional LSTMs (training LSTM from left to right and right to left), are contextualized word embeddings. However, previous approaches to ELMO such as word2vec and GloVe were contextless word embeddings.\n\nContextualized word embeddings: We know that the same word could have different meanings depending on its context. Therefore, the same word would have different embeddings in different context.\nContextless word embeddings: Each word would have the same embedding obtained from pre-training regardless of its context\n\n\n\n#nlp #llm"
  },
  {
    "objectID": "papers-summaries/self-instruct.html",
    "href": "papers-summaries/self-instruct.html",
    "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    "section": "",
    "text": "Self-Instruct: Aligning Language Models with Self-Generated Instructions\n\nThesis: Self-instruct is a framework that uses base LLMs (such as GPT) to generate samples of instructions, inputs, and outputs that will be used, after filtering low quality and very similar instructions, to align LLMs to follow user instructions.\nContributions:\n\nThis framework allows us to generate almost annotation-free instructions that performs very close to InstructGPT\nBecause generated instructions are more creative/diverse than human-created instructions, LLMs fine-tuned on data using self-instruct outperforms the ones fine-tuned on public instruction data\n\n52K synthetic instructions dataset and a set of manually written novel tasks for building and evaluating future instruction-following models\nMethod(s):\n\nVanilla GPT3 is used as the LM in all the steps of the generation pipeline\nThe authors started with \\(175\\) pool of task seed that was written by them\nEach instruction can have \\(&gt;= 1\\) input/output pairs\n\nNot all tasks have inputs\n\nSelf-Instruct pipeline:\n\nInstruction generation:\n\nSample 8 instruction: 6 instructions from seed instructions and 2 from previously generated instructions\nPrompt the model to generate new instructions. Model can generate up to 8 instructions\n\nInstruction classification:\n\nSample 12 classification instructions and 19 non-classification instructions from the seed tasks\nPrompt the model to determine if the instructions generated are classification or non-classification tasks\n\nInstance generation: Generating input (if required) and output for generated tasks\n\nFor classification tasks, model is asked first to determine the class of the task. Then conditioning on the instruction and the class, model is asked to generate the output. This is called output-first approach\nFor non-classification tasks, model is first asked to generate input (if required else [NULL]). Then is it asked to generate the output for the task. This is called input-first approach\n\nFiltering: Generated instructions are cleaned up using some heuristics such as:\n\nToo long or too short\nOutput is the same as input\nSimilar to at least 1 task from the seed tasks with &gt;= 0.7 ROUGE-L\nSame inputs but different outputs\netc.\n\n\nThe final instructions data is used to fine-tune GPT3\n252 high-quality instructions written by the authors are used to evaluate and compare models.\n\nHuman annotators (same authors) rank the response of the models to the these instructions from A (valid and satisfying response) to D (response is invalid or irrelevant)\n\n\nTakeaways:\n\nMore data (instructions) leads to better performance but it plateaus after certain dataset size. This is largely dependent on the evaluations data and its diversity\nData quality plays a critical role in improving performance. It led to \\(10\\%\\) increase in performance when output of each instruction is rewritten by \\(InstructGPT_{003}\\)\n\n\n#nlp #fine-tuning"
  },
  {
    "objectID": "papers-summaries/react.html",
    "href": "papers-summaries/react.html",
    "title": "ReAct: Synergizing Reasoning And Acting In Language Models",
    "section": "",
    "text": "ReAct: Synergizing Reasoning And Acting In Language Models\n\nThesis: Combine reasoning and acting to utilize the synergy between the two. With this approach, we can have reasoning traces as well as actions and handle exceptions when performing actions and interacting with the environment\nMethods:\n\nRaAct prompting template/technique that can be used for various problem-solving tasks\nAugment context with thought, which LLM’s reasoning over the current context. Context is nothing but a set of previous action/feedback pairs\n\nContribution:\n\nReAct framework, which is prompt-based technique to combine reasoning and acting\nReAct is easy to use, generalize to any problem domain, robust and outperform baseline, and finally aligned with humans\n\nTakeaways: There are great synergies between reasoning and acting. CoT is a black box that suffers from hallucination because it is not grounded with external up-to-date relevant knowledge and is dependent on the model’s internal representation for reasoning. ReAct augment this with actions by using tools and external knowledge through actions and refine reasoning based on actions.\nImprovements:\n\nComplex tasks may not see huge improvement with ReAct as its action space requires much more examples that can’t be fit into context window. This problem is kind of resolved with increasing context window sizes of most LLMs\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a DMTS - AI Scientist with many years of experience in the field of Data Science and Machine Learning where I worked in a variety of industries such as Smart Devices, Consumer Goods, Real Estate, Marketing, and Healthcare.\nAmong other things, I am interested in Artificial Intelligence and Machine Learning. I write articles related to machine learning and AI on Medium and contribute to open source.\nHuman potential has no limits and I’m always trying to constantly learn and improve myself. I hope you find the contents of my website useful."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nThe University of Texas at Dallas | Dallas, TX\nMS in Economics | 2014\nLebanese International University | Beirut, Lebanon\nBA in Economics | 2009"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nArtificial Intelligence\nMachine Learning\nData Science"
  },
  {
    "objectID": "til/vim/notes-on-vim.html",
    "href": "til/vim/notes-on-vim.html",
    "title": "Notes on Vim",
    "section": "",
    "text": "To know which scrips Vim loaded, :scriptnames\nVim uses runtimepath to determine which scripts to run at startup or buffer load time. It inspects all directories and check scripts in sub-directories to check which scripts to run\n$VIMRUNTIME is the path for all files distributed with Vim\n.vimrc is the catchall config file that is used for general edition settings. These settings maybe overridden by other plugings/scripts\nCheck :options to know what are all the available options that we can set\nstart would load at startup\nopt on-demand loading using packadd\n\nThis will add the plugin base directory to the runtimepath and sources it plugin and ftdetect scripts\nWe can use :helptags ALL to update help tags documentation\n\nYou can use git submodules for all the packages used under ~/.vim"
  },
  {
    "objectID": "til/vim/clear-register.html",
    "href": "til/vim/clear-register.html",
    "title": "Clearing Registers",
    "section": "",
    "text": "The easiest way to clear a register; in norma mode, is q-register_name-q. The command qa will start recording a macro in the register a, which cleans the register before it starts the key strokes that follow. So when it is followed right away by q, it acts as clearing the register a. This is very useful if we’re trying to copy something to a register, such as copy all TODO from all project files into a register so that we can then past them in new file."
  },
  {
    "objectID": "til/devops/how-to-decide-what-to-include-in-kuberentes-pod.html",
    "href": "til/devops/how-to-decide-what-to-include-in-kuberentes-pod.html",
    "title": "Which Containers Should Be In The Same Pod",
    "section": "",
    "text": "If containers work correctly if they land on different machines, then it is safe to include them in different pods; otherwise, they have to be in the same pod. For example, if two containers interact through local filesystem, then it would be impossible to operate correctly if they land on different machines."
  },
  {
    "objectID": "til/devops/tips-reduce-docker-images.html",
    "href": "til/devops/tips-reduce-docker-images.html",
    "title": "Tips to Reduce Docker Images",
    "section": "",
    "text": "Docker uses layering when building Docker images, which means the next layer is basically the delta (add/remove/modify) from the previous layer. Therefore:\n\nEven when a layer deletes a big file from previous layer, it still exists in the image but just not accessible to the user. This means the size of the image includes the big file.\nEvery time a layer changes, all the layers after it should change. This means when building new image, Docker mayn’t be able to use cached layers due to the changes in previous layers. Therefore, start first with layers that aren’t expected to change much and keep the last layers for things that may change more frequently.\n\nBuild the Docker image using multi-stage build to avoid having multiple build tools that are not needed in the application in the image."
  },
  {
    "objectID": "til/pytest/pytest-fixtures.html",
    "href": "til/pytest/pytest-fixtures.html",
    "title": "Fixtures",
    "section": "",
    "text": "Fixtures are functions decorated by pytest.fixture() that run before (and maybe after) test functions by pytest to do the setup/teardown. Fixtures can do some setup work and return data to the test functions as well. The name of the fixture(s) are passed to the test/fixtures as parameters and pytest will see that and check for a fixture with that name. We can use the fixture name inside test functions to access returned data or other state(s) that the fixtute set up.\nTo conclude, fixtures help us separate the “getting ready for” and “cleaning up after” code of test functions. If the fixture has yield statement, then the fixture would run until the yield before yielding to the test function. Once the test function is done, the code following the yield statement would run even if the test function failed.\nIf we know that some fixtures have to be run always at certain times, we can avoid passing their names to all test functions by using autouse=True parameter in @pytest.fixture which means the fixture will be called always depending on the scope.\nWe can also rename a fixture using name parameter of @pytest.fixture. This is useful if the fixture name collides with some test functions OR we follow the convention to have the fixture name have fixture suffix/prefix.\nTest functions as well as other fixtures can depend on fixtures through their parameter list that would have the same name(s) of the fixture(s)."
  },
  {
    "objectID": "til/pytest/test-non-installable-scripts.html",
    "href": "til/pytest/test-non-installable-scripts.html",
    "title": "Testing Non-installable Scripts",
    "section": "",
    "text": "Sometimes we may want to test scripts that can’t be imported as packages and live in different directories than the tests directory. The easiest way to make our src code directory discoverable at runtime is to update Python’s sys.path. We can do that through pytest config file as follows:\n[pytest]\n...\ntestpaths = tests\npythonpath = src\n...\nThis will add src directory to the sys.path. This is relative to pytest.ini file. Also, by default, pytest add tests directory to the sys.path, but we’re here explicit about what is the tests path."
  },
  {
    "objectID": "til/pytest/pytest-test-outcomes.html",
    "href": "til/pytest/pytest-test-outcomes.html",
    "title": "Test Outcomes",
    "section": "",
    "text": "Possible outcomes of a test:\n\nPASSED (.): The test ran successfully\nFAILED (F): The test did not run successfully\nSKIPPED (s): The test was skipped (@pytest.mark.skip(), @pytest.mark.skipif())\nXFAIL (x): The test was not supposed to pass, and it ran and failed (@pytest.mark.xfail())\nXPASS (X): The test was marked with xfail, but it ran and passed\nERROR (E): An exception happened either during the execution of a fixtureor hook function, and not during the execution of a test function or any of the functions it called"
  },
  {
    "objectID": "til/pytest/hiding-function-traceback.html",
    "href": "til/pytest/hiding-function-traceback.html",
    "title": "Hiding Test Traceback",
    "section": "",
    "text": "Sometime we might want to invoke a test from another test function/method that tests some functionality but we don’t want to include the traceback of the invoked function if the test failed. We can achive this by including __tracebackhide__ = True at the top of the invoked function’s body."
  },
  {
    "objectID": "til/pytest/pytest-test-discovery.html",
    "href": "til/pytest/pytest-test-discovery.html",
    "title": "Test Discovery",
    "section": "",
    "text": "Test discovery process in pytest goes as follows:\n\nIt recursively looks for files in the given directories (use current working directory if no directory/file is given) and all their subdirectories that named with test_*.py or *_test.py.\nIn each of the test files identified, it looks for functions/methods that start with test_ and classes that start with Test\n\nWe can always run tests in specific file by running pytest file or specific test in a test file pytest path/to/file::test_name"
  },
  {
    "objectID": "til/pytest/running-tests-in-pytest.html",
    "href": "til/pytest/running-tests-in-pytest.html",
    "title": "Running Tests",
    "section": "",
    "text": "Below are different ways of running subsets of test in pytest:\n\nTest method inside TestClass: pytest   path/test_module.py::TestClass::test_method\nAll tests in TestClass: pytest path/test_module.py::TestClass\nTest function: pytest path/test_module.py::test_function\nAll tests a file: pytest path/test_module.py\nAll tests in a directory (and its subdirectories): pytest path\nTests matching a name pattern: pytest -k pattern\n\nWe can use not, and, or inside regex pattern\nExamples:\n\npytest -k forward would run all tests that have forward in their name.\npytest -k \"forward and not forward_backward\" would run all tests that have forward in their name but not forward_backward.\npytest -k \"forward or backward and not forward_backward\" would run all tests that have forward or backward in their name but not forward_backward."
  },
  {
    "objectID": "til/pytest/pytest-parametrization.html",
    "href": "til/pytest/pytest-parametrization.html",
    "title": "Parametrization",
    "section": "",
    "text": "Test parametrization allows us to avoid redundant code by writing one test function/fixture that tries to test the same logic with different values/parameters. pytest will generate tests for us by calling the same test function/fixture passing different combination of parameters each time. Example:\npytest.mark.parametrize(\n    [\"param_name1\", \"param_name2\", \"param_name3\"],\n        [\n        (1, \"a\", 3),\n        (1, \"x\", 0),\n        (5, \"d\", 100),\n    ]\n)\ndef test_func(param_name1, param_name2, param_name3):\n    pass\n\nThe first argument to pytest.mark.parametrize should either be a list of parameter names or a string of parameter names separated by “,”\nThe second argument to pytest.mark.parametrize should be a list of test cases\nThe arguments’ names to the test function should match the names in the first argument to pytest.mark.parametrize"
  },
  {
    "objectID": "til/python/first-class-objects.html",
    "href": "til/python/first-class-objects.html",
    "title": "First Class Objects",
    "section": "",
    "text": "First class objects are objects that can be:\n\nCreated at runtime\nAssigned to a variable or element in a data structure\nPassed as an argument to a function\nReturned as a result of a function\n\nExamples of first class objects are intergers, strings, dictionaries, etc."
  },
  {
    "objectID": "til/python/context-mgr-protocol.html",
    "href": "til/python/context-mgr-protocol.html",
    "title": "Context Manager Protocol",
    "section": "",
    "text": "with statement is designed to work with context manager objects to simplify the try/finally pattern, which guarantees that some code is executed (clean up code) after the with block code is aborted because of an exception, a return or sys.exit() call. In a sense, the clean-up that would typically be in the finally clause such as releasing some resources or restoring the temporarily changed state.\nContext manager protocol implements __enter__ & __exit__:\n\n__enter__ will be invoked on the object after with keyword, which is called the context manager object. It takes no arguments.\nThe value __enter__ returns, which could be None, self, or any other value would be assigned to the target variable that comes after as keyword.\nOnce the with code block is terminated for any reason, __exit__ is invoked on the context manager object, which takes 3 arguments (all default to None):\n\nexc_type: Exception type such as ValueError\nexc_val: Exception object`\ntraceback: Traceback object\n\n\nclass ContextProtocal:\n    def __enter__(self):\n        # Setup\n        # return nothing, i.e. None\n        # Or return self\n        return self\n\n    def __exit__(self, exc_type, exc_val, traceback):\n        # Teardown which should typically handle exceptions\n        # to make sure the  Teardown is taken care of even\n        # in the case of exceptions\n        pass"
  },
  {
    "objectID": "til/python/retrieve-object-attributes.html",
    "href": "til/python/retrieve-object-attributes.html",
    "title": "Retrieving Object’s Attributes",
    "section": "",
    "text": "When we try to get any object’s attributes such as obj.x, the interpreter follow more or less the following steps:\n\nIt invokes __getattribute__ special method on the instance. If it fails,\nIt invokes __getattr__ special method. It it fails,\nIt returns AttributeError\n\nclass C:\n    def __init__(self, x):\n        self.x = 10\n    \n    def __getattribute__(self, k):\n        print(\"getattribute\")\n        return super().__getattribute__(k)\n    \n    def __getattr__(self, k):\n        print(\"getattr\")\n        return super().__getattr__(k)\nWhen we want to check if an object has a specific attribute, we can use hasattr(obj, \"attribute\"), which follows the same logic as above to test if the object has the given attribute."
  },
  {
    "objectID": "til/python/itemview-and-keyview.html",
    "href": "til/python/itemview-and-keyview.html",
    "title": "Itemview and Keyview Attributes of Dictionary are Subclasses of Set",
    "section": "",
    "text": "dict.items() and dict.keys() returns ItemsView and KeysView which are subclasses of Set. Therefore, we can do all the set operations on them such as intersection and union and the type of the object returned is a Set. Examples:\nd1 = dict(zip(\"abcd\", range(1, 5))) #=&gt; {'a': 1, 'b': 2, 'c': 3, 'd': 4}\nd2 = dict(zip(\"cdef\", range(3, 7))) #=&gt; {'c': 3, 'd': 4, 'e': 5, 'f': 6}\nNow we can do any set operations on .items() or .keys() views.\nd1.keys() & d2.keys() #=&gt; {'c', 'd'}\nd1.keys() | d2.keys() #=&gt; {'a', 'b', 'c', 'd', 'e', 'f'}\nd1.items() - d2.items() #=&gt; {('a', 1), ('b', 2)}"
  },
  {
    "objectID": "til/python/slots.html",
    "href": "til/python/slots.html",
    "title": "Using __slots__ to Store Instance’s Attributes",
    "section": "",
    "text": "Typically all attributes are stored in the instance’s dictionary __dict__. This will add space overhead due to the hash table being sparse. We can save memory by storing all the attributes in a tuple-like data structure using __slots__ at the beginning of the class definition (see below):\nimport sys\n\n\nclass C:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\nclass C_Slots:\n    __slots__ = (\"x\", \"y\")\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\nprint(sys.getsizeof(C))         #=&gt; 904\nprint(sys.getsizeof(C_Slots))   #=&gt; 1,072\nThe main problems with __slots__:\n\nCan’t dynamically add attributes to the instance. We can work around this issue by adding __dict__ to __slots__ but this negates the benefits of using __slots__.\nDoesn’t support __weakref__ by default. We need to add __weakref__ to __slots__ to be able to use it.\nAll sublcasses have to declare __slots__ because attributes can’t be inheritted and will be ignored by the interpreter.\n\nIn conclusion, use __slots__ only if it is justified."
  },
  {
    "objectID": "til/python/scope-of-variables-in-comprehensions.html",
    "href": "til/python/scope-of-variables-in-comprehensions.html",
    "title": "Scope of Variables in Comprehensions",
    "section": "",
    "text": "Variables used in any form of comprehensions such as list/tuple comprehensions have their own scope and don’t mask/change variables in the outer scope. For example, if we have a global variable x and we use x in a list comprehension, it doesn’t mask or change the global variable x and it is local to the list comprehension.\nx = 10\nl = [x for x in range(100) if x % 2 == 0]\nprint(x) #=&gt; 10"
  },
  {
    "objectID": "til/python/lists-vs-tuples.html",
    "href": "til/python/lists-vs-tuples.html",
    "title": "Lists vs Tuples",
    "section": "",
    "text": "Tuples and Lists are referential data structures (container sequence), which means each element in them is just a pointer to objects but not the objects themselves. This is what allowed them to contain elements of different types. Compare that with arrays (flat sequence) that hold the actual elements, which requires all elements to be of the same type.\nThey share some of the same characteristics such as search time which is O(n). However, there are some differences that makes tuples preferred over lists in certain circumstances as is seen in the Python interpretter especially when the object’s size is not expected to change.\n\nLists are mutable but tuples are immutable. This means that we can’t delete or replace elements in tuples or extend/shrink its size.\nTuples are much faster to create because Python cache them at runtime and doesn’t communicate with the OS to allocate memory as it does for the lists.\n\n%timeit l = [0,1,2,3,4,5,6,7,8,9] #=&gt; 60.5 ns ± 0.357 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n%timeit t = (0,1,2,3,4,5,6,7,8,9) #=&gt; 11.9 ns ± 0.255 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)\n\nTuples can be constant folded: Python interpretter generates bytecode for a tuple in one operation, but generates bytecode for each element is the list as a seperate operation then pushes it to data stack.\n\ndis.dis(\"(1, 2)\")\n  1           0 LOAD_CONST               0 ((1, 2))\n              2 RETURN_VALUE\n\ndis.dis(\"[1, 2]\")\n  1           0 LOAD_CONST               0 (1)\n              2 LOAD_CONST               1 (2)\n              4 BUILD_LIST               2\n              6 RETURN_VALUE\n\nTuples can be reused instead of copied: If a tuple is hashable, tuple(t) returns itself because it should stay fixed. On the other side, list(l) would return new copy.\n\nt = (1, 2)\ntuple(t) is t #=&gt;  True\n\nl = (1, 2)\nlist(l) is l #=&gt;  False\n\nTuples are compact and don’t over-allocate.\n\nsys.getsizeof(tuple(iter(range(10)))) #=&gt; 120\nsys.getsizeof(list(iter(range(10)))) #=&gt; 136\n\nTuples directly reference their elements: references to objects are incorporated directly in a tuple object. In contrast, lists have an extra layer of indirection to an external array of pointers. This gives tuples a small speed advantage for indexed lookups and unpacking:\n\nHere is how the tuple (10, 20) stored:\n\ntypedef struct {\n    Py_ssize_t ob_refcnt;\n    struct _typeobject *ob_type;\n    Py_ssize_t ob_size;\n    PyObject *ob_item[2];     /* store a pointer to 10 and a pointer to 20 */\n} PyTupleObject;\n\nHere is how the list [10, 20] stored:\n\nPyObject arr[2];              /* store a pointer to 10 and a pointer to 20 */\n\ntypedef struct {\n    Py_ssize_t ob_refcnt;\n    struct _typeobject *ob_type;\n    Py_ssize_t ob_size;\n    PyObject **ob_item = arr; /* store a pointer to the two-pointer array */\n    Py_ssize_t allocated;\n} PyListObject;\nConclusion: Use tuples as immutable lists to 1) clarify that its length won’t change and 2) get a little of speed up due to some optimizations the Python interpretter may do as a result of some of the above."
  },
  {
    "objectID": "til/python/operator-overloading.html",
    "href": "til/python/operator-overloading.html",
    "title": "Operator Overloading",
    "section": "",
    "text": "Operator overloading is when a given operator behave differently according to the type of the operand(s). For example, the + operator can add two number, concatenate strings, and merge two lists. This mean int, str, and list classes have their own implementation of the + operator.\nPython has limited operator overloading with some limitations where we can’t create new operators and prevent overloading for:\n\noperators of the built-in types\nis, and, or, not\n\nIf an operator doesn’t know how to handle the type of an operand, it returns NotImplemented so the interpreter tries to check the right-side operand if it knows how to handle it. Below is the flowchart of computing a + b with __add__ and __radd__ (Source):"
  },
  {
    "objectID": "til/python/logical-execution-try-except-finally.html",
    "href": "til/python/logical-execution-try-except-finally.html",
    "title": "Logical Execution of try/except/else/finally Blocks",
    "section": "",
    "text": "The logical execution of try/except/finally/else goes as follows:\ntry:\n    ...\nexcept:\n    ...\nelse:\n    ...\nfinally:\n    ...\n\ntry block gets executed\nIf try block executes successfully, else block starts execution.\nIf try block raises an exception:\n\nException gets saved and finally block gets executed.\n\nexception gets raised if it wasn’t handled\nIf finally block returns -&gt; exception gets suspended\nIf finally raises an exception, the saved exception will be raised in the context of the new exception"
  },
  {
    "objectID": "til/python/python-lookup.html",
    "href": "til/python/python-lookup.html",
    "title": "Python Object Lookup Process",
    "section": "",
    "text": "Whenever a variable, function, or module is invoked in Python, there is a hierarchy that determines where it looks for these objects.\n\nFirst, Python looks inside the locals() array, which has entries for all local variables. Python works hard to make local variable lookups fast, and this is the only part of the chain that doesn’t require a dictionary lookup.\n\n__local variables__ do not need a dictionary lookup to be found; they are stored in a very slim array that has very fast lookup times. Because of this, finding the function is quite fast!\n\nIf it doesn’t exist there, the globals() dictionary is searched.\nFinally, if the object isn’t found there, the __builtin__ object is searched.\n\nIt is important to note that while locals() and globals() are explicitly dictionaries and __builtin__ is technically a module object, when searching __builtin__ for a given property, we are just doing a dictionary lookup inside its locals() map (this is the case for all module objects and class objects!).\nAs a result of the above:\n\nIt is recommended to import specific objects (variables/functions/classess…) from a module instead of just importing the module and use the module name to refer to the objects it contains because this will slow down the run-time.\nThe performance of the dictionary lookup is largely dependent of the hash function. Therefore, the slower the hash function, the slower the lookup and may not have O(1) lookup anymore."
  },
  {
    "objectID": "til/python/attribute-access.html",
    "href": "til/python/attribute-access.html",
    "title": "Attribute Access",
    "section": "",
    "text": "Python follows more or less the following steps when looking up an attribute inside of an object:\n\nFirst looks at obj.__dict__[attribute]. If it is not found,\nThen looks at type(obj).__dict__[attribute]. If it is not found,\nCheck __dict__ of all the superclasses.\n\nHowever, if Python sees that the attribute is implemented using a descriptor, it uses the descriptor methods (__get__)."
  },
  {
    "objectID": "til/python/closures.html",
    "href": "til/python/closures.html",
    "title": "Closures",
    "section": "",
    "text": "A closure is a function that retains the bindings of the free variables that exist when the function is defined, i.e. it would maintain a refernce to the free variables that can be used when the function is invoked later.\ndef func(x):\n    y = 100\n    def nested_func(z):\n        return x + y + z\n    return nested_func\n\nf = func(10)\nprint(f.__closure__)            #=&gt; (&lt;cell at 0x7fd10dc26830: int object at 0x7fd108a00210&gt;, \n                                #=&gt;  &lt;cell at 0x7fd10d7abe50: int object at 0x7fd108a00d50&gt;)\nprint(f.__code__.co_freevars)   #=&gt; ('x', 'y')\nf(10)                           #=&gt; 120\nx and y are free variables here. Each object f.__closure__ corresponds to a name in the f.__code__.co_freevars.\nThis is in some way similar to decorators where the decorated function is nothing but the nested function, which can be written in the following form:\ndef func(x):\n    def decorator_func(func):\n        y = 100\n        def wrapper_func(z):\n            return func(z) + x + y\n        return wrapper_func\n    return decorator_func\n\n@func(10)\ndef nested_func(z):\n    return z\n\nnested_func(10)     #=&gt; 120"
  },
  {
    "objectID": "til/python/coroutines-no-synchronization.html",
    "href": "til/python/coroutines-no-synchronization.html",
    "title": "Coroutines Don’t Need Synchronization",
    "section": "",
    "text": "Coroutines don’t need synchronization in contrary to threads because:\n\nThere can only be one coroutine running since the event loop and all coroutines run in single thread\nCoroutines don’t get interrupted by the OS while executing and have to explicitly yield control back to the event loop with await keyword\nCoroutines can only be cancelled when they are suspended at await statement. This means cancelled coroutine can do cleanup when they receive CancelledError exception because they will be scheduled to run again and do whatever cleanup steps before returning if they catch the exception"
  },
  {
    "objectID": "til/python/property-descriptor.html",
    "href": "til/python/property-descriptor.html",
    "title": "@property",
    "section": "",
    "text": "@property implements full descriptor protocol (class that implements a dynamic protocol consisting of __set__, __get__, and __delete__) that shadows instance attributes of the same name when we try to access/set/delete instance attributes.\nThe signature of property constructor is property(fget=None, fset=None, fdel=None, doc=None). Few important things about instance attributes decorated by @property:\n\nIf the attribute doesn’t have setter -&gt; read-only attribute\nIf the attribute doesn’t have deleter -&gt; we can’t delete the attribute\n\nclass C:\n   def __init__(self):\n       self._x = 10\n\n   @property\n   def x(self):\n       # called when we use obj.x\n       return self._x\n\n   @x.setter\n   def x(self, v):\n       # called when we use obj.x = v\n       self._x = v\n\n   @x.deleter\n   def x(self, v):\n       # called when we use obj.x = v\n       self._x = v"
  },
  {
    "objectID": "til/python/namespace-packages.html",
    "href": "til/python/namespace-packages.html",
    "title": "Namespace Packages",
    "section": "",
    "text": "Namespace packages are packages that don’t have __init__.py. When import machinery is looking for the package, it does not stop when it finds it and assuming there may be a regular package; i.e. package with __init__.py, in some other paths in sys.path but keeps a record of all namespace packages it found during the search.\n\nPython first scans the whole sys.path before deciding that the package is a namespace -&gt; If any name is found with init.py in it, it will give this priority and don’t continue.\n\nIf it finds a regular package with that name -&gt; discard all namespace packages it found and import the regular package.\n\nIf it doesn’t find any regular package with that name -&gt; Use all the namespace packages it found during the search and combine their paths in namespace_path so when we try to import subpackage or modules, it checks all the paths in the namespace_path (which is a list).\n\nThere can be multiple packages of the same name (under different directories) -&gt; They all combined together and the namespace_path list would have the path for all of them. Therefore, the same package can be used to refer to completely different modules in different directories."
  },
  {
    "objectID": "til/python/immutable-objects-and-augmented-assignment-operators.html",
    "href": "til/python/immutable-objects-and-augmented-assignment-operators.html",
    "title": "Immutable Objects and Augmented Assignment Operators",
    "section": "",
    "text": "Augmented assignment operators such as += and *= behave differently depending on whether the Python object is mutable or immutable. Let’s take += as an example: Python first checks if the first object implements __iadd__ special method. If not, Python falls back on __add__. Since immutable objects don’t implement any inplace operations, statement like x += y is the same as x = x + y.\nFor mutable objects such as lists:\nl = [1, 2]\nold_id = id(l)\nl += [3]            # the same as l.extend([3])\nprint(l)            #=&gt; [1, 2, 3]\nold_id == id(l)     #=&gt; True\nFor mutable objects such as tuples:\nt = [1, 2]\nold_id = id(t)\nt += (3,)           # the same as t = t + (3,)\nprint(t)            #=&gt; (1, 2, 3)\nold_id == id(t)     #=&gt; False"
  },
  {
    "objectID": "til/python/truthiness-of-objects.html",
    "href": "til/python/truthiness-of-objects.html",
    "title": "Truthiness of Python Objects",
    "section": "",
    "text": "When trying to evaluate the truthiness of an object x, basically the following is done in order:\n\nx.__bool__() is called. If __bool__ is not implemented, then\nx.__len__() is called. If __len__() is not implemented, then\nreturn True. As a result, all user defined objects are truthy unless __bool__ or __len__ are implemented. Also, we can use if iterable w/o the need to use if len(iterable).\n\nclass A:\n    ...\n\nclass B:\n    def __bool__(self):\n        print(\"In __bool__\")\n        return False\n\nclass C:\n    def __len__(self):\n        print(\"In __len__\")\n        return False\n\nclass D:\n    def __bool__(self):\n        print(\"In __bool__\")\n        return False\n\n    def __len__(self):\n        print(\"In __len__\")\n        return False\nif A(): print(\"Truthy\")     #=&gt; \"Truthy\"\nif B(): print(\"Truthy\")     #=&gt; \"In __bool__\"\nif C(): print(\"Truthy\")     #=&gt; \"In __len__\"\nif D(): print(\"Truthy\")     #=&gt; \"In __bool__\""
  },
  {
    "objectID": "til/python/decorators.html",
    "href": "til/python/decorators.html",
    "title": "Decorators",
    "section": "",
    "text": "Decorator changes the behavior of a function by doing something before calling the function and/or doing/or another thing after calling it. However, it does not affect the body of the main function that is being decorated.\n@decorator\ndef func():\n    pass\nis the same as\nfunc = decorator(func)\nBoilerplate template for any decorator:\nimport functools\ndef decorator(func):\n    @functools.wraps(func)\n    def wrapper_decorator(*args, **kwargs):\n        # Do something\n        value = func(*args, **kwargs)\n        # Do something\n        return value\n    return wrapper_decorator\nWe can also have decorators that take optional arguments:\n# Here _func is positional-only argument and num_times is keyword-only argument\ndef repeat(_func=None, *, num_times=1):\n    def decorator_repeat(func):\n        @functools.wraps(func)\n        def wrapper_repeat(*args, **kwargs):\n            res = 0\n            for _ in range(num_times):\n                res += func(*args, **kwargs)\n            return res\n        return wrapper_repeat\n    \n    # If the decorator is called with arguments\n    if _func is None:\n        return decorator_repeat\n    else:\n        return decorator_repeat(_func)\n\n@repeat\ndef add(x, y):\n    return x + y\nadd(10, 10)         #=&gt; 20\n\n@repeat(num_times=3)\ndef add(x, y):\n    return x + y\nadd(10, 10)         #=&gt; 60\nIf we want the decorator to be stateful, we can use classes as decorators:\nclass Decorator:\n    def __init__(self, func):\n        # We can't use @functools.wrap(func)\n        functools.update_wrapper(self, func)\n        self.func = func\n        # keep state here\n\n    def __call__(self, *args, **kwargs):\n        # Do something, probably update state\n        value = self.func(*args, **kwargs)\n        # Do something, probably update state\n        return value\nDon’t forget to return the value of decorated function 😀"
  },
  {
    "objectID": "til/python/context-mgr-decorator.html",
    "href": "til/python/context-mgr-decorator.html",
    "title": "Context Manager Decorator",
    "section": "",
    "text": "Instead of creating a class to implement the context manager protocol (__enter__ & __exit__), we can use @contextmanager Decorator from contextlib library to decorate any generator function with a single yield statement. This done by wrapping the function in a class that implements the __enter__ and __exit__ methods.\ndef gen():\n    # Anything here for the setup\n    print(\"setup\")\n\n    # expression after yield will be returned after calling the gen \n    # function in `with` block; otherwise, `None` will be returned\n    # equivalent to `__enter__`\n    yield\n\n    # Anything here will be part of the teardown when we exit the \n    # `with` block, equivalent to `__exit__`\n    print(\"teardown\")\n\nwith gen():\n    print(\"inside `with` block\")\nprint(\"outside `with` block\")\n\n# Output\nsetutp\ninside `with` block\nteardown\noutside `with` block\nHere is how generator functions decorated with contextmanager get evaluated:\n\nEvaluates the body of the generator function until the yield statement\n\nInvoke the generator function and store the generator object, for example gen\nCalls next(gen) so the body of generator gets executed until yield statement\nReturns the value returned by yield so it can used to bound it to the target variable of with. Therefore, yield produces values we want __enter__ to return and evaluation stops\n\nContinue evaluating the body of the with block\nOnce we are done from with block (exit with block), execution continue after the yield statement\n\nChecks if there is exception raised in exc_type inside with block. If there is exception, then invoke gen.throw(exception) in the yield line inside the generator function\nElse, it would invoke next(gen), which makes the generator to continue execution after the yield line"
  },
  {
    "objectID": "til/python/overriding-vs-nonoverriding-descriptor.html",
    "href": "til/python/overriding-vs-nonoverriding-descriptor.html",
    "title": "Overriding vs Non-overriding Descriptors",
    "section": "",
    "text": "Overriding descriptors are descriptors that implement __set__. Therefore, the user can’t shadow any attribute that is defined as an overriding descriptor because any assignment at the to the instance’s attribute will be forced to go through the __set__ method. The below class utilizes @property, which is an overriding descriptor, to make an attribute as a read-only attribute.\n\nclass Test:\n    def __init__(self):\n        self._x = 1\n    \n    @property\n    def x(self):\n        return self._x\n\nc = Test()\nprint(c.x)  #=&gt; 1\nc.x = 10    #=&gt; AttributeError: can't set attribute 'x'\n\nNon-overriding descriptors are descriptors that don’t implement __set__. This means that the user can shadow the attribute. The common example of a non-overriding descriptor is the instance methods. They are actually implemented using non-overriding descriptor with __get__ method.\n\nclass Test:\n    def f(self):\n        return 1\n\nc = Test()\ntype(c.f)   #=&gt; method\nprint(c.f())    #=&gt; 1\n\nc.f = 0\ntype(c.f)   #=&gt; int\nprint(c.f)  #=&gt; 0"
  },
  {
    "objectID": "til/python/static-vs-class-vs-instance-methods.html",
    "href": "til/python/static-vs-class-vs-instance-methods.html",
    "title": "Instance vs Class vs Static Method",
    "section": "",
    "text": "A Python class can have three kinds of methods:\nclass C:\n    \n    def instance_method(self):\n        pass\n    \n    @classmethod\n    def class_method(cls):\n        pass\n    \n    @staticmethod\n    def static_method():\n        pass\n\nInstance method is a function that is bound to the instance itself and the first argument will always be the instance object self. It has access to both instance and class variables. Most methods inside the class are typically instance methods.\nClass method is a function that is bound to the class ONLY and the first argument will always be the class object itself cls. It has access ONLY to the class variables. It is typically used as a factory method.\nStatic method is like a plain function that is not bound to neither classs nor instance and doesn’t depend on the state of the object. Therefore, it doesn’t have access to any class or instance variables. It is typically used if it is very close to the class but not exactly belongs to it and is not used from outside the class."
  },
  {
    "objectID": "til/shell/process-substitution.html",
    "href": "til/shell/process-substitution.html",
    "title": "Process Substitution in the Shell",
    "section": "",
    "text": "&lt;(CMD) is called process substitution, which will execute CMD and place the output in a temporary file and substitute the &lt;() with that file’s name. This comes in handy because some commands such as diff expect values to be passed as files instead of standard input. For example, diff &lt;(ls dir1) &lt;(ls dir2) will show differences between files in a & b directories."
  },
  {
    "objectID": "til/shell/command-substitution.html",
    "href": "til/shell/command-substitution.html",
    "title": "Command Substitution in the Shell",
    "section": "",
    "text": "Command substitution is very useful when we want to execute a command and assign its output to a variable. This can be done like this: $(CMD). This will execute CMD, get the output of the command and substitute it in place.\nThe following will execute ls and assigns the output to the variable FILES.\nmkdir test\ntouch test/{a..c}\nFILES=$(ls)\necho FILES\n# will print\na\nb\nc"
  },
  {
    "objectID": "til/shell/input-output-redirection.html",
    "href": "til/shell/input-output-redirection.html",
    "title": "I/O Redirection",
    "section": "",
    "text": "I/O redirection is very useful especially when we want to disgard errors our redirect output of a some shell command/program to a file. There are three kinds of redirections:\n\nRedirect stdin fd to a file such as cat &lt; file\n\nThe file descriptor that we want to redirect has to be infront of &lt;. In the case of redirecting stdin, 0&lt; is the same as &lt;.\n\nRedirect stdout/stderr to a file such as ls dir &gt; tmp or ls dir 2&gt;error\n\nThe file descriptor that we want to redirect has also to be in the front of &gt;. In the case of redirecting stdout, 1&gt; is the same as &gt;.\nIf we want to redirect some file descriptor fd0 to another file descriptor fd1, we can do fd0&gt;fd1.\n&gt; truncates the file we’re redirecting the output to and writes the new output\n&gt;&gt; appends to the file we’re redirecting the output to, and doesn’t overwrite the old context. Example: cat /etc/passwd &gt;&gt; tmp."
  },
  {
    "objectID": "til/shell/execute-vs-source-script.html",
    "href": "til/shell/execute-vs-source-script.html",
    "title": "Executing vs Sourcing Script",
    "section": "",
    "text": "If we execute a program from the shell (./script.sh), we don’t get the side effects from that program. For example, if the program defines variables or change directories, those variables won’t be defined in the shell and we would still be in the same directory. The reason for that is because the shell spin a new shell instance to execute the program and then terminates once done. Therefore, the changes happens in the child process which can’t affect the parent process (process that invoked the sript which happens to be the shell).\nIf we load the program to the shell (source script.sh), then we get the side effects. Therefore, we can then use some of its functions and if some of those functions change directories it would be reflected in out shell. This happens because the script executes in the current process, which means no child process is created to execute it."
  },
  {
    "objectID": "til/unix/directory-permission.html",
    "href": "til/unix/directory-permission.html",
    "title": "Directory Access Permissions/Representation",
    "section": "",
    "text": "Directory is represented as a file where the data block where each files/subdirectory inside it has an entry:\n\nName of file/subdirectory\ninode number\n\nTherefore, when we list directory, we read the names from each entry. When we add/rename/remove files/subdirectories, we update the file by adding/updating/removing the entry that corresponds to the file we added.\nMeaning of access permissions on a directory:\n\nread: List files in the directory\nwrite: Add/remove/rename files in the directory\nexecute: Search/lookup files in the directory. It allows us to change into the directory. execute permission is needed to do anything with a directory. Even if we have read or write permissions, we can NOT do anything w/o execute permission.\n\ntouch test              // would create a directory with rwx permissions\nchmod u-x test          // Remove execute permission\ncd test                 // Error: permission denied\ntouch test/test.txt     // Error: permission denied\nWe as users never update directory file directory, ONLY kernel does the update and we only get to read its entries."
  },
  {
    "objectID": "til/chrome/navigate-and-control-browser-with-vimium.html",
    "href": "til/chrome/navigate-and-control-browser-with-vimium.html",
    "title": "Vimium: Vim Key Bindings for Chrome",
    "section": "",
    "text": "Vimium is a chrome extension that provides keyboard shortcuts to navigate and control the browser inspired by Vim editor. It comes very handy when navigating between pages as well as on the same page and still be mouse-free. Keyboard shortcut examples: - d to go down half page - u to go up half page - gg to go the top of the page - G to go to the bottom of the page - gt to go to next tab to the right - gT to go to previous tab to the left"
  },
  {
    "objectID": "til/tmux/list-sessions.html",
    "href": "til/tmux/list-sessions.html",
    "title": "Listing tmux Sessions",
    "section": "",
    "text": "To list all the tmux sessions that are currently running, you can type tmux ls in the command-line which is a shortcut for tmux list-sessions.\nIf you’re within a tmux session, prefix-w would list the tree of all sessions and the windows/panes withing each session. This is very handy to toggle between sessions/windows."
  },
  {
    "objectID": "til/pytorch/detach-tensor-from-computation-graph.html",
    "href": "til/pytorch/detach-tensor-from-computation-graph.html",
    "title": "Detach Tensor From Computation Graph",
    "section": "",
    "text": "We can think of a Tensor that implements Automatic Differentation as a regular tensor that have, among other attributes, the following attribites that helps to capture its history:\n\ninputs: list of inputs such as tensors and scalars that created the output tensor. Leaf tensors have no inputs\noperation: function that was applied on the inputs to create the output tensor. Leaf tensors have no operations\ndata: the output tensor from applying the operation on the inputs\nrequires_grad: whether we want to track the history of the computations when creating new tensors. If False, inputs and operation attributes will be set to None\n\nTherefore, to detach a tensor from a computation graph Or if we don’t want to track temporary computations done on a tensor (such as during inference), we can do the following:\n\nTensor.detach() returns new tensor sharing the same underlying storage but make it a leaf tensor. Therefore:\n\ninputs will be set to None\noperation will be set to None\nrequires_grad will be set to False\n\nwith torch.no_grad let’s us perform computations on tensors w/o tracking those computations\nOperating directly on Tensor.data avoids recoding the operations. Useful when updating a tensor or for initialization\n\na = torch.tensor([[1., 2.]], requires_grad=True)\na  #=&gt; tensor([[1., 2.]], requires_grad=True)\n\n# The following will record the computation\nb = a + 1\nb.grad_fn  #=&gt; &lt;AddBackward0 at 0x7f81abadc8b0&gt;\n\n# All the following forms allow us to avoid recording the computation on the tensor\nb = a.data + 1\nb.grad_fn           #=&gt; None\nb.requires_grad     #=&gt; False\n\nb = a.detach() + 1\nb.grad_fn           #=&gt; None\nb.requires_grad     #=&gt; False\n\nwith torch.no_grad():\n    b = a + 1\nb.grad_fn           #=&gt; None\nb.requires_grad     #=&gt; False\n\n# Useful for updates/initialization because it keeps requires_grad attribute\na.data = torch.randint(10, size=(1, 2))\na.grad_fn           #=&gt; None\nb.requires_grad     #=&gt; True"
  },
  {
    "objectID": "til/pytorch/lazy-layers.html",
    "href": "til/pytorch/lazy-layers.html",
    "title": "Lazy Layers",
    "section": "",
    "text": "In Pytorch, you can define a layer with only the desired output size without specifying the input size. Later, we the layer if first called, Pytorch would check the input size from the input tensor and creates the required parameters. This is kinda of lazy definition of a layer. It is very helpful when we’re not sure about the input size but know the output size. All the layerer that support lazy definitions are prefixed with Lazy such as LazyLinear and LazyConv2d.\nx = torch.randn(1, 20)\nlayer = nn.LazyLinear(1)\nlayer(x) #=&gt; tensor([[-1.3754]], grad_fn=&lt;AddmmBackward0&gt;)"
  },
  {
    "objectID": "til/c/memory-layout.html",
    "href": "til/c/memory-layout.html",
    "title": "C Program Memory Layout",
    "section": "",
    "text": "Below is how a typical C program memory layout would look like (program was run on macOs, but the arrangement is the same on any UNIX system). As you might expect (with stack grows downward from higher to lower addresses), the memory segments will be in the following order:\n\nEnv & Args\nStack\nShared memory\nHeap\nUninitialized data (BSS)\nInitialized data\nText\n\nI omitted few segments such as read-only data, init, etc. The code that produced the layout can be found at the endof the page (credit: Advanced Programming in the Unix Environment)\nHigh address (args and env):\n----------------------------\nenvp[55] at                                            : 0x7FF7BE0AD5D0\nenviron[55] at                                         : 0x7FF7BE0AD5D0\nenvp[0] at                                             : 0x7FF7BE0AD418\nenviron[0] at                                          : 0x7FF7BE0AD418\nlast arg at                                            : 0x7FF7BE0AD410\nfirst arg at                                           : 0x7FF7BE0AD408\n\nStack:\n------\nFirst variable inside main at                          : 0x7FF7BE0AD294\nfaun_array[] ends at                                   : 0x7FF7BE0AD2C0\nfunc_array[] (like 'array[]', but on stack) begins at  : 0x7FF7BE0AD2B0\nargc at                                                : 0x7FF7BE0AD2A8\nargv at                                                : 0x7FF7BE0AD2A0\nenvp at                                                : 0x7FF7BE0AD298\nfunc2 (from main): frame at                            : 0x7FF7BE0AD254\nfunc frame at                                          : 0x7FF7BE0AD258\nstatic int n within func at                            : 0x   101E5A06C\nfunc (called     0 times): frame at                    : 0x7FF7BE0AD258\nfunc2 (from func): frame at                            : 0x7FF7BE0AD234\n\nShared memory:\n--------------\nshared memory attachment begins at                     : 0x   101F84000\nshared memory attachment ends at                       : 0x   101F9C6A0\n\nHeap:\n-----\nmalloced area ends at                                  : 0x6000026AD120\nmalloced area begins at                                : 0x6000026AD100\n\nUninitialized Data (BSS):\n-------------------------\nSemaphore at                                           : 0x   101E5A0F4\nCond at                                                : 0x   101E5A080\nLock at                                                : 0x   101E5A0B0\narray[] ends at                                        : 0x   101E5A080\narray[] (uninitialized, fixed-size char * on BSS) from : 0x   101E5A070\nnum2 (uninitialized global int) at                     : 0x   101E5A0F0\nstring2 (uninitialized global char *) at               : 0x   101E5A0F8\nextern **environ at                                    : 0x7FF8497239A0\n\nInitialized Data:\n-----------------\nnum (initialized global int) at                        : 0x   101E5A068\nstring (initialized global char *) at                  : 0x   101E5A060\n\nText Segment:\n-------------\nfunc2 (function) at                                    : 0x   101E55440\nfunc (function) at                                     : 0x   101E55470\nmain (function) at                                     : 0x   101E54F20\n#include &lt;errno.h&gt;\n#include &lt;err.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;pthread.h&gt;\n#include &lt;semaphore.h&gt;\n#include &lt;sys/shm.h&gt;\n\n#define ARRAY_SIZE 16\n#define MALLOC_SIZE 32\n#define SHM_SIZE 100000\n#define SHM_MODE 0600\n\nchar array[ARRAY_SIZE];\nchar *string = \"a string\";\nchar *string2;\nint num = 10;\nint num2;\npthread_mutex_t lock;\npthread_cond_t cond;\nsem_t sem;\nextern char **environ;\n\nvoid func(int);\nvoid func2(const char *);\n\nint main(int argc, char **argv, char **envp) {\n    int vars;\n    int shmid;\n    char *ptr;\n\n    char func_array[ARRAY_SIZE];\n\n    vars = 0;\n    char **tmp = envp;\n    while (*tmp++) {\n        vars++;\n    }\n\n    (void)printf(\"High address (args and env):\\n\");\n    (void)printf(\"----------------------------\\n\");\n    (void)printf(\"envp[%d] at                                            : 0x%12lX\\n\", vars, (unsigned long)&envp[vars]);\n    (void)printf(\"environ[%d] at                                         : 0x%12lX\\n\", vars, (unsigned long)&environ[vars]);\n    (void)printf(\"envp[0] at                                             : 0x%12lX\\n\", (unsigned long)envp);\n    (void)printf(\"environ[0] at                                          : 0x%12lX\\n\", (unsigned long)environ);\n    (void)printf(\"last arg at                                            : 0x%12lX\\n\", (unsigned long)&argv[argc]);\n    (void)printf(\"first arg at                                           : 0x%12lX\\n\", (unsigned long)&argv[0]);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Stack:\\n\");\n    (void)printf(\"------\\n\");\n    (void)printf(\"First variable inside main at                          : 0x%12lX\\n\", (unsigned long)&vars);\n    (void)printf(\"func_array[] ends at                                   : 0x%12lX\\n\", (unsigned long)&func_array[ARRAY_SIZE]);\n    (void)printf(\"func_array[] (like 'array[]', but on stack) begins at  : 0x%12lX\\n\", (unsigned long)&func_array[0]);\n\n    (void)printf(\"argc at                                                : 0x%12lX\\n\", (unsigned long)&argc);\n    (void)printf(\"argv at                                                : 0x%12lX\\n\", (unsigned long)&argv);\n    (void)printf(\"envp at                                                : 0x%12lX\\n\", (unsigned long)&envp);\n\n    func2(\"from main\");\n    func(0);\n\n    (void)printf(\"\\n\");\n\n    printf(\"Shared memory:\\n\");\n    printf(\"--------------\\n\");\n    if ((shmid = shmget(IPC_PRIVATE, SHM_SIZE, SHM_MODE)) &lt; 0) {\n            fprintf(stderr, \"Unable to get shared memory: %s\\n\",\n                strerror(errno));\n            exit(1);\n        }\n\n        if ((ptr = shmat(shmid, 0, 0)) == (void *)-1) {\n            fprintf(stderr, \"Unable to map shared memory: %s\\n\",\n                strerror(errno));\n            exit(1);\n        }\n        printf(\"shared memory attachment begins at                     : 0x%12lX\\n\", (unsigned long)ptr);\n        printf(\"shared memory attachment ends at                       : 0x%12lX\\n\", (unsigned long)ptr+SHM_SIZE);  (void)\n    printf(\"\\n\");\n    printf(\"Heap:\\n\");\n    (void)printf(\"-----\\n\");\n    if ((ptr = malloc(MALLOC_SIZE)) == NULL) {\n        err(EXIT_FAILURE, \"unable to allocate memory\");\n        /* NOTREACHED */\n    }\n\n    (void)printf(\"malloced area ends at                                  : 0x%12lX\\n\", (unsigned long)ptr+MALLOC_SIZE);\n    (void)printf(\"malloced area begins at                                : 0x%12lX\\n\", (unsigned long)ptr);\n    free(ptr);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Uninitialized Data (BSS):\\n\");\n    (void)printf(\"-------------------------\\n\");\n    (void)printf(\"Semaphore at                                           : 0x%12lX\\n\", (unsigned long)&sem);\n    (void)printf(\"Cond at                                                : 0x%12lX\\n\", (unsigned long)&cond);\n    (void)printf(\"Lock at                                                : 0x%12lX\\n\", (unsigned long)&lock);\n    (void)printf(\"array[] ends at                                        : 0x%12lX\\n\", (unsigned long)&array[ARRAY_SIZE]);\n    (void)printf(\"array[] (uninitialized, fixed-size char * on BSS) from : 0x%12lX\\n\", (unsigned long)&array[0]);\n    (void)printf(\"num2 (uninitialized global int) at                     : 0x%12lX\\n\", (unsigned long)&num2);\n    (void)printf(\"string2 (uninitialized global char *) at               : 0x%12lX\\n\", (unsigned long)&string2);\n    (void)printf(\"extern **environ at                                    : 0x%12lX\\n\", (unsigned long)&environ);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Initialized Data:\\n\");\n    (void)printf(\"-----------------\\n\");\n    (void)printf(\"num (initialized global int) at                        : 0x%12lX\\n\", (unsigned long)&num);\n    (void)printf(\"string (initialized global char *) at                  : 0x%12lX\\n\", (unsigned long)&string);\n    (void)printf(\"\\n\");\n\n    (void)printf(\"Text Segment:\\n\");\n    (void)printf(\"-------------\\n\");\n    (void)printf(\"func2 (function) at                                    : 0x%12lX\\n\", (unsigned long)&func2);\n    (void)printf(\"func (function) at                                     : 0x%12lX\\n\", (unsigned long)&func);\n    (void)printf(\"main (function) at                                     : 0x%12lX\\n\", (unsigned long)&main);\n    (void)printf(\"\\n\");\n    return EXIT_SUCCESS;\n}\n\n\nvoid func(int recurse) {\n    int fint;\n    char *msg = \"from func\";\n\n    /* Change this value to 0 and note how\n     * the location of where it is stored\n     * changes from the Data to BSS segment. */\n    static int n = 0;\n    (void)printf(\"func frame at                                          : 0x%12lX\\n\", (unsigned long)&fint);\n\n    if (recurse) {\n        msg = \"recursive\";\n    }\n    (void)printf(\"static int n within func at                            : 0x%12lX\\n\", (unsigned long)&n);\n    printf(\"func (called %5d times): frame at                    : 0x%12lX\\n\", n, (unsigned long)&fint);\n\n    n++;\n    func2(msg);\n}\n\nvoid func2(const char *how) {\n    int fint;\n    (void)printf(\"func2 (%s): frame at                            : 0x%12lX\\n\", how, (unsigned long)&fint);\n    func(1);\n}"
  },
  {
    "objectID": "til/c/char-array-vs-string-constant-pointers.html",
    "href": "til/c/char-array-vs-string-constant-pointers.html",
    "title": "Character Array vs String Constant Pointers",
    "section": "",
    "text": "In C, there is two ways to define a string:\n\nDefine it as an array of character such as:\n\nchar aname[] = \"Imad\";\n\nThe array would be of size number of characters plus 1 for the null terminating character.\n\n\nOr define it as a pointer to string constant such as:\n\nchar *pname = \"Imad\";\nThey are almost identical except for 1 subtle difference:\n\naname will always point to the same storage and can’t be changed to point to something else. The characters themselves can be changed.\npname is a pointer so it can be changed to point to something else. If we try to modify the underlying characters, we get an error because the string is stored in .rodata which can’t be modified at runtime."
  },
  {
    "objectID": "til/c/linkage.html",
    "href": "til/c/linkage.html",
    "title": "Linkage in C",
    "section": "",
    "text": "Within a translation unit, all declarations of the same object or function identifier with internal linkage refer to the same thing, and the object or function is unique to that translation unit. All declarations for the same object or function identifier with external linkage refer to the same thing, and the object or function is shared by the entire program.\nThe first external declaration for an identifier gives the identifier internal linkage if the static specifier is used, external linkage otherwise. If a declaration for an identifier within a block does not include the extern specifier, then the identifier has no linkage and is unique to the function. If it does include extern, and an external declaration for the identifier is active in the scope surrounding the block, then the identifier has the same linkage as the external declaration, and refers to the same object or function; but if no external declaration is visible, its linkage is external."
  },
  {
    "objectID": "til/git/navigate-and-control-browser-with-vimium.html",
    "href": "til/git/navigate-and-control-browser-with-vimium.html",
    "title": "Comparing Repo’s History on Github",
    "section": "",
    "text": "We can compare branches/tags/time ranges/etc. on Github by modifying the URL of the Github repository and appending /compare to the URL such as https://github.com/ImadDabbura/cmn_ai/compare. This will take us to a new page that allows us to compare between different commits. It is very useful way to visualize changes betweem branches/tags or just even time ranges."
  },
  {
    "objectID": "til/sql/what-is-null.html",
    "href": "til/sql/what-is-null.html",
    "title": "What Is Null Actually?",
    "section": "",
    "text": "Null is a special value that represents an unknown value. Any arithmatic operation (+,-,*, /) that has at least one Null operand results in Null. The same thing applies with comparisons: All comparisons with Null would result in Null. Even Null = Null returns Null. Therefore,\n\nand operator:\n\ntrue and Null results in Null\nfalse and Null results in false\nNull and Null results in Null\n\nor operator:\n\ntrue or Null results in true\nfalse or Null results in Null\nNull or Null results in Null\n\nnot operator:\n\nnot Null results in Null\n\nTo test if a value is Null, use IS NULL or IS NOT NULL\nIn where clause, recods that evaluates to either false or Null are excluded\nAggregation functions such as sum and average discard Null records\ncount(*) returns the number of records in a table. But count(field) returns the number of not Null records in the table. This means if the field is empty (Null values only), count(field) will return zero."
  },
  {
    "objectID": "til/sql/logical-execution-order-of-sql-query.html",
    "href": "til/sql/logical-execution-order-of-sql-query.html",
    "title": "Logical Execution Order of SQL Query",
    "section": "",
    "text": "The logical execution order of an SQL query is defined by the following sequences:\n\nThe FROM clause is first evaluated to get the output relation\nThen WHERE predicate is applied to the records in the output relation. If WHERE is not present, the predicate is True\nRecords satisfies the WHERE clause then placed into groups according to the GROUPBY clause. If GROUPBY clause is not present, all records will be placed into one group\nThen groups will filtered acoording to HAVING clause. If it is not present, the predicate is True\nThen the SELECT clause uses the remaining groups to generate results and, if they exist, apply aggregate functions on the results\nGenerated results will be sorted according to ORDER by clause if it is present. Otherwise, the results will be displayed randomly\nFinally, if LIMIT is present, display the number of records requested in the LIMIT clause"
  },
  {
    "objectID": "misc-notes/how-to-be-better-programmer.html",
    "href": "misc-notes/how-to-be-better-programmer.html",
    "title": "How to be a Better Programmer",
    "section": "",
    "text": "Practice Drills:\n\nWrite your resume. List all the relevant skills that will still be used in 100 years. Rate yourself on each skill from 1-10.\n\nNice to have an up-to-date resume\nMost likely you will find that math, computer science, writing, and people skills are for the most part timeless, universal skills. Most specific technologies, languages and protocols eventually expire, to be replaced by better alternatives\n\nThe knowledge of a technology or a framework will decrease in value if we don’t keep it current\nStudy by itself is not enough. We need to use what we learned and practice to make it stick\nMake a list of programmers you admire and write down the few things that they seem to do well - things you wish you were better at\nMastering the tools of the trade is very important to make us effective\n\nMake a list of the most common tools/programming languages you use and invest in learning them in depth\n\nRead good and bad code and try to differentiate between both\nRead known libraries’ code to see their coding style/design\nWork on open-source projects\nLearn different programming languages\nUnderstand the hardware effect on what you do\nWrite a blog that documents your learning\n\nHave a file (note) with the title “Things I don’t know”. This should include things important to your career that you either don’t know or know very little.\nNot all experiences are created equal. Strive for mentorship and teams that help you learn more\nFocus your learning on three areas, in the following order:\n\nFundamentals\nThe latest version/feature of the stack(s) you use the most\nIn-demand tech that is backed by market leaders\n\n\n#career-advice"
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html",
    "href": "misc-notes/setting-and-achieving-goals.html",
    "title": "The Science of Setting and Achieving Goals",
    "section": "",
    "text": "Most people struggle in setting/executing/achieving goals due to many reasons that we are not going to cover in this post. I tried in this post to cover notes I’ve taken so far about the whole process of setting and achieving goals from a neuroscience perspective. The psychological aspects are already covered in a lot of books about this topic, but rarely the neuroscience aspect is mentioned. I will keep updating this post in the event of learning/reading something new or from my own experience going through a process over the years. Please feel free to send your feedback with new resources/corrections so we can all benefit from it."
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html#introduction",
    "href": "misc-notes/setting-and-achieving-goals.html#introduction",
    "title": "The Science of Setting and Achieving Goals",
    "section": "",
    "text": "Most people struggle in setting/executing/achieving goals due to many reasons that we are not going to cover in this post. I tried in this post to cover notes I’ve taken so far about the whole process of setting and achieving goals from a neuroscience perspective. The psychological aspects are already covered in a lot of books about this topic, but rarely the neuroscience aspect is mentioned. I will keep updating this post in the event of learning/reading something new or from my own experience going through a process over the years. Please feel free to send your feedback with new resources/corrections so we can all benefit from it."
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html#goals",
    "href": "misc-notes/setting-and-achieving-goals.html#goals",
    "title": "The Science of Setting and Achieving Goals",
    "section": "Goals",
    "text": "Goals\nHumans are particularly good at orienting the brain towards goals with different time trajectories: immediate goals, short-term goals, and long-term/lifetime goals. Also, humans are good at executing multiple goals at the same time.\nNeural circuit is the brain areas that work together in concert when something happens. These areas work in sequence, each area is necessary in making action happen but not sufficient. Therefore, all areas work together to lead to the occurrence of an action.\nBrain circuits involved in all phases of goals (setting/executing/achieving) regardless of the kind of goals (small/big or short-term/long-term) are:\n\nAmygdala: this area is responsible for fear and anxiety.\nBasal Ganglia: it is responsible for action and it has two parts (circuits):\n\nOne that generates GO (initiation of action)\nOne that generates the NO GO (prevention of action).\n\nLateral Prefrontal Cortex: it is involved with planning and thinking across different time scales.\nOrbito Prefrontal Cortex: it is responsible for meshing some emotionality with the current state of progress and comparing that emotionality with what it might be when we achieve the goal. Therefore, it is responsible for how we feel at present compared to how we would feel when reaching some predefined goal.\n\nOne part of the circuits will be involved in placing a value on the goal to determine whether it is worth pursuing. The other part is involved in taking the action; whether to do something or not do something, given the value of the goal.\nThe Value of the goal is so critical in achieving the goal. The reason behind this is that we have neuro-modulater system that governs setting/executing/achieving goals which are Dopamine. Dopamine is the currency by which we assess particular progress towards particular things of particular value. It is the way we use to assess the value of our pursuit."
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html#peripersonal-space-vs.-extrapersonal-space",
    "href": "misc-notes/setting-and-achieving-goals.html#peripersonal-space-vs.-extrapersonal-space",
    "title": "The Science of Setting and Achieving Goals",
    "section": "Peripersonal Space vs. Extrapersonal Space",
    "text": "Peripersonal Space vs. Extrapersonal Space\nPeripersonal Space is the space within our body, the surface of our skin, and in our immediate environment. There are particular neurochemicals (such as ceretonine) and particular neuro-circuits that are responsible for consummatory behaviors such as using and consuming stuff with our immediate reach. Things like drinking from the cup next to us or feeling our internal body/breathing. We don’t have to do much to consume/use stuff that is in our peripersonal space.\nExtrapersonal Space is the space beyond the confinement of our reach in both time and space such as things in a different room or another building or something in the future. The molecule that is most concerned with this is dopamine.\nIt is very important to understand both spaces to be efficient in setting/assessing/achieving our goals and be able to keep transitioning back and forth between those spaces:\n\nWe need to be able to understand our peripersonal space so that we can assess how feel and the progress we made towards goal(s) even if the goal has not been started yet or the stuff we’re evaluating is in the future. Therefore, we use our peripersonal space to assess current feelings/state.\nWe need to be able to move to the extrapersonal space to move towards any goal."
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html#visually-focusing-on-a-goal-line-improves-performance",
    "href": "misc-notes/setting-and-achieving-goals.html#visually-focusing-on-a-goal-line-improves-performance",
    "title": "The Science of Setting and Achieving Goals",
    "section": "Visually Focusing on a Goal Line Improves Performance",
    "text": "Visually Focusing on a Goal Line Improves Performance\n\nMultitasking is not good while doing goal-seeking activities that require focus and a high level of cognition. However, it is good to do some form of multitasking before starting goal-seeking activities because it helps us be in action mode which leads us to focus faster on the next task (due to adrenaline).\n\nBlood pressure:\n\nSystolic blood pressure (top number) which is what is measured when the heart contracts (beats).\nDiastolic blood pressure (bottom number) which is what is measured between beats.\n\nThere are two visual pathways:\n\nWhen we focus on something such as an object or simply a line regardless of how far it is. This helps with focus and increases attention/stress levels. This would raise systolic blood pressure which also releases adrenaline by a small amount as well as other stuff such as more Oxygen and fuel which in turn helps us get ready for action. As a result, visually focusing on one object helps prepare the body and brain to move and be ready to take action.\nWhen we are not focusing and just collecting a wider view of the scene without necessarily focusing on any specific thing. This would reduce systolic blood pressure and any goal-seeking behaviors which also helps with relaxation.\n\n\nVisually forcing ourselves to focus on a line or a dot (narrowing visual focus) on a wall or desk for 30-60 seconds will help us increase our level of cognitive attention and our ability to focus and stay focused. This is because most of our cognition follows our visual perception.\nAlso, visually focusing on the goal line will help us achieve the goal faster with less work (with the help of moving to the extrapersonal space)."
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html#tools",
    "href": "misc-notes/setting-and-achieving-goals.html#tools",
    "title": "The Science of Setting and Achieving Goals",
    "section": "Tools",
    "text": "Tools\n\nTool 1: Learn Faster with the 85% Rule\nNeuroplasticity is how the brain changes the connection between neurons that reflects new learnings. Mainly, it changes based on the errors we make while learning. If we don’t make mistakes, there is nothing to learn and the brain wouldn’t change anything since everything is working perfectly fine. Therefore, part of learning is making mistakes and learning from them. The brain would be in a much better position after errors in terms of focus and ability to learn faster and absorb new material. As a result, we should pursue errors, not as end goals but to make the brain more plastic.\nBut the big question is: What is the rate of errors we should look for to get the best learnings/plasticity?\n\nToo few errors don’t result in learning because the brain is performing everything correctly.\nToo many errors result in frustration and not making progress because the task is much harder that the level we are at.\n\nAccording to a recent paper, it uses the 85% rule: we should target about a 15% error rate to get the best of plasticity/learning and progress. The target should be around 15% since it is hard to measure the precise error rate; however, make sure to not exceed 20% or be below 10%.\n\n\nTool 2: Use Focal Vision to Initiate Goal Pursuit\nGiven the importance of visually focusing on narrow/specific objects, it is highly recommended to do it 30-60 seconds right before starting the goal-seeking focused task. It is okay to blink while focusing on the object.\n\n\nTool 3: Use Aged Self-Images to Self-Motivate\nThere is a phenomenon called delayed discounting. In short, delayed discounting entails how we perceive future rewards for our immediate action. For example, most people struggle to save because; even though they acknowledge the importance of saving for retirement, they discount the rewards since they will be further in the future and tend to prefer immediate short-term rewards over long-term rewards. As a result, the future rewards need to be much bigger (or at least convince ourselves that it is much bigger) for our brain to be motivated to do it compared to other activities that have immediate/shorter-term rewards.\nDue to the importance of the visual system, labs at CMU did a study on two groups regarding the tendency of saving for retirement by showing one group an actual image of themselves after 30-40 years and the other group just told about the need to save for retirement. They found that the group that saw their future images saved much higher than the others. As a result, it is good to use self-images before pursuing goals and always affirm the value of the goal and stay motivated.\n\n\nTool 4: Visualizing Goals\nIt appears, according to multiple studies, that visualizing the goal and how it feels after achieving the goal has effect at the start of the goal ONLY. After that, the effect starts to diminish until it becomes negligible. Therefore, it is only helpful in the process of setting the goals but not necessarily during execution or assessment.\n\n\nTool 5: Visualizing Failure\nMultiple studies showed that thinking about how things can fail and what would be the outcome of failing makes us twice more likely to achieve the goal. When we imagine failure, there would be an increase in the systolic blood pressure and readiness to perform actions which leads us to stay motivated and push extra hard to achieve those goals. The brain and body are much better at moving away from fearful things than moving towards things we like.\n\n\nTool 6: Make Goals Moderately Lofty\nIf we classify goals into 3 categories: easy, moderate (hard but achievable), and very hard (impossible):\n\nEasy goals are too easy that they don’t engage brain circuits and the body and don’t change systolic blood pressure. So people are more likely to drop those goals\nHard (impossible) goals also get dropped due to the same reasons the easy goals get dropped but in the opposite direction (crash systolic blood pressure)\nModerate goals; goals that are hard enough but reachable, are much more likely to be achieved and engage brain circuits and the body. Studies have shown that such goals are at least twice as likely to be pursued and achieved than other goals.\n\nTherefore, set goals that are moderate to hard to increase the likelihood of achieving them. Not too easy and not too hard goals.\n\n\nTool 7: Avoid Goal Distraction; Focus on 1-2 Major Goals Per Year\nWhen our visual system has so many things in the environment to look at, this will drift our attention from what we planned to do and end up doing completely different things that may even contradict what we had in mind and planned to do. Department stores figured this out by stocking their shelves with so many things and varieties that make people much more likely to buy stuff that never thought about or even needed. Therefore, having sparse space helps our brain and visual system stay focused and do what we planned to do. Therefore, 2 (or max 3) major goals per year are more than enough for most people.\n\n\nTool 8: Ensure Specificity of Goals, Weekly Assessment\nWe need to be specific about what success would look like and have an action plan of what it takes to achieve the goal. In other words, there need to be specific steps that have to be performed to achieve the goal so it makes it easier for the brain to know what to do next and reduces the amount of willpower needed to start goal-seeking activities. It also helps us in measuring progress and update action plans.\nIt is highly recommended to have a weekly assessment to measure what has been done in the previous week and what should be done in the coming week by updating the action plan. The assessment should be written and advisable to have a specific day/time every week dedicated to this exercise where the focus will be only on the assessment and revisiting goals/action plans.\n\n\nTool 9: Space-Time Bridging\n\nClose your eyes for 3 short breaths and focus on what is happening inside your body and avoid any external distraction\nOpen your eyes and look at something on the skin of your body for short breaths. The attention should be 90% on the internal body and 10% on the visual cognition of the skin part we’re looking at.\nLook at something 5-15 ft away for 3 short breaths. The focus would be 90% on the object we’re looking at and 10% on our internal body.\nLook at something very far for 3 short breaths. The focus should be around 99% on the object we’re looking at and 1% on our internal body.\nExpand the visual horizon to include everything in the horizon for 3 short breaths. The focus should be 100% on the external (extrapersonal) space.\n\nRepeat the above steps 2-3 times once daily. This should teach our brain system related to goal setting/assessment/execution to adjust and move between different time-frames and allow us to conceive the pursuit of different goals over different periods."
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html#dopamine",
    "href": "misc-notes/setting-and-achieving-goals.html#dopamine",
    "title": "The Science of Setting and Achieving Goals",
    "section": "Dopamine",
    "text": "Dopamine\nDopamine is thought of as the molecule for pleasure and reward. However, it is actually for motivation. Therefore, it affects the motivation of performing the actual steps that lead to pleasure/rewards.\nDopamine reward prediction error:\n\nIf something positive happens that we didn’t expect -&gt; A lot of dopamine is released\nIf we anticipate something positive to happen -&gt; Dopamine is released during the anticipation period (before receiving the reward) and small release happens after receiving the reward. The releases are not as much as if something positive was not expected.\nIf we anticipate something positive to happen but it didn’t happen -&gt; Dopamine is released during the waiting (anticipation) period but then dropped below the initial level when it doesn’t happen.\n\nOur brain can change the nature of the same behavior from positive to negative and vice versa. For example, if you voluntarily exercise versus someone forcing you to exercise. It is the same action but, according to multiple studies, people (or rats) who were forced to exercise generated negative results including blood pressure and other health metrics while others who exercised voluntarily generated positive metrics.\nAs a result, we should pick a milestone (assessment) that we can maintain consistency with the reward schedule (cognitive rewards). Weekly assessment is perfect: it is not too granular neither it is too infrequent. The reward can be as simple as checking a box that certain tasks have been achieved and acknowledging that we did it and we are on the right track. This helps us stay motivated and ready to pursue the goal.\nSome things help increase the release of dopamine such as a cold shower or cold water exposure which has been shown to increase the release of dopamine by 2.5x or consumption of caffeine which increases the dopamine receptors and makes dopamine float around more effectively in activating the motivational state."
  },
  {
    "objectID": "misc-notes/setting-and-achieving-goals.html#resources",
    "href": "misc-notes/setting-and-achieving-goals.html#resources",
    "title": "The Science of Setting and Achieving Goals",
    "section": "Resources",
    "text": "Resources\n\nThe 85% Rule for Optimal Learning\nEffects of Narrowing Visual Attention on Goal Pursuit Behavior\nHuberman Lab Podcast—\n\n#career-advice #personal-growth"
  },
  {
    "objectID": "misc-notes/shipping-projects.html",
    "href": "misc-notes/shipping-projects.html",
    "title": "Notes on Shipping Projects",
    "section": "",
    "text": "Writing for specific audience about specific topic is much easier and faster to deliver\nSet a deadline and ship whatever done by the time you reach the deadline regardless of what has been done or its quality. View deadline as a challenge.\nWork backwards:\n\nWhat’s the end result look like, exactly?\nHow many features?\nWhat is absolutely required, what’s nice to have?\nHow perfect does it have to be?\nHow long will each of these take?\nWhat has to come first?\nWhat do I need to prepare?\nHow long will that take?\nWhat do I need to find out?\nHow long will that take?\nHow much, or how little, defines success? And who controls that?\n\nBreak it into pieces. For projects, break it into separate components and do deep work/focused sessions to finish each component separately. This has the advantage of keeping motivation high as a result of the feeling of accomplishments\nGet super clear on the details of every stage to help you know when you’re done\nStart small. This will give you an idea what it would take to build the project you’re aiming for.\nTrack your progress using tools such as Trello. Keep the completed tasks visible to get the satisfaction from accomplishing tasks always visible.\n\nVisible progress is a great motivator\nEmotional management makes a big difference during the different stages of the project\n\nDon’t reinvent the wheel. Use existing tools/libraries that help you deliver the project faster and get customer feedback. You can always come back later to improve the tools or build your own from scratch if there is a need for that\nEvery version should be better. This means you don’t have to ship perfect product because future iterations will always improve different aspects of the product\nDefine success criteria to ship the first version of the project\nResearch and do literature review before starting the project. This would help determine the tools to use, structure of the project and its components, etc. and the rest would be left for implementation/execution\nWrite down the must-haves and nice-to-have features\nMistakes can happen after shipping the product. Don’t take it personally and just try to fix them\nShipping now is much better than shipping perfect project later. You learn much more from trial and error and faster iterations than perfect project\n\n#productivity #career-advice"
  },
  {
    "objectID": "books-summaries.html",
    "href": "books-summaries.html",
    "title": "Books’ Summaries",
    "section": "",
    "text": "Deep Work: Rules for Focused Success in A Distracted Life\n\n\n\nPersonal Development\n\n\n\n\nImad Dabbura\n\n\nApr 8, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers-summaries/open-hands.html",
    "href": "papers-summaries/open-hands.html",
    "title": "OpenHands: An Open Platform for AI Software Developers as Generalist Agents",
    "section": "",
    "text": "OpenHands: An Open Platform for AI Software Developers as Generalist Agents\n\nThesis: Software engineering agentic platform that facilitates the development of multi-agent systems to colloborate to solve software’s related problems. The platform provides generalist agent that follows tha CodeAct architecture as well as specialized agents that are smaller and do things such as browsing. One important piece is that tools are basically programming-language primitives that extend the agent skills\nContribution:\n\nEvaluation framework of agents across wide range of tasks\nAgentic platform\nAgent skills in the form of programming-language functions\n\nImportance: It is open source and provide key agent skills and capabilites to build SWE multi-agent systems\nImprovements:\n\nImprove agent’s ability to view multiple file formats such as images and videos\nAgents are still struggling with complex tasks -&gt; need stronger agents either through training or inference\nAgents suffer at editing long files\nWeb browsing is still a challenge\nOptimizing multi-agent system is a challenge\n\nNotes:\n\nOpenHands:\n\nAgent Hub: community can provide implementation of different agents\nAgent Stream: Tracks the history of actions and observations\nAgent Runtime: Sandboxed environment to execute actions and collect observations\n\nAgents collaborate, write code, execute code in a Sandboxed environment, and browse the web\nAgent implementation:\n\nState: data structure with previous actions and observations as well ass agent’s own actions and user feedbacks/instructions. It also includes other metadata related to the agent’s operation\nAgent is connected to the environment through some set of code actions in Python and bash following CodeAct architecture and can browse the internet (borrowed from WebArena). This is very flexible and compatible with any actions needed\nTools are provided to the agent using programming-language primitives actions such as functions. Agent may sometime write the tool itself when there is no API available to perform the task(s)\nObservations: Previous results of actions and user messages in the form of instructions/feedbacks\nA user can create/customize agents by changing the logic of the step method that would have the agent logic and expected behavior. The user doesn’t have to worry about how the code gets executed\n\nRuntime environment:\n\nProvides bash terminal to run code and command line tools, Jupyter notebook for writing and executing code, and browing the web to web-based tasks\nIt is based on docker sandbox. OpenHands interact with the sandbox environment using REST API. OpenHands also maintains a server inside the sandbox environment that listens to event stream for requested actions. The results of the actions will be returned to event stream as observations. We can also mount directories to this environment related to the agent’s task(s)\n\nAgent Skills:\n\nAgentSkills is a Python library that provides tools to agents to extend its skills. It can also be imported inside Jupyter notebook and any agent can use the available tools or contribute such tools in the form of functions to the library\nThere is no need to add tools that the LLM knows such as pandas that was part of the pre-training dataset\n\nAgent delegation:\n\nAgents can delegate to another agent some work. For example, generalist agent such as CodeAct agent can delegate web browsing work to BrowsingAgent, which specialized in web browsing\n\nAgentHub:\n\nCodeAct agent is a generalist agent the converses with humans and perform coding tasks using bash commands and Python. It can also do web browsing\nBrowsingAgent is also a generalist that does web browsing similar to that in WebArena\nGPTSwarm is a specialized agent that optimize graphs to construct agent systems where each node is an operation and each edge defines communication pathways and collaboration. This should lead to powerful multi-agent systems\nMicro agent is a small and specialized agent that does specific task. It typically reuses most of the implementations from CodeAct agent but have different optimized prompt that works better in certain use cases\n\nEvaluation:\n\nOpenHands agents, without any optimization and are general, evaluation results may not be the top but are very close to agentic systems that were tuned and optimized for specific tasks\n\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "papers-summaries/swe-agent.html",
    "href": "papers-summaries/swe-agent.html",
    "title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
    "section": "",
    "text": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering\n\nThesis: Agent-Computer Interfaces (ACIs) can significantly enhance the ability of language model agents to autonomously perform software engineering tasks. The paper introduces SWE-agent, a system that leverages these interfaces to enable language models, such as GPT-4 or other large language models, to interact with computers effectively. This interaction allows the agent to autonomously address tasks like bug fixing, competitive coding, and even offensive cybersecurity challenges\nContribution:\n\nIntroduce ACI and its importance to improve LM agent’s performance\nSWE-agent\n\nTakeaways: Spending time on building well-crafted ACI will yield the best performance boost for LM agents. The current state of UI has only human end users in my which is completely different than LM agent than has different needs and abilities. ACI in a major step forward to enhance LM agents to search and execute commands and integrate environment feedback into their state\nNotes:\n\nLM agent represents a new category of end users. Carefully designing the agent-computer interface (ACI) helps improve the LM performance and ability to solve complex tasks\nSWE-agent is a system that solves SWE tasks by creating ACI that allows the agent to create/edit files, navigate repositories, and execute tests/programs\nACI is just an abstraction that provides the LM agent with an interface to simple few commands such as viewing and editing files as well as searching and navigating repositories. It also provides well-formatted/structured feedback for the agent’s actions. Finally, it has some guardrails to prevent common mistakes\n\nThe current UI are built with only human end users in mind. However, LM agent has different abilities and needs. For example, LM agent suffers (as of now) to grasp the GUI and not get destracted with all unnecessary details that would also harm its performance\nACI provides actions that the LM agent can take as well as how the environment feedback is represented to the agent. It should specifies the previous changes, manage history in terms of what to ignore and what to keep an eye on, and actions that it can take efficiently and reliably\n\nFor a good ACI:\n\nActions should be simple with concise documentation\nEnvironment feedback should be informative and concise\nConsolidate actions especially the common ones\nHave guardrails to prevent common mistakes\n\nThe agent follows ReAct framework by generating a thought and action at each step. It then incorporate the feedback from the executed commands\nThe agent has access to common Linux commands\nSearch & Navigation:Agent has access to search for files and strings that are mentioned in the issues. Commands are find_file, search_file, search_directory. Results returned are at most 50 results for each query\nFile viewer: open a file in a window using open command and restrict it to 100 lines. The agent can use scroll_down and scroll_up to move the window as well as goto to jump to specific line. Code localization is provided by presenting the full path of the file and number of lines before/after the current line as well as the total number of lines in the file\nFile editor. edit command lets the agent to edit range of lines while viewing the file. Code linter is used to provide feedback to the agent in terms of syntax errors in the form of file contents before/after error was introduced. If there is an error, the agent is asked to retry the edit\nContext management. The agent receives prompts, instructions, documentation, and demonstration of the bash and ACI commands. If the agent fail to generate a response that strictly follow the thought/action paradigm, it triggers an error until one is produced\n\n\n#nlp #llm #agents"
  },
  {
    "objectID": "posts/dl-systems/automatic-differentiation.html",
    "href": "posts/dl-systems/automatic-differentiation.html",
    "title": "Understanding Forward and Reverse-mode Automatic Differentiation in Deep Learning",
    "section": "",
    "text": "Automatic Differentiation (AD) is a fundamental technique in modern machine learning, particularly crucial for training neural networks and optimizing complex models. In this post, we’ll dive deep into the two main modes of AD: Forward Mode and Reverse-mode, exploring their mechanisms, advantages, and trade-offs."
  },
  {
    "objectID": "posts/dl-systems/automatic-differentiation.html#forward-mode-ad-from-input-to-output",
    "href": "posts/dl-systems/automatic-differentiation.html#forward-mode-ad-from-input-to-output",
    "title": "Understanding Forward and Reverse-mode Automatic Differentiation in Deep Learning",
    "section": "Forward-mode AD: From Input to Output",
    "text": "Forward-mode AD: From Input to Output\n\nForward Mode AD operates by propagating derivatives from input nodes to output nodes in a computational graph. At each step, it calculates the partial derivative of the current node with respect to the input node. This mode implements a straightforward approach to computing derivatives, making it particularly intuitive to understand.\nKey features:\n\nMultiple Passes Required: For a function with n inputs, Forward Mode AD requires n separate forward passes to compute all necessary derivatives. This characteristic makes it less efficient for functions with many input variables, which is often the case in deep learning applications.\nImplementation Using Dual Numbers: Forward Mode AD typically uses dual numbers (a + bε) to track both the value and its derivative simultaneously during computation. This elegant mathematical approach allows for efficient derivative computation alongside function evaluation.\nMemory Efficiency: One advantage of Forward Mode AD is its relatively lower memory requirements compared to Reverse-mode. Since it doesn’t need to store intermediate results for backpropagation, it can be more memory-efficient for certain types of computations."
  },
  {
    "objectID": "posts/dl-systems/automatic-differentiation.html#backward-mode-ad-the-power-of-reverse-computation",
    "href": "posts/dl-systems/automatic-differentiation.html#backward-mode-ad-the-power-of-reverse-computation",
    "title": "Understanding Forward and Backward Mode Automatic Differentiation in Deep Learning",
    "section": "Backward Mode AD: The Power of Reverse Computation",
    "text": "Backward Mode AD: The Power of Reverse Computation\nBackward Mode AD, also known as reverse-mode AD, has become the cornerstone of modern deep learning frameworks. This approach computes derivatives by working backwards from the output to the inputs, making it particularly efficient for functions with many inputs but few outputs.\n\nKey Features:\n\nSingle Pass Efficiency: One of the most significant advantages of Backward Mode AD is its ability to compute gradients with respect to all input variables in a single backward pass. This makes it highly efficient for typical deep learning scenarios where we have scalar loss functions but numerous parameters.\nGradient Computation: The gradients computed at each node indicate how much that node needs to be adjusted to achieve the steepest ascent/descent direction locally. This information is crucial for optimization algorithms like gradient descent.\nHardware Optimization: Backward Mode AD allows for various optimizations that can leverage underlying hardware capabilities. Framework implementations can fuse operations and utilize parallel processing to enhance performance.\n\n\n\nAdvantages and Challenges:\n\nPros:\n\nHighly efficient for functions with many inputs and few outputs (typical in deep learning)\nEnables sophisticated hardware optimizations and operation fusion\nWidely supported in popular frameworks like TensorFlow and PyTorch\n\n\n\nCons:\n\nRequires storing intermediate results during computation\nHigher memory usage due to maintaining input tensors and operations for gradient computation\nMemory requirements can become significant for large models or batch sizes"
  },
  {
    "objectID": "posts/dl-systems/automatic-differentiation.html#memory-usage-and-performance-considerations",
    "href": "posts/dl-systems/automatic-differentiation.html#memory-usage-and-performance-considerations",
    "title": "Understanding Forward and Reverse-mode Automatic Differentiation in Deep Learning",
    "section": "Memory Usage and Performance Considerations",
    "text": "Memory Usage and Performance Considerations\nThe choice between Forward and Reverse-mode AD often comes down to memory-performance trade-offs:\n\nMemory Patterns:\n\nForward Mode AD can achieve up to 1.97 times memory reduction compared to baseline models\nReverse-mode AD requires more memory but offers better computational efficiency for typical deep learning scenarios\n\nPerformance Impact:\n\nForward Mode AD can show up to 20% performance improvement over recomputation methods under similar memory constraints\nReverse-mode AD’s efficiency makes it the preferred choice for deep learning despite higher memory requirements"
  },
  {
    "objectID": "posts/dl-systems/automatic-differentiation.html#conclusion",
    "href": "posts/dl-systems/automatic-differentiation.html#conclusion",
    "title": "Understanding Forward and Reverse-mode Automatic Differentiation in Deep Learning",
    "section": "Conclusion",
    "text": "Conclusion\nBoth Forward and Reverse-mode AD have their place in modern machine learning. While Forward Mode AD offers better memory efficiency and is suitable for functions with few inputs, Reverse-mode AD has become the de facto standard in deep learning due to its computational efficiency with scalar loss functions and many parameters. Understanding these trade-offs is crucial for choosing the right approach for specific applications and optimizing model training processes.\nThe choice between the two modes ultimately depends on your specific use case, considering factors such as:\n\nThe ratio of input to output variables\nAvailable memory resources\nComputational efficiency requirements\nHardware optimization capabilities"
  },
  {
    "objectID": "posts/dl-systems/automatic-differentiation.html#reverse-mode-ad-the-power-of-reverse-computation",
    "href": "posts/dl-systems/automatic-differentiation.html#reverse-mode-ad-the-power-of-reverse-computation",
    "title": "Understanding Forward and Reverse-mode Automatic Differentiation in Deep Learning",
    "section": "Reverse-mode AD: The Power of Reverse Computation",
    "text": "Reverse-mode AD: The Power of Reverse Computation\n\nReverse-mode AD, has become the cornerstone of modern deep learning frameworks. This approach computes derivatives by working backwards from the output to the inputs, making it particularly efficient for functions with many inputs but few outputs.\nkey features:\n\nSingle Pass Efficiency: One of the most significant advantages of Reverse-mode AD is its ability to compute gradients with respect to all input variables in a single backward pass. This makes it highly efficient for typical deep learning scenarios where we have scalar loss functions but numerous parameters.\nGradient Computation: The gradients computed at each node indicate how much that node needs to be adjusted to achieve the steepest ascent/descent direction locally. This information is crucial for optimization algorithms like gradient descent.\nHardware Optimization: Reverse-mode AD allows for various optimizations that can leverage underlying hardware capabilities. Framework implementations can fuse operations and utilize parallel processing to enhance performance.\n\nPros:\n\nHighly efficient for functions with many inputs and few outputs (typical in deep learning)\nEnables sophisticated hardware optimizations and operation fusion\n\nCons:\n\nRequires storing intermediate results during computation\nHigher memory usage due to maintaining input tensors and operations for gradient computation\n\nMemory requirements can become significant for large models or batch sizes"
  }
]