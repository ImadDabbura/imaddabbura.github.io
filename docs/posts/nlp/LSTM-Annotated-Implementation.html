<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Imad Dabbura">
<meta name="dcterms.date" content="2022-12-10">

<title>Imad Dabbura - Inside LSTMs: Implementing and Optimizing Sequential Models from First Principles</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/profile-pic.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "PVDXB8B7OS",
    "search-only-api-key": "eb3007c200831c30465f8a5172690cf0",
    "index-name": "Initial-Website-Search-Index",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-127825273-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(lstm-cell.jpeg);
background-size: cover;
      }
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../assets/posts.css">
<meta property="og:title" content="Imad Dabbura - Inside LSTMs: Implementing and Optimizing Sequential Models from First Principles">
<meta property="og:description" content="A deep dive into LSTM internals—covering the math, gates, performance considerations, and a full PyTorch-aligned implementation from scratch.">
<meta property="og:image" content="https://imaddabbura.github.io/posts/nlp/lstm-cell.jpeg">
<meta property="og:site_name" content="Imad Dabbura">
<meta name="twitter:title" content="Imad Dabbura - Inside LSTMs: Implementing and Optimizing Sequential Models from First Principles">
<meta name="twitter:description" content="A deep dive into LSTM internals—covering the math, gates, performance considerations, and a full PyTorch-aligned implementation from scratch.">
<meta name="twitter:image" content="https://imaddabbura.github.io/posts/nlp/lstm-cell.jpeg">
<meta name="twitter:creator" content="@imaddabbura">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../images/profile-pic.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Imad Dabbura</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../til.html"> <i class="bi bi-lightbulb" role="img">
</i> 
<span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers-summaries.html"> 
<span class="menu-text">Papers’ Summaries</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../dl-tips-tricks.html"> 
<span class="menu-text">DL Tips &amp; Tricks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects-index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-more" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">More</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-more">    
        <li>
    <a class="dropdown-item" href="../../data-index.html">
 <span class="dropdown-text">Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../books-summaries.html">
 <span class="dropdown-text">Books’ Summaries</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../reading-list.html">
 <span class="dropdown-text">Reading List</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resume.html">
 <span class="dropdown-text">Resume</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../misc-notes.html">
 <span class="dropdown-text">Misc. Notes</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/imaddabbura"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Inside LSTMs: Implementing and Optimizing Sequential Models from First Principles</h1>
            <p class="subtitle lead">A deep dive into LSTM internals—covering the math, gates, performance considerations, and a full PyTorch-aligned implementation from scratch.</p>
                                <div class="quarto-categories">
                <div class="quarto-category">NLP</div>
                <div class="quarto-category">Deep Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Imad Dabbura </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 10, 2022</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">May 15, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-implement-an-lstm-from-scratch" id="toc-why-implement-an-lstm-from-scratch" class="nav-link active" data-scroll-target="#why-implement-an-lstm-from-scratch">Why Implement an LSTM from Scratch?</a></li>
  <li><a href="#the-vanishing-gradient-problem" id="toc-the-vanishing-gradient-problem" class="nav-link" data-scroll-target="#the-vanishing-gradient-problem">The Vanishing Gradient Problem</a></li>
  <li><a href="#how-lstms-fix-it" id="toc-how-lstms-fix-it" class="nav-link" data-scroll-target="#how-lstms-fix-it">How LSTMs Fix It</a>
  <ul class="collapse">
  <li><a href="#why-two-states" id="toc-why-two-states" class="nav-link" data-scroll-target="#why-two-states">Why Two States?</a></li>
  <li><a href="#a-concrete-example" id="toc-a-concrete-example" class="nav-link" data-scroll-target="#a-concrete-example">A Concrete Example</a></li>
  </ul></li>
  <li><a href="#inside-the-lstm-cell" id="toc-inside-the-lstm-cell" class="nav-link" data-scroll-target="#inside-the-lstm-cell">Inside the LSTM Cell</a>
  <ul class="collapse">
  <li><a href="#independent-gates-four-operating-modes" id="toc-independent-gates-four-operating-modes" class="nav-link" data-scroll-target="#independent-gates-four-operating-modes">Independent Gates: Four Operating Modes</a></li>
  <li><a href="#gates-as-learned-pattern-detectors" id="toc-gates-as-learned-pattern-detectors" class="nav-link" data-scroll-target="#gates-as-learned-pattern-detectors">Gates as Learned Pattern Detectors</a></li>
  </ul></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a>
  <ul class="collapse">
  <li><a href="#lstmcell" id="toc-lstmcell" class="nav-link" data-scroll-target="#lstmcell"><code>LSTMCell</code></a></li>
  <li><a href="#from-cell-to-sequence-the-full-lstm" id="toc-from-cell-to-sequence-the-full-lstm" class="nav-link" data-scroll-target="#from-cell-to-sequence-the-full-lstm">From Cell to Sequence: The Full <code>LSTM</code></a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  </ul></li>
  <li><a href="#references-resources" id="toc-references-resources" class="nav-link" data-scroll-target="#references-resources">References &amp; Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/imaddabbura/imaddabbura.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="why-implement-an-lstm-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="why-implement-an-lstm-from-scratch">Why Implement an LSTM from Scratch?</h2>
<p>If you’ve used <code>nn.LSTM</code> in PyTorch, you’ve seen it work. But <em>how</em> does it decide what to remember and what to forget? Why does it need four gates instead of one? And why is it so much better than a vanilla RNN at handling long sequences?</p>
<p>The best way to answer these questions is to build one yourself. In this post, we’ll start with the problem that motivated LSTMs (vanishing gradients), build up the intuition for how they solve it, then implement both <code>LSTMCell</code> and a multi-layer <code>LSTM</code> from scratch in PyTorch — verifying each against the official implementation down to floating-point precision.</p>
</section>
<section id="the-vanishing-gradient-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-vanishing-gradient-problem">The Vanishing Gradient Problem</h2>
<p><strong>Long Short-Term Memory (LSTM)</strong> is a recurrent neural network architecture introduced by <a href="https://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter and Schmidhuber (1997)</a> to solve the <strong>vanishing gradient problem</strong> — the central failure mode of vanilla RNNs on long sequences.</p>
<p>To understand why LSTMs exist, we first need to understand what goes wrong. In a vanilla RNN, the hidden state is <em>completely overwritten</em> at every time step:</p>
<p><span class="math display">\[h_t = \tanh(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b)\]</span></p>
<p>During backpropagation, the gradient of the loss with respect to an early hidden state <span class="math inline">\(h_1\)</span> must pass through the <span class="math inline">\(\tanh\)</span> nonlinearity and the weight matrix <span class="math inline">\(W_{hh}\)</span> at <em>every single time step</em> between <span class="math inline">\(h_T\)</span> and <span class="math inline">\(h_1\)</span>. If the sequence has 100 tokens, the gradient is multiplied by <span class="math inline">\(W_{hh}\)</span> roughly 100 times. If the dominant eigenvalue of <span class="math inline">\(W_{hh}\)</span> is even slightly less than 1 — say 0.9 — the gradient shrinks by a factor of <span class="math inline">\(0.9^{100} \approx 0.00003\)</span>. The signal from early tokens effectively disappears.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Fundamental Issue
</div>
</div>
<div class="callout-body-container callout-body">
<p>The problem isn’t just mathematical — it has a concrete consequence: <strong>vanilla RNNs can’t learn long-range dependencies</strong>. If the answer to a question depends on a word 50 tokens earlier in the sentence, the gradient signal connecting them is essentially zero. The model can’t learn that relationship, no matter how long you train.</p>
</div>
</div>
</section>
<section id="how-lstms-fix-it" class="level2">
<h2 class="anchored" data-anchor-id="how-lstms-fix-it">How LSTMs Fix It</h2>
<p>The LSTM introduces a <strong>cell state</strong> <span class="math inline">\(c_t\)</span> — a separate memory channel that runs parallel to the hidden state. The critical difference is in <em>how</em> it gets updated:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Vanilla RNN</th>
<th>LSTM Cell State</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Update rule</strong></td>
<td><span class="math inline">\(h_t = \tanh(W \cdot h_{t-1} + \ldots)\)</span></td>
<td><span class="math inline">\(c_t = f_t \odot c_{t-1} + i_t \odot g_t\)</span></td>
</tr>
<tr class="even">
<td><strong>Mechanism</strong></td>
<td>Complete <em>replacement</em> through nonlinearity</td>
<td>Selective <em>modification</em> via additive gating</td>
</tr>
<tr class="odd">
<td><strong>Gradient flow</strong></td>
<td>Must pass through <span class="math inline">\(\tanh\)</span> and <span class="math inline">\(W\)</span> at every step</td>
<td>Can flow <em>directly</em> through the forget gate <span class="math inline">\(f_t\)</span></td>
</tr>
<tr class="even">
<td><strong>Long-range memory</strong></td>
<td>Exponential decay</td>
<td>Controlled retention</td>
</tr>
</tbody>
</table>
<p>The cell state update is <strong>additive</strong>: when the forget gate <span class="math inline">\(f_t\)</span> is close to 1 and the input gate <span class="math inline">\(i_t\)</span> is close to 0, the cell state passes through <em>unchanged</em>: <span class="math inline">\(c_t \approx c_{t-1}\)</span>. Gradients flow backward through time with minimal decay — no weight matrix or nonlinearity in the way.</p>
<p>If this looks familiar, it should — it’s the same principle behind <strong>residual connections</strong> in ResNets. In a ResNet, each layer computes <span class="math inline">\(y = F(x) + x\)</span>: the input passes through unchanged, and the layer only learns the <em>residual</em>. The LSTM cell state works the same way, but across <strong>time instead of depth</strong>: the previous cell state passes through (scaled by <span class="math inline">\(f_t\)</span>), and the network adds a residual update (<span class="math inline">\(i_t \odot g_t\)</span>). Both create a gradient highway. ResNets made it possible to train 100+ layer networks; the LSTM cell state makes it possible to learn dependencies across 100+ time steps. Same insight, different axis.</p>
<p align="center">
<img src="lstm-cell.jpeg" style="width: 500px;"><br>

</p><center>
<u><b><font color="00b7e4">Figure 1:</font></b></u> The LSTM cell. The horizontal line at the top is the cell state — the “highway” through time. The four yellow boxes (<span class="math inline">\(\sigma, \sigma, \tanh, \sigma\)</span>) are the forget, input, cell, and output gates respectively. The cell state is updated additively (the ⊕ node), while the gates use element-wise multiplication (⊗) to control information flow.
</center>

<p></p>
<section id="why-two-states" class="level3">
<h3 class="anchored" data-anchor-id="why-two-states">Why Two States?</h3>
<p>A vanilla RNN has a single hidden state that must do <em>everything</em>: store long-term memory, carry short-term context, and produce the output that downstream layers consume. That’s too many jobs for one vector — optimizing the hidden state for the current prediction destroys the long-term information stored in it.</p>
<p>LSTMs split this into two specialized roles:</p>
<p><strong>Cell state (<span class="math inline">\(c_t\)</span>): the long-term internal memory.</strong> The cell state is the LSTM’s private memory — never directly exposed to the rest of the network. Its job is to <em>retain information across long distances</em> without interference. Because it’s updated additively, gradients can flow through it across hundreds of time steps. Think of it as a notebook that the LSTM writes to and reads from, but never shows to anyone directly.</p>
<p><strong>Hidden state (<span class="math inline">\(h_t\)</span>): the short-term working output.</strong> The hidden state is what the LSTM <em>exposes</em> to the outside world — the input to the next layer, the softmax, or whatever comes next. It’s computed by selectively reading from the cell state via the output gate: <span class="math inline">\(h_t = o_t \odot \tanh(c_t)\)</span>. The output gate decides: <em>“Given everything I know and the current context, what’s relevant right now?”</em></p>
<p>This separation is crucial. The cell state can hold information like “the subject is plural” or “we’re inside a quotation” for as long as needed, without being distorted by the demands of predicting intermediate tokens. When it <em>is</em> needed — the output gate reads it out at exactly the right moment.</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Cell State (<span class="math inline">\(c_t\)</span>)</th>
<th>Hidden State (<span class="math inline">\(h_t\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Role</strong></td>
<td>Long-term memory</td>
<td>Short-term working output</td>
</tr>
<tr class="even">
<td><strong>Visible to</strong></td>
<td>Only the LSTM itself (internal)</td>
<td>Next layer, softmax, classifier (external)</td>
</tr>
<tr class="odd">
<td><strong>Updated by</strong></td>
<td>Forget gate (erase) + input gate (write)</td>
<td>Output gate reading from cell state</td>
</tr>
<tr class="even">
<td><strong>Gradient flow</strong></td>
<td>Additive — gradients pass through cleanly</td>
<td>Through tanh and output gate — more lossy</td>
</tr>
<tr class="odd">
<td><strong>Analogy</strong></td>
<td>A notebook you write in privately</td>
<td>The answer you speak aloud when asked</td>
</tr>
</tbody>
</table>
</section>
<section id="a-concrete-example" class="level3">
<h3 class="anchored" data-anchor-id="a-concrete-example">A Concrete Example</h3>
<p>Consider: <em>“The cat, which sat on the mat in the living room near the window overlooking the garden, <strong>was</strong> sleeping.”</em> The verb “was” must agree with “cat” (singular), not “garden” or “window” — a dependency spanning ~15 tokens. A vanilla RNN’s gradient signal from “was” back to “cat” would be multiplied by <span class="math inline">\(W_{hh}\)</span> fifteen times — likely vanishing. An LSTM can keep “cat = singular noun” in its cell state with the forget gate near 1, preserving the information until it’s needed at “was.”</p>
<p>One important constraint: RNNs and LSTMs are <strong>sequential models</strong> — the output at time <span class="math inline">\(t\)</span> depends on the hidden state from <span class="math inline">\(t-1\)</span>. We cannot parallelize across time steps; we must iterate one token at a time. This is the limitation that the Transformer (<a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a>) later addressed with self-attention.</p>
</section>
</section>
<section id="inside-the-lstm-cell" class="level2">
<h2 class="anchored" data-anchor-id="inside-the-lstm-cell">Inside the LSTM Cell</h2>
<p>An <code>LSTMCell</code> computes four gates, then uses them to update the cell and hidden states. Each gate has the same dimension as the hidden state:</p>
<span class="math display">\[\begin{array}{ll} \\
i_t = \sigma(W_{ii} x_t + b_{ii} + W_{ih} h_{t-1} + b_{hi}) \\
f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\
o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\
c_t = f_t \odot c_{t-1} + i_t \odot g_t \\
h_t = o_t \odot \tanh(c_t) \\
\end{array}\]</span>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Gate</th>
<th>Name</th>
<th>Activation</th>
<th>What It Does</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(i_t\)</span></td>
<td><strong>Input gate</strong></td>
<td>Sigmoid (0–1)</td>
<td>How much of the <em>new</em> candidate values to write into the cell</td>
</tr>
<tr class="even">
<td><span class="math inline">\(f_t\)</span></td>
<td><strong>Forget gate</strong></td>
<td>Sigmoid (0–1)</td>
<td>How much of the <em>old</em> cell state to keep (1 = remember everything, 0 = forget everything)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(g_t\)</span></td>
<td><strong>Cell gate</strong></td>
<td>Tanh (-1 to 1)</td>
<td>The candidate new values to potentially add to the cell state</td>
</tr>
<tr class="even">
<td><span class="math inline">\(o_t\)</span></td>
<td><strong>Output gate</strong></td>
<td>Sigmoid (0–1)</td>
<td>How much of the cell state to expose as the hidden state output</td>
</tr>
</tbody>
</table>
<p>Notice the activation functions: three gates use <strong>sigmoid</strong>, but the cell gate uses <strong>tanh</strong>. This isn’t arbitrary — it reflects their different roles. The sigmoid gates (<span class="math inline">\(i_t, f_t, o_t\)</span>) answer <em>“how much?”</em> questions: how much to write, how much to keep, how much to expose. Sigmoid squashes values to (0, 1), making each gate a dimmer switch that scales its input between “fully off” and “fully on.” The cell gate <span class="math inline">\(g_t\)</span> answers a different question: <em>“what values?”</em> It proposes candidate content to write into the cell state. Tanh maps to (-1, 1), which is critical — it allows the cell state to both <strong>increase and decrease</strong>. If <span class="math inline">\(g_t\)</span> used sigmoid (0, 1), the additive update <span class="math inline">\(i_t \odot g_t\)</span> could only ever push the cell state upward, and it would grow without bound. Tanh lets the network write negative corrections, keeping the cell state centered and bounded.</p>
<section id="independent-gates-four-operating-modes" class="level3">
<h3 class="anchored" data-anchor-id="independent-gates-four-operating-modes">Independent Gates: Four Operating Modes</h3>
<p>A critical design choice is that the input gate and forget gate are <strong>completely independent</strong> — computed from separate weight matrices and biases, with nothing constraining them to sum to 1. The network is free to set both high, both low, or any combination.</p>
<p>Contrast this with the GRU (Gated Recurrent Unit), where the equivalent gates <em>are</em> complementary: a single update gate <span class="math inline">\(z_t\)</span> weights new content by <span class="math inline">\(z_t\)</span> and old content by <span class="math inline">\((1 - z_t)\)</span>, forcing a trade-off. The GRU is more parameter-efficient, but less expressive — it can only interpolate between “keep old” and “write new.”</p>
<p>The LSTM’s independence gives it four distinct operating modes:</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Forget <span class="math inline">\(f_t\)</span></th>
<th>Input <span class="math inline">\(i_t\)</span></th>
<th>Mode</th>
<th>Effect</th>
<th>When It’s Useful</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\approx 1\)</span></td>
<td><span class="math inline">\(\approx 1\)</span></td>
<td><strong>Accumulate</strong></td>
<td>Keep old state <em>and</em> write new info</td>
<td>Building up a running representation (e.g., accumulating features of a described entity)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\approx 0\)</span></td>
<td><span class="math inline">\(\approx 1\)</span></td>
<td><strong>Replace</strong></td>
<td>Flush old state, write new info</td>
<td>Topic change, sentence boundary — start fresh with new content</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\approx 1\)</span></td>
<td><span class="math inline">\(\approx 0\)</span></td>
<td><strong>Preserve</strong></td>
<td>Keep old state, ignore current input</td>
<td>Carrying information across irrelevant tokens (e.g., remembering subject across a parenthetical)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\approx 0\)</span></td>
<td><span class="math inline">\(\approx 0\)</span></td>
<td><strong>Reset</strong></td>
<td>Forget old state <em>and</em> ignore input</td>
<td>Clearing a dimension that’s no longer needed</td>
</tr>
</tbody>
</table>
<p>The GRU can only express the diagonal of this table. This is why LSTMs tend to outperform GRUs on tasks requiring long-range memory: the accumulate mode lets information persist indefinitely while still absorbing new inputs, and the reset mode provides a clean mechanism for freeing capacity.</p>
</section>
<section id="gates-as-learned-pattern-detectors" class="level3">
<h3 class="anchored" data-anchor-id="gates-as-learned-pattern-detectors">Gates as Learned Pattern Detectors</h3>
<p>It’s tempting to think of gates as simple switches, but each gate is a <strong>learned pattern detector</strong> — analogous to how a CNN filter activates on specific visual patterns, a gate’s weight matrix learns to activate on specific <em>contextual patterns</em> in the input and hidden state. A CNN filter produces a high activation when the input patch matches its learned pattern; a gate weight matrix produces a high activation (close to 1 after sigmoid) when the combination of <span class="math inline">\(x_t\)</span> and <span class="math inline">\(h_{t-1}\)</span> matches <em>its</em> learned pattern. CNN filters detect <em>spatial</em> patterns in pixel neighborhoods; gate weights detect <em>contextual</em> patterns across the current token and sequence history.</p>
<p>Consider the forget gate: <span class="math inline">\(f_t = \sigma(W_{if} \cdot x_t + W_{hf} \cdot h_{t-1} + b_f)\)</span>. After training, specific rows of these weight matrices become specialized detectors:</p>
<ul>
<li>Some rows might detect <strong>“end of clause”</strong> patterns (a period, “but”) — signaling that old context should be flushed</li>
<li>Other rows might detect <strong>“continuation”</strong> patterns (a comma, “which”) — signaling that existing context should be preserved</li>
<li>Rows in the input gate might detect <strong>“salient new information”</strong> patterns (a named entity, a negation word) — signaling that this input should be written into memory</li>
</ul>
<p>This happens <strong>per dimension</strong> of the hidden state. The gate output is a vector, not a scalar — dimension 42 of the forget gate might be close to 0 (forget) while dimension 73 is close to 1 (keep), because each dimension stores different information and each gate dimension detects different patterns.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Single-Matrix Trick
</div>
</div>
<div class="callout-body-container callout-body">
<p>Even though we describe four separate gates, in practice we compute them all in <strong>one matrix multiplication</strong> by concatenating the four weight matrices into a single <code>4 * hidden_size</code> matrix. We then split the result into four chunks. This is much faster because it replaces four small matmuls with one large one — better utilizing GPU parallelism and memory bandwidth.</p>
</div>
</div>
</section>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>With the conceptual foundation in place, let’s turn these equations into code. We’ll build two modules — <code>LSTMCell</code> (one time step) and <code>LSTM</code> (full sequences with multiple layers) — verifying each against PyTorch’s official implementation.</p>
<section id="lstmcell" class="level3">
<h3 class="anchored" data-anchor-id="lstmcell"><code>LSTMCell</code></h3>
<p>We implement two versions: a verbose one that makes every operation explicit (separate weight matrices for each gate), and a compact one using <code>nn.Linear</code> with the single-matrix trick. Both produce identical results — the compact version is what you’d use in practice.</p>
<div id="4f325555-fdab-47c3-94a6-38a4a2e6dd11" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="e7aa6509-79db-4a54-9d47-e8c656fa28d7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Long version</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMCellNew(nn.Module):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_sz, hidden_sz, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_ih <span class="op">=</span> nn.Parameter(torch.randn((input_sz, hidden_sz <span class="op">*</span> <span class="dv">4</span>)))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_hh <span class="op">=</span> nn.Parameter(torch.randn((hidden_sz, hidden_sz <span class="op">*</span> <span class="dv">4</span>)))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias_ih <span class="op">=</span> nn.Parameter(torch.zeros(hidden_sz <span class="op">*</span> <span class="dv">4</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias_hh <span class="op">=</span> nn.Parameter(torch.zeros(hidden_sz <span class="op">*</span> <span class="dv">4</span>))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, h, c):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">## B x hidden_sz</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight_ih <span class="op">+</span> h <span class="op">@</span> <span class="va">self</span>.weight_hh <span class="op">+</span> <span class="va">self</span>.bias_ih <span class="op">+</span> <span class="va">self</span>.bias_hh</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        i, f, g, o <span class="op">=</span> torch.split(out, <span class="dv">100</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        i, f, o <span class="op">=</span> torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> torch.tanh(g)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        c_t <span class="op">=</span> f <span class="op">*</span> c <span class="op">+</span> i <span class="op">*</span> g</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        h_t <span class="op">=</span> o <span class="op">*</span> torch.tanh(c_t)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h_t, c_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d5630f35-d6a3-435a-8a8d-0ae5c2b87601" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Short version utilizing linear layer module</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMCellNew(nn.Module):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_sz, hidden_sz, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ih <span class="op">=</span> nn.Linear(input_sz, hidden_sz <span class="op">*</span> <span class="dv">4</span>, bias<span class="op">=</span>bias)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hh <span class="op">=</span> nn.Linear(hidden_sz, hidden_sz <span class="op">*</span> <span class="dv">4</span>, bias<span class="op">=</span>bias)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, h, c):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.ih(x) <span class="op">+</span> <span class="va">self</span>.hh(h)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        i, f, g, o <span class="op">=</span> torch.split(out, <span class="dv">100</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        i, f, o <span class="op">=</span> torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> torch.tanh(g)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        c_t <span class="op">=</span> f <span class="op">*</span> c <span class="op">+</span> i <span class="op">*</span> g</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        h_t <span class="op">=</span> o <span class="op">*</span> torch.tanh(c_t)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h_t, c_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2c9e3a64-ba74-4c22-84ad-77d212fe2f31" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>batch_sz <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>input_sz <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>hidden_sz <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>num_layers <span class="op">=</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="18f4dcb4-49b0-4f90-bf15-961483ed0471" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(seq_len, batch_sz, input_sz, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>c_0 <span class="op">=</span> torch.randn(num_layers, batch_sz, hidden_sz, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>h_0 <span class="op">=</span> torch.randn(num_layers, batch_sz, hidden_sz, dtype<span class="op">=</span>torch.float32)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="13915d7b-7746-480b-8bfe-85d2000c21a1" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>pytorch_cell <span class="op">=</span> nn.LSTMCell(input_sz, hidden_sz, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    pytorch_cell.weight_hh.shape,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    pytorch_cell.weight_ih.shape,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    pytorch_cell.bias_ih.shape,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    pytorch_cell.bias_hh.shape,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(torch.Size([400, 100]),
 torch.Size([400, 20]),
 torch.Size([400]),
 torch.Size([400]))</code></pre>
</div>
</div>
<div id="36dbbe52-f7c4-4dff-9361-6757dffbd917" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">## h: B x hidden_sz</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">## c: B x hidden_sz</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>pytorch_h, pytorch_c <span class="op">=</span> pytorch_cell(X[<span class="dv">0</span>], (h_0[<span class="dv">0</span>], c_0[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9f7dd706-59f6-4162-b00c-8c3610c573d3" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>cell <span class="op">=</span> LSTMCellNew(input_sz, hidden_sz)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">## To make sure pytorch and our implementation both</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">## have the same weights so we can compare them</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>cell.ih.weight.data <span class="op">=</span> pytorch_cell.weight_ih.data</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>cell.hh.weight.data <span class="op">=</span> pytorch_cell.weight_hh.data</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>cell.ih.bias.data <span class="op">=</span> pytorch_cell.bias_ih.data</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>cell.hh.bias.data <span class="op">=</span> pytorch_cell.bias_hh.data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1a03238d-994b-427e-92bd-7fb1abff8b62" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>h_t, c_t <span class="op">=</span> cell(X[<span class="dv">0</span>], h_0[<span class="dv">0</span>], c_0[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0bc1356c-0d3e-4e79-a174-0b717407882c" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    np.linalg.norm(pytorch_h.detach().numpy() <span class="op">-</span> h_t.detach().numpy()),</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    np.linalg.norm(pytorch_c.detach().numpy() <span class="op">-</span> c_t.detach().numpy()),</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.0 0.0</code></pre>
</div>
</div>
</section>
<section id="from-cell-to-sequence-the-full-lstm" class="level3">
<h3 class="anchored" data-anchor-id="from-cell-to-sequence-the-full-lstm">From Cell to Sequence: The Full <code>LSTM</code></h3>
<p>With <code>LSTMCell</code> verified, let’s build the full <code>LSTM</code> module that handles entire sequences and optionally stacks multiple layers.</p>
<p>There are several important design decisions in a production LSTM implementation:</p>
<p><strong>Memory layout: sequence-first (<code>T × B × D</code>).</strong> We use the sequence length as the first dimension instead of batch-first. Why? We iterate over time steps in the inner loop, and we want each <code>x[t]</code> to be a contiguous slice of memory. If batch were first, each time step’s data would be non-contiguous, requiring a copy on every iteration.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Contiguity Trap
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you pass batch-first tensors (<code>B × T × D</code>) to an LSTM that expects sequence-first, it will still “work” — but each time step access triggers an implicit copy because the memory isn’t contiguous along the time dimension. This can silently slow down training. PyTorch’s <code>nn.LSTM</code> has a <code>batch_first</code> flag that handles the transpose for you, but internally it still processes sequence-first.</p>
</div>
</div>
<p><strong>Truncated Backpropagation Through Time (TBPTT).</strong> Since weights are shared across all time steps within a layer, backpropagating through very long sequences causes severe vanishing/exploding gradients <em>and</em> extreme memory usage (all intermediate activations must be stored). The standard solution: <strong>detach</strong> the hidden and cell states from the computation graph after each batch. Gradients can flow within a batch’s time steps but not across batch boundaries.</p>
<p><strong>Multi-layer stacking.</strong> We can stack LSTMs by feeding the hidden state output of layer <span class="math inline">\(l\)</span> as the input to layer <span class="math inline">\(l+1\)</span>. Each layer has its own <code>LSTMCell</code> with independent weights. The first layer’s cell takes input of size <code>input_sz</code>; all subsequent layers take input of size <code>hidden_sz</code>. This increases model capacity — deeper layers can learn more abstract representations.</p>
<p><strong>Layer iteration order.</strong> With multiple layers, there are two valid iteration orders: (1) iterate all time steps for layer 0, then all time steps for layer 1, etc., or (2) at each time step, iterate through all layers before moving to the next time step. Our implementation uses option (1), which is simpler and matches PyTorch’s behavior.</p>
<p><strong>Handling variable-length sequences.</strong> Not all sequences have the same length. Two approaches:</p>
<ol type="1">
<li><strong>Padding</strong>: pad shorter sequences to the longest length with zeros (pre- or post-padding). Simple but wasteful — the model does unnecessary computation on padding tokens.</li>
<li><strong>Packed sequences</strong>: combine all sequences together with index metadata marking boundaries. More efficient but more complex to implement. PyTorch provides <code>pack_padded_sequence</code> and <code>pad_packed_sequence</code> utilities for this.</li>
</ol>
<div id="d7691b4e-235e-434c-b22f-a5130c6ad864" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMNew(nn.Module):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_sz, hidden_sz, num_layers<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_sz <span class="op">=</span> hidden_sz</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cells <span class="op">=</span> nn.ModuleList(</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                LSTMCellNew(input_sz, hidden_sz)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span> LSTMCellNew(hidden_sz, hidden_sz)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, h_t, c_t):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">## x  :      T     x B x hidden_sz</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">## h_t: num_layers x B x hidden_sz</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">## c_t: num_layers x B x hidden_sz</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        T, B, _ <span class="op">=</span> x.shape</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> torch.zeros(T, B, <span class="va">self</span>.hidden_sz)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, cell <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.cells):</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            h, c <span class="op">=</span> h_t[i], c_t[i]</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> H</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>                h, c <span class="op">=</span> cell(x[t], h, c)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>                H[t] <span class="op">=</span> h</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>            <span class="co">## last hidden state for each layer</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>            h_t[i], c_t[i] <span class="op">=</span> h, c</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Truncated BPTT</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> H, (h_t.detach(), c_t.detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d2c608d9-11f0-49a6-bcbf-e8a7387cb205" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pytorch_lstm <span class="op">=</span> nn.LSTM(input_sz, hidden_sz, num_layers<span class="op">=</span>num_layers)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>pytorch_H, (pytorch_h, pytorch_c) <span class="op">=</span> pytorch_lstm(X, (h_0, c_0))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0966232f-132c-4e2f-b97c-8fa2fab3ba56" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>lstm <span class="op">=</span> LSTMNew(input_sz, hidden_sz, num_layers<span class="op">=</span>num_layers)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    lstm.cells[i].ih.weight.data <span class="op">=</span> <span class="bu">getattr</span>(pytorch_lstm, <span class="ss">f"weight_ih_l</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>).data</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    lstm.cells[i].hh.weight.data <span class="op">=</span> <span class="bu">getattr</span>(pytorch_lstm, <span class="ss">f"weight_hh_l</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>).data</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    lstm.cells[i].ih.bias.data <span class="op">=</span> <span class="bu">getattr</span>(pytorch_lstm, <span class="ss">f"bias_ih_l</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>).data</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    lstm.cells[i].hh.bias.data <span class="op">=</span> <span class="bu">getattr</span>(pytorch_lstm, <span class="ss">f"bias_hh_l</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>).data</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>H, (h_t, c_t) <span class="op">=</span> lstm(X, h_0, c_0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="652fc3bb-7371-489b-85c4-8cd2e97fd60f" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    np.linalg.norm(pytorch_H.detach().numpy() <span class="op">-</span> H.detach().numpy()),</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    np.linalg.norm(pytorch_h.detach().numpy() <span class="op">-</span> h_t.detach().numpy()),</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    np.linalg.norm(pytorch_c.detach().numpy() <span class="op">-</span> c_t.detach().numpy()),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.0 0.0 0.0</code></pre>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>LSTMs were the dominant architecture for sequence modeling in NLP for years — powering machine translation, text classification, language modeling, and speech recognition before Transformers took over. In this post, we implemented both <code>LSTMCell</code> and a multi-layer <code>LSTM</code> from scratch, verified them against PyTorch’s official implementation, and discussed the performance decisions that go into a production implementation.</p>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<ol type="1">
<li><p><strong>LSTMs solve vanishing gradients through additive cell state updates.</strong> The forget gate can stay close to 1, allowing gradients to flow through many time steps without exponential decay. This is fundamentally different from vanilla RNNs, where the hidden state is completely overwritten at each step.</p></li>
<li><p><strong>Four gates, one matrix multiplication.</strong> The input, forget, cell, and output gates are computed together in a single fused operation, then split — a practical optimization that significantly improves throughput by better utilizing hardware parallelism.</p></li>
<li><p><strong>Sequential processing is the fundamental bottleneck.</strong> The output at time <span class="math inline">\(t\)</span> depends on the hidden state from <span class="math inline">\(t-1\)</span>, making parallelization across time steps impossible. This is the limitation that motivated the Transformer’s self-attention mechanism.</p></li>
<li><p><strong>Truncated BPTT is essential for long sequences.</strong> Detaching hidden states between batches prevents gradient computation from spanning the entire sequence, reducing both memory usage and gradient instability.</p></li>
<li><p><strong>Memory layout matters.</strong> Using sequence-first tensors (<code>T × B × D</code>) ensures contiguous memory access at each time step, avoiding hidden performance penalties from implicit copies.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LSTMs in the Transformer Era
</div>
</div>
<div class="callout-body-container callout-body">
<p>While Transformers have largely replaced LSTMs for most NLP tasks, understanding LSTMs remains valuable. They’re still used in streaming/online settings where you process one token at a time, in resource-constrained environments where the <span class="math inline">\(O(n^2)\)</span> attention cost is prohibitive, and as components in hybrid architectures. More importantly, the concepts — gating, cell states, truncated BPTT — appear in many modern architectures in different forms.</p>
</div>
</div>
</section>
</section>
<section id="references-resources" class="level2">
<h2 class="anchored" data-anchor-id="references-resources">References &amp; Resources</h2>
<ul>
<li><strong>Hochreiter, S. &amp; Schmidhuber, J.</strong> (1997). <a href="https://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a>. <em>Neural Computation</em>, 9(8), 1735–1780. The original LSTM paper.</li>
<li><strong>Olah, C.</strong> (2015). <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>. The classic visual explainer — the single best resource for building intuition about LSTM gates.</li>
<li><strong>Greff, K. et al.</strong> (2017). <a href="https://arxiv.org/abs/1503.04069">LSTM: A Search Space Odyssey</a>. <em>IEEE TNNLS</em>. Comprehensive study of LSTM variants — concludes that the forget gate and output activation are the most critical components.</li>
<li><strong>Vaswani, A. et al.</strong> (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. <em>NeurIPS 2017</em>. The Transformer architecture that largely replaced LSTMs for NLP tasks.</li>
<li><strong>Merity, S. et al.</strong> (2018). <a href="https://arxiv.org/abs/1708.02182">Regularizing and Optimizing LSTM Language Models</a>. <em>ICLR 2018</em>. AWD-LSTM — pushed LSTM language models to their limits with careful regularization.</li>
<li><strong>PyTorch Documentation</strong>. <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">nn.LSTM</a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html">nn.LSTMCell</a>. Official reference for the implementation we verified against.</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script>
document.addEventListener("DOMContentLoaded", function() {
    // Only add citation to post pages (not index, about, etc.)
    if (window.location.pathname.includes('/posts/')) {
        // Find the main content area
        const mainContent = document.querySelector('main');
        if (mainContent) {
            // Get metadata from the page
            const titleElement = document.querySelector('h1.title');
            const dateElement = document.querySelector('.date');
            
            const title = titleElement ? titleElement.textContent : 'Untitled';
            const dateText = dateElement ? dateElement.textContent : new Date().toLocaleDateString();
            
            // Get the current post URL - construct proper GitHub Pages URL
            const postPath = window.location.pathname;
            const postUrl = 'https://imaddabbura.github.io' + postPath;
            
            // Parse date and format it
            const date = new Date(dateText);
            const monthNames = ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                               "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"];
            const formattedDate = monthNames[date.getMonth()] + " " + date.getFullYear();
            const year = date.getFullYear();
            const month = monthNames[date.getMonth()];
            
            // Create citation HTML
            const citationHTML = '<hr>' +
                '<div class="callout callout-style-default callout-note no-icon callout-titled">' +
                    '<div class="callout-header d-flex align-content-center">' +
                        '<div class="callout-icon-container">' +
                            '<i class="callout-icon no-icon"></i>' +
                        '</div>' +
                        '<div class="callout-title-container flex-fill">' +
                            'How to Cite This Post' +
                        '</div>' +
                    '</div>' +
                    '<div class="callout-body-container callout-body">' +
                        '<p>If you found this useful, please cite this write-up as:</p>' +
                        '<blockquote class="blockquote">' +
                            '<p>Dabbura, Imad. (' + formattedDate + '). ' + title + '. https://imaddabbura.github.io. ' + postUrl + '.</p>' +
                        '</blockquote>' +
                        '<p>or</p>' +
                        '<div class="sourceCode"><pre class="sourceCode bibtex code-with-copy"><code class="sourceCode bibtex">@article{dabbura' + year + ',\n' +
                        '  title   = {' + title + '},\n' +
                        '  author  = {Dabbura, Imad},\n' +
                        '  journal = {https://imaddabbura.github.io},\n' +
                        '  year    = {' + year + '},\n' +
                        '  month   = {' + month + '},\n' +
                        '  url     = {' + postUrl + '}\n' +
                        '}</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>' +
                    '</div>' +
                '</div>';
            
            // Insert citation at the end of main content, before closing tag
            mainContent.insertAdjacentHTML('beforeend', citationHTML);
        }
    }
});
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/imaddabbura\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="imaddabbura/imaddabbura.github.io" data-repo-id="R_kgDOIEwRMg" data-category="General" data-category-id="DIC_kwDOIEwRMs4CRprP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Blog made with Quarto, by Imad Dabbura</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/imaddabbura/imaddabbura.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/imadphd">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/imaddabbura/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/imaddabbura">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/@ImadPhd">
      <i class="bi bi-medium" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:imad.dabbura@hotmail.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>