<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Imad Dabbura">
<meta name="dcterms.date" content="2024-04-10">

<title>Imad Dabbura - Coding GPT2/3 (124M) From Scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<script src="../../site_libs/quarto-search/autocomplete-preset-algolia.umd.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/profile.jpg" rel="icon" type="image/jpeg">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "algolia": {
    "application-id": "PVDXB8B7OS",
    "search-only-api-key": "eb3007c200831c30465f8a5172690cf0",
    "index-name": "Initial-Website-Search-Index",
    "analytics-events": true,
    "show-logo": true,
    "libDir": "site_libs"
  },
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js"></script>


<script type="text/javascript">
var ALGOLIA_INSIGHTS_SRC = "https://cdn.jsdelivr.net/npm/search-insights/dist/search-insights.iife.min.js";
!function(e,a,t,n,s,i,c){e.AlgoliaAnalyticsObject=s,e[s]=e[s]||function(){
(e[s].queue=e[s].queue||[]).push(arguments)},i=a.createElement(t),c=a.getElementsByTagName(t)[0],
i.async=1,i.src=n,c.parentNode.insertBefore(i,c)
}(window,document,"script",ALGOLIA_INSIGHTS_SRC,"aa");
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/autocomplete-plugin-algolia-insights">

</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-127825273-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Imad Dabbura - Coding GPT2/3 (124M) From Scratch">
<meta property="og:description" content="">
<meta property="og:site_name" content="Imad Dabbura">
<meta name="twitter:title" content="Imad Dabbura - Coding GPT2/3 (124M) From Scratch">
<meta name="twitter:description" content="">
<meta name="twitter:creator" content="@imaddabbura">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../images/palestine-flag.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Imad Dabbura</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers-summaries.html"> 
<span class="menu-text">Papers’ Summaries</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../til.html"> 
<span class="menu-text">Today I Learned</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../mlops.html"> 
<span class="menu-text">MLOps</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../dl-tips-tricks.html"> 
<span class="menu-text">DL Tips &amp; Tricks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cmn-ai.html"> 
<span class="menu-text">cmn_ai</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tiny-pytorch.html"> 
<span class="menu-text">Tiny-PyTorch</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-more" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">More</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-more">    
        <li>
    <a class="dropdown-item" href="../../projects-index.html">
 <span class="dropdown-text">Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../books-summaries.html">
 <span class="dropdown-text">Books’ Summaries</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../misc-notes.html">
 <span class="dropdown-text">Misc. Notes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../reading-list.html">
 <span class="dropdown-text">Reading List</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resume.html">
 <span class="dropdown-text">Resume</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/imaddabbura"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/imadphd"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Coding GPT2/3 (124M) From Scratch</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">NLP</div>
                <div class="quarto-category">Deep Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Imad Dabbura </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 10, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/imaddabbura/imaddabbura.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>TODO</p>
<ul>
<li>In the first iteration of the training, we want all tokens to have almost same probability and thus the loss on each one would be the same. The probability of each token would be <span class="math inline">\(1/vocab\_sz\)</span> -&gt; <span class="math inline">\(loss \approx -log(1/vocab\_sz)\)</span> because the probability distribution would be diffused.</li>
<li>3e-4 learning rate is good for <code>AdamW</code> optimizer for debugging</li>
<li>We want the model to overfit 1 batch to make sure it is running correctly</li>
<li>Weight sharing between token embedding and the final linear layer (also called classifier or LM head) because we want the tokens that are semantically similar to have similar probability when predicting next token.
<ul>
<li>This also has huge advantage on computational efficiency as those matrices have a lot parameters. For GPT2, each one has <span class="math inline">\(50257 * 768 \approx 38.5M\)</span> which is <span class="math inline">\(1/3\)</span> of the GPT2 model.</li>
<li>As a result of Weight sharing, gradient update will be addition from the two branches: classifier and token embedding</li>
</ul></li>
<li>For tokens that don’t appear in the training data, we want their probabilities to be very close to zero</li>
<li>CPU can continue running even if Cuda kernels are not done. This is because CPU is kinda scheduling the kernels on the GPU and doesn’t wait for them to finish -&gt; Use <code>torch.cuda.synchronize()</code> so CPU only presumes when scheduled kernels finish execution to get better timings</li>
</ul>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<div id="660e14ce-2270-4cf4-9078-36759f24db29" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial, wraps</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tiktoken</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> opt</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> destroy_process_group, init_process_group</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.parallel <span class="im">import</span> DistributedDataParallel <span class="im">as</span> DDP</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="4fd6e093-b414-4fb9-a165-04ae9e312c03" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> annealer(func: Callable):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    wraps(func)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> annealer_wrapper(<span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> partial(func, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> annealer_wrapper</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="at">@annealer</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lin_sched(start, end, pos):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Linear scheduler."""</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> start <span class="op">+</span> (end <span class="op">-</span> start) <span class="op">*</span> pos</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="at">@annealer</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cos_sched(start, end, pos):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Cosine scheduler."""</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> start <span class="op">+</span> (<span class="dv">1</span> <span class="op">+</span> math.cos(math.pi <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> pos))) <span class="op">*</span> (end <span class="op">-</span> start) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> combine_scheds(pcts, scheds):</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Combine multiple schedulers, each run for a given percentage of the</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co">    training process.</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(pcts) <span class="op">==</span> <span class="bu">len</span>(scheds), <span class="st">"Each scheduler should have its `pct`."</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">sum</span>(pcts) <span class="op">==</span> <span class="fl">1.0</span>, <span class="st">"Sum of the `pcts` should be equal to 1."</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    pcts <span class="op">=</span> torch.tensor([<span class="dv">0</span>] <span class="op">+</span> listify(pcts))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (pcts <span class="op">&gt;=</span> <span class="dv">0</span>).<span class="bu">all</span>(), <span class="st">"All percentages should be non-negative."</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    pcts <span class="op">=</span> torch.cumsum(pcts, <span class="dv">0</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _inner(pos):</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> (pos <span class="op">&gt;=</span> pcts).nonzero().<span class="bu">max</span>()</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        actual_pos <span class="op">=</span> (pos <span class="op">-</span> pcts[idx]) <span class="op">/</span> (pcts[idx <span class="op">+</span> <span class="dv">1</span>] <span class="op">-</span> pcts[idx])</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scheds[idx](actual_pos)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _inner</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="96f98324-58a9-49f1-9ad1-789c1a6b20e4" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPTConfig:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    block_sz: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    vocab_sz: <span class="bu">int</span> <span class="op">=</span> (</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="dv">50257</span>  <span class="co"># 50000 BPE merges + 256 byte tokens + 1 for &lt;|endoftext|&gt; token</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># which will delimits different documents. This token's index is 50256</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    n_layer: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    n_embd: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    n_head: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    lr: <span class="bu">int</span> <span class="op">=</span> <span class="fl">3e-4</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    batch_sz: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: GPTConfig):</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Point-wise feed-forward network that applies non-linearity</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># on every token sepearately. THERE IS NO INTERACTION BETWEEN TOKENS</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_fc <span class="op">=</span> nn.Linear(config.n_embd, config.n_embd <span class="op">*</span> <span class="dv">4</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gelu <span class="op">=</span> nn.GELU(approximate<span class="op">=</span><span class="st">"tanh"</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(config.n_embd <span class="op">*</span> <span class="dv">4</span>, config.n_embd)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj.NANOGPT_SCALE_INIT <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.c_proj(<span class="va">self</span>.gelu(<span class="va">self</span>.c_fc(x)))</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: GPTConfig):</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> config.n_head</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_embd <span class="op">=</span> config.n_embd</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_attn <span class="op">=</span> nn.Linear(config.n_embd, config.n_embd <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(config.n_embd, config.n_embd)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj.NANOGPT_SCALE_INIT <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: Bias is not needed when we use Pytorch's Flash attention</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.register_buffer(</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     "bias",</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     torch.tril(torch.ones(config.block_sz, config.block_sz)).view(</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         1, 1, config.block_sz, config.block_sz</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     ),</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># )</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> x.shape</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.c_attn(x)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># q/k/v is B x T x n_embd each</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> torch.split(qkv, <span class="va">self</span>.n_embd, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape q/k/v to B x n_head x T x (n_embd / n_head)</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># So each head would be learning different kind of</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># relationships</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn is B x T x T</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.shape[-1]))</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Mask out future tokens</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn = attn.masked_fill(self.bias[:, :, :T, :T] == 0, float("-inf"))</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn = F.softmax(attn, dim=-1)</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # y is B x T x n_embd</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y = attn @ v</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Uses Flash attention that never materialize attention matrices for</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># each head and is aware of the memory hierarchy and tries to reduce</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># read/writes with more FLOPs -&gt; Speed up since we're memory bound</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> F.scaled_dot_product_attention(q, k, v, is_causal<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.c_proj(y)</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: GPTConfig):</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_1 <span class="op">=</span> nn.LayerNorm(config.n_embd)</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_2 <span class="op">=</span> nn.LayerNorm(config.n_embd)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> MLP(config)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(config)</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use Pre-layer normalization which deviates from the</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># transformer original paper that uses post-layer normalization.</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This should help stabilize training</span></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln_1(x))</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.ln_2(x))</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT2(nn.Module):</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: GPTConfig):</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.ModuleDict(</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>            <span class="bu">dict</span>(</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>                wte<span class="op">=</span>nn.Embedding(config.vocab_sz, config.n_embd),</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>                wpe<span class="op">=</span>nn.Embedding(config.block_sz, config.n_embd),</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>                h<span class="op">=</span>nn.ModuleList(</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>                    [Block(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.n_layer)]</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>                ),</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Final layer norm after all transformer layers</span></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>                ln_f<span class="op">=</span>nn.LayerNorm(config.n_embd),</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(config.n_embd, config.vocab_sz, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Weigth sharing between the token embedding layer and</span></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># last linear layer (LM head classifier). The rationale is</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># that tokens that are semantically similar to each other in</span></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the embedding space should have similar probabilities in the</span></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># softmax of the LM head layer</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Also, these matrices are one of the biggest matrices in the the model</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This means, for model like GPT2, we save almost 30 % of the parameters</span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># by sharing the weight matrices (50257 * 768) / 124M = ~31%</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer.wte.weight <span class="op">=</span> <span class="va">self</span>.lm_head.weight</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The following initialization comes from gpt2 src code</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: Becuase token embedding and classifier weights are shared,</span></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out initialization logic will initialize the weight matrix twice</span></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but shouldn't be an issue since they're being initialized with the</span></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># same std and mean</span></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>            std <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We're changing std because residual path affect std</span></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>            <span class="co"># by increasing it on every layer so we need to adjust</span></span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>            <span class="co"># it so we still have the same std = 0.02</span></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(module, <span class="st">"NANOGPT_SCALE_INIT"</span>):</span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>                <span class="co"># `2` here becauase every layer has two blocks:</span></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Attention block and MLP block</span></span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>                std <span class="op">*=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.config.n_layer) <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>            nn.init.normal_(module.weight, std<span class="op">=</span>std)</span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>                nn.init.zeros_(module.bias)</span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We're initializing the token and positional embeddings</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>            <span class="co"># with the same std but the paper initialized the positional</span></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>            <span class="co"># embedding with std = 0.01</span></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>            nn.init.normal_(module.weight, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>        T <span class="op">=</span> x.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> (</span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>            T <span class="op">&lt;=</span> <span class="va">self</span>.config.block_sz</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a>        ), <span class="ss">f"Sequence length must be &lt;= </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>config<span class="sc">.</span>block_sz<span class="sc">}</span><span class="ss">, got </span><span class="sc">{</span>T<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.transformer.wpe(</span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>            torch.arange(<span class="dv">0</span>, T, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>x.device)</span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.transformer.wte(x)</span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> pos_emb <span class="op">+</span> tok_emb</span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> block(x)</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.ln_f(x)</span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>        <span class="co"># logits is B x T x vocab_sz</span></span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>            <span class="co"># F.cross_entropy expects the 2nd dimension to be probabilities</span></span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> F.cross_entropy(</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>                logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.config.vocab_sz), targets.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> configure_optimizer(<span class="va">self</span>, weight_decay, lr, device):</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>        params_dict <span class="op">=</span> {</span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>            pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters() <span class="cf">if</span> p.requires_grad</span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>        decay_params <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> params_dict.values() <span class="cf">if</span> p.ndim <span class="op">&gt;=</span> <span class="dv">2</span>]</span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a>        nondecay_params <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> params_dict.values() <span class="cf">if</span> p.ndim <span class="op">&lt;</span> <span class="dv">2</span>]</span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>        params_groups <span class="op">=</span> [</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"params"</span>: decay_params, <span class="st">"weight_decay"</span>: weight_decay},</span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"params"</span>: nondecay_params, <span class="st">"weight_decay"</span>: <span class="fl">0.0</span>},</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>        fused_available <span class="op">=</span> <span class="st">"fused"</span> <span class="kw">in</span> inspect.signature(opt.AdamW).parameters</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a>        use_fused <span class="op">=</span> fused_available <span class="kw">and</span> <span class="st">"cuda"</span> <span class="kw">in</span> device</span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> opt.AdamW(</span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>            params_groups, lr<span class="op">=</span>lr, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.95</span>), eps<span class="op">=</span><span class="fl">1e-8</span>, fused<span class="op">=</span>use_fused</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span></span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idxs: torch.tensor, max_tokens: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_tokens):</span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>            <span class="co"># x would be B x T x vocab_sz</span></span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a>            idxs <span class="op">=</span> idxs[:, <span class="op">-</span><span class="va">self</span>.config.block_sz :]</span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>            logits, _ <span class="op">=</span> <span class="va">self</span>(idxs)</span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get probs for last token to predict next token</span></span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>            <span class="co"># This would be B x vocab_sz</span></span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply softmax to get probabilities</span></span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Each would be B x 50</span></span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a>            topk_probs, topk_idxs <span class="op">=</span> torch.topk(probs, <span class="dv">50</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a>            <span class="co"># idx is B x 1</span></span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">=</span> torch.multinomial(topk_probs, <span class="dv">1</span>)</span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">=</span> torch.gather(topk_idxs, <span class="op">-</span><span class="dv">1</span>, idx)</span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a>            idxs <span class="op">=</span> torch.cat([idxs, idx], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> idxs</span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DataLoaderLight:</span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a>        batch_sz: <span class="bu">int</span>,</span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a>        block_sz: <span class="bu">int</span>,</span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a>        process_rank: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a>        number_processes: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_sz <span class="op">=</span> batch_sz</span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_sz <span class="op">=</span> block_sz</span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.process_rank <span class="op">=</span> process_rank</span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.number_processes <span class="op">=</span> number_processes</span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"input.txt"</span>, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> f.read()</span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a>        encoder <span class="op">=</span> tiktoken.get_encoding(<span class="st">"gpt2"</span>)</span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokens <span class="op">=</span> torch.tensor(encoder.encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_pos <span class="op">=</span> batch_sz <span class="op">*</span> block_sz <span class="op">*</span> process_rank</span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.tokens) <span class="op">//</span> (<span class="va">self</span>.batch_sz <span class="op">*</span> <span class="va">self</span>.block_sz)</span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> next_batch(<span class="va">self</span>):</span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a>        buf <span class="op">=</span> <span class="va">self</span>.tokens[</span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.current_pos : <span class="va">self</span>.current_pos</span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> <span class="va">self</span>.batch_sz <span class="op">*</span> <span class="va">self</span>.block_sz</span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-225"><a href="#cb3-225" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-226"><a href="#cb3-226" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> buf[:<span class="op">-</span><span class="dv">1</span>].view(<span class="va">self</span>.batch_sz, <span class="va">self</span>.block_sz)</span>
<span id="cb3-227"><a href="#cb3-227" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> buf[<span class="dv">1</span>:].view(<span class="va">self</span>.batch_sz, <span class="va">self</span>.block_sz)</span>
<span id="cb3-228"><a href="#cb3-228" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each process will process batch_sz x block_sz tokens in each</span></span>
<span id="cb3-229"><a href="#cb3-229" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iteration -&gt; with number_processes processes, total tokens processed</span></span>
<span id="cb3-230"><a href="#cb3-230" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in each iteration is batch_sz x block_sz x number_processes. In the</span></span>
<span id="cb3-231"><a href="#cb3-231" aria-hidden="true" tabindex="-1"></a>        <span class="co"># case of one process, total tokens would be batch_sz x block_sz</span></span>
<span id="cb3-232"><a href="#cb3-232" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_pos <span class="op">+=</span> (</span>
<span id="cb3-233"><a href="#cb3-233" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.batch_sz <span class="op">*</span> <span class="va">self</span>.block_sz <span class="op">*</span> <span class="va">self</span>.number_processes</span>
<span id="cb3-234"><a href="#cb3-234" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-235"><a href="#cb3-235" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.current_pos <span class="op">+</span> (</span>
<span id="cb3-236"><a href="#cb3-236" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.batch_sz <span class="op">*</span> <span class="va">self</span>.block_sz <span class="op">*</span> <span class="va">self</span>.number_processes</span>
<span id="cb3-237"><a href="#cb3-237" aria-hidden="true" tabindex="-1"></a>        ) <span class="op">+</span> <span class="va">self</span>.number_processes <span class="op">&gt;</span> <span class="bu">len</span>(<span class="va">self</span>):</span>
<span id="cb3-238"><a href="#cb3-238" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.current_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-239"><a href="#cb3-239" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, y</span>
<span id="cb3-240"><a href="#cb3-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-241"><a href="#cb3-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-242"><a href="#cb3-242" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb3-243"><a href="#cb3-243" aria-hidden="true" tabindex="-1"></a>    <span class="co">###########</span></span>
<span id="cb3-244"><a href="#cb3-244" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Distributed Data Parallel</span></span>
<span id="cb3-245"><a href="#cb3-245" aria-hidden="true" tabindex="-1"></a>    <span class="co">###########</span></span>
<span id="cb3-246"><a href="#cb3-246" aria-hidden="true" tabindex="-1"></a>    <span class="co"># torchrun command sets the following environment variables:</span></span>
<span id="cb3-247"><a href="#cb3-247" aria-hidden="true" tabindex="-1"></a>    <span class="co"># RANK: Id of the process in the process group. It is an int 0-WORLD_SIZE</span></span>
<span id="cb3-248"><a href="#cb3-248" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LOCAL_RANK: In the case of multi-nodes, LOCAL_RANK is the id of</span></span>
<span id="cb3-249"><a href="#cb3-249" aria-hidden="true" tabindex="-1"></a>    <span class="co">#             the process in the same node</span></span>
<span id="cb3-250"><a href="#cb3-250" aria-hidden="true" tabindex="-1"></a>    <span class="co"># WORLD_SIZE: Total number of processes</span></span>
<span id="cb3-251"><a href="#cb3-251" aria-hidden="true" tabindex="-1"></a>    ddp <span class="op">=</span> <span class="bu">int</span>(os.getenv(<span class="st">"RANK"</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span>  <span class="co"># Check if it is a ddp run</span></span>
<span id="cb3-252"><a href="#cb3-252" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ddp:</span>
<span id="cb3-253"><a href="#cb3-253" aria-hidden="true" tabindex="-1"></a>        <span class="co"># DDP requires CUDA so we need to set the device for each process</span></span>
<span id="cb3-254"><a href="#cb3-254" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so only one process can run per device</span></span>
<span id="cb3-255"><a href="#cb3-255" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> torch.cuda.is_available(), <span class="st">"DDP requires CUDA"</span></span>
<span id="cb3-256"><a href="#cb3-256" aria-hidden="true" tabindex="-1"></a>        init_process_group(backend<span class="op">=</span><span class="st">"nccl"</span>)</span>
<span id="cb3-257"><a href="#cb3-257" aria-hidden="true" tabindex="-1"></a>        ddp_rank <span class="op">=</span> <span class="bu">int</span>(os.getenv(<span class="st">"RANK"</span>))</span>
<span id="cb3-258"><a href="#cb3-258" aria-hidden="true" tabindex="-1"></a>        ddp_local_rank <span class="op">=</span> <span class="bu">int</span>(os.getenv(<span class="st">"LOCAL_RANK"</span>))</span>
<span id="cb3-259"><a href="#cb3-259" aria-hidden="true" tabindex="-1"></a>        ddp_world_size <span class="op">=</span> <span class="bu">int</span>(os.getenv(<span class="st">"WORLD_SIZE"</span>))</span>
<span id="cb3-260"><a href="#cb3-260" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="ss">f"cuda:</span><span class="sc">{</span>ddp_local_rank<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-261"><a href="#cb3-261" aria-hidden="true" tabindex="-1"></a>        torch.cuda.set_device(device)</span>
<span id="cb3-262"><a href="#cb3-262" aria-hidden="true" tabindex="-1"></a>        <span class="co"># master process will do more things such as checkpointing and logging</span></span>
<span id="cb3-263"><a href="#cb3-263" aria-hidden="true" tabindex="-1"></a>        <span class="co"># while other processes would assist in the computations</span></span>
<span id="cb3-264"><a href="#cb3-264" aria-hidden="true" tabindex="-1"></a>        master_process <span class="op">=</span> ddp_rank <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb3-265"><a href="#cb3-265" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-266"><a href="#cb3-266" aria-hidden="true" tabindex="-1"></a>        ddp_rank <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-267"><a href="#cb3-267" aria-hidden="true" tabindex="-1"></a>        ddp_local_rank <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-268"><a href="#cb3-268" aria-hidden="true" tabindex="-1"></a>        ddp_world_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-269"><a href="#cb3-269" aria-hidden="true" tabindex="-1"></a>        master_process <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-270"><a href="#cb3-270" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb3-271"><a href="#cb3-271" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> <span class="st">"cuda"</span></span>
<span id="cb3-272"><a href="#cb3-272" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> torch.backends.mps.is_built():</span>
<span id="cb3-273"><a href="#cb3-273" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> <span class="st">"mps"</span></span>
<span id="cb3-274"><a href="#cb3-274" aria-hidden="true" tabindex="-1"></a>            torch.mps.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb3-275"><a href="#cb3-275" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-276"><a href="#cb3-276" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> <span class="st">"cpu"</span></span>
<span id="cb3-277"><a href="#cb3-277" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(device)</span>
<span id="cb3-278"><a href="#cb3-278" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb3-279"><a href="#cb3-279" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb3-280"><a href="#cb3-280" aria-hidden="true" tabindex="-1"></a>        torch.cuda.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb3-281"><a href="#cb3-281" aria-hidden="true" tabindex="-1"></a>    <span class="co">##########</span></span>
<span id="cb3-282"><a href="#cb3-282" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize model and optimizer</span></span>
<span id="cb3-283"><a href="#cb3-283" aria-hidden="true" tabindex="-1"></a>    <span class="co">##########</span></span>
<span id="cb3-284"><a href="#cb3-284" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Everything in GPUs is a power of 2 such as tiling ops</span></span>
<span id="cb3-285"><a href="#cb3-285" aria-hidden="true" tabindex="-1"></a>    <span class="co"># So try to always have matrices be power of 2. Here we</span></span>
<span id="cb3-286"><a href="#cb3-286" aria-hidden="true" tabindex="-1"></a>    <span class="co"># change the vocab_sz by rounding it up to the closest</span></span>
<span id="cb3-287"><a href="#cb3-287" aria-hidden="true" tabindex="-1"></a>    <span class="co"># number that is power of. This will increase space overhead</span></span>
<span id="cb3-288"><a href="#cb3-288" aria-hidden="true" tabindex="-1"></a>    <span class="co"># but would speed up computations</span></span>
<span id="cb3-289"><a href="#cb3-289" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT2(GPTConfig(vocab_sz<span class="op">=</span><span class="dv">50304</span>)).to(device)</span>
<span id="cb3-290"><a href="#cb3-290" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Speed up model by building statis graph that analyzes all ops</span></span>
<span id="cb3-291"><a href="#cb3-291" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and optimizes them such as fusing some of them to avoid unnecessary</span></span>
<span id="cb3-292"><a href="#cb3-292" aria-hidden="true" tabindex="-1"></a>    <span class="co"># trips to memory</span></span>
<span id="cb3-293"><a href="#cb3-293" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model = torch.compile(model)</span></span>
<span id="cb3-294"><a href="#cb3-294" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ddp:</span>
<span id="cb3-295"><a href="#cb3-295" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> DDP(model, device_ids<span class="op">=</span>[ddp_local_rank])</span>
<span id="cb3-296"><a href="#cb3-296" aria-hidden="true" tabindex="-1"></a>    raw_model <span class="op">=</span> model.module <span class="cf">if</span> ddp <span class="cf">else</span> model</span>
<span id="cb3-297"><a href="#cb3-297" aria-hidden="true" tabindex="-1"></a>    max_lr <span class="op">=</span> <span class="fl">3e-4</span></span>
<span id="cb3-298"><a href="#cb3-298" aria-hidden="true" tabindex="-1"></a>    min_lr <span class="op">=</span> max_lr <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb3-299"><a href="#cb3-299" aria-hidden="true" tabindex="-1"></a>    warmup_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-300"><a href="#cb3-300" aria-hidden="true" tabindex="-1"></a>    max_steps <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb3-301"><a href="#cb3-301" aria-hidden="true" tabindex="-1"></a>    sched <span class="op">=</span> combine_scheds(</span>
<span id="cb3-302"><a href="#cb3-302" aria-hidden="true" tabindex="-1"></a>        [warmup_steps <span class="op">/</span> max_steps, <span class="dv">1</span> <span class="op">-</span> (warmup_steps <span class="op">/</span> max_steps)],</span>
<span id="cb3-303"><a href="#cb3-303" aria-hidden="true" tabindex="-1"></a>        [lin_sched(min_lr, max_lr), cos_sched(max_lr, min_lr)],</span>
<span id="cb3-304"><a href="#cb3-304" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-305"><a href="#cb3-305" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Move building of optimizer inside GPT2</span></span>
<span id="cb3-306"><a href="#cb3-306" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Don't decay biases and 1D tensors such as layer norm tensors (scales and</span></span>
<span id="cb3-307"><a href="#cb3-307" aria-hidden="true" tabindex="-1"></a>    <span class="co"># biases) linear layer's tensors</span></span>
<span id="cb3-308"><a href="#cb3-308" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optimizer = opt.AdamW(</span></span>
<span id="cb3-309"><a href="#cb3-309" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     model.parameters(), lr=GPTConfig.lr, betas=(0.9, 0.95), eps=1e-8</span></span>
<span id="cb3-310"><a href="#cb3-310" aria-hidden="true" tabindex="-1"></a>    <span class="co"># )</span></span>
<span id="cb3-311"><a href="#cb3-311" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> raw_model.configure_optimizer(</span>
<span id="cb3-312"><a href="#cb3-312" aria-hidden="true" tabindex="-1"></a>        weight_decay<span class="op">=</span><span class="fl">0.1</span>, lr<span class="op">=</span>max_lr, device<span class="op">=</span>device</span>
<span id="cb3-313"><a href="#cb3-313" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-314"><a href="#cb3-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-315"><a href="#cb3-315" aria-hidden="true" tabindex="-1"></a>    <span class="co">##########</span></span>
<span id="cb3-316"><a href="#cb3-316" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run training loop</span></span>
<span id="cb3-317"><a href="#cb3-317" aria-hidden="true" tabindex="-1"></a>    <span class="co">#########</span></span>
<span id="cb3-318"><a href="#cb3-318" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">NOTE</span><span class="co">: In order to run 0.5M tokens per fwd/bwd iteration, we need to</span></span>
<span id="cb3-319"><a href="#cb3-319" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use gradient accumulation because we can't fit it in almost any commodity</span></span>
<span id="cb3-320"><a href="#cb3-320" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GPY -&gt; We only do backward after we loop through ~0.5M tokens.</span></span>
<span id="cb3-321"><a href="#cb3-321" aria-hidden="true" tabindex="-1"></a>    total_batch_sz <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">19</span>  <span class="co"># closest number to 0.5M</span></span>
<span id="cb3-322"><a href="#cb3-322" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (</span>
<span id="cb3-323"><a href="#cb3-323" aria-hidden="true" tabindex="-1"></a>        total_batch_sz</span>
<span id="cb3-324"><a href="#cb3-324" aria-hidden="true" tabindex="-1"></a>        <span class="op">%</span> (GPTConfig.batch_sz <span class="op">*</span> GPTConfig.block_sz <span class="op">*</span> ddp_world_size)</span>
<span id="cb3-325"><a href="#cb3-325" aria-hidden="true" tabindex="-1"></a>        <span class="op">==</span> <span class="dv">0</span>,</span>
<span id="cb3-326"><a href="#cb3-326" aria-hidden="true" tabindex="-1"></a>        <span class="st">"total batch size must be divisible by micro batch_sz x block_sz x ddp_world_size"</span>,</span>
<span id="cb3-327"><a href="#cb3-327" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-328"><a href="#cb3-328" aria-hidden="true" tabindex="-1"></a>    grad_accum_steps <span class="op">=</span> total_batch_sz <span class="op">//</span> (</span>
<span id="cb3-329"><a href="#cb3-329" aria-hidden="true" tabindex="-1"></a>        GPTConfig.batch_sz <span class="op">*</span> GPTConfig.block_sz <span class="op">*</span> ddp_world_size</span>
<span id="cb3-330"><a href="#cb3-330" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-331"><a href="#cb3-331" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> master_process:</span>
<span id="cb3-332"><a href="#cb3-332" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Total desired batch size: </span><span class="sc">{</span>total_batch_sz<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-333"><a href="#cb3-333" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Calculated gradient accumulation steps: </span><span class="sc">{</span>grad_accum_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-334"><a href="#cb3-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-335"><a href="#cb3-335" aria-hidden="true" tabindex="-1"></a>    train_dl <span class="op">=</span> DataLoaderLight(</span>
<span id="cb3-336"><a href="#cb3-336" aria-hidden="true" tabindex="-1"></a>        batch_sz<span class="op">=</span>GPTConfig.batch_sz, block_sz<span class="op">=</span>GPTConfig.block_sz</span>
<span id="cb3-337"><a href="#cb3-337" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-338"><a href="#cb3-338" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pytorch will use TensorFloat32 if available, else use FP32</span></span>
<span id="cb3-339"><a href="#cb3-339" aria-hidden="true" tabindex="-1"></a>    <span class="co"># But the weights will still be stored as FP32. It is just the</span></span>
<span id="cb3-340"><a href="#cb3-340" aria-hidden="true" tabindex="-1"></a>    <span class="co"># operations would be executed as TF32 if available</span></span>
<span id="cb3-341"><a href="#cb3-341" aria-hidden="true" tabindex="-1"></a>    torch.set_float32_matmul_precision(<span class="st">"high"</span>)</span>
<span id="cb3-342"><a href="#cb3-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-343"><a href="#cb3-343" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb3-344"><a href="#cb3-344" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> time.time()</span>
<span id="cb3-345"><a href="#cb3-345" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> train_dl.next_batch()</span>
<span id="cb3-346"><a href="#cb3-346" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(device)</span>
<span id="cb3-347"><a href="#cb3-347" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.to(device)</span>
<span id="cb3-348"><a href="#cb3-348" aria-hidden="true" tabindex="-1"></a>        <span class="co"># code.interact(local=locals())</span></span>
<span id="cb3-349"><a href="#cb3-349" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb3-350"><a href="#cb3-350" aria-hidden="true" tabindex="-1"></a>        loss_accum <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-351"><a href="#cb3-351" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> macro_step <span class="kw">in</span> <span class="bu">range</span>(grad_accum_steps):</span>
<span id="cb3-352"><a href="#cb3-352" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">==</span> <span class="st">"cuda"</span>:</span>
<span id="cb3-353"><a href="#cb3-353" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Tensors that will be greatly affected by less precission such</span></span>
<span id="cb3-354"><a href="#cb3-354" aria-hidden="true" tabindex="-1"></a>                <span class="co"># loss, layernorm would still be in FP32</span></span>
<span id="cb3-355"><a href="#cb3-355" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span>device, dtype<span class="op">=</span>torch.bfloat16):</span>
<span id="cb3-356"><a href="#cb3-356" aria-hidden="true" tabindex="-1"></a>                    logits, loss <span class="op">=</span> model(x, y)</span>
<span id="cb3-357"><a href="#cb3-357" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb3-358"><a href="#cb3-358" aria-hidden="true" tabindex="-1"></a>                logits, loss <span class="op">=</span> model(x, y)</span>
<span id="cb3-359"><a href="#cb3-359" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Just accumulation gradients yield to summation of objective but</span></span>
<span id="cb3-360"><a href="#cb3-360" aria-hidden="true" tabindex="-1"></a>            <span class="co"># we want mean so we weight each loss by 1/grad_accum_steps</span></span>
<span id="cb3-361"><a href="#cb3-361" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">/=</span> grad_accum_steps</span>
<span id="cb3-362"><a href="#cb3-362" aria-hidden="true" tabindex="-1"></a>            loss_accum <span class="op">+=</span> loss.detach()</span>
<span id="cb3-363"><a href="#cb3-363" aria-hidden="true" tabindex="-1"></a>            <span class="co"># To avoid syncing the gradients between the processes after every</span></span>
<span id="cb3-364"><a href="#cb3-364" aria-hidden="true" tabindex="-1"></a>            <span class="co"># macro step, we disable it and only allows the sync up of</span></span>
<span id="cb3-365"><a href="#cb3-365" aria-hidden="true" tabindex="-1"></a>            <span class="co"># gradients after we finish all gradient accumulation in each</span></span>
<span id="cb3-366"><a href="#cb3-366" aria-hidden="true" tabindex="-1"></a>            <span class="co"># process</span></span>
<span id="cb3-367"><a href="#cb3-367" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ddp:</span>
<span id="cb3-368"><a href="#cb3-368" aria-hidden="true" tabindex="-1"></a>                model.require_backward_grad_sync <span class="op">=</span> (</span>
<span id="cb3-369"><a href="#cb3-369" aria-hidden="true" tabindex="-1"></a>                    macro_step <span class="op">==</span> grad_accum_steps <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-370"><a href="#cb3-370" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb3-371"><a href="#cb3-371" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb3-372"><a href="#cb3-372" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each process would have its own loss_accum tensor, so to get the</span></span>
<span id="cb3-373"><a href="#cb3-373" aria-hidden="true" tabindex="-1"></a>        <span class="co"># average loss_accum across all processes, we to compute the average of</span></span>
<span id="cb3-374"><a href="#cb3-374" aria-hidden="true" tabindex="-1"></a>        <span class="co"># all loss_accum in all processes</span></span>
<span id="cb3-375"><a href="#cb3-375" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ddp:</span>
<span id="cb3-376"><a href="#cb3-376" aria-hidden="true" tabindex="-1"></a>            dist.all_reduce(loss_accum, op<span class="op">=</span>dist.ReduceOp.AVG)</span>
<span id="cb3-377"><a href="#cb3-377" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Clips gradient to global norm. It is very useful to avoid having a</span></span>
<span id="cb3-378"><a href="#cb3-378" aria-hidden="true" tabindex="-1"></a>        <span class="co"># very high loss for some batch(es) that would have very high loss</span></span>
<span id="cb3-379"><a href="#cb3-379" aria-hidden="true" tabindex="-1"></a>        <span class="co"># which would learn to high gradients and huge updates</span></span>
<span id="cb3-380"><a href="#cb3-380" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In the beginning of training it is normal to have high norms as the</span></span>
<span id="cb3-381"><a href="#cb3-381" aria-hidden="true" tabindex="-1"></a>        <span class="co"># model initialized randomly</span></span>
<span id="cb3-382"><a href="#cb3-382" aria-hidden="true" tabindex="-1"></a>        norm <span class="op">=</span> nn.utils.clip_grad_norm_(model.parameters(), <span class="fl">1.0</span>)</span>
<span id="cb3-383"><a href="#cb3-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-384"><a href="#cb3-384" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Use ParamScheduler from `cmn_ai`</span></span>
<span id="cb3-385"><a href="#cb3-385" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> sched(step <span class="op">/</span> max_steps)</span>
<span id="cb3-386"><a href="#cb3-386" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> pg <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb3-387"><a href="#cb3-387" aria-hidden="true" tabindex="-1"></a>            pg[<span class="st">"lr"</span>] <span class="op">=</span> lr</span>
<span id="cb3-388"><a href="#cb3-388" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb3-389"><a href="#cb3-389" aria-hidden="true" tabindex="-1"></a>        end <span class="op">=</span> time.time()</span>
<span id="cb3-390"><a href="#cb3-390" aria-hidden="true" tabindex="-1"></a>        elapsed_time <span class="op">=</span> end <span class="op">-</span> start</span>
<span id="cb3-391"><a href="#cb3-391" aria-hidden="true" tabindex="-1"></a>        token_per_sec <span class="op">=</span> (</span>
<span id="cb3-392"><a href="#cb3-392" aria-hidden="true" tabindex="-1"></a>            GPTConfig.batch_sz</span>
<span id="cb3-393"><a href="#cb3-393" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> GPTConfig.block_sz</span>
<span id="cb3-394"><a href="#cb3-394" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> grad_accum_steps</span>
<span id="cb3-395"><a href="#cb3-395" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span> ddp_world_size</span>
<span id="cb3-396"><a href="#cb3-396" aria-hidden="true" tabindex="-1"></a>        ) <span class="op">/</span> (elapsed_time)</span>
<span id="cb3-397"><a href="#cb3-397" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb3-398"><a href="#cb3-398" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, lr </span><span class="sc">{</span>lr<span class="sc">:.4e}</span><span class="ss">, norm: </span><span class="sc">{</span>norm<span class="sc">:.2f}</span><span class="ss">, time: </span><span class="sc">{</span>elapsed_time<span class="sc">:.2f}</span><span class="ss">s, tok/sec: </span><span class="sc">{</span>token_per_sec<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb3-399"><a href="#cb3-399" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-400"><a href="#cb3-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-401"><a href="#cb3-401" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ddp:</span>
<span id="cb3-402"><a href="#cb3-402" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Kills all processes</span></span>
<span id="cb3-403"><a href="#cb3-403" aria-hidden="true" tabindex="-1"></a>        destroy_process_group()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>TODO</p>
</section>
<section id="resources" class="level1">
<h1>Resources</h1>
<ul>
<li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT2: Language Models are Unsupervised Multitask Learners</a></li>
<li><a href="http://arxiv.org/abs/2005.14165">GPT3: Language Models are Few-Shot Learners</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/imaddabbura\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="imaddabbura/imaddabbura.github.io" data-repo-id="R_kgDOIEwRMg" data-category="General" data-category-id="DIC_kwDOIEwRMs4CRprP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Blog made with Quarto, by Imad Dabbura</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/imaddabbura/imaddabbura.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/imaddabbura/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:imad.dabbura@hotmail.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/imaddabbura">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/imadphd">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>