---
title: "Designing ML Systems"
date: "2023-12-12"
categories: MLOps
---

![](images/mlops.png){.preview-image}

Below are notes I gathered from my readings in MLOps as well as deploying ML systems at different companies.

- During training, we care more about throughput. However, once we deploy the model, we care more about latency.
    - Increased latency in production leads to reduction in customer satisfaction and conversion rates, which are more important than a relatively more accurate predictions.
    - It is common to focus on the high percentiles of the latency such as 90th/95th/99th or even 99.9th percentiles.
- Map ML metrics to business metrics and see how improvement in ML model would lead to an improvement in business metrics.
    - In some cases, ML is just a small part of the overall process that makes it hard to attribute loss/revenue to a particular component in the process.
- Requirements of ML systems:
  - Reliability: ML systems performs correct function in the case of failures. How do we check if predictions are not wrong?
  - Scalability: ML system can grow in:
      - Model complexity 
      - Number of models
      - Number of requests served by each ML model
  - Maintainability: Code should be documented. Code, data, and artifacts should be versioned. Experiments and models should be able to be reproduced
  - Adaptability: ML system should adapt quickly to change in data distribution and business requirements
- Developing an ML system is an iterative process that we typically go back and forth between different steps such as: scoping project, data engineering, model development, model deployment, monitoring and retraining, business analysis, etc.
- For multiclass classification problems with a lot of classes, it may be helpful to frame it as a hierarchical classification where each example is first classified into few major classes then another classifier is used to classify subclasses and so on.
- For mutlilabel classification problems, we either build a binary classifier for each label or use one model for all labels
    - It is more challenging to build one model with multilabel because now we need to figure out how to get predictions out of raw probabilities
- If we have multiple objectives, it is better to decouple the objectives and train different model for each objective. Finally, get the final score of each prediction by combining the output from each model using different weight for each objective (which can be tuned based on business priorities). This way we can change one model without the need to retrain all the models.
- Data-centric approach to ML model development tends to lead to the best results as compared to algorithm-centric approach. Data is the critical piece in obtaining good performance if we have decent architecture.
- Types of data:
    - first-party data: Data collected by companies about their customers
    - Second-party data: Data collected by another companies about their customers
    - Third-party data: Data collected on the public on who aren't their customers
    - User data are the most messy and require heavy cleanups
- Data passing modes through:
    - Databases. Not recommended for applications with strong latency requirements
    - Services using requests such as REST or RPC. Recommended for applications that rely more on logic than data. It is called `request-driven`/microservice architecture, which is the most common
    - Real-time transport such as Kafka. Recommended for applications that are data-heavy. It is called `event-driven` architecture.
- Batch features (also called static features) are features that aren't expected to change frequently, which are computed using batch processing.
- Streaming features (also called dynamic features) are features that changes quickly, which are computed using stream processing.
- Sampling Methods:
    - Nonprobability sampling such as selecting the most recent data
    - Random sampling
    - Stratified random sampling
    - Weighted sampling
    - Reservoir sampling
    - Importance sampling
- Extracting labels from feedback: User feedback goes through different stages where each stage has different volume, strength of signal, and feedback loop length
- There is a trade-off between accuracy and speed of the feedback loop window. The shorter the feedback loop window the less accurate the labels. But also the longer the feedback loop window the longer to notice and fix model problems.
- Data labeling:
  - Weak supervision: Have no labeled data and use labeling functions to label data (Snorkel)
  - Semi supervision: Have limited labeled data that are used to generate more labels through different methods such as perturbation
  - Transfer learning
  - Active learning: Selectively pick the samples to train the model on such as the most confident wrong samples
- Class imbalance: There are different degrees of class imbalance.
    - It affects learning algorithms in many ways:
        - Algorithm didn't have sufficient signals from rare classes
        - Algorithm may use simple heuristics to always output majority class to get best metric
        - Rare classes have typically asymmetric costs of errors such as if the X-ray is cancerous. Therefore, we might need to build a model that is more accurate on the rare classes and less accurate on the majority class(es)
    - The more complex the problem -> the more sensitive the algorithm(s) to class imbalance. If classes are linearly separable, class imbalance has no effect.
    - Solutions:
        - Data resampling methods:
            - Undersampling
            - Oversampling
            - SMOTE: oversampling technique
            - Tomek links: undersampling technique where two similar samples from opposite classes are chosen and the one from majority class is dropped
            - Dynamic sampling: Undersample majority class to all classes have the same number of samples and train the model on the resampled data. Then fine tune the model on the original imbalanced data
        - Algorithm methods:
            - Define cost sensitive matrix that will be used in the loss function
            - Class-balanced loss where each sample has weight of misclassification that is inversely proportional to the number of samples it belongs to
            - Focal loss: Focus on learning the samples that model has difficulty in classifying; i.e. has the low probability of being right
            - Ensembles sometimes help with class imbalance
- Data augmentation
    - Label-preserving transformations such as randomly flipping images or replace word with its synonym in NLP tasks
    - Perturbation that adds noise to the data but still preserve the label. BERT uses such technique where 15% of the tokens are chosen randomly and 10% of such chosen tokens will be replaced by random tokens.
    - Data synthesis: create data from existing data such as mixup or using templates in NLP
- Feature engineering. Having the right features give us the biggest performance boost compared to algorithmic techniques and hyperparameter-tuning. It requires domain-specific knowledge, SME, and algorithm-specific knowledge.
    - Missing values. Be careful that not all missing values are `NaN`. Some systems use predefined values such as empty string or 1999 for missing values.
        - Reasons for missing values:
            - Missing not at random: When the value is missing due to related observed value such as some type of customers don't disclose their age.
            - Missing at random: When the value is missing due to the value itself such as when people with high income mostly decide to not disclose their income.
            - Missing completely at random: When there is no pattern for missing values such as customer forgot to insert the value. This is very rare in practice.
        - Handling missing values:
            - Deletion:
                - Columns deletion. Not recommended for almost all cases.
                - Row deletion. 
                    - It might be okay if the % of rows with missing values are very small especially for large datasets and it is missing completely at random
                    - It is very risking to delete such rows if the missing are either not at random or at random
            - Imputation. The most common is median/mode imputation. Make sure that if the filled value is constant to not be a possible value for the feature such as `-999`
            - It is almost always a good idea to add an indicator for each feature that indicates whether a value is missing
    - Scaling. Most classical ML algorithms such as logistic regression and gradient-boosted trees as well as NN require that features are normalized.
        - We can normalize to have range [0, 1] or [-1, 1] if we don't want to make any assumption on the distribution of the data. This would change the distribution of the data.
        - Standardization. This would preserve the distribution. Be careful that the statistics used from training data may be out of date when used during inference if the distribution of the feature changes dramatically. Therefore, we need to train the model frequently to update such statistics.
        - Other transformation: `log(1 + x)` or `sqrt(x)`
    - Discretization/Binning. Convert the feature into small buckets (bins) which will make the feature categorical. It is useful if values within a bucket is not that different in terms of the feature itself or the behavior of the customer. It is rarely proven to be useful. It is a challenge to pick the number of bins or the bin boundaries. Quantiles are commonly used for bins boundaries.
    - Encoding categorical features. Some features have fixed categories such as marital status while others have categories that change all the time such as product names.
        - It is risky to encode infrequent/unknown categories to one category.
        - Hashing is pretty good especially if the hashing space is big enough where collision rate is low. We don't have to worry about unknown categories in production.
    - Feature crossing. It helps learn nonlinear relationships between features especially for models that aren't good at learning nonlinear relationships such as logistic regression and tree-based models. The caveat is that it makes the feature space of the new feature blow up and requires more data to learn all these possible values. For example, if two features have 10 unique categories each, then the new feature would possible have 10 x 10 = 100 possible values.
- Data leakage. It refers to the case where some form of the label leaks into the feature(s) that aren't available during inference. Common causes for data leakage:
  - Splitting time-correlated data randomly. This is due to the fact that trends are time-correlated and the model would have access to future things it wouldn't have otherwise.
  - Scaling before splitting
  - Fill missing values before splitting
  - Poor handling data duplication before splitting. Check for duplicates or near-duplicates before/after splitting
  - Leakage from data generation process. Need to check with SME because this is the hardest cause of data leakage to uncover because it requires detailed knowledge about the process.
  - Group leakage: group of data have correlated labels but are spread into different splits such as images of the same person are spread into training and test
- Feature importance is very important to avoid useless features as well as checking for data leakage. It is also helpful to interpret the model.
    - Be careful of features that have very high importance
    - It is better to remove features if they are  useless even if the model can ignore them
- Feature generalization: 
    - Features coverage (% of missing values). Check whether the coverage is similar between splits. Also, if the coverage is low, check if it adds value to the model; otherwise, delete it.
    - Distribution of values between data splits. Aim for 100% overlaps of values for all features between the splits.
        - Build a model to predict whether a row is in train or valid using the features used to build the main model. If the new model is good enough, check the most important features because those will be the features that have different distributions between train and valid splits.
        - There is a trade-off between generalization and specifity. IS_RUSH_HOUR is more generalizable than HOUR_OF_DAY.
- Model development:
    - Baslines:
        - Random
        - Most common
        - Simple heuristics
        - Random Forest is also a good baseline
    - Evaluation methods:
        - Perturbation tests. Ideally, we don't want the output of the model to change much if inputs are close to each other. We can add random noise to the test data to see how the model behaves and check its sensitivity.
        - Directional expectation tests. Check if changes in some features would lead to changes in the output in the expected direction; otherwise, the model might be learning the wrong thing.
        - Model calibration. Unless the model is explicitly trained using `LogLoss`, the probabilities from the model won't be calibrated. We need to calibrate the model so that the probability produced by the model should match the underlying probability distribution of the data.
        - Confidence level measurement. We can avoid showing uncertain predictions and maybe involve human in the loop or ask the user for more info. For example, predict positive class if probability >= 80% and negative class if probability <= 20%.
        - Slice-based evaluation. Evaluate the model on different slices of data to either make sure the model's performance across different slices are acceptable OR if we care about critical slices we can confirm our requirements. This also gives us indication of how to improve the model. We can discover slices by:
            - Simple heuristics that come from domain knowledge and heavy EDA of the data
            - Error analysis
            - Slice finder algorithms
- Model Deployment:
    - Online prediction: Predictions generated on demand using either streaming features OR both streaming and batch features.
        - Pros: Always adapt to new behaviors/trends. Also don't have to compute predictions not needed and waste compute/storage
        - Cons: Constrained by latency 
    - Batch prediction: Predictions generated periodically using batch features.
        - Pros: Not constrained by latency and utilize vectorization/parallel computations and allows to use very complex models that takes time to generate predictions
        - Cons: Model is not responsive to changes in customers behaviors/trends
    - Model compression:
        - Knowledge distillation
        - Pruning
        - Quantization
    - Cloud vs Edge:
        - Cloud is much easier to setup and very flexible at the cost of privacy and network latency.
        - Edge avoid network latence and privacy concerns but device must have enough memory, compute, and battery. Also, It takes more engineering work and model compression to make the model run on devices with a reasonable latency.
