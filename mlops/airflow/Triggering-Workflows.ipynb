{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a8df91-0b76-4abe-a256-ca7df2fd17dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Airflow Part 7 - Triggering Workflows\"\n",
    "date: \"2022-03-14\"\n",
    "image: feature.png\n",
    "categories: [\"Data Engineering\", \"MLOps\", \"Airflow\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48243cb5-f66b-4d0c-9dc3-fb567d6cc643",
   "metadata": {},
   "source": [
    "![](feature.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fa8c7-b38b-4c2b-9b8f-3269465ebe9f",
   "metadata": {},
   "source": [
    "- Workflows are most commonly triggered based on schedule intervals provided using `start_date`, `end_date` , `schedule_interval`. Airflow would calculate when the next schedule would be and start the first task(s) to run at the next data/time.\n",
    "- However, sometimes we want the workflow to run based on the occurance of external events such as a file is available in specific location OR code is changed on git repo etc.\n",
    "- One way to execute workflows based on the occurance of external exents is using Airflow's **sensors**. Sensor is a subclass of operators that checks if certain condition is true. If true, execute the step (workflow). If false, wait for a given period (default 60 seconds) and tries again. It keeps doing so for *timeout* period. This is a form of **Poking**, which is checking for the existence of file in the case of FileSensor.\n",
    "\n",
    "```Python\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "wait_for_file_1 = FileSensor(\n",
    "    task_id=\"wait_for_file_1\", filepath=\"/data/file_1.csv\"\n",
    "    )\n",
    "```\n",
    "- We can also use **globbing** with FileSensors by using wildcards to check for the existence of file(s)\n",
    "- We can also use PythonSensor which checks for certain condition and must return a Boolean. It is more flexible and easier to read than using globbing within FileSensor. It is the same as PythonOperator in terms of taking a Python callable\n",
    "\n",
    "```Python\n",
    "from pathlib import Path\n",
    "from airflow.sensors.python import PythonSensor\n",
    "\n",
    "# Check whether there is any data for a given supermarker\n",
    "# and there is _SUCCESS path which indicates whether the \n",
    "# data for the given supermarket is all uploaded\n",
    "def _wait_for_supermarket(supermarket):\n",
    "    supermarket_path = Path(\"/data\") / supermarket\n",
    "    success_path = Path(\"/data\") / \"_SUCCESS\"\n",
    "    data_files = supermarketpath.glob(\"*.csv\")\n",
    "    return data_files and success_path.exists()\n",
    "\n",
    "wait_for_supermarket_1 = PythonSensor(\n",
    "    task_id=\"wait_for_supermarket_1\",\n",
    "    python_callable=_wait_for_supermarket,\n",
    "    op_kwargs={\"supermarket\": \"supermarket_1\"},\n",
    "    dag=dag\n",
    "    )\n",
    "```\n",
    "![](images/python-sensor.png)\n",
    "\n",
    "- All sensors take a `timeout` arguments, which has default value of 7 days\n",
    "- There is also a limit on the number of tasks Airflow can run concurrently per DAG (default is 16). DAG takes `concurrency` argument that can change this number. There is also a limit on the number of tasks per global Airflow and the number DAG runs per DAG\n",
    "```Python\n",
    "wait_for_supermarket_1 = PythonSensor(\n",
    "    task_id=\"wait_for_supermarket_1\",\n",
    "    python_callable=_wait_for_supermarket,\n",
    "    op_kwargs={\"supermarket\": \"supermarket_1\"},\n",
    "    concurreny=20, # Default is 16\n",
    "    dag=dag\n",
    "    )\n",
    "```\n",
    "- There is snowball effect when sensors don't succeed. The occupy slots that DAG has (which is determined by the concurrency argument. From the above figure, if only task 1 succeeds and the rest keeps polling and the DAG is scheduled daily with default concurrency of 16 slots and default timeout of 7 days, this is what will happen (sensor deadlock):\n",
    "    - Day 1: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 3 tasks.\n",
    "    - Day 2: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 6 tasks.\n",
    "    - Day 3: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 9 tasks.\n",
    "    - Day 4: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 12 tasks.\n",
    "    - Day 5: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 15 tasks.\n",
    "    - Day 6: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 16 tasks; two new tasks cannot run, and any other task trying to run is blocked.\n",
    "\n",
    "![](images/python-sensor-deadlock.png)\n",
    "\n",
    "- This also affect the global Airflow limit of maximum number of tasks that can run concurrently, which may lead to whole system get stalled. \n",
    "- For sensor task, it pokes to check the condition and block if it is false. So it would run for a little bit and wait for the most part. It keeps poking untel the timeout period is completed, which means it keeps occupying the slot until the condition becomes true or timeout is reached\n",
    "- `mode` argument which has two values: {`poking`, `reschedule`}. The default is poking. Reschedule can solve the sensor deadlock and snowball effect because it releases the slot the sensor task is occupying after the slot has finished poking. In other words, sensor task would poke, if condition if false, the system will reschedule it and take its slot and make it available to other tasks. It is the same concept as **process scheduling** that the OS does when a process does a blocking system call.\n",
    "\n",
    "```Python\n",
    "wait_for_supermarket_1 = PythonSensor(\n",
    "    task_id=\"wait_for_supermarket_1\",\n",
    "    python_callable=_wait_for_supermarket,\n",
    "    op_kwargs={\"supermarket\": \"supermarket_1\"},\n",
    "    mode=\"reschedule\",\n",
    "    dag=dag\n",
    "    )\n",
    "```\n",
    "- We can trigger another DAG to run from inside another DAG using `TriggerDagRunOperator`. This will cause another DAG to run once the trigger_operator runs which is useful if we want to split DAGs and make some DAGs available to other DAGs instead of repearing functionality. See below for both approaches:\n",
    "![](images/complicated-dag-logic.png)\n",
    "![](images/triggered-dag.png)\n",
    "\n",
    "```Python\n",
    "from pathlib import Path\n",
    "\n",
    "import airflow.utils.dates\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
    "from airflow.sensors.python import PythonSensor\n",
    "\n",
    "dag1 = DAG(\n",
    "    dag_id=\"ingest_supermarket_data\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=\"0 16 * * *\",\n",
    ")\n",
    "dag2 = DAG(\n",
    "    dag_id=\"create_metrics\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=None, # Since it will be triggered\n",
    ")\n",
    "\n",
    "\n",
    "def _wait_for_supermarket(supermarket_id_):\n",
    "    supermarket_path = Path(\"/data/\" + supermarket_id_)\n",
    "    data_files = supermarket_path.glob(\"data-*.csv\")\n",
    "    success_file = supermarket_path / \"_SUCCESS\"\n",
    "    return data_files and success_file.exists()\n",
    "\n",
    "\n",
    "for supermarket_id in range(1, 5):\n",
    "    wait = PythonSensor(\n",
    "        task_id=f\"wait_for_supermarket_{supermarket_id}\",\n",
    "        python_callable=_wait_for_supermarket,\n",
    "        op_kwargs={\"supermarket_id_\": f\"supermarket{supermarket_id}\"},\n",
    "        dag=dag1,\n",
    "    )\n",
    "    copy = DummyOperator(task_id=f\"copy_to_raw_supermarket_{supermarket_id}\", dag=dag1)\n",
    "    process = DummyOperator(task_id=f\"process_supermarket_{supermarket_id}\", dag=dag1)\n",
    "    trigger_create_metrics_dag = TriggerDagRunOperator(\n",
    "        task_id=f\"trigger_create_metrics_dag_supermarket_{supermarket_id}\",\n",
    "        trigger_dag_id=\"create_metrics\", # Has to be the same dag_id as dag2\n",
    "        dag=dag1,\n",
    "    )\n",
    "    wait >> copy >> process >> trigger_create_metrics_dag\n",
    "\n",
    "compute_differences = DummyOperator(task_id=\"compute_differences\", dag=dag2)\n",
    "update_dashboard = DummyOperator(task_id=\"update_dashboard\", dag=dag2)\n",
    "notify_new_data = DummyOperator(task_id=\"notify_new_data\", dag=dag2)\n",
    "compute_differences >> update_dashboard\n",
    "\n",
    "```\n",
    "- Each DAG run has a run_id that starts with one of the following:\n",
    "    - `scheduled__` to indicate the DAG run started because of its schedule\n",
    "    - `backfill__` to indicate the DAG run started by a backfill job\n",
    "    - `manual__` to indicate the DAG run started by a manual action (e.g., pressing the Trigger Dag button, or triggered by a TriggerDagRunOperator)\n",
    "- From the UI, scheduled DAGs have their task instance in black border while Triggered DAGs don't\n",
    "- Clearing a task in a DAG will clear the task and all its downstream tasks and trigger a run (backfill)\n",
    "    - It only clears tasks within the same DAG, NOT downstream tasks in another DAG of TriggerDagRunOperator\n",
    "- If the triggered DAG has dependency on multiple triggering DAGs to be completed before it can run, then we can use `ExternalTaskSensor` that checks whether the task has been completed successfully (sensor poking the state of tasks in another DAGs). Each `ExternalTaskSensor` checks for only 1 task by querying the metastore database\n",
    "    - By default, it uses the same execution_date as itself\n",
    "    - If the task runs on different schedule, we then need to provide timedelta object to `execution_delta` argument to get what would be the execution_date of the task it tries to sense\n",
    "\n",
    "```Python\n",
    "import datetime\n",
    "\n",
    "import airflow.utils.dates\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "\n",
    "dag1 = DAG(\n",
    "    dag_id=\"ingest_supermarket_data\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=\"0 16 * * *\",\n",
    ")\n",
    "dag2 = DAG(\n",
    "    dag_id=\"create_metrics\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=\"0 18 * * *\",\n",
    ")\n",
    "\n",
    "DummyOperator(task_id=\"copy_to_raw\", dag=dag1) >> DummyOperator(\n",
    "    task_id=\"process_supermarket\", dag=dag1\n",
    ")\n",
    "\n",
    "wait = ExternalTaskSensor(\n",
    "    task_id=\"wait_for_process_supermarket\",\n",
    "    external_dag_id=\"figure_6_20_dag_1\",\n",
    "    external_task_id=\"process_supermarket\",\n",
    "    # positive # will be subtracted from the execution_date of task sensor\n",
    "    # to get the execution_date of the task it is trying to sense\n",
    "    execution_delta=datetime.timedelta(hours=6),  \n",
    "    dag=dag2,\n",
    ")\n",
    "report = DummyOperator(task_id=\"report\", dag=dag2)\n",
    "wait >> report\n",
    "```\n",
    "- We can also trigger DAGs from CLI which will have execution_date of the current data and time\n",
    "    - `airflow dags trigger dag1`\n",
    "    - With configuration; which will be available in the context of each task using context[\"dag_run\"].conf:\n",
    "        - `airflow dags trigger -c '{\"supermarket_id\": 1}' dag1`\n",
    "        - `airflow dags trigger --conf '{\"supermarket_id\": 1}' dag1`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
