{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a8df91-0b76-4abe-a256-ca7df2fd17dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Airflow Part 5 - Dependencies Between Tasks\"\n",
    "date: \"2022-02-14\"\n",
    "image: feature.png\n",
    "categories: [\"Data Engineering\", \"MLOps\", \"Airflow\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48243cb5-f66b-4d0c-9dc3-fb567d6cc643",
   "metadata": {},
   "source": [
    "![](feature.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcc752-0125-4d8a-9acb-8ded313c970e",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Other than my experience and the documentation, the main resource behind this post and figures is the fantastic book: [Data Pipelines with Apache. Airflow](https://www.manning.com/books/data-pipelines-with-apache-airflow).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d2fc7-98b3-4b1d-a734-9a538e8615f2",
   "metadata": {},
   "source": [
    "- Dependencies in airflow is specified using the right shift symbol `>>`. It tells Airflow which tasks should be run first before running other tasks.\n",
    "- Basic Dependenices:\n",
    "    1. **Linear dependenies**: `a >> b >> c`. This means that `a` has to run before `b` which should run before `c`. If any task fails, the downstream task won't run and the errors are propagated to them from preceding tasks. They can only run after the errors are fixed for that interval.\n",
    "    2. **Fan-in/Fan-out dependencies**:\n",
    "    <img src=\"images/fan-in-out.png\">\n",
    "        - Fan-in: When 1 task is dependent on >= 2 tasks to run. *join_datasets* is fan-in task. Fan-in tasks can be specified as: `[clean_sales, clean_weather] >> join_datasets`\n",
    "        - Fan-out: When >= 2 tasks are dependent on 1 task to run. *start* is a fan-out task. Fan-out tasks can be specified as: `start >> [fetch_sales, fetch_weather]`\n",
    "        - This is how we can specify dependencies for the DAG in the above picture:\n",
    "        ```python\n",
    "        start >> [fetch_sales, fetch_weather]\n",
    "        fetch_sales >> clean_sales\n",
    "        fetch_weather >> clean_weather\n",
    "        [clean_sales, clean_weather] >> join_datasets\n",
    "        join_datasets >> train_model\n",
    "        train_model >> deploy_model\n",
    "        ```\n",
    "- Branching: \n",
    "    1. We can take care of conditional execution of code paths inside the task, i.e. inside Python script in the case of PythonOperator. Depending on some condition during execution, different code paths and logic will be followed. The main disadvantages of this approach is that 1) it is hard to figure out with code path is being executed on each run from tree/graph view unless we have logging enabled, 2) Adds more complexity to the code structure, 3) May not let us use specialized operators that abstract aways a lot of the boilerplate code such as PostgresOperator. For example, if we have fetch data from either CSV or SQL database depending on condition at execution.\n",
    "    2. We can add `BrachPythonOperator`task that takes a Python callable to determine which tasks to execute next. The Python callable has to return the task_id of the task (or list of task_id) that Airflow should execute next. Example: \n",
    "    ![](images/branching-v1.png)\n",
    "    ```python\n",
    "    def _pick_erp_system(**context):\n",
    "        if context[\"execution_date\"] < ERP_SWITCH_DATE:\n",
    "           return \"fetch_sales_old\"\n",
    "        else:\n",
    "           return \"fetch_sales_new\"\n",
    "\n",
    "    pick_erp_system = BranchPythonOperator(\n",
    "        task_id=\"pick_erp_system\",\n",
    "        python_callable=_pick_erp_system,\n",
    "        )\n",
    "\n",
    "    start >> [pick_erp_system, fetch_weather]\n",
    "    pick_erp_system >> [fetch_sales_old, fetch_sales_new]\n",
    "    fetch_sales_old >> clean_sales_old\n",
    "    fetch_sales_new >> clean_sales_new\n",
    "    fetch_weather >> clean_weather\n",
    "    [clean_sales_old, clean_sales_new, clean_weather] >> join_datasets\n",
    "    join_datasets >> train_model\n",
    "    train_model >> deploy_model\n",
    "    ```\n",
    "    - Since downstream tasks only get scheduled & executed if all thier downstream tasks finished successfully, `jon_datasets` task will never success because with the above dependency either `clean_sales_old` or `clean_sales_new` would execute BUT NOT BOTH. We can adjust this using `trigger_rule` argument (default is `\"all_success\"` in the operatror by specifying `\"non_failed\"`. This will run downstream task if all downstream tasks haven't failed even if they never executed. Therefore, we can change trigger_rule for `join_datasest` task.\n",
    "    - A better approach is to create DummyOperator that does nothing but join both branches and become the upstream task before `join_datasets` such as below: \n",
    "    ![](images/branching-v2.png)\n",
    "    ```python\n",
    "    join_branch = DummyOperator(\n",
    "       task_id=\"join_erp_branch\",\n",
    "       trigger_rule=\"none_failed\"\n",
    "        )\n",
    "\n",
    "    start >> [pick_erp_system, fetch_weather]\n",
    "    pick_erp_system >> [fetch_sales_old, fetch_sales_new]\n",
    "    fetch_sales_old >> clean_sales_old\n",
    "    fetch_sales_new >> clean_sales_new\n",
    "    [clean_sales_old, clean_sales_new] >> join_branch\n",
    "    fetch_weather >> clean_weather\n",
    "    [joen_erp_branch, clean_weather] >> join_datasets\n",
    "    join_datasets >> train_model\n",
    "    train_model >> deploy_model\n",
    "    ```\n",
    "- **Conditional tasks**. Sometimes we only want to execute a task if a condition is true, otherwise, the task should be skipped. For example, if we want to only deploy the model on the most recent data and we don't want `deploy_model` to always execute if we are doing **backfilling** -> Create a conditional upstream task that checks the condition and raise Exception if the condition is False so `deploy_model` will be skipped. \n",
    "![](images/conditional-task.png)\n",
    "```python\n",
    "from airflow.exceptions import AirflowSkipException\n",
    "from airflow.operators.python import PythonOperator\n",
    "def _latest_only(**context):\n",
    "    # execution_time is the first time in the schedule interval\n",
    "    # So following_schedule is the next execution_date\n",
    "    left_window = context[\"dag\"].following_schedule(context[\"execution_date\"])\n",
    "    right_window = context[\"dag\"].following_schedule(left_window)\n",
    "    now = pendulum.now(\"utc\")\n",
    "    # Since execution of DAG starts after last time point passed of the \n",
    "    # schedule interval -> \n",
    "    if not left_window < now <= right_window:\n",
    "        raise AirflowSkipException(\"Not the most recent run!\")\n",
    "\n",
    "latest_only = PythonOperator(task_id=\"latest_only\", python_callable=_latest_only, dag=dag)\n",
    "latest_only >> deplpy_model\n",
    "```\n",
    "- **Trigger rules**: The triggering of Airflow tasks is controlled by the trigger rules which define the behavior of tasks and allow us to configure each task to respond to different situations.\n",
    "    1. Be default, scheduler picks tasks ready to be executed when all its upstreams tasks were executed successfully and put it in the execute queue. The scheduler always checks downstream tasks if they are ready by checking all their downstream task completion state. Once there is a slot/worker, it will be executed. If any of the upstream tasks failed, it would have failed state and the upstream task won't be scheduled and have **state=upstream_failed**. This is called **progagation** because the error is propagated from upstream to downstream tasks. This is the default trigger_rule which is **all_success**. If any of the down\n",
    "    2. If any of the upstream task is skipped -> downstream task will be skipped as well (propagation).\n",
    "    3. Trigger rules:\n",
    "        - `all_success`: Triggers when all parent tasks have executed successfully\n",
    "        - `all_failed`: Triggers when all parent tasks have failed or due to failure in their parents\n",
    "        - `all_done`: Triggers when all parent tasks finished executing regardless of their state. Good to cleanup and shutdown resources regardless of the execution state of the workflow\n",
    "        - `one_failed`: Triggers when at least 1 parent task failed and doesn't wait for other parent tasks to finish\n",
    "        - `one_success`: Triggers when at least 1 parent task succeeded and doesn't wait for other parent tasks to finish\n",
    "        - `none_failed`: Triggers if no parent task has failed but either completed successfully or skipped\n",
    "        - `none_skipped`: Triggers if no parent task has skipped but either completed successfully or failed\n",
    "        - `dummy`: Triggers regardless of the parent tasks state. Useful for testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
