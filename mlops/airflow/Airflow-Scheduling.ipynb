{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a8df91-0b76-4abe-a256-ca7df2fd17dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: Airflow Part 3 - DAG Scheduling\n",
    "date: \"2022-01-24\"\n",
    "image: feature.png\n",
    "categories: [\"Data Engineering\", \"MLOps\", \"Airflow\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48243cb5-f66b-4d0c-9dc3-fb567d6cc643",
   "metadata": {},
   "source": [
    "![](feature.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e6e43-c8e3-4faa-a3a0-085f92d2ccc7",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Other than my experience and the documentation, the main resource behind this post and figures is the fantastic book: [Data Pipelines with Apache. Airflow](https://www.manning.com/books/data-pipelines-with-apache-airflow).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dded1b-3f72-4276-9f82-96f0119f7fcb",
   "metadata": {},
   "source": [
    "(@) Airflow will schedule the first execution of DAG at the end of the interval; which means after the last time point in the interval has passed. For example, if we schedule it to run `@daily`, it will run t midnight of each day starting from the `start_date` until (optionally) `end_date`. In other words, as soon as `23:59:59` has passed which means any time after `00:00:00`.\n",
    "    - Example: if start_date=\"2022-01-01\" and schedule_interval=\"@daily\" -> The first time it runs is any time soon after \"2022-01-02 00:00\" which is midnight of January second.\n",
    "(@) We can use convenience string (such as `@daily`), timedetla objects (such as timedelta(days=3), or cron expressions (such as `0 0 * * 0` which means weekly on Sunday 00:00)\n",
    "(@) Frequency scheduling intervals (shorthands):\n",
    "    - `@once`: Schedule once and only once.\n",
    "    - `@hourly`: Run once an hour at the beginning of the hour.\n",
    "    - `@daily`: Run once a day at midnight.\n",
    "    - `@weekly`: Run once a week at midnight on Sunday morning.\n",
    "    - `@monthly`: Run once a month at midnight on the first day of the month. Run once a year at midnight on January 1.\n",
    "\n",
    "(@) Cron-based intervals:\n",
    "```\n",
    "# ┌─────── minute (0 - 59)\n",
    "# │ ┌────── hour (0 - 23)\n",
    "# │ │ ┌───── dayofthemonth(1-31)\n",
    "# │ │ │ ┌───── month(1-12)\n",
    "# │ │ │ │ ┌──── dayoftheweek(0-6)(SundaytoSaturday; \n",
    "# │ │ │ │ │ 7 is also Sunday on some systems) \n",
    "# * * * * *\n",
    "```\n",
    "    - \"*\" means don't care values.\n",
    "    - Examples:\n",
    "        1. 0**** means hourly\n",
    "        2. 00*** means daily at midnight\n",
    "        3. 00**0 means weekly at midnight on Sunday\n",
    "    - Useful link to check meaning of cron-based intervals: https://crontab.guru/\n",
    "(@) Cron expressions have limitations when trying to specify frequency-based intervals such as every three days. The reason for this behavior is that cron expressions are stateless and don't look at previous runs to determine next run, they only look at the current time to see if it matches the expression.\n",
    "(@) Airflow allows us to use frequency-based intervals using `timedelta` from datetime library. This way we can use previous run to determine the next run.\n",
    "    - Example: schedule_interval=\"timedelta(days=3)\" means to run every 3 days after start_date.\n",
    "(@) We can use dynamic time reference that uses execution dates which allows us to do the work incrementally. Airflow will pass those dates to the tasks to determine which schedule interval is being executed.\n",
    "    - `execution_date` is a timestamp of the start time of the schedule interval\n",
    "    - `next_execution_date` is a timestamp of the end time of the schedule interval\n",
    "    - `previous_execution_date` is a timestamp of the start time of the previous schedule interval\n",
    "    - Airflow uses Jinja-based templating such as {{variable_name}}:\n",
    "\n",
    "```bash\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data && \"\n",
    "        \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n",
    "        \"start_date={{execution_date.strftime('%Y-%m-%d')}}\" \n",
    "        \"&end_date={{next_execution_date.strftime('%Y-%m-%d')}}\"\n",
    "    ),\n",
    "dag=dag,\n",
    ")\n",
    "```\n",
    "    - Or we can use shorthands:\n",
    "```bash\n",
    "fetch_events = BashOperator(\n",
    "    task_id=\"fetch_events\",\n",
    "    bash_command=(\n",
    "        \"mkdir -p /data && \"\n",
    "        \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n",
    "        \"start_date={{ds}}\" \n",
    "        \"&end_date={{next_ds}}\"\n",
    "    ),\n",
    "dag=dag,\n",
    ")\n",
    "```\n",
    "    - `ds` has `YYYY-MM-DD` format while `ds_nodash` has `YYYYMMDD` format\n",
    "    - Shorthands: ds, ds_nodash, next_ds, next_ds_nodash, ps, ps_nodash execution date of the next interval.\n",
    "\n",
    "(@) We can also use dates or any dynamic parameters to Python function using `templates_dict` argument and the python callable will be passed the context that has the `templates_dict` For example:\n",
    "```bash\n",
    "    calculate_stats = PythonOperator(\n",
    "       task_id=\"calculate_stats\",\n",
    "       python_callable=_calculate_stats,\n",
    "       templates_dict={\n",
    "            \"input_path\": \"/data/events/{{ds}}.json\",\n",
    "           \"output_path\": \"/data/stats/{{ds}}.csv\",\n",
    "    },\n",
    "    dag=dag\n",
    "    )\n",
    "```\n",
    "```python\n",
    "    def _calculate_stats(**context):\n",
    "        \"\"\"Calculates event statistics.\"\"\"\n",
    "            input_path = context[\"templates_dict\"][\"input_path\"] \n",
    "            output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "```\n",
    "(@) Because Airlfow follows **Interval-Based Scheduling**, that means DAGs run only after the last time point of schedule interval passed. If we run the DAG daily starting from `2022-01-01`, the first time it runs is soon after `2022-01-02 00:00:00` has passed and the `execution_date` would be `2022-01-01` even though it is running in `2022-01-02`. This is because it is running for the corresponding interval.\n",
    "    - The end of the previous interval is `execution_date`\n",
    "    - **One caveat for this is that `previous_execution_date` and `next_execution_date` are only defined for DAGs that run on schedule interval. This means that those values are undefined when the DAGs are run from the UI or CLI**\n",
    "(@) Airflow allows us to have `start_date` in the past. This will help us in backfilling. By default, Airflow will run all the schedule intervals from the past until current time once the DAG is activated. We can control this behavior using `catchup` parameter to the `DAG()` class. If we set it to `False`, it won't run previous schedule intervals.\n",
    "    - Backfilling is also helpful if we change the code for the DAG. It would run all previous schedules after we clear them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91ef2b-c5c0-422c-bbe4-6b521f22428b",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    "\n",
    "- Task needs to be **atomic** which means a single coherent unit of work. This allows us to split work into smaller units where if one fails we know exactly what is it and recover easily.\n",
    "- Task needs to be **idempotent** which means it has no side effects on the system when it reruns. If the task is given the same input, it should produce the same output.\n",
    "    - In database systems, we can use upsert, which allows us to overwrite existing row.\n",
    "    - When writing to files, make sure that rerunning the same task for the same interval don't write data again. *Append* doesn't let us make the task idempotent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
