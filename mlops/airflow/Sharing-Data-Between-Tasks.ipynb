{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a8df91-0b76-4abe-a256-ca7df2fd17dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Airflow Part 6 - Sharing Data Between Tasks\"\n",
    "date: \"2022-02-28\"\n",
    "image: feature.png\n",
    "categories: [\"Data Engineering\", \"MLOps\", \"Airflow\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48243cb5-f66b-4d0c-9dc3-fb567d6cc643",
   "metadata": {},
   "source": [
    "![](feature.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2a1c7-946b-49a5-897c-1f3417e1d70b",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Other than my experience and the documentation, the main resource behind this post and figures is the fantastic book: [Data Pipelines with Apache. Airflow](https://www.manning.com/books/data-pipelines-with-apache-airflow).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638d166-331d-40b0-a727-94a68dc4cde5",
   "metadata": {},
   "source": [
    "- It is meant to exchange messages between tasks, which is some form of shared state\n",
    "- We can use dag instance to push/pull data between tasks:\n",
    "    - `conext[\"dag_instance\"].xcom_push(key=\"data_name\", value=\"value\")` to push data to metastore. It also store the `dag_id`, `task_id`, & `execution_date`.\n",
    "    - `conext[\"dag_instance\"].xcom_pull(key=\"data_name\")` which pull the shared data. We can also specify `dag_id` and `execution_date`.\n",
    "    - We can also access push/pull methods in templates using `task_instance.xcom_push()` or `task_instance.xcom_pull()`\n",
    "    - We can view the shared data on the UI by going to Admin -> XComs\n",
    "\n",
    "- **Limitations**:\n",
    "    - XComs data will be pickled and stored in the database -> The objects have to be serializable\n",
    "    - Size limitations:\n",
    "        - SQLite—Stored as BLOB type, 2GB limit\n",
    "        - PostgreSQL—Stored as BYTEA type, 1 GB limit \n",
    "        - MySQL—Stored as BLOB type, 64 KB limit\n",
    "    - It create hidden dependency between tasks because now the task the pushes the shared state has to push the data before the task that pulls the data. Airflow won't manage/respect this dependency the developer has to document this and make sure this is not an issue based on the tasks' order\n",
    "- Due to its limitations in terms of size, we can create custom backends for XComs by defining a class that inherits from `BaseXCom` and implements two static methods. Airflow will use this class. It can be added to `xcom_backend` parameter in the Airflow configWe can use cheap/large storage services on the cloud such as Amazon S3, Azure Blob Storage, or Google GCS.\n",
    "\n",
    "```python\n",
    "from typing import Any\n",
    "from airflow.models.xcom import BaseXCom\n",
    "\n",
    "class CustomXComBackend(BaseXCom):\n",
    "    \n",
    "    @staticmethod\n",
    "    def serialize(value: Any):\n",
    "        ...\n",
    "    \n",
    "    @staticmethod\n",
    "    def deserialize(result):\n",
    "        ...\n",
    "```\n",
    "- If most of tasks are PythonOperators, we can use `Taskflow` API that takes care of passing state between tasks and avoid the boilerplate code that we have to write with regular API. We need to just decorate the function that we use in the PythonOperator with `@task` and Airflow will take care of the rest by passed XCom data between tasks. Example:\n",
    "\n",
    "![](images/taskflow-example.png)\n",
    "\n",
    "```python\n",
    "from airflow.decorators import task\n",
    "\n",
    "\n",
    "with DAG(...) as dag:\n",
    "    start = DummyOperator(task_id=\"start\")\n",
    "    start >> fetch_sales\n",
    "    start >> fetch_weather\n",
    "    fetch_sales >> clean_sales\n",
    "    fetch_weather >> clean_weather\n",
    "    [clean_sales, clean_weather] >> join_datasets\n",
    "    \n",
    "    @task\n",
    "    def train_model():\n",
    "        model_id = str(uuid.uuid4())\n",
    "        # Airflow will figure out that the return value is XCom\n",
    "        # and would take care of pushing it\n",
    "        return model_id\n",
    "\n",
    "    @task\n",
    "    def deploy_model(model_id: str):\n",
    "        # Airflow would realize that this task uses XCom so it passes\n",
    "        # it from XCom\n",
    "        print(f\"Deploying model {model_id}\")\n",
    "\n",
    "model_id = train_model()\n",
    "deploy_model(model_id)\n",
    "\n",
    "# Now train_model and deploy_model will be new tasks\n",
    "# with explicit dependeny. \n",
    "# The task type is PythonDecoratedOperator\n",
    "join_datasets >> model_id\n",
    "```\n",
    "- Any data passed between Taskflow-style tasks will be stored as XComs and subject to the same limitations of XCom\n",
    "- The main limitation of Taskflow API is that it is still only for PythonOperators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
